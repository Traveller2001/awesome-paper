{
  "arxiv_id": "2510.00726v1",
  "title": "CroSTAta: Cross-State Transition Attention Transformer for Robotic\n  Manipulation",
  "summary": "Learning robotic manipulation policies through supervised learning from\ndemonstrations remains challenging when policies encounter execution variations\nnot explicitly covered during training. While incorporating historical context\nthrough attention mechanisms can improve robustness, standard approaches\nprocess all past states in a sequence without explicitly modeling the temporal\nstructure that demonstrations may include, such as failure and recovery\npatterns. We propose a Cross-State Transition Attention Transformer that\nemploys a novel State Transition Attention (STA) mechanism to modulate standard\nattention weights based on learned state evolution patterns, enabling policies\nto better adapt their behavior based on execution history. Our approach\ncombines this structured attention with temporal masking during training, where\nvisual information is randomly removed from recent timesteps to encourage\ntemporal reasoning from historical context. Evaluation in simulation shows that\nSTA consistently outperforms standard cross-attention and temporal modeling\napproaches like TCN and LSTM networks across all tasks, achieving more than 2x\nimprovement over cross-attention on precision-critical tasks.",
  "authors": [
    "Giovanni Minelli",
    "Giulio Turrisi",
    "Victor Barasuol",
    "Claudio Semini"
  ],
  "published": "2025-10-01T10:09:05Z",
  "primary_category": "cs.RO",
  "arxiv_url": "https://arxiv.org/abs/2510.00726v1",
  "primary_area": "vla_models",
  "secondary_focus": "model_architecture",
  "application_domain": "general_purpose",
  "tldr_zh": "本文提出CroSTAta模型，一种用于机器人操作的跨状态转换注意力Transformer。该模型通过新颖的状态转换注意力机制学习状态演化模式来调节标准注意力权重，结合训练时的时序掩码技术，使策略能基于执行历史更好地适应行为变化。在仿真评估中，该方法在精度关键任务上比标准交叉注意力提升2倍以上，优于TCN和LSTM等时序建模方法。",
  "order": 256,
  "papers_cool_url": "https://papers.cool/arxiv/2510.00726v1"
}