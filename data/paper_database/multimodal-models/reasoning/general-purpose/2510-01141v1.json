{
  "arxiv_id": "2510.01141v1",
  "title": "Apriel-1.5-15b-Thinker",
  "summary": "We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights\nmultimodal reasoning model that achieves frontier-level performance through\ntraining design rather than sheer scale. Starting from Pixtral-12B, we apply a\nprogressive three-stage methodology: (1) depth upscaling to expand reasoning\ncapacity without pretraining from scratch, (2) staged continual pre-training\nthat first develops foundational text and vision understanding, then enhances\nvisual reasoning through targeted synthetic data generation addressing spatial\nstructure, compositional understanding, and fine-grained perception, and (3)\nhigh-quality text-only supervised fine-tuning on curated instruction-response\npairs with explicit reasoning traces spanning mathematics, coding, science, and\ntool use. Notably, our model achieves competitive results without reinforcement\nlearning or preference optimization, isolating the contribution of our\ndata-centric continual pre-training approach. On the Artificial Analysis\nIntelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching\nDeepSeek-R1-0528 despite requiring significantly fewer computational resources.\nAcross ten image benchmarks, its performance is on average within five points\nof Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model\noperating within single-GPU deployment constraints. Our results demonstrate\nthat thoughtful mid-training 2 design can close substantial capability gaps\nwithout massive scale, making frontier-level multimodal reasoning accessible to\norganizations with limited infrastructure. We release the model checkpoint, all\ntraining recipes, and evaluation protocols under the MIT license to to advance\nopen-source research.",
  "authors": [
    "Shruthan Radhakrishna",
    "Aman Tiwari",
    "Aanjaneya Shukla",
    "Masoud Hashemi",
    "Rishabh Maheshwary",
    "Shiva Krishna Reddy Malay",
    "Jash Mehta",
    "Pulkit Pattnaik",
    "Saloni Mittal",
    "Khalil Slimi",
    "Kelechi Ogueji",
    "Akintunde Oladipo",
    "Soham Parikh",
    "Oluwanifemi Bamgbose",
    "Toby Liang",
    "Ahmed Masry",
    "Khyati Mahajan",
    "Sai Rajeswar Mudumba",
    "Vikas Yadav",
    "Sathwik Tejaswi Madhusudhan",
    "Torsten Scholak",
    "Sagar Davasam",
    "Srinivas Sunkara",
    "Nicholas Chapados"
  ],
  "published": "2025-10-01T17:29:35Z",
  "primary_category": "cs.AI",
  "arxiv_url": "https://arxiv.org/abs/2510.01141v1",
  "primary_area": "multimodal_models",
  "secondary_focus": "reasoning",
  "application_domain": "general_purpose",
  "tldr_zh": "Apriel-1.5-15B-Thinker是一个150亿参数的多模态推理模型，通过三阶段渐进训练方法（深度扩展、持续预训练、监督微调）实现前沿性能，无需强化学习即可在多个基准测试中媲美更大模型，且支持单GPU部署，为资源有限的组织提供先进的多模态推理能力。",
  "order": 188,
  "papers_cool_url": "https://papers.cool/arxiv/2510.01141v1"
}