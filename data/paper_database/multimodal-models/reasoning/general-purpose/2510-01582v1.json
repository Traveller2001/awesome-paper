{
  "arxiv_id": "2510.01582v1",
  "title": "ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal\n  Reasoning for Vision Language Models",
  "summary": "We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the\ndevelopment of Vision Language Models (VLMs) with explicit reasoning\ncapabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,\nproviding structured thinking tokens and corresponding answers. Our synthetic\ndataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and\nKimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of\nthinking-answer sequences, creating a resource for training and evaluating\nmultimodal reasoning models. We capture the step-by-step reasoning process of\nVLMs and the final descriptive answers. Our goal with this dataset is to enable\nthe development of more robust VLMs while contributing to the broader\nunderstanding of multimodal reasoning mechanisms. The dataset and evaluation\nbenchmarks will be publicly available to aid research in reasoning/thinking\nmultimodal VLMs.",
  "authors": [
    "Krishna Teja Chitty-Venkata",
    "Murali Emani"
  ],
  "published": "2025-10-02T02:02:45Z",
  "primary_category": "cs.CV",
  "arxiv_url": "https://arxiv.org/abs/2510.01582v1",
  "primary_area": "multimodal_models",
  "secondary_focus": "reasoning",
  "application_domain": "general_purpose",
  "tldr_zh": "ImageNet-Think-250K是一个大规模合成多模态推理数据集，基于25万张ImageNet21k图像构建，包含结构化思维标记和对应答案。该数据集由GLM-4.1V和Kimi-VL两个先进视觉语言模型生成，每张图像配备两对思维-答案序列，旨在促进具有显式推理能力的视觉语言模型开发，推动多模态推理机制研究。",
  "order": 570,
  "papers_cool_url": "https://papers.cool/arxiv/2510.01582v1"
}