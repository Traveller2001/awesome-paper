{
  "arxiv_id": "2510.06131v1",
  "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal\n  Generation",
  "summary": "Recent advances in generative medical models are constrained by\nmodality-specific scenarios that hinder the integration of complementary\nevidence from imaging, pathology, and clinical notes. This fragmentation limits\ntheir evolution into foundation models that can learn and reason across the\nfull spectrum of biomedical data. We propose MeDiM, the first medical discrete\ndiffusion model that learns shared distributions across modalities without\nmodality-specific components. MeDiM unifies multiple generative tasks:\ntranslating between images and text, and jointly producing image-report pairs\nacross domains in response to prompts. Built on a discrete diffusion framework,\nMeDiM bridges vision and language representations through a shared\nprobabilistic space. To enable unified and flexible medical generation, we\nemploy a multimodal large language model (MLLM) as the diffusion backbone,\nleveraging its prior knowledge and cross-modal reasoning. Two key designs are\nintroduced: (1) removing the causal attention mask for bidirectional context,\nand (2) injecting continuous timestep embeddings for diffusion awareness.\nExperiments demonstrate high-fidelity medical generation (FID 16.60 on\nMIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR\n0.2650 and 0.2580). Jointly generated image-report pairs further enhance\ndownstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,\nplus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports\ncoherent and clinically grounded multimodal outputs.",
  "authors": [
    "Jiawei Mao",
    "Yuhan Wang",
    "Lifeng Chen",
    "Can Zhao",
    "Yucheng Tang",
    "Dong Yang",
    "Liangqiong Qu",
    "Daguang Xu",
    "Yuyin Zhou"
  ],
  "published": "2025-10-07T17:06:57Z",
  "primary_category": "cs.CV",
  "arxiv_url": "https://arxiv.org/abs/2510.06131v1",
  "primary_area": "multimodal_models",
  "secondary_focus": "['model_architecture', 'diffusion_models']",
  "application_domain": "medical_ai",
  "tldr_zh": "本文提出MeDiM，首个医学离散扩散模型，通过共享概率空间统一学习多模态分布，无需特定模态组件。采用多模态大语言模型作为扩散主干，实现图像-文本互转和联合生成图像-报告对，在医学影像和病理数据上取得高保真生成效果，显著提升下游任务性能。",
  "order": 65,
  "papers_cool_url": "https://papers.cool/arxiv/2510.06131v1"
}