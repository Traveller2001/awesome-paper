{
  "arxiv_id": "2510.05644v1",
  "title": "The African Languages Lab: A Collaborative Approach to Advancing\n  Low-Resource African NLP",
  "summary": "Despite representing nearly one-third of the world's languages, African\nlanguages remain critically underserved by modern NLP technologies, with 88\\%\nclassified as severely underrepresented or completely ignored in computational\nlinguistics. We present the African Languages Lab (All Lab), a comprehensive\nresearch initiative that addresses this technological gap through systematic\ndata collection, model development, and capacity building. Our contributions\ninclude: (1) a quality-controlled data collection pipeline, yielding the\nlargest validated African multi-modal speech and text dataset spanning 40\nlanguages with 19 billion tokens of monolingual text and 12,628 hours of\naligned speech data; (2) extensive experimental validation demonstrating that\nour dataset, combined with fine-tuning, achieves substantial improvements over\nbaseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points\nacross 31 evaluated languages; and (3) a structured research program that has\nsuccessfully mentored fifteen early-career researchers, establishing\nsustainable local capacity. Our comparative evaluation against Google Translate\nreveals competitive performance in several languages while identifying areas\nthat require continued development.",
  "authors": [
    "Sheriff Issaka",
    "Keyi Wang",
    "Yinka Ajibola",
    "Oluwatumininu Samuel-Ipaye",
    "Zhaoyi Zhang",
    "Nicte Aguillon Jimenez",
    "Evans Kofi Agyei",
    "Abraham Lin",
    "Rohan Ramachandran",
    "Sadick Abdul Mumin",
    "Faith Nchifor",
    "Mohammed Shuraim",
    "Lieqi Liu",
    "Erick Rosas Gonzalez",
    "Sylvester Kpei",
    "Jemimah Osei",
    "Carlene Ajeneza",
    "Persis Boateng",
    "Prisca Adwoa Dufie Yeboah",
    "Saadia Gabriel"
  ],
  "published": "2025-10-07T07:42:52Z",
  "primary_category": "cs.CL",
  "arxiv_url": "https://arxiv.org/abs/2510.05644v1",
  "primary_area": "['text_models', 'audio_models']",
  "secondary_focus": "['training_optimization']",
  "application_domain": "['general_purpose']",
  "tldr_zh": "本文介绍非洲语言实验室项目，通过系统化数据收集、模型开发和能力建设，解决非洲低资源语言在自然语言处理中的严重不足。项目构建了涵盖40种语言的最大验证多模态数据集（190亿文本标记和1.2万小时语音），微调后模型性能显著提升（平均ChrF++提高23.69），并培养了15名早期研究人员建立本地可持续能力。",
  "order": 43,
  "papers_cool_url": "https://papers.cool/arxiv/2510.05644v1"
}