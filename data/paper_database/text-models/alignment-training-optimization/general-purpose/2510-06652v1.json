{
  "arxiv_id": "2510.06652v1",
  "title": "Aligning Large Language Models via Fully Self-Synthetic Data",
  "summary": "Traditional reinforcement learning from human feedback (RLHF) for large\nlanguage models (LLMs) relies on expensive human-annotated datasets, while\nReinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,\nrequiring the collection of diverse prompts and corresponding responses, often\nnecessitating external reward models or proprietary models like GPT-4 to\nannotate preference pairs. In this work, we introduce Self-Alignment\nOptimization (SAO), a fully self-synthetic framework for LLM alignment, where\nall training data, including prompts (i.e., user queries), responses, and\npreferences, are generated by the model itself. Specifically, SAO first\ninstructs the LLM to engage in persona role-play and generate diverse prompts\nand responses, which are then self-evaluated for preference optimization.\nExtensive experiments demonstrate that SAO effectively enhances the model's\nchat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining\nstrong performance on downstream objective tasks (e.g., question-answering,\nmath reasoning). Our work provides a practical solution for self-improvement in\naligning LLMs, and the code for reproducing our results is available at:\nhttps://github.com/SJY8460/SAO.",
  "authors": [
    "Shangjian Yin",
    "Zhepei Wei",
    "Xinyu Zhu",
    "Wei-Lin Chen",
    "Yu Meng"
  ],
  "published": "2025-10-08T05:07:45Z",
  "primary_category": "cs.CL",
  "arxiv_url": "https://arxiv.org/abs/2510.06652v1",
  "primary_area": "text_models",
  "secondary_focus": "['alignment', 'training_optimization']",
  "application_domain": "general_purpose",
  "tldr_zh": "本文提出自对齐优化(SAO)框架，通过完全自生成数据实现大语言模型对齐。该方法让模型自主生成多样化提示、回复并进行自评估偏好优化，无需依赖昂贵的人工标注或外部奖励模型。实验表明SAO在AlpacaEval~2.0等基准上有效提升对话能力，同时保持下游任务性能，为大模型自改进对齐提供了实用解决方案。",
  "order": 94,
  "papers_cool_url": "https://papers.cool/arxiv/2510.06652v1"
}