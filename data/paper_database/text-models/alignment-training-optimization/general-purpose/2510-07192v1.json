{
  "arxiv_id": "2510.07192v1",
  "title": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison\n  Samples",
  "summary": "Poisoning attacks can compromise the safety of large language models (LLMs)\nby injecting malicious documents into their training data. Existing work has\nstudied pretraining poisoning assuming adversaries control a percentage of the\ntraining corpus. However, for large models, even small percentages translate to\nimpractically large amounts of data. This work demonstrates for the first time\nthat poisoning attacks instead require a near-constant number of documents\nregardless of dataset size. We conduct the largest pretraining poisoning\nexperiments to date, pretraining models from 600M to 13B parameters on\nchinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned\ndocuments similarly compromise models across all model and dataset sizes,\ndespite the largest models training on more than 20 times more clean data. We\nalso run smaller-scale experiments to ablate factors that could influence\nattack success, including broader ratios of poisoned to clean data and\nnon-random distributions of poisoned samples. Finally, we demonstrate the same\ndynamics for poisoning during fine-tuning. Altogether, our results suggest that\ninjecting backdoors through data poisoning may be easier for large models than\npreviously believed as the number of poisons required does not scale up with\nmodel size, highlighting the need for more research on defences to mitigate\nthis risk in future models.",
  "authors": [
    "Alexandra Souly",
    "Javier Rando",
    "Ed Chapman",
    "Xander Davies",
    "Burak Hasircioglu",
    "Ezzeldin Shereen",
    "Carlos Mougan",
    "Vasilios Mavroudis",
    "Erik Jones",
    "Chris Hicks",
    "Nicholas Carlini",
    "Yarin Gal",
    "Robert Kirk"
  ],
  "published": "2025-10-08T16:25:05Z",
  "primary_category": "cs.LG",
  "arxiv_url": "https://arxiv.org/abs/2510.07192v1",
  "primary_area": "text_models",
  "secondary_focus": "['alignment', 'training_optimization']",
  "application_domain": "general_purpose",
  "tldr_zh": "本研究首次证明，对大语言模型进行投毒攻击所需的恶意样本数量几乎恒定，不随模型规模或训练数据量增加而增长。通过在6亿至130亿参数模型上进行大规模实验，发现仅需250个投毒文档即可在所有规模模型上实现相似的攻击效果，即使最大模型训练数据量超过最小模型20倍以上。该发现表明大型模型可能比预想中更容易遭受数据投毒攻击，凸显了防御机制研究的紧迫性。",
  "order": 175,
  "papers_cool_url": "https://papers.cool/arxiv/2510.07192v1"
}