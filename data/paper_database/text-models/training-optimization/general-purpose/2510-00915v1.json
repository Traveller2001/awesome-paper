{
  "arxiv_id": "2510.00915v1",
  "title": "Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect\n  Verifiers",
  "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) trains policies against\nautomated verifiers to avoid costly human labeling. To reduce vulnerability to\nverifier hacking, many RLVR systems collapse rewards to binary $\\{0,1\\}$ during\ntraining. This choice carries a cost: it introduces \\textit{false negatives}\n(rejecting correct answers, FNs) and \\textit{false positives} (accepting\nincorrect ones, FPs). For instance, a rule-based checker may mark the correct\nfraction $\\frac{12}{36}$ as wrong when compared against the canonical\n$\\frac{1}{3}$ due to brittle parsing/equivalence rules (FN), while a large\nlanguage model (LLM) judges can be gamed by superficial cues or even a single\nadversarial token, yielding inflated correctness for wrong solutions (FP). We\nformalize verifier unreliability by modeling the verifier as a stochastic\nreward channel with asymmetric noise rates. From this abstraction, we derive\ntwo correction algorithms for verifier errors. The first is a \\textit{backward}\ncorrection that de-biases the observed binary reward to recover an\n\\textit{unbiased} estimator of the clean policy gradient. The second is a\n\\textit{forward} correction that reweights score-function terms so that the\nexpected update direction aligns with the \\textit{clean gradient}; notably, it\nrequires only the FN rate. We implement both as lightweight hooks in a group\nrelative policy optimization (GRPO)-based RLVR pipeline and evaluate them on\nmath-reasoning models and benchmarks. Across models and datasets, both\ncorrections improve over uncorrected training; the forward variant converges\nfaster and remains stable under heavier noise. Finally, we show a practical\nappeal mechanism in which a lightweight LLM verifier estimates the FN rate\nonline by rechecking rule-based negatives, obtaining outperformance compared\nwith other state-of-the-art contenders.",
  "authors": [
    "Xin-Qiang Cai",
    "Wei Wang",
    "Feng Liu",
    "Tongliang Liu",
    "Gang Niu",
    "Masashi Sugiyama"
  ],
  "published": "2025-10-01T13:56:44Z",
  "primary_category": "cs.LG",
  "arxiv_url": "https://arxiv.org/abs/2510.00915v1",
  "primary_area": "text_models",
  "secondary_focus": "training_optimization",
  "application_domain": "general_purpose",
  "tldr_zh": "本文提出在验证器不可靠的强化学习环境中，通过建模验证器为具有不对称噪声率的随机奖励通道，开发了前向和后向两种校正算法来修正验证错误。这些轻量级方法在数学推理任务中表现出优于未校正训练的性能，其中前向校正收敛更快且在强噪声下更稳定。",
  "order": 219,
  "papers_cool_url": "https://papers.cool/arxiv/2510.00915v1"
}