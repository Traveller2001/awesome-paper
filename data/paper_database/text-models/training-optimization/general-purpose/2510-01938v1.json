{
  "arxiv_id": "2510.01938v1",
  "title": "StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold",
  "summary": "Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient\ntechnique for fine-tuning large-scale pre-trained models. However, it still\nlags behind full fine-tuning in performance, partly due to its insufficient\nexploitation of the geometric structure underlying low-rank manifolds. In this\npaper, we propose a geometry-aware extension of LoRA that uses a three-factor\ndecomposition $U\\!SV^\\top$. Analogous to the structure of singular value\ndecomposition (SVD), it separates the adapter's input and output subspaces, $V$\nand $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie\non the Stiefel manifold, ensuring their orthonormality throughout the training.\nTo optimize on the Stiefel manifold, we employ a flexible and modular geometric\noptimization design that converts any Euclidean optimizer to a Riemannian one.\nIt enables efficient subspace learning while remaining compatible with existing\nfine-tuning pipelines. Empirical results across a wide range of downstream\ntasks, including commonsense reasoning, math and code generation, image\nclassification, and image generation, demonstrate the superior performance of\nour approach against the recent state-of-the-art variants of LoRA. Code is\navailable at https://github.com/SonyResearch/stella.",
  "authors": [
    "Zhizhong Li",
    "Sina Sajadmanesh",
    "Jingtao Li",
    "Lingjuan Lyu"
  ],
  "published": "2025-10-02T11:59:13Z",
  "primary_category": "cs.LG",
  "arxiv_url": "https://arxiv.org/abs/2510.01938v1",
  "primary_area": "text_models",
  "secondary_focus": "training_optimization",
  "application_domain": "general_purpose",
  "tldr_zh": "StelLA是一种基于Stiefel流形的低秩自适应改进方法，通过三因子分解U·S·V⊤将适配器的输入输出子空间与缩放因子分离，并约束U和V在Stiefel流形上保持正交归一化。该方法采用几何优化设计，在常识推理、数学与代码生成、图像分类与生成等多项任务中优于现有LoRA变体。",
  "order": 773,
  "papers_cool_url": "https://papers.cool/arxiv/2510.01938v1"
}