{
  "arxiv_id": "2510.07172v1",
  "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents",
  "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.",
  "authors": [
    "Tianshi Zheng",
    "Kelvin Kiu-Wai Tam",
    "Newt Hue-Nam K. Nguyen",
    "Baixuan Xu",
    "Zhaowei Wang",
    "Jiayang Cheng",
    "Hong Ting Tsang",
    "Weiqi Wang",
    "Jiaxin Bai",
    "Tianqing Fang",
    "Yangqiu Song",
    "Ginny Y. Wong",
    "Simon See"
  ],
  "published": "2025-10-08T16:12:11Z",
  "primary_category": "cs.AI",
  "arxiv_url": "https://arxiv.org/abs/2510.07172v1",
  "primary_area": "text_models",
  "secondary_focus": "reasoning",
  "application_domain": "general_purpose",
  "tldr_zh": "本文提出NewtonBench基准测试，包含12个物理领域的324项科学定律发现任务，解决了现有基准在科学相关性、可扩展性和抗记忆性之间的权衡困境。研究发现前沿大语言模型具备发现能力但很脆弱：随系统复杂度增加而急剧下降，对观测噪声极度敏感，且工具辅助可能阻碍更优解决方案的探索。该基准为衡量AI科学发现能力提供了关键工具。",
  "order": 3,
  "papers_cool_url": "https://papers.cool/arxiv/2510.07172v1"
}