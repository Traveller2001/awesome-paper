{
  "arxiv_id": "2510.01165v1",
  "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient\n  Few-Shot Reasoning",
  "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks,\nbut their effectiveness often depends on the quality of the provided context.\nRetrieval-Augmented Generation (RAG) enriches prompts with external\ninformation, but its reliance on static databases constrains adaptability and\ncan result in irrelevant demonstrations. In this work, we propose a Generative\nRetrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach\nwhere an LLM model is trained to generate input-specific concise\ndemonstrations. By tailoring demonstrations to each input, our method offers\nbetter contextual support than traditional RAG approaches. We demonstrate the\nsuperiority of GRAD under budget constraints, where we limit both the number of\ntokens used per demonstration and the number of tokens used for the final\noutput. Trained solely on a math dataset, GRAD consistently outperforms strong\nbaselines on Qwen2.5-14B across mathematical reasoning and advanced STEM\nquestions, highlighting GRAD's robust generalization to out-of-distribution\n(OOD) domains such as physics, chemistry, and computer science. Furthermore, we\nshow that demonstrations generated by trained smaller models can effectively\nguide larger target models, reducing training costs while maintaining\ncompetitive accuracy. Overall, this work introduces a scalable demonstration\ngenerator model presenting the first step toward a dynamic few-shot learning\nparadigm in resource-constrained settings. We release the code used for the\nproject.",
  "authors": [
    "Oussama Gabouj",
    "Kamel Charaf",
    "Ivan Zakazov",
    "Nicolas Baldwin",
    "Robert West"
  ],
  "published": "2025-10-01T17:52:41Z",
  "primary_category": "cs.CL",
  "arxiv_url": "https://arxiv.org/abs/2510.01165v1",
  "primary_area": "text_models",
  "secondary_focus": "reasoning",
  "application_domain": "general_purpose",
  "tldr_zh": "本文提出GRAD方法，通过训练LLM生成针对特定输入的简洁示例，替代传统检索增强生成中的静态数据库。该方法在数学推理和STEM问题上表现优异，能有效泛化至物理、化学等新领域，并在计算资源受限条件下保持竞争力。",
  "order": 420,
  "papers_cool_url": "https://papers.cool/arxiv/2510.01165v1"
}