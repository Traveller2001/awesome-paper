{
  "arxiv_id": "2510.02190v1",
  "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
  "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.",
  "authors": [
    "Yang Yao",
    "Yixu Wang",
    "Yuxuan Zhang",
    "Yi Lu",
    "Tianle Gu",
    "Lingyu Li",
    "Dingyi Zhao",
    "Keming Wu",
    "Haozhe Wang",
    "Ping Nie",
    "Yan Teng",
    "Yingchun Wang"
  ],
  "published": "2025-10-02T16:40:02Z",
  "primary_category": "cs.AI",
  "arxiv_url": "https://arxiv.org/abs/2510.02190v1",
  "primary_area": "text_models",
  "secondary_focus": "reasoning",
  "application_domain": "general_purpose",
  "tldr_zh": "本文针对深度研究代理(DRAs)提出了一套严谨的基准测试与多维评估框架，包含214个专家精选查询和10个主题领域，通过语义质量、主题聚焦和检索可信度等指标评估长篇幅报告，验证了DRAs优于增强推理模型但仍有改进空间。",
  "order": 348,
  "papers_cool_url": "https://papers.cool/arxiv/2510.02190v1"
}