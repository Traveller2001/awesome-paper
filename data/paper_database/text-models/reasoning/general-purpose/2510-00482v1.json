{
  "arxiv_id": "2510.00482v1",
  "title": "Agent Fine-tuning through Distillation for Domain-specific LLMs in\n  Microdomains",
  "summary": "Agentic large language models (LLMs) have become prominent for autonomously\ninteracting with external environments and performing multi-step reasoning\ntasks. Most approaches leverage these capabilities via in-context learning with\nfew-shot prompts, but this often results in lengthy inputs and higher\ncomputational costs. Agent fine-tuning offers an alternative by enabling LLMs\nto internalize procedural reasoning and domain-specific knowledge through\ntraining on relevant data and demonstration trajectories. While prior studies\nhave focused on general domains, their effectiveness in specialized technical\nmicrodomains remains unclear. This paper explores agent fine-tuning for domain\nadaptation within Hitachi's JP1 middleware, a microdomain for specialized IT\noperations. We fine-tuned LLMs using JP1-specific datasets derived from domain\nmanuals and distilled reasoning trajectories generated by LLMs themselves,\nenhancing decision making accuracy and search efficiency. During inference, we\nused an agentic prompt with retrieval-augmented generation and introduced a\ncontext-answer extractor to improve information relevance. On JP1 certification\nexam questions, our method achieved a 14% performance improvement over the base\nmodel, demonstrating the potential of agent fine-tuning for domain-specific\nreasoning in complex microdomains.",
  "authors": [
    "Yawen Xue",
    "Masaya Tsunokake",
    "Yuta Koreeda",
    "Ekant Muljibhai Amin",
    "Takashi Sumiyoshi",
    "Yasuhiro Sogawa"
  ],
  "published": "2025-10-01T04:04:53Z",
  "primary_category": "cs.CL",
  "arxiv_url": "https://arxiv.org/abs/2510.00482v1",
  "primary_area": "text_models",
  "secondary_focus": "reasoning",
  "application_domain": "general_purpose",
  "tldr_zh": "本文提出一种通过蒸馏技术对智能体大语言模型进行微调的方法，专门针对日立JP1中间件这一技术微领域。通过使用领域手册数据和模型自生成的推理轨迹进行微调，结合检索增强生成和上下文答案提取器，在JP1认证考试中相比基础模型性能提升14%，展示了在复杂微领域中领域特定推理的潜力。",
  "order": 479,
  "papers_cool_url": "https://papers.cool/arxiv/2510.00482v1"
}