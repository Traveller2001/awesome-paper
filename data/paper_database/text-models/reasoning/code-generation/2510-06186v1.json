{
  "arxiv_id": "2510.06186v1",
  "title": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback",
  "summary": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation",
  "authors": [
    "Chunyu Miao",
    "Henry Peng Zou",
    "Yangning Li",
    "Yankai Chen",
    "Yibo Wang",
    "Fangxin Wang",
    "Yifan Li",
    "Wooseong Yang",
    "Bowei He",
    "Xinni Zhang",
    "Dianzhi Yu",
    "Hanchen Yang",
    "Hoang H Nguyen",
    "Yue Zhou",
    "Jie Yang",
    "Jizhou Guo",
    "Wenzhe Fan",
    "Chin-Yuan Yeh",
    "Panpan Meng",
    "Liancheng Fang",
    "Jinhu Qi",
    "Wei-Chieh Huang",
    "Zhengyao Gu",
    "Yuwei Han",
    "Langzhou He",
    "Yuyao Yang",
    "Xue Liu",
    "Irwin King",
    "Philip S. Yu"
  ],
  "published": "2025-10-07T17:45:35Z",
  "primary_category": "cs.CL",
  "arxiv_url": "https://arxiv.org/abs/2510.06186v1",
  "primary_area": "text_models",
  "secondary_focus": "reasoning",
  "application_domain": "code_generation",
  "tldr_zh": "本文提出RECODE-H基准测试，针对科研代码开发场景，通过102个任务评估LLM在多轮人机交互反馈下的代码生成能力。该基准包含结构化指令、单元测试和五级反馈体系，并展示了ReCodeAgent框架如何通过反馈迭代提升代码质量。实验表明丰富反馈能显著提升GPT-5等主流模型的性能，但复杂科研代码生成仍是挑战。",
  "order": 10,
  "papers_cool_url": "https://papers.cool/arxiv/2510.06186v1"
}