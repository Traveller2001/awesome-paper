{
  "arxiv_id": "2510.06213v1",
  "title": "Training Dynamics Impact Post-Training Quantization Robustness",
  "summary": "While post-training quantization is widely adopted for efficient deployment\nof large language models, the mechanisms underlying quantization robustness\nremain unclear. We conduct a comprehensive analysis of quantization degradation\nacross open-source language model training trajectories up to 32B parameters\nand 15T training tokens to accurately assess the relationship between training\ndynamics and quantization performance. Our key finding is that quantization\nerrors in large-scale training runs are driven by a complex interplay between\nlearning rate and other training hyperparameters. Specifically, once learning\nrates decay, validation loss and quantization error diverge, largely\nindependent of training data scale. To investigate interventions on the\ntraining dynamics and identify specific configurations that can modulate\nquantization robustness favorably, we train our own models in controlled\nexperiments up to 100B tokens. Our results challenge the assumption that\nincreasing dataset scale inherently compromises quantization effectiveness,\ndemonstrating instead that strategic training hyperparameter interventions can\nimprove quantization quality at scale.",
  "authors": [
    "Albert Catalan-Tatjer",
    "Niccolò Ajroldi",
    "Jonas Geiping"
  ],
  "published": "2025-10-07T17:59:07Z",
  "primary_category": "cs.LG",
  "arxiv_url": "https://arxiv.org/abs/2510.06213v1",
  "primary_area": "text_models",
  "secondary_focus": "['model_compression', 'training_optimization']",
  "application_domain": "general_purpose",
  "tldr_zh": "本文系统研究了训练动态对大型语言模型后训练量化鲁棒性的影响。通过对高达32B参数和15T训练token的模型训练轨迹分析，发现学习率衰减会导致验证损失与量化误差出现分歧。研究挑战了数据集规模增大会损害量化效果的假设，证明通过优化训练超参数可在保持模型规模的同时提升量化质量。",
  "order": 80,
  "papers_cool_url": "https://papers.cool/arxiv/2510.06213v1"
}