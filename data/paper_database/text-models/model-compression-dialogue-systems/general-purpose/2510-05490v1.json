{
  "arxiv_id": "2510.05490v1",
  "title": "LANTERN: Scalable Distillation of Large Language Models for Job-Person\n  Fit and Explanation",
  "summary": "Large language models (LLMs) have achieved strong performance across a wide\nrange of natural language processing tasks. However, deploying LLMs at scale\nfor domain specific applications, such as job-person fit and explanation in job\nseeking platforms, introduces distinct challenges. At LinkedIn, the job person\nfit task requires analyzing a candidate's public profile against job\nrequirements to produce both a fit assessment and a detailed explanation.\nDirectly applying open source or finetuned LLMs to this task often fails to\nyield high quality, actionable feedback due to the complexity of the domain and\nthe need for structured outputs. Moreover, the large size of these models leads\nto high inference latency and limits scalability, making them unsuitable for\nonline use. To address these challenges, we introduce LANTERN, a novel LLM\nknowledge distillation framework tailored specifically for job person fit\ntasks. LANTERN involves modeling over multiple objectives, an encoder model for\nclassification purpose, and a decoder model for explanation purpose. To better\ndistill the knowledge from a strong black box teacher model to multiple\ndownstream models, LANTERN incorporates multi level knowledge distillation that\nintegrates both data and logit level insights. In addition to introducing the\nknowledge distillation framework, we share our insights on post training\ntechniques and prompt engineering, both of which are crucial for successfully\nadapting LLMs to domain specific downstream tasks. Extensive experimental\nresults demonstrate that LANTERN significantly improves task specific metrics\nfor both job person fit and explanation. Online evaluations further confirm its\neffectiveness, showing measurable gains in job seeker engagement, including a\n0.24\\% increase in apply rate and a 0.28\\% increase in qualified applications.",
  "authors": [
    "Zhoutong Fu",
    "Yihan Cao",
    "Yi-Lin Chen",
    "Aman Lunia",
    "Liming Dong",
    "Neha Saraf",
    "Ruijie Jiang",
    "Yun Dai",
    "Qingquan Song",
    "Tan Wang",
    "Guoyao Li",
    "Derek Koh",
    "Haichao Wei",
    "Zhipeng Wang",
    "Aman Gupta",
    "Chengming Jiang",
    "Jianqiang Shen",
    "Liangjie Hong",
    "Wenjing Zhang"
  ],
  "published": "2025-10-07T01:10:02Z",
  "primary_category": "cs.CL",
  "arxiv_url": "https://arxiv.org/abs/2510.05490v1",
  "primary_area": "text_models",
  "secondary_focus": "['model_compression', 'dialogue_systems']",
  "application_domain": "general_purpose",
  "tldr_zh": "本文提出LANTERN框架，通过多层次知识蒸馏技术将大型语言模型压缩为适用于职位匹配与解释任务的高效模型。该方案在LinkedIn平台上验证有效，显著提升求职者参与度，申请率增加0.24%，合格申请率提升0.28%。",
  "order": 54,
  "papers_cool_url": "https://papers.cool/arxiv/2510.05490v1"
}