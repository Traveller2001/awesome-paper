{
  "arxiv_id": "2510.01910v1",
  "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under\n  Deficiencies with Iterative Refinement",
  "summary": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement.",
  "authors": [
    "Zhaoyan Wang",
    "Zheng Gao",
    "Arogya Kharel",
    "In-Young Ko"
  ],
  "published": "2025-10-02T11:30:51Z",
  "primary_category": "cs.LG",
  "arxiv_url": "https://arxiv.org/abs/2510.01910v1",
  "primary_area": "text_models",
  "secondary_focus": "model_architecture",
  "application_domain": "general_purpose",
  "tldr_zh": "本研究首次系统比较传统图神经网络与LLM增强方法在复合缺陷图数据上的表现，挑战了LLM增强必然更优的假设。提出RoGRAD框架，采用检索增强生成和迭代对比学习实现动态优化，实验显示其性能显著优于基线方法，平均提升达82.43%。",
  "order": 777,
  "papers_cool_url": "https://papers.cool/arxiv/2510.01910v1"
}