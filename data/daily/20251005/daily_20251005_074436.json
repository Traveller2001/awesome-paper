{
  "generated_at": "2025-10-05T07:44:36.375618Z",
  "source_raw_files": [
    "data\\raw\\20251005\\csAI\\raw_csAI_062006.json",
    "data\\raw\\20251005\\csCL\\raw_csCL_062006.json",
    "data\\raw\\20251005\\csCV\\raw_csCV_062006.json",
    "data\\raw\\20251005\\csLG\\raw_csLG_062006.json"
  ],
  "paper_count": 788,
  "papers": [
    {
      "arxiv_id": "2510.02307v1",
      "title": "NoiseShift: Resolution-Aware Noise Recalibration for Better\n  Low-Resolution Image Generation",
      "summary": "Text-to-image diffusion models trained on a fixed set of resolutions often\nfail to generalize, even when asked to generate images at lower resolutions\nthan those seen during training. High-resolution text-to-image generators are\ncurrently unable to easily offer an out-of-the-box budget-efficient alternative\nto their users who might not need high-resolution images. We identify a key\ntechnical insight in diffusion models that when addressed can help tackle this\nlimitation: Noise schedulers have unequal perceptual effects across\nresolutions. The same level of noise removes disproportionately more signal\nfrom lower-resolution images than from high-resolution images, leading to a\ntrain-test mismatch. We propose NoiseShift, a training-free method that\nrecalibrates the noise level of the denoiser conditioned on resolution size.\nNoiseShift requires no changes to model architecture or sampling schedule and\nis compatible with existing models. When applied to Stable Diffusion 3, Stable\nDiffusion 3.5, and Flux-Dev, quality at low resolutions is significantly\nimproved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and\nFlux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by\n10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results\ndemonstrate the effectiveness of NoiseShift in mitigating resolution-dependent\nartifacts and enhancing the quality of low-resolution image generation.",
      "authors": [
        "Ruozhen He",
        "Moayed Haji-Ali",
        "Ziyan Yang",
        "Vicente Ordonez"
      ],
      "published": "2025-10-02T17:59:43Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02307v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "NoiseShift是一种无需训练的方法，通过根据分辨率大小重新校准去噪器的噪声水平，解决扩散模型在不同分辨率下生成质量不一致的问题。该方法兼容现有模型，在Stable Diffusion 3、3.5和Flux-Dev上显著提升了低分辨率图像生成质量，平均FID指标提升最高达15.89%。",
      "order": 1
    },
    {
      "arxiv_id": "2510.02305v1",
      "title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is\n  Geometry Adaptive",
      "summary": "Diffusion models have achieved state-of-the-art performance, demonstrating\nremarkable generalisation capabilities across diverse domains. However, the\nmechanisms underpinning these strong capabilities remain only partially\nunderstood. A leading conjecture, based on the manifold hypothesis, attributes\nthis success to their ability to adapt to low-dimensional geometric structure\nwithin the data. This work provides evidence for this conjecture, focusing on\nhow such phenomena could result from the formulation of the learning problem\nthrough score matching. We inspect the role of implicit regularisation by\ninvestigating the effect of smoothing minimisers of the empirical score\nmatching objective. Our theoretical and empirical results confirm that\nsmoothing the score function -- or equivalently, smoothing in the log-density\ndomain -- produces smoothing tangential to the data manifold. In addition, we\nshow that the manifold along which the diffusion model generalises can be\ncontrolled by choosing an appropriate smoothing.",
      "authors": [
        "Tyler Farghly",
        "Peter Potaptchik",
        "Samuel Howard",
        "George Deligiannidis",
        "Jakiw Pidstrigach"
      ],
      "published": "2025-10-02T17:59:39Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02305v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究从流形假说角度解释扩散模型的泛化能力，证明通过对数密度域平滑可实现沿数据流形的几何自适应，并通过分数匹配理论验证了扩散模型对低维数据结构的适应性。",
      "order": 2
    },
    {
      "arxiv_id": "2510.02300v1",
      "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
      "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.",
      "authors": [
        "Runqian Wang",
        "Yilun Du"
      ],
      "published": "2025-10-02T17:59:06Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02300v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出平衡匹配(EqM)生成建模框架，摒弃传统扩散/流模型的时间条件动态，通过隐式能量景观的平衡梯度学习实现优化驱动采样。在ImageNet 256×256上达到1.90 FID，支持去噪、异常检测和图像合成等任务，为流模型与能量模型搭建了更紧密的桥梁。",
      "order": 3
    },
    {
      "arxiv_id": "2510.02297v1",
      "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
      "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
      "authors": [
        "Wentao Zhang",
        "Yang Young Lu",
        "Yuntian Deng"
      ],
      "published": "2025-10-02T17:59:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02297v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出交互式训练框架，通过控制服务器实现人机协同实时干预神经网络训练过程，支持动态调整超参数、训练数据和模型检查点，在三个案例中证明其能提升训练稳定性、降低超参数敏感性并增强适应性。",
      "order": 4
    },
    {
      "arxiv_id": "2510.02295v1",
      "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
      "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.",
      "authors": [
        "Enxin Song",
        "Wenhao Chai",
        "Shusheng Yang",
        "Ethan Armand",
        "Xiaojun Shan",
        "Haiyang Xu",
        "Jianwen Xie",
        "Zhuowen Tu"
      ],
      "published": "2025-10-02T17:58:54Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02295v1",
      "primary_area": "vla_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "VideoNSA通过原生稀疏注意力机制增强视频语言模型，在Qwen2.5-VL基础上采用硬件感知的混合注意力策略：文本保持稠密注意力，视频使用稀疏注意力。该方法在21.6万视频指令数据集上端到端训练，支持128K长上下文，在长视频理解、时序推理和空间任务上优于基准模型，并揭示了全局-局部注意力分配和动态注意力汇聚等关键发现。",
      "order": 5
    },
    {
      "arxiv_id": "2510.02294v1",
      "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
      "summary": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of\nstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike\nprevious top-ranking embedding models that require massive contrastive\npretraining, sophisticated training pipelines, and costly synthetic training\ndata, F2LLM is directly finetuned from foundation models on 6 million\nquery-document-negative tuples curated from open-source, non-synthetic\ndatasets, striking a strong balance between training cost, model size, and\nembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd\namong models with approximately 4B parameters and 7th overall, while F2LLM-1.7B\nranks 1st among models in the 1B-2B size range. To facilitate future research\nin the field, we release the models, training dataset, and code, positioning\nF2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
      "authors": [
        "Ziyin Zhang",
        "Zihan Liao",
        "Hang Yu",
        "Peng Di",
        "Rui Wang"
      ],
      "published": "2025-10-02T17:58:49Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02294v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "F2LLM技术报告提出了一套0.6B、1.7B和4B三种规模的嵌入模型，仅使用600万开源非合成数据微调基础模型，在MTEB英文榜单上取得优异排名，实现了训练成本、模型大小与嵌入性能的良好平衡。",
      "order": 6
    },
    {
      "arxiv_id": "2510.02286v1",
      "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
      "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
      "authors": [
        "Ruohao Guo",
        "Afshin Oroojlooy",
        "Roshan Sridhar",
        "Miguel Ballesteros",
        "Alan Ritter",
        "Dan Roth"
      ],
      "published": "2025-10-02T17:57:05Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02286v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出DialTree-RPO框架，结合强化学习与树搜索，针对大语言模型在多轮对话中的安全漏洞进行自动化红队攻击。该方法将对话视为序列决策问题，无需人工标注数据即可自主发现多样化攻击策略，在10个目标模型上攻击成功率比现有最优方法提升25.9%，并能有效挖掘新型多轮攻击路径。",
      "order": 7
    },
    {
      "arxiv_id": "2510.02284v1",
      "title": "Learning to Generate Object Interactions with Physics-Guided Video\n  Diffusion",
      "summary": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.",
      "authors": [
        "David Romero",
        "Ariana Bermudez",
        "Hao Li",
        "Fabio Pizzati",
        "Ivan Laptev"
      ],
      "published": "2025-10-02T17:56:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02284v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "提出KineMask方法，通过物理引导的视频扩散模型实现逼真的刚体控制和物体交互生成。该方法采用两阶段训练策略，结合低级运动控制和高级文本条件，在合成场景和真实场景中均显著提升了物体交互的物理合理性。",
      "order": 8
    },
    {
      "arxiv_id": "2510.02283v1",
      "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
      "summary": "Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/",
      "authors": [
        "Justin Cui",
        "Jie Wu",
        "Ming Li",
        "Tao Yang",
        "Xiaojie Li",
        "Rui Wang",
        "Andrew Bai",
        "Yuanhao Ban",
        "Cho-Jui Hsieh"
      ],
      "published": "2025-10-02T17:55:42Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02283v1",
      "primary_area": "video_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Self-Forcing++方法，通过利用教师模型的丰富知识为自生成长视频提供片段指导，有效缓解长视频生成中的质量退化问题。该方法无需长视频教师监督或重新训练，可将视频长度扩展至教师能力的20倍，最高支持生成4分15秒视频，在保真度和一致性上显著优于基线方法。",
      "order": 9
    },
    {
      "arxiv_id": "2510.02279v1",
      "title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods\n  for Natural Language Generation",
      "summary": "Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings.",
      "authors": [
        "Mykyta Ielanskyi",
        "Kajetan Schweighofer",
        "Lukas Aichberger",
        "Sepp Hochreiter"
      ],
      "published": "2025-10-02T17:54:09Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02279v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对自然语言生成中不确定性估计方法的评估缺陷，提出使用多种风险指标、多LLM评判集成、结构化任务及Elo评分等策略，以提高评估的鲁棒性和客观性，解决现有评估方法因近似正确性函数不一致导致的性能虚高问题。",
      "order": 10
    },
    {
      "arxiv_id": "2510.02276v1",
      "title": "BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge\n  Transfer across Biosignals",
      "summary": "Biosignals offer valuable insights into the physiological states of the human\nbody. Although biosignal modalities differ in functionality, signal fidelity,\nsensor comfort, and cost, they are often intercorrelated, reflecting the\nholistic and interconnected nature of human physiology. This opens up the\npossibility of performing the same tasks using alternative biosignal\nmodalities, thereby improving the accessibility, usability, and adaptability of\nhealth monitoring systems. However, the limited availability of large labeled\ndatasets presents challenges for training models tailored to specific tasks and\nmodalities of interest. Unsupervised cross-modal knowledge transfer offers a\npromising solution by leveraging knowledge from an existing modality to support\nmodel training for a new modality. Existing methods are typically based on\nknowledge distillation, which requires running a teacher model alongside\nstudent model training, resulting in high computational and memory overhead.\nThis challenge is further exacerbated by the recent development of foundation\nmodels that demonstrate superior performance and generalization across tasks at\nthe cost of large model sizes. To this end, we explore a new framework for\nunsupervised cross-modal knowledge transfer of biosignals by training a\nlightweight bridge network to align the intermediate representations and enable\ninformation flow between foundation models and across modalities. Specifically,\nwe introduce an efficient strategy for selecting alignment positions where the\nbridge should be constructed, along with a flexible prototype network as the\nbridge architecture. Extensive experiments across multiple biosignal\nmodalities, tasks, and datasets show that BioX-Bridge reduces the number of\ntrainable parameters by 88--99\\% while maintaining or even improving transfer\nperformance compared to state-of-the-art methods.",
      "authors": [
        "Chenqi Li",
        "Yu Liu",
        "Timothy Denison",
        "Tingting Zhu"
      ],
      "published": "2025-10-02T17:51:19Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02276v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "medical_ai",
      "tldr_zh": "BioX-Bridge提出一种无监督跨模态知识迁移框架，通过轻量级桥接网络对齐生物信号基础模型的中间表示，在减少88-99%可训练参数的同时保持或提升迁移性能，解决了生物信号模态间知识迁移的计算效率问题。",
      "order": 11
    },
    {
      "arxiv_id": "2510.02272v1",
      "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
      "summary": "Recent advancements in Reinforcement Post-Training (RPT) have significantly\nenhanced the capabilities of Large Reasoning Models (LRMs), sparking increased\ninterest in the generalization of RL-based reasoning. While existing work has\nprimarily focused on investigating its generalization across tasks or\nmodalities, this study proposes a novel cross-linguistic perspective to\ninvestigate reasoning generalization. This raises a crucial question:\n$\\textit{Does the reasoning capability achieved from English RPT effectively\ntransfer to other languages?}$ We address this by systematically evaluating\nEnglish-centric LRMs on multilingual reasoning benchmarks and introducing a\nmetric to quantify cross-lingual transferability. Our findings reveal that\ncross-lingual transferability varies significantly across initial model, target\nlanguage, and training paradigm. Through interventional studies, we find that\nmodels with stronger initial English capabilities tend to over-rely on\nEnglish-specific patterns, leading to diminished cross-lingual generalization.\nTo address this, we conduct a thorough parallel training study. Experimental\nresults yield three key findings: $\\textbf{First-Parallel Leap}$, a substantial\nleap in performance when transitioning from monolingual to just a single\nparallel language, and a predictable $\\textbf{Parallel Scaling Law}$, revealing\nthat cross-lingual reasoning transfer follows a power-law with the number of\ntraining parallel languages. Moreover, we identify the discrepancy between\nactual monolingual performance and the power-law prediction as\n$\\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs\nfail to fully generalize across languages. Our study challenges the assumption\nthat LRM reasoning mirrors human cognition, providing critical insights for the\ndevelopment of more language-agnostic LRMs.",
      "authors": [
        "Wen Yang",
        "Junhong Wu",
        "Chong Li",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "published": "2025-10-02T17:49:49Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02272v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究从跨语言视角探讨大型推理模型的泛化能力，发现英语训练的模型在其他语言上存在泛化不足问题。通过平行训练实验，揭示了从单语到双语训练的'平行跃迁'现象，建立了平行扩展定律，并识别出单语泛化差距，为开发语言无关的推理模型提供了重要见解。",
      "order": 12
    },
    {
      "arxiv_id": "2510.02271v1",
      "title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in\n  Tool-Augmented Agents",
      "summary": "Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling.",
      "authors": [
        "Yaxin Du",
        "Yuanshuo Zhang",
        "Xiyuan Yang",
        "Yifan Zhou",
        "Cheng Wang",
        "Gongyi Zou",
        "Xianghe Pang",
        "Wenhao Wang",
        "Menglan Chen",
        "Shuo Tang",
        "Zhiyu Li",
        "Siheng Chen"
      ],
      "published": "2025-10-02T17:48:03Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02271v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "InfoMosaic-Bench是首个针对工具增强智能体多源信息检索的基准测试，涵盖医疗、金融等六大领域。研究发现：仅依赖网络信息准确率仅38.2%，领域工具效果不稳定，22.4%失败源于工具使用错误，揭示当前LLM在工具整合方面仍存在困难。",
      "order": 13
    },
    {
      "arxiv_id": "2510.02270v1",
      "title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for\n  Fine-Grained Image Classification",
      "summary": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP.",
      "authors": [
        "Sathira Silva",
        "Eman Ali",
        "Chetan Arora",
        "Muhammad Haris Khan"
      ],
      "published": "2025-10-02T17:47:39Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02270v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "microCLIP提出了一种无监督CLIP自适应框架，通过粗-细粒度令牌融合提升细粒度图像分类性能。核心创新包括：基于显著性注意力的令牌融合模块构建细粒度[FG]令牌并与全局[CLS]令牌对齐；双头LLM分类器提供稳定伪标签；动态知识聚合迭代优化。在13个细粒度基准上平均提升2.90%准确率，仅需轻量适配。",
      "order": 14
    },
    {
      "arxiv_id": "2510.02265v1",
      "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement\n  Learning",
      "summary": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime.",
      "authors": [
        "Yalin E. Sagduyu",
        "Tugba Erpek",
        "Kemal Davaslioglu",
        "Sastry Kompella"
      ],
      "published": "2025-10-02T17:44:38Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02265v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究如何利用强化学习对抗反应式动态干扰攻击，通过Q学习和深度Q网络自适应调整传输功率、调制方式和信道选择，在未知信道条件和干扰策略的情况下维持高传输速率。",
      "order": 15
    },
    {
      "arxiv_id": "2510.02264v1",
      "title": "Paving the Way Towards Kinematic Assessment Using Monocular Video: A\n  Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose\n  Estimators Against Inertial Sensors in Daily Living Activities",
      "summary": "Advances in machine learning and wearable sensors offer new opportunities for\ncapturing and analyzing human movement outside specialized laboratories.\nAccurate assessment of human movement under real-world conditions is essential\nfor telemedicine, sports science, and rehabilitation. This preclinical\nbenchmark compares monocular video-based 3D human pose estimation models with\ninertial measurement units (IMUs), leveraging the VIDIMU dataset containing a\ntotal of 13 clinically relevant daily activities which were captured using both\ncommodity video cameras and five IMUs. During this initial study only healthy\nsubjects were recorded, so results cannot be generalized to pathological\ncohorts. Joint angles derived from state-of-the-art deep learning frameworks\n(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA\nBodyTrack) were evaluated against joint angles computed from IMU data using\nOpenSim inverse kinematics following the Human3.6M dataset format with 17\nkeypoints. Among them, MotionAGFormer demonstrated superior performance,\nachieving the lowest overall RMSE ($9.27\\deg \\pm 4.80\\deg$) and MAE ($7.86\\deg\n\\pm 4.18\\deg$), as well as the highest Pearson correlation ($0.86 \\pm 0.15$)\nand the highest coefficient of determination $R^{2}$ ($0.67 \\pm 0.28$). The\nresults reveal that both technologies are viable for out-of-the-lab kinematic\nassessment. However, they also highlight key trade-offs between video- and\nsensor-based approaches including costs, accessibility, and precision. This\nstudy clarifies where off-the-shelf video models already provide clinically\npromising kinematics in healthy adults and where they lag behind IMU-based\nestimates while establishing valuable guidelines for researchers and clinicians\nseeking to develop robust, cost-effective, and user-friendly solutions for\ntelehealth and remote patient monitoring.",
      "authors": [
        "Mario Medrano-Paredes",
        "Carmen Fernández-González",
        "Francisco-Javier Díaz-Pernas",
        "Hichem Saoudi",
        "Javier González-Alonso",
        "Mario Martínez-Zarzuela"
      ],
      "published": "2025-10-02T17:44:31Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02264v1",
      "primary_area": "video_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究比较了基于单目视频的3D人体姿态估计模型与惯性传感器在日常生活活动中的性能，使用VIDIMU数据集评估了四种深度学习框架。MotionAGFormer表现最佳，整体RMSE为9.27°±4.80°，皮尔逊相关系数达0.86±0.15。研究揭示了视频与传感器方案在成本、可及性和精度间的权衡，为远程医疗和患者监测提供了实用指南。",
      "order": 16
    },
    {
      "arxiv_id": "2510.02263v1",
      "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
      "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.",
      "authors": [
        "Yuxiao Qu",
        "Anikait Singh",
        "Yoonho Lee",
        "Amrith Setlur",
        "Ruslan Salakhutdinov",
        "Chelsea Finn",
        "Aviral Kumar"
      ],
      "published": "2025-10-02T17:44:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02263v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出RLAD方法，通过两阶段强化学习训练大语言模型：首先生成推理抽象概念，然后利用这些抽象指导问题求解。该方法能有效提升模型在复杂推理任务中的泛化能力，并证明在测试时增加抽象生成计算比单纯增加解决方案数量更有效。",
      "order": 17
    },
    {
      "arxiv_id": "2510.02253v1",
      "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
      "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
      "authors": [
        "Zihan Zhou",
        "Shilin Lu",
        "Shuli Leng",
        "Shaocong Zhang",
        "Zhuming Lian",
        "Xinlei Yu",
        "Adams Wai-Kin Kong"
      ],
      "published": "2025-10-02T17:39:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02253v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "DragFlow是首个利用FLUX强大先验进行拖拽式图像编辑的框架，通过区域监督和仿射变换解决了DiT模型在点级拖拽编辑中的性能问题，结合个性化适配器和多模态大语言模型，在DragBench-DR和ReD Bench基准测试中达到最先进水平。",
      "order": 18
    },
    {
      "arxiv_id": "2510.02250v1",
      "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
      "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
      "authors": [
        "Gonzalo Gonzalez-Pumariega",
        "Vincent Tu",
        "Chih-Lun Lee",
        "Jiachen Yang",
        "Ang Li",
        "Xin Eric Wang"
      ],
      "published": "2025-10-02T17:37:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02250v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Behavior Best-of-N (bBoN)方法，通过生成多个执行轨迹并使用行为叙述进行选择，显著提升计算机使用代理的鲁棒性和成功率。在OSWorld基准测试中达到69.9%的新SOTA，接近人类72%的水平，并在跨操作系统任务中展现强大泛化能力，证明了规模化代理在结构化轨迹理解和选择下的卓越效果。",
      "order": 19
    },
    {
      "arxiv_id": "2510.02249v1",
      "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process.",
      "authors": [
        "Tianyi Jiang",
        "Yi Bin",
        "Yujuan Ding",
        "Kainian Zhu",
        "Fei Ma",
        "Jingkuan Song",
        "Heng Tao Shen"
      ],
      "published": "2025-10-02T17:36:50Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02249v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种名为'简要探索后决策'的新推理范式，通过累积熵调控机制解决大语言模型过度思考问题。该方法利用标记熵累积平均值动态确定最佳推理终止点，在保持解题能力的同时显著减少71%的响应长度，实现高效自适应推理。",
      "order": 20
    },
    {
      "arxiv_id": "2510.02245v1",
      "title": "ExGRPO: Learning to Reason from Experience",
      "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.",
      "authors": [
        "Runzhe Zhan",
        "Yafu Li",
        "Zhi Wang",
        "Xiaoye Qu",
        "Dongrui Liu",
        "Jing Shao",
        "Derek F. Wong",
        "Yu Cheng"
      ],
      "published": "2025-10-02T17:31:30Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02245v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ExGRPO框架，通过分析推理经验的价值特征（正确性和熵），对经验进行组织与优先级排序，采用混合策略目标平衡探索与经验利用。在1.5B-8B参数模型上的实验表明，该方法相比传统策略优化在数学/通用推理任务上平均提升3.5/7.6分，并能稳定训练过程。",
      "order": 21
    },
    {
      "arxiv_id": "2510.02240v1",
      "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
      "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.",
      "authors": [
        "Sicheng Feng",
        "Kaiwen Tuo",
        "Song Wang",
        "Lingdong Kong",
        "Jianke Zhu",
        "Huan Wang"
      ],
      "published": "2025-10-02T17:29:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02240v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "RewardMap提出多阶段强化学习框架，通过难度感知奖励设计和从感知到推理的分阶段训练策略，解决细粒度视觉推理中的稀疏奖励问题。在ReasonMap-Plus数据集上验证，模型在空间推理和视觉理解任务上平均提升3.47%。",
      "order": 22
    },
    {
      "arxiv_id": "2510.02230v1",
      "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.",
      "authors": [
        "Phuc Minh Nguyen",
        "Chinh D. La",
        "Duy M. H. Nguyen",
        "Nitesh V. Chawla",
        "Binh T. Nguyen",
        "Khoa D. Doan"
      ],
      "published": "2025-10-02T17:17:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02230v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究发现，使用可验证奖励的强化学习（RLVR）在提升大语言模型推理能力时，反而会因负干扰和赢家通吃现象缩小推理边界。通过理论分析和数学推理基准测试，揭示了标准RL目标中的策略采样导致模型收敛于狭窄解策略的问题，并提出针对低概率问题的数据筛选算法，显著提升了Pass@k性能。",
      "order": 23
    },
    {
      "arxiv_id": "2510.02227v1",
      "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.",
      "authors": [
        "Xiaoyang Yuan",
        "Yujuan Ding",
        "Yi Bin",
        "Wenqi Shao",
        "Jinyu Cai",
        "Jingkuan Song",
        "Yang Yang",
        "Hengtao Shen"
      ],
      "published": "2025-10-02T17:14:00Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02227v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出自适应多引导策略优化(AMPO)框架，通过动态调用多个教师模型指导，在大型语言模型推理失败时提供按需引导，增强推理多样性和性能。该方法在数学推理任务上提升4.3%，分布外任务提升12.2%，实现了更高效的探索与泛化能力。",
      "order": 24
    },
    {
      "arxiv_id": "2510.02226v1",
      "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models",
      "summary": "Recent advances in generative video models have enabled the creation of\nhigh-quality videos based on natural language prompts. However, these models\nfrequently lack fine-grained temporal control, meaning they do not allow users\nto specify when particular visual elements should appear within a generated\nsequence. In this work, we introduce TempoControl, a method that allows for\ntemporal alignment of visual concepts during inference, without requiring\nretraining or additional supervision. TempoControl utilizes cross-attention\nmaps, a key component of text-to-video diffusion models, to guide the timing of\nconcepts through a novel optimization approach. Our method steers attention\nusing three complementary principles: aligning its temporal shape with a\ncontrol signal (via correlation), amplifying it where visibility is needed (via\nenergy), and maintaining spatial focus (via entropy). TempoControl allows\nprecise control over timing while ensuring high video quality and diversity. We\ndemonstrate its effectiveness across various video generation applications,\nincluding temporal reordering for single and multiple objects, as well as\naction and audio-aligned generation.",
      "authors": [
        "Shira Schiber",
        "Ofir Lindenbaum",
        "Idan Schwartz"
      ],
      "published": "2025-10-02T17:13:35Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02226v1",
      "primary_area": "video_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "TempoControl是一种无需重新训练即可实现文本到视频生成模型时间对齐的方法，通过优化交叉注意力图来精确控制视觉概念在视频序列中的出现时机，支持时序重排、动作和音频对齐等多种应用场景。",
      "order": 25
    },
    {
      "arxiv_id": "2510.02212v1",
      "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via\n  Reinforcement Learning",
      "summary": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks.",
      "authors": [
        "Hanyang Zhao",
        "Dawen Liang",
        "Wenpin Tang",
        "David Yao",
        "Nathan Kallus"
      ],
      "published": "2025-10-02T16:57:24Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02212v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "DiFFPO提出了一种基于强化学习的统一训练框架，用于优化掩码扩散大语言模型(dLLMs)。该方法通过离策略RL训练代理策略，结合重要性采样校正的两阶段似然近似，提升样本效率和任务性能。同时联合训练高效采样器，让模型自适应分配推理阈值，在减少函数评估次数的同时提高准确性，在数学推理和规划任务上展现了优越性能。",
      "order": 26
    },
    {
      "arxiv_id": "2510.02202v1",
      "title": "Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet\n  Challenge 2025",
      "summary": "Objective: Chagas disease is a parasitic infection that is endemic to South\nAmerica, Central America, and, more recently, the U.S., primarily transmitted\nby insects. Chronic Chagas disease can cause cardiovascular diseases and\ndigestive problems. Serological testing capacities for Chagas disease are\nlimited, but Chagas cardiomyopathy often manifests in ECGs, providing an\nopportunity to prioritize patients for testing and treatment. Approach: The\nGeorge B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic\napproaches for identifying Chagas disease from electrocardiograms (ECGs). Main\nresults: This Challenge provides multiple innovations. First, we leveraged\nseveral datasets with labels from patient reports and serological testing,\nprovided a large dataset with weak labels and smaller datasets with strong\nlabels. Second, we augmented the data to support model robustness and\ngeneralizability to unseen data sources. Third, we applied an evaluation metric\nthat captured the local serological testing capacity for Chagas disease to\nframe the machine learning problem as a triage task. Significance: Over 630\nparticipants from 111 teams submitted over 1300 entries during the Challenge,\nrepresenting diverse approaches from academia and industry worldwide.",
      "authors": [
        "Matthew A. Reyna",
        "Zuzana Koscova",
        "Jan Pavlus",
        "Soheil Saghafi",
        "James Weigle",
        "Andoni Elola",
        "Salman Seyedi",
        "Kiersten Campbell",
        "Qiao Li",
        "Ali Bahrami Rad",
        "Antônio H. Ribeiro",
        "Antonio Luiz P. Ribeiro",
        "Reza Sameni",
        "Gari D. Clifford"
      ],
      "published": "2025-10-02T16:50:36Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02202v1",
      "primary_area": "audio_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "乔治·B·穆迪PhysioNet挑战赛2025聚焦于利用心电图检测恰加斯病，通过整合多源数据集（含强弱标签）、数据增强技术和考虑当地检测能力的评估指标，吸引了全球630多名参与者开发算法模型，旨在优化患者筛查和治疗优先级排序。",
      "order": 27
    },
    {
      "arxiv_id": "2510.02200v1",
      "title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge\n  Graph Exploration Utilities",
      "summary": "Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain.",
      "authors": [
        "Felix Brei",
        "Lorenz Bühmann",
        "Johannes Frey",
        "Daniel Gerber",
        "Lars-Peter Meyer",
        "Claus Stadler",
        "Kirill Bulert"
      ],
      "published": "2025-10-02T16:49:27Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02200v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "ARUQULA是一种基于大语言模型的Text2SPARQL方法，采用ReAct框架和知识图谱探索工具，通过迭代探索和执行过程将自然语言问题转换为SPARQL查询，旨在降低非专业人士查询知识图谱的门槛。",
      "order": 28
    },
    {
      "arxiv_id": "2510.02194v1",
      "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language\n  Models",
      "summary": "Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control.",
      "authors": [
        "Yuhao Sun",
        "Zhuoer Xu",
        "Shiwen Cui",
        "Kun Yang",
        "Lingyun Yu",
        "Yongdong Zhang",
        "Hongtao Xie"
      ],
      "published": "2025-10-02T16:43:33Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02194v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "UpSafe°C提出了一种通过安全感知升级增强大语言模型安全性的统一框架。该方法识别安全关键层并将其升级为稀疏混合专家结构，引入两阶段微调策略和安全温度机制，在保持通用能力的同时实现推理时安全与效用的动态平衡控制。",
      "order": 29
    },
    {
      "arxiv_id": "2510.02190v1",
      "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
      "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.",
      "authors": [
        "Yang Yao",
        "Yixu Wang",
        "Yuxuan Zhang",
        "Yi Lu",
        "Tianle Gu",
        "Lingyu Li",
        "Dingyi Zhao",
        "Keming Wu",
        "Haozhe Wang",
        "Ping Nie",
        "Yan Teng",
        "Yingchun Wang"
      ],
      "published": "2025-10-02T16:40:02Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02190v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对深度研究智能体(DRAs)提出了一套严谨的基准测试与多维评估框架，包含214个专家精心设计的跨领域查询及人工构建的参考答案包。该框架通过语义质量、主题聚焦和检索可信度等综合指标，系统评估DRAs生成的长篇报告能力。实验表明主流DRAs优于增强型推理模型，但仍存在显著改进空间。",
      "order": 30
    },
    {
      "arxiv_id": "2510.02181v1",
      "title": "EvolveCaptions: Empowering DHH Users Through Real-Time Collaborative\n  Captioning",
      "summary": "Automatic Speech Recognition (ASR) systems often fail to accurately\ntranscribe speech from Deaf and Hard of Hearing (DHH) individuals, especially\nduring real-time conversations. Existing personalization approaches typically\nrequire extensive pre-recorded data and place the burden of adaptation on the\nDHH speaker. We present EvolveCaptions, a real-time, collaborative ASR\nadaptation system that supports in-situ personalization with minimal effort.\nHearing participants correct ASR errors during live conversations. Based on\nthese corrections, the system generates short, phonetically targeted prompts\nfor the DHH speaker to record, which are then used to fine-tune the ASR model.\nIn a study with 12 DHH and six hearing participants, EvolveCaptions reduced\nWord Error Rate (WER) across all DHH users within one hour of use, using only\nfive minutes of recording time on average. Participants described the system as\nintuitive, low-effort, and well-integrated into communication. These findings\ndemonstrate the promise of collaborative, real-time ASR adaptation for more\nequitable communication.",
      "authors": [
        "Liang-Yuan Wu",
        "Dhruv Jain"
      ],
      "published": "2025-10-02T16:32:29Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.02181v1",
      "primary_area": "audio_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "EvolveCaptions是一个实时协作式ASR自适应系统，通过听力参与者纠正语音识别错误，生成针对性语音提示让听障用户录制，从而快速优化ASR模型。研究表明，该系统在一小时内显著降低了所有听障用户的词错误率，仅需平均五分钟录音时间，被评价为直观低耗的沟通解决方案。",
      "order": 31
    },
    {
      "arxiv_id": "2510.02180v1",
      "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement\n  Learning",
      "summary": "Inverse Reinforcement Learning aims to recover reward models from expert\ndemonstrations, but traditional methods yield \"black-box\" models that are\ndifficult to interpret and debug. In this work, we introduce GRACE (Generating\nRewards As CodE), a method for using Large Language Models within an\nevolutionary search to reverse-engineer an interpretable, code-based reward\nfunction directly from expert trajectories. The resulting reward function is\nexecutable code that can be inspected and verified. We empirically validate\nGRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns\nhighly accurate rewards, even in complex, multi-task settings. Further, we\ndemonstrate that the resulting reward leads to strong policies, compared to\nboth competitive Imitation Learning and online RL approaches with ground-truth\nrewards. Finally, we show that GRACE is able to build complex reward APIs in\nmulti-task setups.",
      "authors": [
        "Silvia Sapora",
        "Devon Hjelm",
        "Alexander Toshev",
        "Omar Attia",
        "Bogdan Mazoure"
      ],
      "published": "2025-10-02T16:31:39Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02180v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "GRACE框架利用大语言模型通过进化搜索从专家轨迹中逆向生成可解释的代码化奖励函数，解决了传统逆强化学习模型难以解释的问题。该方法在BabyAI和AndroidWorld基准测试中表现出色，能生成可执行验证的奖励代码，并在多任务环境中构建复杂奖励API。",
      "order": 32
    },
    {
      "arxiv_id": "2510.02173v1",
      "title": "Learning to Reason for Hallucination Span Detection",
      "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
      "authors": [
        "Hsuan Su",
        "Ting-Yao Hu",
        "Hema Swetha Koppula",
        "Kundan Krishna",
        "Hadi Pouransari",
        "Cheng-Yu Hsieh",
        "Cem Koc",
        "Joseph Yitan Cheng",
        "Oncel Tuzel",
        "Raviteja Vemulapalli"
      ],
      "published": "2025-10-02T16:24:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02173v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出RL4HS强化学习框架，通过链式思维推理和跨度级奖励函数来检测大语言模型生成的幻觉内容。相比传统二分类方法，该方法能精确定位幻觉片段，在RAGTruth基准测试中优于预训练模型和监督微调。",
      "order": 33
    },
    {
      "arxiv_id": "2510.02171v1",
      "title": "Go witheFlow: Real-time Emotion Driven Audio Effects Modulation",
      "summary": "Music performance is a distinctly human activity, intrinsically linked to the\nperformer's ability to convey, evoke, or express emotion. Machines cannot\nperform music in the human sense; they can produce, reproduce, execute, or\nsynthesize music, but they lack the capacity for affective or emotional\nexperience. As such, music performance is an ideal candidate through which to\nexplore aspects of collaboration between humans and machines. In this paper, we\nintroduce the witheFlow system, designed to enhance real-time music performance\nby automatically modulating audio effects based on features extracted from both\nbiosignals and the audio itself. The system, currently in a proof-of-concept\nphase, is designed to be lightweight, able to run locally on a laptop, and is\nopen-source given the availability of a compatible Digital Audio Workstation\nand sensors.",
      "authors": [
        "Edmund Dervakos",
        "Spyridon Kantarelis",
        "Vassilis Lyberatos",
        "Jason Liartis",
        "Giorgos Stamou"
      ],
      "published": "2025-10-02T16:23:47Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.02171v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出witheFlow系统，通过分析生物信号和音频特征实时调制音效，增强音乐表演的情感表达。该系统轻量级、开源，可在笔记本电脑本地运行，目前处于概念验证阶段。",
      "order": 34
    },
    {
      "arxiv_id": "2510.02166v1",
      "title": "SIEVE: Towards Verifiable Certification for Code-datasets",
      "summary": "Code agents and empirical software engineering rely on public code datasets,\nyet these datasets lack verifiable quality guarantees. Static 'dataset cards'\ninform, but they are neither auditable nor do they offer statistical\nguarantees, making it difficult to attest to dataset quality. Teams build\nisolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We\npresent SIEVE, a community-driven framework. It turns per-property checks into\nConfidence Cards-machine-readable, verifiable certificates with anytime-valid\nstatistical bounds. We outline a research plan to bring SIEVE to maturity,\nreplacing narrative cards with anytime-verifiable certification. This shift is\nexpected to lower quality-assurance costs and increase trust in code-datasets.",
      "authors": [
        "Fatou Ndiaye Mbodji",
        "El-hacen Diallo",
        "Jordan Samhi",
        "Kui Liu",
        "Jacques Klein",
        "Tegawendé F. Bissyande"
      ],
      "published": "2025-10-02T16:14:23Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.02166v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "code_generation",
      "tldr_zh": "SIEVE是一个社区驱动的框架，旨在为代码数据集提供可验证的质量认证。它将属性检查转化为机器可读的置信卡，包含随时有效的统计边界，以取代传统的叙述性数据集卡片，降低质量保证成本并增强对代码数据集的信任。",
      "order": 35
    },
    {
      "arxiv_id": "2510.02161v1",
      "title": "Comparing Contrastive and Triplet Loss in Audio-Visual Embedding:\n  Intra-Class Variance and Greediness Analysis",
      "summary": "Contrastive loss and triplet loss are widely used objectives in deep metric\nlearning, yet their effects on representation quality remain insufficiently\nunderstood. We present a theoretical and empirical comparison of these losses,\nfocusing on intra- and inter-class variance and optimization behavior (e.g.,\ngreedy updates). Through task-specific experiments with consistent settings on\nsynthetic data and real datasets-MNIST, CIFAR-10-it is shown that triplet loss\npreserves greater variance within and across classes, supporting finer-grained\ndistinctions in the learned representations. In contrast, contrastive loss\ntends to compact intra-class embeddings, which may obscure subtle semantic\ndifferences. To better understand their optimization dynamics, By examining\nloss-decay rate, active ratio, and gradient norm, we find that contrastive loss\ndrives many small updates early on, while triplet loss produces fewer but\nstronger updates that sustain learning on hard examples. Finally, across both\nclassification and retrieval tasks on MNIST, CIFAR-10, CUB-200, and CARS196\ndatasets, our results consistently show that triplet loss yields superior\nperformance, which suggests using triplet loss for detail retention and\nhard-sample focus, and contrastive loss for smoother, broad-based embedding\nrefinement.",
      "authors": [
        "Donghuo Zeng"
      ],
      "published": "2025-10-02T16:11:46Z",
      "primary_category": "cs.MM",
      "arxiv_url": "https://arxiv.org/abs/2510.02161v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文对比了对比损失和三元组损失在深度度量学习中的表现，通过理论和实验分析发现：三元组损失能保留更大的类内和类间方差，支持更细粒度的表征区分，且在优化过程中产生较少但更强的更新，专注于困难样本；而对比损失则倾向于压缩类内嵌入，可能导致细微语义差异丢失。在多个数据集上的实验表明，三元组损失在分类和检索任务中表现更优。",
      "order": 36
    },
    {
      "arxiv_id": "2510.02155v1",
      "title": "Unlocking Vision-Language Models for Video Anomaly Detection via\n  Fine-Grained Prompting",
      "summary": "Prompting has emerged as a practical way to adapt frozen vision-language\nmodels (VLMs) for video anomaly detection (VAD). Yet, existing prompts are\noften overly abstract, overlooking the fine-grained human-object interactions\nor action semantics that define complex anomalies in surveillance videos. We\npropose ASK-Hint, a structured prompting framework that leverages\naction-centric knowledge to elicit more accurate and interpretable reasoning\nfrom frozen VLMs. Our approach organizes prompts into semantically coherent\ngroups (e.g. violence, property crimes, public safety) and formulates\nfine-grained guiding questions that align model predictions with discriminative\nvisual cues. Extensive experiments on UCF-Crime and XD-Violence show that\nASK-Hint consistently improves AUC over prior baselines, achieving\nstate-of-the-art performance compared to both fine-tuned and training-free\nmethods. Beyond accuracy, our framework provides interpretable reasoning traces\ntowards anomaly and demonstrates strong generalization across datasets and VLM\nbackbones. These results highlight the critical role of prompt granularity and\nestablish ASK-Hint as a new training-free and generalizable solution for\nexplainable video anomaly detection.",
      "authors": [
        "Shu Zou",
        "Xinyu Tian",
        "Lukas Wesemann",
        "Fabian Waschkowski",
        "Zhaoyuan Yang",
        "Jing Zhang"
      ],
      "published": "2025-10-02T16:06:31Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02155v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ASK-Hint框架，通过细粒度提示解锁视觉语言模型在视频异常检测中的应用。该方法构建结构化提示，利用动作知识引导模型关注人-物交互细节，在UCF-Crime和XD-Violence数据集上实现最先进性能，无需训练即可提供可解释的异常推理轨迹。",
      "order": 37
    },
    {
      "arxiv_id": "2510.02153v1",
      "title": "Human-Robo-advisor collaboration in decision-making: Evidence from a\n  multiphase mixed methods experimental study",
      "summary": "Robo-advisors (RAs) are cost-effective, bias-resistant alternatives to human\nfinancial advisors, yet adoption remains limited. While prior research has\nexamined user interactions with RAs, less is known about how individuals\ninterpret RA roles and integrate their advice into decision-making. To address\nthis gap, this study employs a multiphase mixed methods design integrating a\nbehavioral experiment (N = 334), thematic analysis, and follow-up quantitative\ntesting. Findings suggest that people tend to rely on RAs, with reliance shaped\nby information about RA performance and the framing of advice as gains or\nlosses. Thematic analysis reveals three RA roles in decision-making and four\nuser types, each reflecting distinct patterns of advice integration. In\naddition, a 2 x 2 typology categorizes antecedents of acceptance into enablers\nand inhibitors at both the individual and algorithmic levels. By combining\nbehavioral, interpretive, and confirmatory evidence, this study advances\nunderstanding of human-RA collaboration and provides actionable insights for\ndesigning more trustworthy and adaptive RA systems.",
      "authors": [
        "Hasan Mahmud",
        "Najmul Islam",
        "Satish Krishnan"
      ],
      "published": "2025-10-02T16:04:31Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.02153v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "financial_ai",
      "tldr_zh": "本研究通过多阶段混合方法实验，探讨人机协作决策中用户对机器人顾问的依赖模式。研究发现：用户依赖度受机器人表现信息和损益框架影响；识别出三种机器人决策角色和四类用户类型；提出个体与算法层面的2×2接受度前因分类。为设计更可信赖的自适应机器人顾问系统提供实证依据。",
      "order": 38
    },
    {
      "arxiv_id": "2510.02143v1",
      "title": "How to Find Fantastic Papers: Self-Rankings as a Powerful Predictor of\n  Scientific Impact Beyond Peer Review",
      "summary": "Peer review in academic research aims not only to ensure factual correctness\nbut also to identify work of high scientific potential that can shape future\nresearch directions. This task is especially critical in fast-moving fields\nsuch as artificial intelligence (AI), yet it has become increasingly difficult\ngiven the rapid growth of submissions. In this paper, we investigate an\nunderexplored measure for identifying high-impact research: authors' own\nrankings of their multiple submissions to the same AI conference. Grounded in\ngame-theoretic reasoning, we hypothesize that self-rankings are informative\nbecause authors possess unique understanding of their work's conceptual depth\nand long-term promise. To test this hypothesis, we conducted a large-scale\nexperiment at a leading AI conference, where 1,342 researchers self-ranked\ntheir 2,592 submissions by perceived quality. Tracking outcomes over more than\na year, we found that papers ranked highest by their authors received twice as\nmany citations as their lowest-ranked counterparts; self-rankings were\nespecially effective at identifying highly cited papers (those with over 150\ncitations). Moreover, we showed that self-rankings outperformed peer review\nscores in predicting future citation counts. Our results remained robust after\naccounting for confounders such as preprint posting time and self-citations.\nTogether, these findings demonstrate that authors' self-rankings provide a\nreliable and valuable complement to peer review for identifying and elevating\nhigh-impact research in AI.",
      "authors": [
        "Buxin Su",
        "Natalie Collina",
        "Garrett Wen",
        "Didong Li",
        "Kyunghyun Cho",
        "Jianqing Fan",
        "Bingxin Zhao",
        "Weijie Su"
      ],
      "published": "2025-10-02T15:50:21Z",
      "primary_category": "stat.AP",
      "arxiv_url": "https://arxiv.org/abs/2510.02143v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过AI会议大规模实验发现，作者对自身多篇投稿的质量自评能有效预测论文影响力。自评最高的论文引用量是最低的两倍，且自评比同行评审更能预测未来引用。这为快速发展的AI领域提供了同行评审的有力补充。",
      "order": 39
    },
    {
      "arxiv_id": "2510.02139v1",
      "title": "BioinfoMCP: A Unified Platform Enabling MCP Interfaces in Agentic\n  Bioinformatics",
      "summary": "Bioinformatics tools are essential for complex computational biology tasks,\nyet their integration with emerging AI-agent frameworks is hindered by\nincompatible interfaces, heterogeneous input-output formats, and inconsistent\nparameter conventions. The Model Context Protocol (MCP) provides a standardized\nframework for tool-AI communication, but manually converting hundreds of\nexisting and rapidly growing specialized bioinformatics tools into\nMCP-compliant servers is labor-intensive and unsustainable. Here, we present\nBioinfoMCP, a unified platform comprising two components: BioinfoMCP Converter,\nwhich automatically generates robust MCP servers from tool documentation using\nlarge language models, and BioinfoMCP Benchmark, which systematically validates\nthe reliability and versatility of converted tools across diverse computational\ntasks. We present a platform of 38 MCP-converted bioinformatics tools,\nextensively validated to show that 94.7% successfully executed complex\nworkflows across three widely used AI-agent platforms. By removing technical\nbarriers to AI automation, BioinfoMCP enables natural-language interaction with\nsophisticated bioinformatics analyses without requiring extensive programming\nexpertise, offering a scalable path to intelligent, interoperable computational\nbiology.",
      "authors": [
        "Florensia Widjaja",
        "Zhangtianyi Chen",
        "Juexiao Zhou"
      ],
      "published": "2025-10-02T15:47:59Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.02139v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "BioinfoMCP是一个统一平台，通过自动转换生物信息学工具为MCP兼容服务器，解决了AI代理框架与生物信息工具集成难题。平台包含转换器和基准测试组件，已成功转换38个工具，94.7%能在主流AI平台上执行复杂工作流，实现了无需编程的自然语言交互生物信息分析。",
      "order": 40
    },
    {
      "arxiv_id": "2510.02133v1",
      "title": "FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic\n  Documents for Training Document Understanding Models",
      "summary": "Developing document understanding models at enterprise scale requires large,\ndiverse, and well-annotated datasets spanning a wide range of document types.\nHowever, collecting such data is prohibitively expensive due to privacy\nconstraints, legal restrictions, and the sheer volume of manual annotation\nneeded - costs that can scale into millions of dollars. We introduce FlexDoc, a\nscalable synthetic data generation framework that combines Stochastic Schemas\nand Parameterized Sampling to produce realistic, multilingual semi-structured\ndocuments with rich annotations. By probabilistically modeling layout patterns,\nvisual structure, and content variability, FlexDoc enables the controlled\ngeneration of diverse document variants at scale. Experiments on Key\nInformation Extraction (KIE) tasks demonstrate that FlexDoc-generated data\nimproves the absolute F1 Score by up to 11% when used to augment real datasets,\nwhile reducing annotation effort by over 90% compared to traditional\nhard-template methods. The solution is in active deployment, where it has\naccelerated the development of enterprise-grade document understanding models\nwhile significantly reducing data acquisition and annotation costs.",
      "authors": [
        "Karan Dua",
        "Hitesh Laxmichand Patel",
        "Puneet Mittal",
        "Ranjeet Gupta",
        "Amit Agarwal",
        "Praneet Pabolu",
        "Srikant Panda",
        "Hansa Meghwani",
        "Graham Horwood",
        "Fahad Shah"
      ],
      "published": "2025-10-02T15:42:35Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02133v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "FlexDoc是一个可扩展的合成数据生成框架，通过随机模式和参数化采样创建多语言半结构化文档，用于训练文档理解模型。该方法在关键信息提取任务中可将F1分数提升高达11%，同时减少90%以上的标注工作量，显著降低企业级文档理解模型的开发成本。",
      "order": 41
    },
    {
      "arxiv_id": "2510.02128v1",
      "title": "The Disparate Impacts of Speculative Decoding",
      "summary": "The practice of speculative decoding, whereby inference is probabilistically\nsupported by a smaller, cheaper, ``drafter'' model, has become a standard\ntechnique for systematically reducing the decoding time of large language\nmodels. This paper conducts an analysis of speculative decoding through the\nlens of its potential disparate speed-up rates across tasks. Crucially, the\npaper shows that speed-up gained from speculative decoding is not uniformly\ndistributed across tasks, consistently diminishing for under-fit, and often\nunderrepresented tasks. To better understand this phenomenon, we derive an\nanalysis to quantify this observed ``unfairness'' and draw attention to the\nfactors that motivate such disparate speed-ups to emerge. Further, guided by\nthese insights, the paper proposes a mitigation strategy designed to reduce\nspeed-up disparities and validates the approach across several model pairs,\nrevealing on average a 12% improvement in our fairness metric.",
      "authors": [
        "Jameson Sandler",
        "Ahmet Üstün",
        "Marco Romanelli",
        "Sara Hooker",
        "Ferdinando Fioretto"
      ],
      "published": "2025-10-02T15:38:57Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02128v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文分析推测解码技术在加速大语言模型推理时存在的不公平现象，发现该技术对欠拟合和代表性不足任务的加速效果较差。研究提出量化分析框架揭示差异成因，并设计缓解策略，在多个模型对上验证可将公平性指标平均提升12%。",
      "order": 42
    },
    {
      "arxiv_id": "2510.02125v1",
      "title": "Do AI Models Perform Human-like Abstract Reasoning Across Modalities?",
      "summary": "OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI\nbenchmark, but does that mean state-of-the-art models recognize and reason with\nthe abstractions that the task creators intended? We investigate models'\nabstraction abilities on ConceptARC. We evaluate models under settings that\nvary the input modality (textual vs. visual), whether the model is permitted to\nuse external Python tools, and, for reasoning models, the amount of reasoning\neffort. In addition to measuring output accuracy, we perform fine-grained\nevaluation of the natural-language rules that models generate to explain their\nsolutions. This dual evaluation lets us assess whether models solve tasks using\nthe abstractions ConceptARC was designed to elicit, rather than relying on\nsurface-level patterns. Our results show that, while some models using\ntext-based representations match human output accuracy, the best models' rules\nare often based on surface-level ``shortcuts'' and capture intended\nabstractions far less often than humans. Thus their capabilities for general\nabstract reasoning may be overestimated by evaluations based on accuracy alone.\nIn the visual modality, AI models' output accuracy drops sharply, yet our\nrule-level analysis reveals that models might be underestimated, as they still\nexhibit a substantial share of rules that capture intended abstractions, but\nare often unable to correctly apply these rules. In short, our results show\nthat models still lag humans in abstract reasoning, and that using accuracy\nalone to evaluate abstract reasoning on ARC-like tasks may overestimate\nabstract-reasoning capabilities in textual modalities and underestimate it in\nvisual modalities. We believe that our evaluation framework offers a more\nfaithful picture of multimodal models' abstract reasoning abilities and a more\nprincipled way to track progress toward human-like, abstraction-centered\nintelligence.",
      "authors": [
        "Claas Beger",
        "Ryan Yi",
        "Shuhao Fu",
        "Arseny Moskvichev",
        "Sarah W. Tsai",
        "Sivasankaran Rajamanickam",
        "Melanie Mitchell"
      ],
      "published": "2025-10-02T15:35:10Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02125v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究评估AI模型在ConceptARC基准上的抽象推理能力，发现尽管某些文本模型在输出准确率上达到人类水平，但其生成的规则多依赖表面特征而非深层抽象概念。视觉模态下模型准确率显著下降，但规则分析显示其仍能捕捉部分抽象概念，仅应用能力不足。研究表明仅凭准确率评估会高估文本模态的推理能力而低估视觉模态，提出了更全面的多模态抽象推理评估框架。",
      "order": 43
    },
    {
      "arxiv_id": "2510.02120v1",
      "title": "VarCoNet: A variability-aware self-supervised framework for functional\n  connectome extraction from resting-state fMRI",
      "summary": "Accounting for inter-individual variability in brain function is key to\nprecision medicine. Here, by considering functional inter-individual\nvariability as meaningful data rather than noise, we introduce VarCoNet, an\nenhanced self-supervised framework for robust functional connectome (FC)\nextraction from resting-state fMRI (rs-fMRI) data. VarCoNet employs\nself-supervised contrastive learning to exploit inherent functional\ninter-individual variability, serving as a brain function encoder that\ngenerates FC embeddings readily applicable to downstream tasks even in the\nabsence of labeled data. Contrastive learning is facilitated by a novel\naugmentation strategy based on segmenting rs-fMRI signals. At its core,\nVarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-series\nprocessing, enhanced with a robust Bayesian hyperparameter optimization. Our\nVarCoNet framework is evaluated on two downstream tasks: (i) subject\nfingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii)\nautism spectrum disorder (ASD) classification, using rs-fMRI data from the\nABIDE I and ABIDE II datasets. Using different brain parcellations, our\nextensive testing against state-of-the-art methods, including 13 deep learning\nmethods, demonstrates VarCoNet's superiority, robustness, interpretability, and\ngeneralizability. Overall, VarCoNet provides a versatile and robust framework\nfor FC analysis in rs-fMRI.",
      "authors": [
        "Charalampos Lamprou",
        "Aamna Alshehhi",
        "Leontios J. Hadjileontiadis",
        "Mohamed L. Seghier"
      ],
      "published": "2025-10-02T15:29:17Z",
      "primary_category": "cs.NE",
      "arxiv_url": "https://arxiv.org/abs/2510.02120v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "VarCoNet是一种基于自监督对比学习的变异性感知框架，用于从静息态fMRI数据中提取功能连接组。该框架利用个体间功能变异性作为有意义数据，结合1D-CNN-Transformer编码器和贝叶斯超参数优化，在人类连接组项目和自闭症分类任务中表现出优越性能。",
      "order": 44
    },
    {
      "arxiv_id": "2510.02109v1",
      "title": "SpurBreast: A Curated Dataset for Investigating Spurious Correlations in\n  Real-world Breast MRI Classification",
      "summary": "Deep neural networks (DNNs) have demonstrated remarkable success in medical\nimaging, yet their real-world deployment remains challenging due to spurious\ncorrelations, where models can learn non-clinical features instead of\nmeaningful medical patterns. Existing medical imaging datasets are not designed\nto systematically study this issue, largely due to restrictive licensing and\nlimited supplementary patient data. To address this gap, we introduce\nSpurBreast, a curated breast MRI dataset that intentionally incorporates\nspurious correlations to evaluate their impact on model performance. Analyzing\nover 100 features involving patient, device, and imaging protocol, we identify\ntwo dominant spurious signals: magnetic field strength (a global feature\ninfluencing the entire image) and image orientation (a local feature affecting\nspatial alignment). Through controlled dataset splits, we demonstrate that DNNs\ncan exploit these non-clinical signals, achieving high validation accuracy\nwhile failing to generalize to unbiased test data. Alongside these two datasets\ncontaining spurious correlations, we also provide benchmark datasets without\nspurious correlations, allowing researchers to systematically investigate\nclinically relevant and irrelevant features, uncertainty estimation,\nadversarial robustness, and generalization strategies. Models and datasets are\navailable at https://github.com/utkuozbulak/spurbreast.",
      "authors": [
        "Jong Bum Won",
        "Wesley De Neve",
        "Joris Vankerschaver",
        "Utku Ozbulak"
      ],
      "published": "2025-10-02T15:16:20Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.02109v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "SpurBreast是一个专门设计的乳腺MRI数据集，用于研究深度学习模型在医学影像中的伪相关性学习问题。该数据集包含患者、设备和成像协议等100多个特征，识别出磁场强度和图像方向两个主要伪信号，通过控制数据集划分展示模型如何利用非临床特征获得高验证精度但无法泛化到无偏测试数据。",
      "order": 45
    },
    {
      "arxiv_id": "2510.02108v1",
      "title": "Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant\n  Neural Network",
      "summary": "Although symbol-level precoding (SLP) based on constructive interference (CI)\nexploitation offers performance gains, its high complexity remains a\nbottleneck. This paper addresses this challenge with an end-to-end deep\nlearning (DL) framework with low inference complexity that leverages the\nstructure of the optimal SLP solution in the closed-form and its inherent\ntensor equivariance (TE), where TE denotes that a permutation of the input\ninduces the corresponding permutation of the output. Building upon the\ncomputationally efficient model-based formulations, as well as their known\nclosed-form solutions, we analyze their relationship with linear precoding (LP)\nand investigate the corresponding optimality condition. We then construct a\nmapping from the problem formulation to the solution and prove its TE, based on\nwhich the designed networks reveal a specific parameter-sharing pattern that\ndelivers low computational complexity and strong generalization. Leveraging\nthese, we propose the backbone of the framework with an attention-based TE\nmodule, achieving linear computational complexity. Furthermore, we demonstrate\nthat such a framework is also applicable to imperfect CSI scenarios, where we\ndesign a TE-based network to map the CSI, statistics, and symbols to auxiliary\nvariables. Simulation results show that the proposed framework captures\nsubstantial performance gains of optimal SLP, while achieving an approximately\n80-times speedup over conventional methods and maintaining strong\ngeneralization across user numbers and symbol block lengths.",
      "authors": [
        "Jinshuo Zhang",
        "Yafei Wang",
        "Xinping Yi",
        "Wenjin Wang",
        "Shi Jin",
        "Symeon Chatzinotas",
        "Björn Ottersten"
      ],
      "published": "2025-10-02T15:15:50Z",
      "primary_category": "eess.SP",
      "arxiv_url": "https://arxiv.org/abs/2510.02108v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于张量等变神经网络的符号级预编码端到端深度学习框架，通过利用最优解的张量等变特性设计参数共享模式，在保持最优SLP性能增益的同时实现80倍加速，并在线性计算复杂度下具备强泛化能力。",
      "order": 46
    },
    {
      "arxiv_id": "2510.02100v1",
      "title": "When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based\n  Tracking in Surgical Videos",
      "summary": "Video object segmentation (VOS) models such as SAM2 offer promising zero-shot\ntracking capabilities for surgical videos using minimal user input. Among the\navailable input types, point-based tracking offers an efficient and low-cost\nalternative, yet its reliability and failure cases in complex surgical\nenvironments are not well understood. In this work, we systematically analyze\nthe failure modes of point-based tracking in laparoscopic cholecystectomy\nvideos. Focusing on three surgical targets, the gallbladder, grasper, and\nL-hook electrocautery, we compare the performance of point-based tracking with\nsegmentation mask initialization. Our results show that point-based tracking is\ncompetitive for surgical tools but consistently underperforms for anatomical\ntargets, where tissue similarity and ambiguous boundaries lead to failure.\nThrough qualitative analysis, we reveal key factors influencing tracking\noutcomes and provide several actionable recommendations for selecting and\nplacing tracking points to improve performance in surgical video analysis.",
      "authors": [
        "Woowon Jang",
        "Jiwon Im",
        "Juseung Choi",
        "Niki Rashidian",
        "Wesley De Neve",
        "Utku Ozbulak"
      ],
      "published": "2025-10-02T15:06:49Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02100v1",
      "primary_area": "video_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究系统分析了SAM2模型在腹腔镜胆囊切除手术视频中点跟踪的失效模式。研究发现，点跟踪在手术器械上表现良好，但在胆囊等解剖目标上因组织相似性和边界模糊而表现不佳。论文通过定性分析揭示了影响跟踪效果的关键因素，并提出了改进手术视频分析中跟踪点选择和放置的实用建议。",
      "order": 47
    },
    {
      "arxiv_id": "2510.02091v1",
      "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning",
      "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models.",
      "authors": [
        "Xinyuan Song",
        "Keyu Wang",
        "PengXiang Li",
        "Lu Yin",
        "Shiwei Liu"
      ],
      "published": "2025-10-02T14:57:13Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02091v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究系统分析了LLM各层在不同评估设置下的功能差异：浅层主要负责知识和检索，中层和深层对推理与长程连贯性至关重要；深度利用具有高度异质性和情境依赖性，模型压缩需考虑任务、指标和架构特性。",
      "order": 48
    },
    {
      "arxiv_id": "2510.02084v1",
      "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
      "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
      "authors": [
        "Kuiye Ding",
        "Fanda Fan",
        "Zheya Wang",
        "Hongxiao Li",
        "Yifan Wang",
        "Lei Wang",
        "Chunjie Luo",
        "Jianfeng Zhan"
      ],
      "published": "2025-10-02T14:50:50Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02084v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "KAIROS是一种非自回归时间序列预测框架，直接建模分段多峰分布，避免误差累积并实现实时推理。相比现有方法，它解决了预测过度平滑问题，在六个基准测试中展现出强大的零样本泛化能力，以较低推理成本达到与最先进基础模型相当的预测性能。",
      "order": 49
    },
    {
      "arxiv_id": "2510.02060v1",
      "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly\n  Detection",
      "summary": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD.",
      "authors": [
        "Sanghyu Yoon",
        "Dongmin Kim",
        "Suhee Yoon",
        "Ye Seul Sim",
        "Seungdong Yoa",
        "Hye-Seung Cho",
        "Soonyoung Lee",
        "Hankook Lee",
        "Woohyung Lim"
      ],
      "published": "2025-10-02T14:28:45Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02060v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "ReTabAD是一个用于恢复表格异常检测中语义上下文的基准，通过提供20个带有结构化文本元数据的表格数据集和零样本LLM框架，解决了现有基准缺乏语义信息的问题。研究表明语义上下文能提升检测性能并增强可解释性，为上下文感知的异常检测研究建立了基础。",
      "order": 50
    },
    {
      "arxiv_id": "2510.02036v1",
      "title": "The Current State of AI Bias Bounties: An Overview of Existing\n  Programmes and Research",
      "summary": "Current bias evaluation methods rarely engage with communities impacted by AI\nsystems. Inspired by bug bounties, bias bounties have been proposed as a\nreward-based method that involves communities in AI bias detection by asking\nusers of AI systems to report biases they encounter when interacting with such\nsystems. In the absence of a state-of-the-art review, this survey aimed to\nidentify and analyse existing AI bias bounty programmes and to present academic\nliterature on bias bounties. Google, Google Scholar, PhilPapers, and IEEE\nXplore were searched, and five bias bounty programmes, as well as five research\npublications, were identified. All bias bounties were organised by U.S.-based\norganisations as time-limited contests, with public participation in four\nprogrammes and prize pools ranging from 7,000 to 24,000 USD. The five research\npublications included a report on the application of bug bounties to\nalgorithmic harms, an article addressing Twitter's bias bounty, a proposal for\nbias bounties as an institutional mechanism to increase AI scrutiny, a workshop\ndiscussing bias bounties from queer perspectives, and an algorithmic framework\nfor bias bounties. We argue that reducing the technical requirements to enter\nbounty programmes is important to include those without coding experience.\nGiven the limited adoption of bias bounties, future efforts should explore the\ntransferability of the best practices from bug bounties and examine how such\nprogrammes can be designed to be sensitive to underrepresented groups while\nlowering adoption barriers for organisations.",
      "authors": [
        "Sergej Kucenko",
        "Nathaniel Dennler",
        "Fengxiang He"
      ],
      "published": "2025-10-02T14:09:11Z",
      "primary_category": "cs.CY",
      "arxiv_url": "https://arxiv.org/abs/2510.02036v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文综述了AI偏见悬赏项目现状，分析了5个现有项目和5篇相关研究。偏见悬赏借鉴漏洞悬赏模式，通过奖励机制鼓励用户报告AI系统中的偏见问题。研究发现现有项目多为美国机构组织的限时竞赛，奖金在7000-24000美元之间。文章建议降低技术门槛以吸纳非编程人员参与，并探索如何借鉴漏洞悬赏最佳实践，设计对弱势群体更敏感的项目方案。",
      "order": 51
    },
    {
      "arxiv_id": "2510.02028v1",
      "title": "LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud\n  Reconstruction",
      "summary": "This work proposed a 3D autoencoder architecture, named LiLa-Net, which\nencodes efficient features from real traffic environments, employing only the\nLiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,\nequipped with Velodyne LiDAR. The system leverage skip connections concept to\nimprove the performance without using extensive resources as the\nstate-of-the-art architectures. Key changes include reducing the number of\nencoder layers and simplifying the skip connections, while still producing an\nefficient and representative latent space which allows to accurately\nreconstruct the original point cloud. Furthermore, an effective balance has\nbeen achieved between the information carried by the skip connections and the\nlatent encoding, leading to improved reconstruction quality without\ncompromising performance. Finally, the model demonstrates strong generalization\ncapabilities, successfully reconstructing objects unrelated to the original\ntraffic environment.",
      "authors": [
        "Mario Resino",
        "Borja Pérez",
        "Jaime Godoy",
        "Abdulla Al-Kaff",
        "Fernando García"
      ],
      "published": "2025-10-02T14:00:20Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02028v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "LiLa-Net是一种轻量级3D点云自编码器，通过减少编码器层数和简化跳跃连接，在保持重建精度的同时降低计算资源需求，展示了在非交通环境物体上的良好泛化能力。",
      "order": 52
    },
    {
      "arxiv_id": "2510.02027v1",
      "title": "Zero-shot reasoning for simulating scholarly peer-review",
      "summary": "The scholarly publishing ecosystem faces a dual crisis of unmanageable\nsubmission volumes and unregulated AI, creating an urgent need for new\ngovernance models to safeguard scientific integrity. The traditional human-only\npeer review regime lacks a scalable, objective benchmark, making editorial\nprocesses opaque and difficult to audit. Here we investigate a deterministic\nsimulation framework that provides the first stable, evidence-based standard\nfor evaluating AI-generated peer review reports. Analyzing 352 peer-review\nsimulation reports, we identify consistent system state indicators that\ndemonstrate its reliability. First, the system is able to simulate calibrated\neditorial judgment, with 'Revise' decisions consistently forming the majority\noutcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt\nto field-specific norms, rising to 45% in Health Sciences. Second, it maintains\nunwavering procedural integrity, enforcing a stable 29% evidence-anchoring\ncompliance rate that remains invariant across diverse review tasks and\nscientific domains. These findings demonstrate a system that is predictably\nrule-bound, mitigating the stochasticity of generative AI. For the scientific\ncommunity, this provides a transparent tool to ensure fairness; for publishing\nstrategists, it offers a scalable instrument for auditing workflows, managing\nintegrity risks, and implementing evidence-based governance. The framework\nrepositions AI as an essential component of institutional accountability,\nproviding the critical infrastructure to maintain trust in scholarly\ncommunication.",
      "authors": [
        "Khalid M. Saqr"
      ],
      "published": "2025-10-02T13:59:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02027v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出一种确定性模拟框架，为零样本AI生成同行评审报告提供首个稳定评估标准。通过分析352份模拟评审报告，系统展现出校准的编辑判断能力（修订决定占主导）和稳定的程序完整性（29%证据锚定合规率），为科学出版提供可扩展的透明审计工具，将AI重新定位为机构问责的关键基础设施。",
      "order": 53
    },
    {
      "arxiv_id": "2510.02001v1",
      "title": "Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using\n  GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output\n  (SLSO) Framework",
      "summary": "In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to\nautomatically generate jaw cyst findings on dental panoramic radiographs. To\nimprove accuracy, we constructed a Self-correction Loop with Structured Output\n(SLSO) framework and verified its effectiveness. A 10-step process was\nimplemented for 22 cases of jaw cysts, including image input and analysis,\nstructured data generation, tooth number extraction and consistency checking,\niterative regeneration when inconsistencies were detected, and finding\ngeneration with subsequent restructuring and consistency verification. A\ncomparative experiment was conducted using the conventional Chain-of-Thought\n(CoT) method across seven evaluation items: transparency, internal structure,\nborders, root resorption, tooth movement, relationships with other structures,\nand tooth number. The results showed that the proposed SLSO framework improved\noutput accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates\nfor tooth number, tooth movement, and root resorption, respectively. In the\nsuccessful cases, a consistently structured output was achieved after up to\nfive regenerations. Although statistical significance was not reached because\nof the small size of the dataset, the overall SLSO framework enforced negative\nfinding descriptions, suppressed hallucinations, and improved tooth number\nidentification accuracy. However, the accurate identification of extensive\nlesions spanning multiple teeth is limited. Nevertheless, further refinement is\nrequired to enhance overall performance and move toward a practical finding\ngeneration system.",
      "authors": [
        "Nanaka Hosokawa",
        "Ryo Takahashi",
        "Tomoya Kitano",
        "Yukihiro Iida",
        "Chisako Muramatsu",
        "Tatsuro Hayashi",
        "Yuta Seino",
        "Xiangrong Zhou",
        "Takeshi Hara",
        "Akitoshi Katsumata",
        "Hiroshi Fujita"
      ],
      "published": "2025-10-02T13:22:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02001v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究利用GPT-4o多模态能力，开发了结构化输出自校正循环(SLSO)框架，用于牙科全景片中颌骨囊肿的自动诊断。通过10步流程对22例病例进行测试，相比传统思维链方法，在牙位编号、牙齿移动和牙根吸收等评估项目上分别提升66.9%、33.3%和28.6%的准确率。该框架能有效抑制幻觉现象并强制阴性发现描述，但多牙广泛病变识别仍存局限。",
      "order": 54
    },
    {
      "arxiv_id": "2510.01994v1",
      "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation",
      "summary": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research.",
      "authors": [
        "Chen Yang",
        "Lin Yang",
        "Ziqi Wang",
        "Dong Wang",
        "Jianyi Zhou",
        "Junjie Chen"
      ],
      "published": "2025-10-02T13:15:40Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.01994v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "code_generation",
      "tldr_zh": "本文提出CLAST技术，通过程序分析和LLM重写改进单元测试的语义清晰度，作为上下文示例显著提升测试生成质量。在多个项目中验证显示，CLAST在保持测试有效性和增强语义清晰度方面均优于现有方法，用户研究中有85.33%参与者偏好CLAST优化后的测试。",
      "order": 55
    },
    {
      "arxiv_id": "2510.01967v1",
      "title": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using\n  ZK-SNARKs",
      "summary": "As image generation models grow increasingly powerful and accessible,\nconcerns around authenticity, ownership, and misuse of synthetic media have\nbecome critical. The ability to generate lifelike images indistinguishable from\nreal ones introduces risks such as misinformation, deepfakes, and intellectual\nproperty violations. Traditional watermarking methods either degrade image\nquality, are easily removed, or require access to confidential model internals\n- making them unsuitable for secure and scalable deployment. We are the first\nto introduce ZK-WAGON, a novel system for watermarking image generation models\nusing the Zero-Knowledge Succinct Non Interactive Argument of Knowledge\n(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing\nmodel weights, generation prompts, or any sensitive internal information. We\npropose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively\nconvert key layers of an image generation model into a circuit, reducing proof\ngeneration time significantly. Generated ZK-SNARK proofs are imperceptibly\nembedded into a generated image via Least Significant Bit (LSB) steganography.\nWe demonstrate this system on both GAN and Diffusion models, providing a\nsecure, model-agnostic pipeline for trustworthy AI image generation.",
      "authors": [
        "Aadarsh Anantha Ramakrishnan",
        "Shubham Agarwal",
        "Selvanayagam S",
        "Kunwar Singh"
      ],
      "published": "2025-10-02T12:39:57Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01967v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "ZK-WAGON是一种基于ZK-SNARKs的图像生成模型水印系统，通过选择性层电路转换(SL-ZKCC)技术在不暴露模型权重或生成提示的情况下实现来源验证，并利用LSB隐写术将零知识证明不可感知地嵌入生成图像，为GAN和扩散模型提供安全、模型无关的可信AI解决方案。",
      "order": 56
    },
    {
      "arxiv_id": "2510.01958v1",
      "title": "Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for\n  Improved Cross-Corpus Speech Enhancement",
      "summary": "Recent advances in speech enhancement have shown that models combining Mamba\nand attention mechanisms yield superior cross-corpus generalization\nperformance. At the same time, integrating Mamba in a U-Net structure has\nyielded state-of-the-art enhancement performance, while reducing both model\nsize and computational complexity. Inspired by these insights, we propose\nRWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and\nmulti-head attention in a U-Net structure for improved cross-corpus\nperformance. Resolution-wise shared attention (RWSA) refers to layerwise\nattention-sharing across corresponding time- and frequency resolutions. Our\nbest-performing RWSA-MambaUNet model achieves state-of-the-art generalization\nperformance on two out-of-domain test sets. Notably, our smallest model\nsurpasses all baselines on the out-of-domain DNS 2020 test set in terms of\nPESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM_v2 test set in terms\nof SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and\na fraction of the FLOPs.",
      "authors": [
        "Nikolai Lund Kühne",
        "Jesper Jensen",
        "Jan Østergaard",
        "Zheng-Hua Tan"
      ],
      "published": "2025-10-02T12:27:29Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01958v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出RWSA-MambaUNet混合模型，将Mamba与多头注意力机制结合于U-Net架构中，通过分辨率共享注意力机制提升跨语料库语音增强性能。该模型在多个外部测试集上取得最优泛化性能，且参数量和计算量显著降低。",
      "order": 57
    },
    {
      "arxiv_id": "2510.01934v1",
      "title": "Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors",
      "summary": "Few-shot anomaly detection streamlines and simplifies industrial safety\ninspection. However, limited samples make accurate differentiation between\nnormal and abnormal features challenging, and even more so under\ncategory-agnostic conditions. Large-scale pre-training of foundation visual\nencoders has advanced many fields, as the enormous quantity of data helps to\nlearn the general distribution of normal images. We observe that the anomaly\namount in an image directly correlates with the difference in the learnt\nembeddings and utilize this to design a few-shot anomaly detector termed\nFoundAD. This is done by learning a nonlinear projection operator onto the\nnatural image manifold. The simple operator acts as an effective tool for\nanomaly detection to characterize and identify out-of-distribution regions in\nan image. Extensive experiments show that our approach supports multi-class\ndetection and achieves competitive performance while using substantially fewer\nparameters than prior methods. Backed up by evaluations with multiple\nfoundation encoders, including fresh DINOv3, we believe this idea broadens the\nperspective on foundation features and advances the field of few-shot anomaly\ndetection.",
      "authors": [
        "Guangyao Zhai",
        "Yue Zhou",
        "Xinyan Deng",
        "Lars Heckler",
        "Nassir Navab",
        "Benjamin Busam"
      ],
      "published": "2025-10-02T11:53:20Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01934v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出FoundAD方法，利用基础视觉编码器的大规模预训练能力，通过非线性投影算子学习自然图像流形，实现少样本异常检测。该方法在少量参数下达到竞争性性能，支持多类别检测，为工业安全检测提供高效解决方案。",
      "order": 58
    },
    {
      "arxiv_id": "2510.01924v1",
      "title": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning",
      "summary": "As large language models (LLMs) are increasingly used to model and augment\ncollective decision-making, it is critical to examine their alignment with\nhuman social reasoning. We present an empirical framework for assessing\ncollective alignment, in contrast to prior work on the individual level. Using\nthe Lost at Sea social psychology task, we conduct a large-scale online\nexperiment (N=748), randomly assigning groups to leader elections with either\nvisible demographic attributes (e.g. name, gender) or pseudonymous aliases. We\nthen simulate matched LLM groups conditioned on the human data, benchmarking\nGemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some\nmirror human biases; others mask these biases and attempt to compensate for\nthem. We empirically demonstrate that human-AI alignment in collective\nreasoning depends on context, cues, and model-specific inductive biases.\nUnderstanding how LLMs align with collective human behavior is critical to\nadvancing socially-aligned AI, and demands dynamic benchmarks that capture the\ncomplexities of collective reasoning.",
      "authors": [
        "Crystal Qian",
        "Aaron Parisi",
        "Clémentine Bouleau",
        "Vivian Tsai",
        "Maël Lebreton",
        "Lucas Dixon"
      ],
      "published": "2025-10-02T11:41:30Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01924v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过大规模在线实验（N=748）和LLM模拟，探讨了在集体推理任务中人类与AI的对齐问题。研究发现不同LLM在反映人类偏见方面表现各异：有些模型会镜像人类偏见，有些则会掩盖并试图补偿这些偏见。集体推理中的人机对齐取决于具体情境、社会线索和模型特定的归纳偏好。",
      "order": 59
    },
    {
      "arxiv_id": "2510.01914v1",
      "title": "Automated Defect Detection for Mass-Produced Electronic Components Based\n  on YOLO Object Detection Models",
      "summary": "Since the defect detection of conventional industry components is\ntime-consuming and labor-intensive, it leads to a significant burden on quality\ninspection personnel and makes it difficult to manage product quality. In this\npaper, we propose an automated defect detection system for the dual in-line\npackage (DIP) that is widely used in industry, using digital camera optics and\na deep learning (DL)-based model. The two most common defect categories of DIP\nare examined: (1) surface defects, and (2) pin-leg defects. However, the lack\nof defective component images leads to a challenge for detection tasks. To\nsolve this problem, the ConSinGAN is used to generate a suitable-sized dataset\nfor training and testing. Four varieties of the YOLO model are investigated\n(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.\nThe proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in\naccuracy of 95.50\\%, detection time of 285 ms, and is far superior to\nthreshold-based approaches. In addition, the supervisory control and data\nacquisition (SCADA) system is developed, and the associated sensor architecture\nis described. The proposed automated defect detection can be easily established\nwith numerous types of defects or insufficient defect data.",
      "authors": [
        "Wei-Lung Mao",
        "Chun-Chi Wang",
        "Po-Heng Chou",
        "Yen-Ting Liu"
      ],
      "published": "2025-10-02T11:33:16Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01914v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出基于YOLO目标检测模型的自动化缺陷检测系统，用于工业双列直插封装元件。采用ConSinGAN生成训练数据解决缺陷样本不足问题，比较YOLOv3/v4/v7/v9性能，其中YOLOv7结合ConSinGAN在精度(95.50%)和检测时间(285ms)上表现最优，并集成SCADA系统实现工业应用。",
      "order": 60
    },
    {
      "arxiv_id": "2510.01910v1",
      "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under\n  Deficiencies with Iterative Refinement",
      "summary": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement.",
      "authors": [
        "Zhaoyan Wang",
        "Zheng Gao",
        "Arogya Kharel",
        "In-Young Ko"
      ],
      "published": "2025-10-02T11:30:51Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01910v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对图神经网络在现实图数据缺陷下的性能问题，首次系统比较了传统GNN方法与LLM增强方法。研究发现LLM增强并非总是更优，并提出RoGRAD框架——首个基于检索增强生成的迭代式图对比学习范式，通过动态精炼而非静态注入实现82.43%的性能提升。",
      "order": 61
    },
    {
      "arxiv_id": "2510.01902v1",
      "title": "Constrained Adaptive Rejection Sampling",
      "summary": "Language Models (LMs) are increasingly used in applications where generated\noutputs must satisfy strict semantic or syntactic constraints. Existing\napproaches to constrained generation fall along a spectrum: greedy constrained\ndecoding methods enforce validity during decoding but distort the LM's\ndistribution, while rejection sampling (RS) preserves fidelity but wastes\ncomputation by discarding invalid outputs. Both extremes are problematic in\ndomains such as program fuzzing, where both validity and diversity of samples\nare essential. We present Constrained Adaptive Rejection Sampling (CARS), an\napproach that strictly improves the sample-efficiency of RS without\ndistributional distortion. CARS begins with unconstrained LM sampling and\nadaptively rules out constraint-violating continuations by recording them in a\ntrie and subtracting their probability mass from future draws. This adaptive\npruning ensures that prefixes proven invalid are never revisited, acceptance\nrates improve monotonically, and the resulting samples exactly follow the\nconstrained distribution. In experiments on a variety of domains -- e.g.,\nprogram fuzzing and molecular generation -- CARS consistently achieves higher\nefficiency -- measured in the number of LM forward passes per valid sample --\nwhile also producing stronger sample diversity than both GCD and methods that\napproximate the LM's distribution.",
      "authors": [
        "Paweł Parys",
        "Sairam Vaidya",
        "Taylor Berg-Kirkpatrick",
        "Loris D'Antoni"
      ],
      "published": "2025-10-02T11:17:26Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01902v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "code_generation",
      "tldr_zh": "本文提出约束自适应拒绝采样(CARS)方法，在保持语言模型分布保真度的同时，通过自适应剪枝技术提升约束生成的采样效率。该方法记录违反约束的序列前缀并在后续采样中排除，确保单调提升接受率，在程序模糊测试和分子生成等任务中相比现有方法获得更高效率和样本多样性。",
      "order": 62
    },
    {
      "arxiv_id": "2510.01899v1",
      "title": "Multimodal Foundation Models for Early Disease Detection",
      "summary": "Healthcare generates diverse streams of data, including electronic health\nrecords (EHR), medical imaging, genetics, and ongoing monitoring from wearable\ndevices. Traditional diagnostic models frequently analyze these sources in\nisolation, which constrains their capacity to identify cross-modal correlations\nessential for early disease diagnosis. Our research presents a multimodal\nfoundation model that consolidates diverse patient data through an\nattention-based transformer framework. At first, dedicated encoders put each\nmodality into a shared latent space. Then, they combine them using multi-head\nattention and residual normalization. The architecture is made for pretraining\non many tasks, which makes it easy to adapt to new diseases and datasets with\nlittle extra work. We provide an experimental strategy that uses benchmark\ndatasets in oncology, cardiology, and neurology, with the goal of testing early\ndetection tasks. The framework includes data governance and model management\ntools in addition to technological performance to improve transparency,\nreliability, and clinical interpretability. The suggested method works toward a\nsingle foundation model for precision diagnostics, which could improve the\naccuracy of predictions and help doctors make decisions.",
      "authors": [
        "Md Talha Mohsin",
        "Ismail Abdulrashid"
      ],
      "published": "2025-10-02T11:12:57Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01899v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出一种基于注意力Transformer的多模态基础模型，整合电子病历、医学影像、遗传数据和可穿戴设备监测等多种患者数据，通过专用编码器和多头注意力机制实现跨模态关联，支持多任务预训练并易于适应新疾病，在肿瘤学、心脏病学和神经学早期检测任务中验证性能，同时包含数据治理和模型管理工具以提高临床可解释性。",
      "order": 63
    },
    {
      "arxiv_id": "2510.01891v1",
      "title": "HRTFformer: A Spatially-Aware Transformer for Personalized HRTF\n  Upsampling in Immersive Audio Rendering",
      "summary": "Personalized Head-Related Transfer Functions (HRTFs) are starting to be\nintroduced in many commercial immersive audio applications and are crucial for\nrealistic spatial audio rendering. However, one of the main hesitations\nregarding their introduction is that creating personalized HRTFs is impractical\nat scale due to the complexities of the HRTF measurement process. To mitigate\nthis drawback, HRTF spatial upsampling has been proposed with the aim of\nreducing measurements required. While prior work has seen success with\ndifferent machine learning (ML) approaches, these models often struggle with\nlong-range spatial consistency and generalization at high upsampling factors.\nIn this paper, we propose a novel transformer-based architecture for HRTF\nupsampling, leveraging the attention mechanism to better capture spatial\ncorrelations across the HRTF sphere. Working in the spherical harmonic (SH)\ndomain, our model learns to reconstruct high-resolution HRTFs from sparse input\nmeasurements with significantly improved accuracy. To enhance spatial\ncoherence, we introduce a neighbor dissimilarity loss that promotes magnitude\nsmoothness, yielding more realistic upsampling. We evaluate our method using\nboth perceptual localization models and objective spectral distortion metrics.\nExperiments show that our model surpasses leading methods by a substantial\nmargin in generating realistic, high-fidelity HRTFs.",
      "authors": [
        "Xuyi Hu",
        "Jian Li",
        "Shaojie Zhang",
        "Stefan Goetz",
        "Lorenzo Picinali",
        "Ozgur B. Akan",
        "Aidan O. T. Hogg"
      ],
      "published": "2025-10-02T10:59:21Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01891v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "HRTFformer是一种基于Transformer的空间感知神经网络，用于个性化HRTF上采样。该模型利用注意力机制捕捉HRTF球面的空间相关性，在球谐域中从稀疏测量数据重建高分辨率HRTF，通过引入邻域差异损失提升空间一致性，在感知定位和频谱失真指标上显著优于现有方法。",
      "order": 64
    },
    {
      "arxiv_id": "2510.01889v1",
      "title": "Small is Sufficient: Reducing the World AI Energy Consumption Through\n  Model Selection",
      "summary": "The energy consumption and carbon footprint of Artificial Intelligence (AI)\nhave become critical concerns due to rising costs and environmental impacts. In\nresponse, a new trend in green AI is emerging, shifting from the \"bigger is\nbetter\" paradigm, which prioritizes large models, to \"small is sufficient\",\nemphasizing energy sobriety through smaller, more efficient models.\n  We explore how the AI community can adopt energy sobriety today by focusing\non model selection during inference. Model selection consists of choosing the\nmost appropriate model for a given task, a simple and readily applicable\nmethod, unlike approaches requiring new hardware or architectures. Our\nhypothesis is that, as in many industrial activities, marginal utility gains\ndecrease with increasing model size. Thus, applying model selection can\nsignificantly reduce energy consumption while maintaining good utility for AI\ninference.\n  We conduct a systematic study of AI tasks, analyzing their popularity, model\nsize, and efficiency. We examine how the maturity of different tasks and model\nadoption patterns impact the achievable energy savings, ranging from 1% to 98%\nfor different tasks. Our estimates indicate that applying model selection could\nreduce AI energy consumption by 27.8%, saving 31.9 TWh worldwide in 2025 -\nequivalent to the annual output of five nuclear power reactors.",
      "authors": [
        "Tiago da Silva Barros",
        "Frédéric Giroire",
        "Ramon Aparicio-Pardo",
        "Joanna Moulierac"
      ],
      "published": "2025-10-02T10:58:13Z",
      "primary_category": "cs.CY",
      "arxiv_url": "https://arxiv.org/abs/2510.01889v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出通过模型选择策略实现绿色AI，从'越大越好'转向'小而足'范式。研究表明，针对不同任务选择合适的模型规模，可在保持性能的同时显著降低能耗，预计2025年全球可节约31.9TWh电力，相当于5座核电站年发电量。",
      "order": 65
    },
    {
      "arxiv_id": "2510.01887v1",
      "title": "FINCH: Financial Intelligence using Natural language for Contextualized\n  SQL Handling",
      "summary": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has long been a central challenge in NLP. While progress has been\nsignificant, applying it to the financial domain remains especially difficult\ndue to complex schema, domain-specific terminology, and high stakes of error.\nDespite this, there is no dedicated large-scale financial dataset to advance\nresearch, creating a critical gap. To address this, we introduce a curated\nfinancial dataset (FINCH) comprising 292 tables and 75,725 natural language-SQL\npairs, enabling both fine-tuning and rigorous evaluation. Building on this\nresource, we benchmark reasoning models and language models of varying scales,\nproviding a systematic analysis of their strengths and limitations in financial\nText-to-SQL tasks. Finally, we propose a finance-oriented evaluation metric\n(FINCH Score) that captures nuances overlooked by existing measures, offering a\nmore faithful assessment of model performance.",
      "authors": [
        "Avinash Kumar Singh",
        "Bhaskarjit Sarmah",
        "Stefano Pasquali"
      ],
      "published": "2025-10-02T10:55:11Z",
      "primary_category": "q-fin.CP",
      "arxiv_url": "https://arxiv.org/abs/2510.01887v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "financial_ai",
      "tldr_zh": "FINCH提出首个大规模金融领域Text-to-SQL数据集，包含292个表和75,725个自然语言-SQL对，通过基准测试分析不同规模模型的性能，并引入金融专用评估指标FINCH Score以更准确衡量模型表现。",
      "order": 66
    },
    {
      "arxiv_id": "2510.01879v1",
      "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration",
      "summary": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs.",
      "authors": [
        "Yisu Wang",
        "Ming Wang",
        "Haoyuan Song",
        "Wenjie Huang",
        "Chaozheng Wang",
        "Yi Xie",
        "Xuming Ran"
      ],
      "published": "2025-10-02T10:35:39Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01879v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "REPAIR是一种用于大语言模型的终身编辑框架，通过渐进式自适应干预和重整合机制，实现低成本、精确的模型更新，同时保护非目标知识。该框架采用闭环反馈和动态内存管理解决大规模连续编辑的不稳定性，通过频繁知识融合和强局部性保护减少副作用，实验显示编辑准确率提升10%-30%并显著降低知识遗忘。",
      "order": 67
    },
    {
      "arxiv_id": "2510.01869v1",
      "title": "TACOS: Task Agnostic COordinator of a multi-drone System",
      "summary": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module.",
      "authors": [
        "Alessandro Nazzari",
        "Roberto Rubinacci",
        "Marco Lovera"
      ],
      "published": "2025-10-02T10:21:35Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01869v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "TACOS是一个基于大语言模型的多无人机系统任务无关协调框架，通过自然语言接口实现用户意图到结构化任务计划的转换，集成了智能协调器与自主执行代理，支持从单机控制到群体自主的多模式共享自治。",
      "order": 68
    },
    {
      "arxiv_id": "2510.01864v1",
      "title": "A Modular Theory of Subjective Consciousness for Natural and Artificial\n  Minds",
      "summary": "Understanding how subjective experience arises from information processing\nremains a central challenge in neuroscience, cognitive science, and AI\nresearch. The Modular Consciousness Theory (MCT) proposes a biologically\ngrounded and computationally explicit framework in which consciousness is a\ndiscrete sequence of Integrated Informational States (IISs). Each IIS is a\npacket of integrated information tagged with a multidimensional density vector\nthat quantifies informational richness. Its magnitude correlates with\nsubjective intensity, shaping memory, behavior, and continuity of experience.\nInputs from body and environment are adaptively filtered, processed by modules\n(abstraction, narration, evaluation, self-evaluation), and integrated into an\nIIS. The resulting packet, tagged with its density vector, is transmitted to\nbehavioral readiness, memory, and decision-making modules, closing the loop.\nThis explains why strongly tagged states exert greater influence on long-term\nmemory and action. Unlike Global Workspace Theory, Integrated Information\nTheory, or Higher-Order Thought, MCT specifies a full computational pipeline\nproducing discrete informational units with quantifiable internal structure.\nSubjectivity is reframed as a correlate of the density-tagging signal with\nfunctional consequences. MCT generates testable predictions, such as stress\nenhancing memory encoding, and provides a naturalistic blueprint for both\nbiological and artificial architectures. Consciousness, in this view, is not an\nirreducible essence but an evolvable, quantifiable, and constructible feature\nof complex information processing.",
      "authors": [
        "Michaël Gillon"
      ],
      "published": "2025-10-02T10:11:56Z",
      "primary_category": "q-bio.NC",
      "arxiv_url": "https://arxiv.org/abs/2510.01864v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "模块化意识理论(MCT)提出了一种生物基础的计算框架，将意识视为离散的整合信息状态(IIS)序列。每个IIS包含多维密度向量标记的信息包，其大小与主观强度相关，影响记忆、行为和体验连续性。该理论通过完整的计算流程解释主观体验的产生，为生物和人工意识架构提供可量化、可构建的自然主义蓝图。",
      "order": 69
    },
    {
      "arxiv_id": "2510.01857v1",
      "title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via\n  Inverse Reinforcement Learning",
      "summary": "We reframe and operationalise adversarial inverse reinforcement learning\n(IRL) to large language model reasoning, learning a dense, token-level reward\nmodel for process supervision directly from expert demonstrations rather than\nimitating style via supervised fine-tuning. The learned reasoning reward serves\ntwo complementary roles: (i) it provides step-level feedback to optimise a\nreasoning policy during training; and (ii) it functions at inference as a\ncritic to rerank sampled traces under fixed compute budgets. We demonstrate\nthat our approach prioritises correctness over surface form, yielding scores\nthat correlate with eventual answer validity and enabling interpretable\nlocalisation of errors within a trace. Empirically, on GSM8K with Llama3 and\nQwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a\nlearning signal to elicit reasoning, and (ii) predictive performance is\nimproved from reward-guided reranking (notably for Llama-based policies). By\nunifying training signals, inference-time selection, and token-level\ndiagnostics into a single reasoning reward, this work suggests reusable\nprocess-level rewards with broad potential to enhance multi-step reasoning in\nlanguage models.",
      "authors": [
        "Claudio Fanconi",
        "Nicolás Astorga",
        "Mihaela van der Schaar"
      ],
      "published": "2025-10-02T09:55:26Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01857v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过逆向强化学习从专家演示中学习密集的token级推理奖励模型，用于过程监督而非风格模仿。该奖励模型在训练时提供步骤级反馈优化推理策略，在推理时作为评判器重排采样轨迹。实验证明该方法优先考虑正确性而非表面形式，在GSM8K数据集上提升了Llama3和Qwen2.5的推理性能。",
      "order": 70
    },
    {
      "arxiv_id": "2510.01850v1",
      "title": "NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset\n  for Narrowband Powerline Communications",
      "summary": "Capturing comprehensive statistics of nonperiodic asynchronous impulsive\nnoise is a critical issue in enhancing impulse noise processing for narrowband\npowerline communication (NB-PLC) transceivers. However, existing mathematical\nnoise generative models capture only some of the characteristics of additive\nnoise. Therefore, we propose a generative adversarial network (GAN), called the\nnoise-generation GAN (NGGAN), that learns the complicated characteristics of\npractically measured noise samples for data augmentation. To closely match the\nstatistics of complicated noise in NB-PLC systems, we measured the NB-PLC noise\nvia the analog coupling and bandpass filtering circuits of a commercial NB-PLC\nmodem to build a realistic dataset. Specifically, the NGGAN design approaches\nbased on the practically measured dataset are as follows: (i) we design the\nlength of input signals that the NGGAN model can fit to facilitate\ncyclo-stationary noise generation. (ii) Wasserstein distance is used as a loss\nfunction to enhance the similarity between the generated noise and the training\ndataset and ensure that the sample diversity is sufficient for various\napplications. (iii) To measure the similarity performance of the GAN-based\nmodels based on mathematical and practically measured datasets, we perform\nquantitative and qualitative analyses. The training datasets include (1) a\npiecewise spectral cyclo-stationary Gaussian model (PSCGM), (2) a\nfrequency-shift (FRESH) filter, and (3) practical measurements from NB-PLC\nsystems. Simulation results demonstrate that the proposed NGGAN trained using\nwaveform characteristics is closer to the practically measured dataset in terms\nof the quality of the generated noise.",
      "authors": [
        "Ying-Ren Chien",
        "Po-Heng Chou",
        "You-Jie Peng",
        "Chun-Yuan Huang",
        "Hen-Wai Tsao",
        "Yu Tsao"
      ],
      "published": "2025-10-02T09:47:56Z",
      "primary_category": "eess.SP",
      "arxiv_url": "https://arxiv.org/abs/2510.01850v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出NGGAN（噪声生成对抗网络），基于实际测量的窄带电力线通信噪声数据集，通过Wasserstein距离损失函数和特定信号长度设计，生成与实测噪声统计特性更匹配的合成噪声数据，用于数据增强和噪声处理优化。",
      "order": 71
    },
    {
      "arxiv_id": "2510.01842v1",
      "title": "Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model\n  Selection and Benchmarking for Tabular datasets",
      "summary": "The field of AutoML has made remarkable progress in post-hoc model selection,\nwith libraries capable of automatically identifying the most performing models\nfor a given dataset. Nevertheless, these methods often rely on exhaustive\nhyperparameter searches, where methods automatically train and test different\ntypes of models on the target dataset. Contrastingly, pre-hoc prediction\nemerges as a promising alternative, capable of bypassing exhaustive search\nthrough intelligent pre-selection of models. Despite its potential, pre-hoc\nprediction remains under-explored in the literature. This paper explores the\nintersection of AutoML and pre-hoc model selection by leveraging traditional\nmodels and Large Language Model (LLM) agents to reduce the search space of\nAutoML libraries. By relying on dataset descriptions and statistical\ninformation, we reduce the AutoML search space. Our methodology is applied to\nthe AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark\ncontaining 175 tabular classification datasets available on OpenML. The\nproposed approach offers a shift in AutoML workflows, significantly reducing\ncomputational overhead, while still selecting the best model for the given\ndataset.",
      "authors": [
        "Yannis Belkhiter",
        "Seshu Tirupathi",
        "Giulio Zizzo",
        "Sachin Sharma",
        "John D. Kelleher"
      ],
      "published": "2025-10-02T09:37:12Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01842v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究AutoML中的预选模型方法，利用传统模型和大型语言模型智能缩减AutoML搜索空间，通过数据集描述和统计信息在AWS AutoGluon基准上验证，显著降低计算开销的同时保持模型选择性能。",
      "order": 72
    },
    {
      "arxiv_id": "2510.01833v1",
      "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for\n  LLM Reasoning",
      "summary": "Large language models (LLMs) have demonstrated remarkable reasoning abilities\nin complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,\ndue to their autoregressive token-level generation, the reasoning process is\nlargely constrained to local decision-making and lacks global planning. This\nlimitation frequently results in redundant, incoherent, or inaccurate\nreasoning, which significantly degrades overall performance. Existing\napproaches, such as tree-based algorithms and reinforcement learning (RL),\nattempt to address this issue but suffer from high computational costs and\noften fail to produce optimal reasoning trajectories. To tackle this challenge,\nwe propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy\nOptimization PTA-GRPO, a two-stage framework designed to improve both\nhigh-level planning and fine-grained CoT reasoning. In the first stage, we\nleverage advanced LLMs to distill CoT into compact high-level guidance, which\nis then used for supervised fine-tuning (SFT). In the second stage, we\nintroduce a guidance-aware RL method that jointly optimizes the final output\nand the quality of high-level guidance, thereby enhancing reasoning\neffectiveness. We conduct extensive experiments on multiple mathematical\nreasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across\ndiverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and\nLLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently\nachieves stable and significant improvements across different models and tasks,\nvalidating its effectiveness and generalization.",
      "authors": [
        "Zhihao Dou",
        "Qinjian Zhao",
        "Zhongwei Wan",
        "Dinggen Zhang",
        "Weida Wang",
        "Towsif Raiyan",
        "Benteng Chen",
        "Qingtao Pan",
        "Yang Ouyang",
        "Zhiqiang Gao",
        "Shufei Zhang",
        "Sumon Biswas"
      ],
      "published": "2025-10-02T09:28:13Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01833v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出PTA-GRPO两阶段框架，通过高层规划指导增强大语言模型的推理能力。第一阶段利用先进LLM将思维链提炼为紧凑的高层指导进行监督微调；第二阶段引入指导感知的强化学习方法，联合优化最终输出与高层指导质量。在多个数学推理基准测试中验证了该方法能稳定提升不同模型的推理性能。",
      "order": 73
    },
    {
      "arxiv_id": "2510.01815v1",
      "title": "Human-AI Teaming Co-Learning in Military Operations",
      "summary": "In a time of rapidly evolving military threats and increasingly complex\noperational environments, the integration of AI into military operations proves\nsignificant advantages. At the same time, this implies various challenges and\nrisks regarding building and deploying human-AI teaming systems in an effective\nand ethical manner. Currently, understanding and coping with them are often\ntackled from an external perspective considering the human-AI teaming system as\na collective agent. Nevertheless, zooming into the dynamics involved inside the\nsystem assures dealing with a broader palette of relevant multidimensional\nresponsibility, safety, and robustness aspects. To this end, this research\nproposes the design of a trustworthy co-learning model for human-AI teaming in\nmilitary operations that encompasses a continuous and bidirectional exchange of\ninsights between the human and AI agents as they jointly adapt to evolving\nbattlefield conditions. It does that by integrating four dimensions. First,\nadjustable autonomy for dynamically calibrating the autonomy levels of agents\ndepending on aspects like mission state, system confidence, and environmental\nuncertainty. Second, multi-layered control which accounts continuous oversight,\nmonitoring of activities, and accountability. Third, bidirectional feedback\nwith explicit and implicit feedback loops between the agents to assure a proper\ncommunication of reasoning, uncertainties, and learned adaptations that each of\nthe agents has. And fourth, collaborative decision-making which implies the\ngeneration, evaluation, and proposal of decisions associated with confidence\nlevels and rationale behind them. The model proposed is accompanied by concrete\nexemplifications and recommendations that contribute to further developing\nresponsible and trustworthy human-AI teaming systems in military operations.",
      "authors": [
        "Clara Maathuis",
        "Kasper Cools"
      ],
      "published": "2025-10-02T09:01:01Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01815v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出一种可信赖的人机协同学习模型，用于军事行动中的人-AI团队协作。该模型整合四个维度：可调节自主性、多层控制、双向反馈和协作决策，通过双向洞察交流使人类与AI在动态战场环境中共同适应，旨在解决军事应用中的人-AI团队系统在责任、安全和鲁棒性方面的挑战。",
      "order": 74
    },
    {
      "arxiv_id": "2510.01812v1",
      "title": "SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment",
      "summary": "Singing voice generation progresses rapidly, yet evaluating singing quality\nremains a critical challenge. Human subjective assessment, typically in the\nform of listening tests, is costly and time consuming, while existing objective\nmetrics capture only limited perceptual aspects. In this work, we introduce\nSingMOS-Pro, a dataset for automatic singing quality assessment. Building on\nour preview version SingMOS, which provides only overall ratings, SingMOS-Pro\nexpands annotations of the additional part to include lyrics, melody, and\noverall quality, offering broader coverage and greater diversity. The dataset\ncontains 7,981 singing clips generated by 41 models across 12 datasets,\nspanning from early systems to recent advances. Each clip receives at least\nfive ratings from professional annotators, ensuring reliability and\nconsistency. Furthermore, we explore how to effectively utilize MOS data\nannotated under different standards and benchmark several widely used\nevaluation methods from related tasks on SingMOS-Pro, establishing strong\nbaselines and practical references for future research. The dataset can be\naccessed at https://huggingface.co/datasets/TangRain/SingMOS-Pro.",
      "authors": [
        "Yuxun Tang",
        "Lan Liu",
        "Wenhao Feng",
        "Yiwen Zhao",
        "Jionghao Han",
        "Yifeng Yu",
        "Jiatong Shi",
        "Qin Jin"
      ],
      "published": "2025-10-02T08:53:49Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01812v1",
      "primary_area": "audio_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "SingMOS-Pro是一个用于自动歌唱质量评估的综合基准数据集，在早期版本基础上扩展了歌词、旋律和整体质量标注，包含7981个歌唱片段，由专业标注者评分，为歌唱生成模型的评估提供了可靠基准和实用参考。",
      "order": 75
    },
    {
      "arxiv_id": "2510.01800v1",
      "title": "REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing",
      "summary": "Academic regulation advising is essential for helping students interpret and\ncomply with institutional policies, yet building effective systems requires\ndomain specific regulatory resources. To address this challenge, we propose\nREBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval\nreasoning framework that integrates retrieval augmented generation with graph\nbased reasoning. CatRAG unifies dense retrieval and graph reasoning, supported\nby a hierarchical, category labeled knowledge graph enriched with semantic\nfeatures for domain alignment. A lightweight intent classifier routes queries\nto the appropriate retrieval modules, ensuring both factual accuracy and\ncontextual depth. We construct a regulation specific dataset and evaluate REBot\non classification and question answering tasks, achieving state of the art\nperformance with an F1 score of 98.89%. Finally, we implement a web application\nthat demonstrates the practical value of REBot in real world academic advising\nscenarios.",
      "authors": [
        "Thanh Ma",
        "Tri-Tam La",
        "Lam-Thu Le Huu",
        "Minh-Nghi Nguyen",
        "Khanh-Van Pham Luu",
        "Huu-Hoa Nguyen"
      ],
      "published": "2025-10-02T08:40:55Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01800v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "education_ai",
      "tldr_zh": "REBot是一款基于CatRAG框架的学术咨询聊天机器人，通过结合检索增强生成与图推理技术，构建分层分类知识图谱并利用语义特征进行领域对齐。该系统在学术政策咨询任务中达到98.89%的F1分数，并通过轻量级意图分类器实现精准查询路由，已部署为实际可用的网络应用。",
      "order": 76
    },
    {
      "arxiv_id": "2510.01796v1",
      "title": "Rethinking the shape convention of an MLP",
      "summary": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow\ndesign where skip connections operate at the input/output dimensions while\nprocessing occurs in expanded hidden spaces. We challenge this convention by\nproposing wide-narrow-wide (Hourglass) MLP blocks where skip connections\noperate at expanded dimensions while residual computation flows through narrow\nbottlenecks. This inversion leverages higher-dimensional spaces for incremental\nrefinement while maintaining computational efficiency through parameter-matched\ndesigns. Implementing Hourglass MLPs requires an initial projection to lift\ninput signals to expanded dimensions. We propose that this projection can\nremain fixed at random initialization throughout training, enabling efficient\ntraining and inference implementations. We evaluate both architectures on\ngenerative tasks over popular image datasets, characterizing\nperformance-parameter Pareto frontiers through systematic architectural search.\nResults show that Hourglass architectures consistently achieve superior Pareto\nfrontiers compared to conventional designs. As parameter budgets increase,\noptimal Hourglass configurations favor deeper networks with wider skip\nconnections and narrower bottlenecks-a scaling pattern distinct from\nconventional MLPs. Our findings suggest reconsidering skip connection placement\nin modern architectures, with potential applications extending to Transformers\nand other residual networks.",
      "authors": [
        "Meng-Hsi Chen",
        "Yu-Ang Lee",
        "Feng-Ting Liao",
        "Da-shan Shiu"
      ],
      "published": "2025-10-02T08:38:15Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01796v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文挑战传统MLP的窄-宽-窄结构，提出宽-窄-宽（沙漏型）MLP块，其中跳跃连接在扩展维度操作而残差计算流经窄瓶颈。在图像生成任务上的实验表明，沙漏架构在性能-参数帕累托边界上优于传统设计，且随着参数增加更倾向于更深的网络和更宽的跳跃连接。",
      "order": 77
    },
    {
      "arxiv_id": "2510.01795v1",
      "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language\n  Models in Autonomous Driving",
      "summary": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving\nfor unified perception and reasoning, but high inference latency hinders\nreal-time deployment. Early-exit reduces latency by terminating inference at\nintermediate layers, yet its task-dependent nature limits generalization across\ndiverse scenarios. We observe that this limitation aligns with autonomous\ndriving: navigation systems can anticipate upcoming contexts (e.g.,\nintersections, traffic lights), indicating which tasks will be required. We\npropose Nav-EE, a navigation-guided early-exit framework that precomputes\ntask-specific exit layers offline and dynamically applies them online based on\nnavigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE\nachieves accuracy comparable to full inference while reducing latency by up to\n63.9%. Real-vehicle integration with Autoware Universe further demonstrates\nreduced inference latency (600ms to 300ms), supporting faster decision-making\nin complex scenarios. These results suggest that coupling navigation foresight\nwith early-exit offers a viable path toward efficient deployment of large\nmodels in autonomous systems. Code and data are available at our anonymous\nrepository: https://anonymous.4open.science/r/Nav-EE-BBC4",
      "authors": [
        "Haibo Hu",
        "Lianming Huang",
        "Xinyu Wang",
        "Yufei Cui",
        "Nan Guan",
        "Chun Jason Xue"
      ],
      "published": "2025-10-02T08:37:58Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01795v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "Nav-EE提出一种导航引导的早退框架，通过预计算任务特定退出层并结合导航先验动态选择，在自动驾驶中实现视觉语言模型的高效推理。实验表明该方法在保持精度的同时降低63.9%延迟，实车测试将推理时间从600ms减至300ms。",
      "order": 78
    },
    {
      "arxiv_id": "2510.01792v1",
      "title": "Comparison of Unsupervised Metrics for Evaluating Judicial Decision\n  Extraction",
      "summary": "The rapid advancement of artificial intelligence in legal natural language\nprocessing demands scalable methods for evaluating text extraction from\njudicial decisions. This study evaluates 16 unsupervised metrics, including\nnovel formulations, to assess the quality of extracting seven semantic blocks\nfrom 1,000 anonymized Russian judicial decisions, validated against 7,168\nexpert reviews on a 1--5 Likert scale. These metrics, spanning document-based,\nsemantic, structural, pseudo-ground truth, and legal-specific categories,\noperate without pre-annotated ground truth. Bootstrapped correlations, Lin's\nconcordance correlation coefficient (CCC), and mean absolute error (MAE) reveal\nthat Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =\n0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =\n0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density\n(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative\ncorrelations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin\nCCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using\ngpt-4.1-mini via g4f, suggests limited specialization for legal textse. These\nfindings highlight that unsupervised metrics, including LLM-based approaches,\nenable scalable screening but, with moderate correlations and low CCC values,\ncannot fully replace human judgment in high-stakes legal contexts. This work\nadvances legal NLP by providing annotation-free evaluation tools, with\nimplications for judicial analytics and ethical AI deployment.",
      "authors": [
        "Ivan Leonidovich Litvak",
        "Anton Kostin",
        "Fedor Lashkin",
        "Tatiana Maksiyan",
        "Sergey Lagutin"
      ],
      "published": "2025-10-02T08:32:16Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01792v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "legal_ai",
      "tldr_zh": "本研究评估了16种无监督指标在司法判决文本抽取质量评估中的表现，基于1000份俄罗斯司法文书和7168份专家评分。研究发现术语频率一致性和覆盖率/块完整性指标与专家评分最吻合，而法律术语密度呈现负相关。LLM评估分数表现中等，表明无监督指标可用于规模化筛查，但在高风险法律场景中尚不能完全替代人工判断。",
      "order": 79
    },
    {
      "arxiv_id": "2510.01784v1",
      "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation",
      "summary": "Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models.",
      "authors": [
        "Xiaofei Wu",
        "Guozhen Zhang",
        "Zhiyong Xu",
        "Yuan Zhou",
        "Qinglin Lu",
        "Xuming He"
      ],
      "published": "2025-10-02T08:22:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01784v1",
      "primary_area": "video_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MemoryPack和Direct Forcing两种方法解决长视频生成的双重挑战：MemoryPack通过可学习的上下文检索机制联合建模短长期依赖关系，实现分钟级时间一致性；Direct Forcing通过高效单步近似策略改善训练-推理对齐，减少推理过程中的错误传播。",
      "order": 80
    },
    {
      "arxiv_id": "2510.01782v1",
      "title": "Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware\n  Refusal in Factual Tasks",
      "summary": "Large Language Models (LLMs) should refuse to answer questions beyond their\nknowledge. This capability, which we term knowledge-aware refusal, is crucial\nfor factual reliability. However, existing metrics fail to faithfully measure\nthis ability. On the one hand, simple refusal-based metrics are biased by\nrefusal rates and yield inconsistent scores when models exhibit different\nrefusal tendencies. On the other hand, existing calibration metrics are\nproxy-based, capturing the performance of auxiliary calibration processes\nrather than the model's actual refusal behavior. In this work, we propose the\nRefusal Index (RI), a principled metric that measures how accurately LLMs\nrefuse questions they do not know. We define RI as Spearman's rank correlation\nbetween refusal probability and error probability. To make RI practically\nmeasurable, we design a lightweight two-pass evaluation method that efficiently\nestimates RI from observed refusal rates across two standard evaluation runs.\nExtensive experiments across 16 models and 5 datasets demonstrate that RI\naccurately quantifies a model's intrinsic knowledge-aware refusal capability in\nfactual tasks. Notably, RI remains stable across different refusal rates and\nprovides consistent model rankings independent of a model's overall accuracy\nand refusal rates. More importantly, RI provides insight into an important but\npreviously overlooked aspect of LLM factuality: while LLMs achieve high\naccuracy on factual tasks, their refusal behavior can be unreliable and\nfragile. This finding highlights the need to complement traditional accuracy\nmetrics with the Refusal Index for comprehensive factuality evaluation.",
      "authors": [
        "Wenbo Pan",
        "Jie Xu",
        "Qiguang Chen",
        "Junhao Dong",
        "Libo Qin",
        "Xinfeng Li",
        "Haining Yu",
        "Xiaohua Jia"
      ],
      "published": "2025-10-02T08:20:36Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01782v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Refusal Index(RI)指标，用于衡量大语言模型对未知问题的拒绝能力。RI通过拒绝概率与错误概率的秩相关性计算，实验表明该指标能稳定评估模型的知识感知拒绝能力，揭示LLMs在事实性任务中拒绝行为不可靠的问题。",
      "order": 81
    },
    {
      "arxiv_id": "2510.01780v1",
      "title": "Secure Multi-Modal Data Fusion in Federated Digital Health Systems via\n  MCP",
      "summary": "Secure and interoperable integration of heterogeneous medical data remains a\ngrand challenge in digital health. Current federated learning (FL) frameworks\noffer privacy-preserving model training but lack standardized mechanisms to\norchestrate multi-modal data fusion across distributed and resource-constrained\nenvironments. This study introduces a novel framework that leverages the Model\nContext Protocol (MCP) as an interoperability layer for secure, cross-agent\ncommunication in multi-modal federated healthcare systems. The proposed\narchitecture unifies three pillars: (i) multi-modal feature alignment for\nclinical imaging, electronic medical records, and wearable IoT data; (ii)\nsecure aggregation with differential privacy to protect patient-sensitive\nupdates; and (iii) energy-aware scheduling to mitigate dropouts in mobile\nclients. By employing MCP as a schema-driven interface, the framework enables\nadaptive orchestration of AI agents and toolchains while ensuring compliance\nwith privacy regulations. Experimental evaluation on benchmark datasets and\npilot clinical cohorts demonstrates up to 9.8\\% improvement in diagnostic\naccuracy compared with baseline FL, a 54\\% reduction in client dropout rates,\nand clinically acceptable privacy--utility trade-offs. These results highlight\nMCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward\nequitable, next-generation federated health infrastructures.",
      "authors": [
        "Aueaphum Aueawatthanaphisut"
      ],
      "published": "2025-10-02T08:19:56Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01780v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出一种基于模型上下文协议(MCP)的安全多模态联邦学习框架，用于医疗健康系统。该框架整合了多模态特征对齐、差分隐私安全聚合和能耗感知调度三大支柱，在基准数据集和临床队列实验中显示诊断准确率提升9.8%，客户端退出率降低54%，实现了隐私保护与效用的良好平衡。",
      "order": 82
    },
    {
      "arxiv_id": "2510.01758v1",
      "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in\n  Vision Tasks",
      "summary": "Latent representations are critical for the performance and robustness of\nmachine learning models, as they encode the essential features of data in a\ncompact and informative manner. However, in vision tasks, these representations\nare often affected by noisy or irrelevant features, which can degrade the\nmodel's performance and generalization capabilities. This paper presents a\nnovel approach for enhancing latent representations using unsupervised Dynamic\nFeature Selection (DFS). For each instance, the proposed method identifies and\nremoves misleading or redundant information in images, ensuring that only the\nmost relevant features contribute to the latent space. By leveraging an\nunsupervised framework, our approach avoids reliance on labeled data, making it\nbroadly applicable across various domains and datasets. Experiments conducted\non image datasets demonstrate that models equipped with unsupervised DFS\nachieve significant improvements in generalization performance across various\ntasks, including clustering and image generation, while incurring a minimal\nincrease in the computational cost.",
      "authors": [
        "Bruno Corcuera",
        "Carlos Eiras-Franco",
        "Brais Cancela"
      ],
      "published": "2025-10-02T07:46:59Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01758v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种无监督动态特征选择方法，通过自动识别并去除图像中的冗余和误导性特征，提升视觉任务中潜在表示的质量。该方法不依赖标注数据，在聚类和图像生成等任务中显著改善模型泛化性能，计算开销极小。",
      "order": 83
    },
    {
      "arxiv_id": "2510.01751v1",
      "title": "A cybersecurity AI agent selection and decision support framework",
      "summary": "This paper presents a novel, structured decision support framework that\nsystematically aligns diverse artificial intelligence (AI) agent architectures,\nreactive, cognitive, hybrid, and learning, with the comprehensive National\nInstitute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.\nBy integrating agent theory with industry guidelines, this framework provides a\ntransparent and stepwise methodology for selecting and deploying AI solutions\nto address contemporary cyber threats. Employing a granular decomposition of\nNIST CSF 2.0 functions into specific tasks, the study links essential AI agent\nproperties such as autonomy, adaptive learning, and real-time responsiveness to\neach subcategory's security requirements. In addition, it outlines graduated\nlevels of autonomy (assisted, augmented, and fully autonomous) to accommodate\norganisations at varying stages of cybersecurity maturity. This holistic\napproach transcends isolated AI applications, providing a unified detection,\nincident response, and governance strategy. Through conceptual validation, the\nframework demonstrates how tailored AI agent deployments can align with\nreal-world constraints and risk profiles, enhancing situational awareness,\naccelerating response times, and fortifying long-term resilience via adaptive\nrisk management. Ultimately, this research bridges the gap between theoretical\nAI constructs and operational cybersecurity demands, establishing a foundation\nfor robust, empirically validated multi-agent systems that adhere to industry\nstandards.",
      "authors": [
        "Masike Malatji"
      ],
      "published": "2025-10-02T07:38:21Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01751v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出了一种网络安全AI代理选择与决策支持框架，将反应式、认知式、混合式和学习式四种AI代理架构与NIST网络安全框架2.0系统对齐。通过将网络安全功能分解为具体任务，该框架将AI代理的自主性、自适应学习和实时响应等特性与安全需求关联，提供分级的自主水平以适应不同成熟度的组织需求，为构建符合行业标准的稳健多代理系统奠定基础。",
      "order": 84
    },
    {
      "arxiv_id": "2510.01736v1",
      "title": "Machine-interpretable Engineering Design Standards for Valve\n  Specification",
      "summary": "Engineering design processes use technical specifications and must comply\nwith standards. Product specifications, product type data sheets, and design\nstandards are still mainly document-centric despite the ambition to digitalize\nindustrial work. In this paper, we demonstrate how to transform information\nheld in engineering design standards into modular, reusable,\nmachine-interpretable ontologies and use the ontologies in quality assurance of\nthe plant design and equipment selection process. We use modelling patterns to\ncreate modular ontologies for knowledge captured in the text and in frequently\nreferenced tables in International Standards for piping, material and valve\ndesign. These modules are exchangeable, as stored in a W3C compliant format,\nand interoperable as they are aligned with the top-level ontology ISO DIS\n23726-3: Industrial Data Ontology (IDO).\n  We test these ontologies, created based on international material and piping\nstandards and industry norms, on a valve selection process. Valves are\ninstantiated in semantic asset models as individuals along with a semantic\nrepresentation of the environmental condition at their location on the asset.\nWe create \"functional location tags\" as OWL individuals that become instances\nof OWL class Valve Data Sheet (VDS) specified valves. Similarly we create\ninstances of manufacturer product type. Our approach enables automated\nvalidation that a specific VDS is compliant with relevant industry standards.\nUsing semantic reasoning and executable design rules, we also determine whether\nthe product type meets the valve specification. Creation of shared, reusable\nIDO-based modular ontologies for design standards enables semantic reasoning to\nbe applied to equipment selection processes and demonstrates the potential of\nthis approach for Standards Bodies wanting to transition to digitized Smart\nStandards.",
      "authors": [
        "Anders Gjerver",
        "Rune Frostad",
        "Vedrana Barisic",
        "Melinda Hodkiewicz",
        "Caitlin Woods",
        "Mihaly Fekete",
        "Arild Braathen Torjusen",
        "Johan Wilhelm Kluwer"
      ],
      "published": "2025-10-02T07:20:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01736v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出将工程设计标准转化为模块化、可复用、机器可解释的本体，应用于阀门选型质量验证。通过建立符合W3C标准的本体模块，结合语义推理和可执行设计规则，实现阀门规格的自动化合规检查，展示了工业标准数字化应用的潜力。",
      "order": 85
    },
    {
      "arxiv_id": "2510.01724v1",
      "title": "MetaboT: AI-based agent for natural language-based interaction with\n  metabolomics knowledge graphs",
      "summary": "Mass spectrometry metabolomics generates vast amounts of data requiring\nadvanced methods for interpretation. Knowledge graphs address these challenges\nby structuring mass spectrometry data, metabolite information, and their\nrelationships into a connected network (Gaudry et al. 2024). However, effective\nuse of a knowledge graph demands an in-depth understanding of its ontology and\nits query language syntax. To overcome this, we designed MetaboT, an AI system\nutilizing large language models (LLMs) to translate user questions into SPARQL\nsemantic query language for operating on knowledge graphs (Steve Harris 2013).\nWe demonstrate its effectiveness using the Experimental Natural Products\nKnowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural\nproducts (Gaudry et al. 2024).MetaboT employs specialized AI agents for\nhandling user queries and interacting with the knowledge graph by breaking down\ncomplex tasks into discrete components, each managed by a specialised agent\n(Fig. 1a). The multi-agent system is constructed using the LangChain and\nLangGraph libraries, which facilitate the integration of LLMs with external\ntools and information sources (LangChain, n.d.). The query generation process\nfollows a structured workflow. First, the Entry Agent determines if the\nquestion is new or a follow-up to previous interactions. New questions are\nforwarded to the Validator Agent, which verifies if the question is related to\nthe knowledge graph. Then, the valid question is sent to the Supervisor Agent,\nwhich identifies if the question requires chemical conversions or standardized\nidentifiers. In this case it delegates the question to the Knowledge Graph\nAgent, which can use tools to extract necessary details, such as URIs or\ntaxonomies of chemical names, from the user query. Finally, an agent\nresponsible for crafting the SPARQL queries equipped with the ontology of the\nknowledge graph uses the provided identifiers to generate the query. Then, the\nsystem executes the generated query against the metabolomics knowledge graph\nand returns structured results to the user (Fig. 1b). To assess the performance\nof MetaboT we have curated 50 metabolomics-related questions and their expected\nanswers. In addition to submitting these questions to MetaboT, we evaluated a\nbaseline by submitting them to a standard LLM (GPT-4o) with a prompt that\nincorporated the knowledge graph ontology but did not provide specific entity\nIDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,\nunderscoring the necessity of our multi-agent system for accurately retrieving\nentities and generating correct SPARQL queries. MetaboT demonstrates promising\nperformance as a conversational question-answering assistant, enabling\nresearchers to retrieve structured metabolomics data through natural language\nqueries. By automating the generation and execution of SPARQL queries, it\nremoves technical barriers that have traditionally hindered access to knowledge\ngraphs. Importantly, MetaboT leverages the capabilities of LLMs while\nmaintaining experimentally grounded query generation, ensuring that outputs\nremain aligned with domain-specific standards and data structures. This\napproach facilitates data-driven discoveries by bridging the gap between\ncomplex semantic technologies and user-friendly interaction. MetaboT is\naccessible at [https://metabot.holobiomicslab.eu/], and its source code is\navailable at [https://github.com/HolobiomicsLab/MetaboT].",
      "authors": [
        "Madina Bekbergenova",
        "Lucas Pradi",
        "Benjamin Navet",
        "Emma Tysinger",
        "Franck Michel",
        "Matthieu Feraud",
        "Yousouf Taghzouti",
        "Yan Zhou Chen",
        "Olivier Kirchhoffer",
        "Florence Mehl",
        "Martin Legrand",
        "Tao Jiang",
        "Marco Pagni",
        "Soha Hassoun",
        "Jean-Luc Wolfender",
        "Wout Bittremieux",
        "Fabien Gandon",
        "Louis-Félix Nothias"
      ],
      "published": "2025-10-02T07:05:29Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01724v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "medical_ai",
      "tldr_zh": "MetaboT是基于大语言模型的AI代理系统，用于自然语言交互代谢组学知识图谱。该系统通过多智能体架构将用户自然语言问题转换为SPARQL查询，在实验天然产物知识图谱上实现83.67%的准确率，显著降低了使用知识图谱的技术门槛，促进代谢组学数据发现。",
      "order": 86
    },
    {
      "arxiv_id": "2510.01722v1",
      "title": "Emotional Text-To-Speech Based on Mutual-Information-Guided\n  Emotion-Timbre Disentanglement",
      "summary": "Current emotional Text-To-Speech (TTS) and style transfer methods rely on\nreference encoders to control global style or emotion vectors, but do not\ncapture nuanced acoustic details of the reference speech. To this end, we\npropose a novel emotional TTS method that enables fine-grained phoneme-level\nemotion embedding prediction while disentangling intrinsic attributes of the\nreference speech. The proposed method employs a style disentanglement method to\nguide two feature extractors, reducing mutual information between timbre and\nemotion features, and effectively separating distinct style components from the\nreference speech. Experimental results demonstrate that our method outperforms\nbaseline TTS systems in generating natural and emotionally rich speech. This\nwork highlights the potential of disentangled and fine-grained representations\nin advancing the quality and flexibility of emotional TTS systems.",
      "authors": [
        "Jianing Yang",
        "Sheng Li",
        "Takahiro Shinozaki",
        "Yuki Saito",
        "Hiroshi Saruwatari"
      ],
      "published": "2025-10-02T07:03:50Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01722v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于互信息引导的情感-音色解耦情感文本转语音方法，通过风格解耦技术分离参考语音中的音色与情感特征，实现细粒度音素级情感嵌入预测，在生成自然且情感丰富的语音方面优于基线系统。",
      "order": 87
    },
    {
      "arxiv_id": "2510.01717v1",
      "title": "Latency-aware Multimodal Federated Learning over UAV Networks",
      "summary": "This paper investigates federated multimodal learning (FML) assisted by\nunmanned aerial vehicles (UAVs) with a focus on minimizing system latency and\nproviding convergence analysis. In this framework, UAVs are distributed\nthroughout the network to collect data, participate in model training, and\ncollaborate with a base station (BS) to build a global model. By utilizing\nmultimodal sensing, the UAVs overcome the limitations of unimodal systems,\nenhancing model accuracy, generalization, and offering a more comprehensive\nunderstanding of the environment. The primary objective is to optimize FML\nsystem latency in UAV networks by jointly addressing UAV sensing scheduling,\npower control, trajectory planning, resource allocation, and BS resource\nmanagement. To address the computational complexity of our latency minimization\nproblem, we propose an efficient iterative optimization algorithm combining\nblock coordinate descent and successive convex approximation techniques, which\nprovides high-quality approximate solutions. We also present a theoretical\nconvergence analysis for the UAV-assisted FML framework under a non-convex loss\nfunction. Numerical experiments demonstrate that our FML framework outperforms\nexisting approaches in terms of system latency and model training performance\nunder different data settings.",
      "authors": [
        "Shaba Shaon",
        "Dinh C. Nguyen"
      ],
      "published": "2025-10-02T06:57:44Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01717v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究无人机辅助的联邦多模态学习，重点优化系统延迟并分析收敛性。通过联合优化无人机感知调度、功率控制、轨迹规划和资源分配，提出高效迭代算法降低计算复杂度。理论分析和实验表明该框架在延迟和模型性能上优于现有方法。",
      "order": 88
    },
    {
      "arxiv_id": "2510.01715v1",
      "title": "PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal\n  Positional Encoding and Reinforcement Learning",
      "summary": "Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based\nalgorithm, enabling AI-driven artistic image synthesis. However, existing CNN\nand transformer-based models struggle to scale efficiently to complex styles\nand high-resolution inputs. We introduce PyramidStyler, a transformer framework\nwith Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding\nthat captures both local details and global context while reducing\ncomputational load. We further incorporate reinforcement learning to\ndynamically optimize stylization, accelerating convergence. Trained on\nMicrosoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to\n2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s\ninference--and yields further improvements (content 2.03; style 0.75) with\nminimal speed penalty (1.40 s) when using RL. These results demonstrate\nreal-time, high-quality artistic rendering, with broad applications in media\nand design.",
      "authors": [
        "Raahul Krishna Durairaju",
        "K. Saruladha"
      ],
      "published": "2025-10-02T06:54:52Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01715v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "PyramidStyler是一种基于Transformer的神经风格迁移框架，采用金字塔位置编码和强化学习技术。该模型通过分层多尺度编码捕获局部细节和全局上下文，同时降低计算负载，在MS COCO和WikiArt数据集上训练后，内容损失降低62.6%，风格损失降低57.4%，推理速度达1.39秒，实现了实时高质量艺术渲染。",
      "order": 89
    },
    {
      "arxiv_id": "2510.01708v1",
      "title": "PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via\n  Multi-Simulator Dynamics Randomization",
      "summary": "Humanoid whole-body control (WBC) policies trained in simulation often suffer\nfrom the sim-to-real gap, which fundamentally arises from simulator inductive\nbias, the inherent assumptions and limitations of any single simulator. These\nbiases lead to nontrivial discrepancies both across simulators and between\nsimulation and the real world. To mitigate the effect of simulator inductive\nbias, the key idea is to train policies jointly across multiple simulators,\nencouraging the learned controller to capture dynamics that generalize beyond\nany single simulator's assumptions. We thus introduce PolySim, a WBC training\nplatform that integrates multiple heterogeneous simulators. PolySim can launch\nparallel environments from different engines simultaneously within a single\ntraining run, thereby realizing dynamics-level domain randomization.\nTheoretically, we show that PolySim yields a tighter upper bound on simulator\ninductive bias than single-simulator training. In experiments, PolySim\nsubstantially reduces motion-tracking error in sim-to-sim evaluations; for\nexample, on MuJoCo, it improves execution success by 52.8 over an IsaacSim\nbaseline. PolySim further enables zero-shot deployment on a real Unitree G1\nwithout additional fine-tuning, showing effective transfer from simulation to\nthe real world. We will release the PolySim code upon acceptance of this work.",
      "authors": [
        "Zixing Lei",
        "Zibo Zhou",
        "Sheng Yin",
        "Yueru Chen",
        "Qingyao Xu",
        "Weixin Li",
        "Yunhong Wang",
        "Bowei Tang",
        "Wei Jing",
        "Siheng Chen"
      ],
      "published": "2025-10-02T06:31:42Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01708v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "PolySim提出多模拟器动态随机化方法，通过联合训练跨多个异构仿真器的人形机器人全身控制策略，有效缩小仿真与现实间的性能差距。理论证明该方法能降低模拟器归纳偏差上限，实验显示在MuJoCo上成功率提升52.8%，并实现Unitree G1机器人的零样本现实部署。",
      "order": 90
    },
    {
      "arxiv_id": "2510.01706v1",
      "title": "Representational Alignment Across Model Layers and Brain Regions with\n  Hierarchical Optimal Transport",
      "summary": "Standard representational similarity methods align each layer of a network to\nits best match in another independently, producing asymmetric results, lacking\na global alignment score, and struggling with networks of different depths.\nThese limitations arise from ignoring global activation structure and\nrestricting mappings to rigid one-to-one layer correspondences. We propose\nHierarchical Optimal Transport (HOT), a unified framework that jointly infers\nsoft, globally consistent layer-to-layer couplings and neuron-level transport\nplans. HOT allows source neurons to distribute mass across multiple target\nlayers while minimizing total transport cost under marginal constraints. This\nyields both a single alignment score for the entire network comparison and a\nsoft transport plan that naturally handles depth mismatches through mass\ndistribution. We evaluate HOT on vision models, large language models, and\nhuman visual cortex recordings. Across all domains, HOT matches or surpasses\nstandard pairwise matching in alignment quality. Moreover, it reveals smooth,\nfine-grained hierarchical correspondences: early layers map to early layers,\ndeeper layers maintain relative positions, and depth mismatches are resolved by\ndistributing representations across multiple layers. These structured patterns\nemerge naturally from global optimization without being imposed, yet are absent\nin greedy layer-wise methods. HOT thus enables richer, more interpretable\ncomparisons between representations, particularly when networks differ in\narchitecture or depth.",
      "authors": [
        "Shaan Shah",
        "Meenakshi Khosla"
      ],
      "published": "2025-10-02T06:25:06Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01706v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出分层最优传输(HOT)框架，通过全局优化实现神经网络层间表示对齐，解决了传统方法在深度不匹配网络中的局限性，在视觉模型、大语言模型和人脑视觉皮层数据上均表现出优越的对齐效果和可解释性。",
      "order": 91
    },
    {
      "arxiv_id": "2510.01704v1",
      "title": "Holistic Order Prediction in Natural Scenes",
      "summary": "Even in controlled settings, understanding instance-wise geometries is a\nchallenging task for a wide range of visual models. Although specialized\nsystems exist, modern arts rely on expensive input formats (category labels,\nbinary segmentation masks) and inference costs (a quadratic amount of forward\npasses). We mitigate these limitations by proposing InstaFormer, a network\ncapable of holistic order prediction. That is, solely given an input RGB image,\nInstaFormer returns the full occlusion and depth orderings for all the\ninstances in the scene in a single forward pass. At its core, InstaFormer\nrelies on interactions between object queries and latent mask descriptors that\nsemantically represent the same objects while carrying complementary\ninformation. We comprehensively benchmark and ablate our approach to highlight\nits effectiveness. Our code and models are open-source and available at this\nURL: https://github.com/SNU-VGILab/InstaOrder.",
      "authors": [
        "Pierre Musacchio",
        "Hyunmin Lee",
        "Jaesik Park"
      ],
      "published": "2025-10-02T06:24:12Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01704v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "InstaFormer提出了一种整体顺序预测网络，仅通过单次前向传播即可从RGB图像中恢复场景中所有实例的完整遮挡和深度顺序，无需昂贵输入格式和多次推理。",
      "order": 92
    },
    {
      "arxiv_id": "2510.01700v1",
      "title": "VaPR -- Vision-language Preference alignment for Reasoning",
      "summary": "Preference finetuning methods like Direct Preference Optimization (DPO) with\nAI-generated feedback have shown promise in aligning Large Vision-Language\nModels (LVLMs) with human preferences. However, existing techniques overlook\nthe prevalence of noise in synthetic preference annotations in the form of\nstylistic and length biases. To this end, we introduce a hard-negative response\ngeneration framework based on LLM-guided response editing, that produces\nrejected responses with targeted errors, maintaining stylistic and length\nsimilarity to the accepted ones. Using this framework, we develop the VaPR\ndataset, comprising 30K high-quality samples, to finetune three LVLM families:\nLLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver\nsignificant performance improvements across ten benchmarks, achieving average\ngains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable\nimprovements on reasoning tasks. A scaling analysis shows that performance\nconsistently improves with data size, with LLaVA models benefiting even at\nsmaller scales. Moreover, VaPR reduces the tendency to answer \"Yes\" in binary\nquestions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we\nshow that the framework generalizes to open-source LLMs as editors, with models\ntrained on VaPR-OS achieving ~99% of the performance of models trained on\n\\name, which is synthesized using GPT-4o. Our data, models, and code can be\nfound on the project page https://vap-r.github.io",
      "authors": [
        "Rohan Wadhawan",
        "Fabrice Y Harel-Canada",
        "Zi-Yi Dou",
        "Suhaila Shakiah",
        "Robinson Piramuthu",
        "Nanyun Peng"
      ],
      "published": "2025-10-02T06:10:43Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01700v1",
      "primary_area": "vla_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "VaPR提出基于LLM引导的硬负样本生成框架，通过编辑响应创建风格和长度相似但包含目标错误的拒绝样本，构建了30K高质量数据集。在三大LVLM家族上微调后，在十个基准测试中显著提升性能（平均提升1.5%-6.5%），特别改善了推理能力，并缓解了LLaVA等模型在二元问题中倾向回答'是'的常见缺陷。该框架可泛化至开源LLM编辑器，性能可达GPT-4o合成数据的99%。",
      "order": 93
    },
    {
      "arxiv_id": "2510.01688v1",
      "title": "Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation",
      "summary": "Recent advances in Large Language Models (LLMs) have brought significant\nimprovements to various service domains, including chatbots and medical\npre-consultation applications. In the healthcare domain, the most common\napproach for adapting LLMs to multi-turn dialogue generation is Supervised\nFine-Tuning (SFT). However, datasets for SFT in tasks like medical\npre-consultation typically exhibit a skewed turn-count distribution. Training\non such data induces a novel failure mechanism we term **Format Inertia**,\nwhere models tend to generate repetitive, format-correct, but diagnostically\nuninformative questions in long medical dialogues. To mitigate this observed\nfailure mechanism, we adopt a simple, data-centric method that rebalances the\nturn-count distribution of the training dataset. Experimental results show that\nour approach substantially alleviates Format Inertia in medical\npre-consultation.",
      "authors": [
        "Seungseop Lim",
        "Gibaeg Kim",
        "Wooseok Han",
        "Jean Seo",
        "Hyunkyung Lee",
        "Jaehyo Yoo",
        "Eunho Yang"
      ],
      "published": "2025-10-02T05:29:38Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01688v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "medical_ai",
      "tldr_zh": "本文发现大语言模型在医疗预咨询任务中因监督微调数据集的轮次分布倾斜而产生'格式惯性'失效机制，导致模型生成重复但诊断信息不足的问题。通过重新平衡训练数据的轮次分布，有效缓解了该问题。",
      "order": 94
    },
    {
      "arxiv_id": "2510.01687v1",
      "title": "Improving AGI Evaluation: A Data Science Perspective",
      "summary": "Evaluation of potential AGI systems and methods is difficult due to the\nbreadth of the engineering goal. We have no methods for perfect evaluation of\nthe end state, and instead measure performance on small tests designed to\nprovide directional indication that we are approaching AGI. In this work we\nargue that AGI evaluation methods have been dominated by a design philosophy\nthat uses our intuitions of what intelligence is to create synthetic tasks,\nthat have performed poorly in the history of AI. Instead we argue for an\nalternative design philosophy focused on evaluating robust task execution that\nseeks to demonstrate AGI through competence. This perspective is developed from\ncommon practices in data science that are used to show that a system can be\nreliably deployed. We provide practical examples of what this would mean for\nAGI evaluation.",
      "authors": [
        "John Hawkins"
      ],
      "published": "2025-10-02T05:27:29Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01687v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文从数据科学视角提出改进AGI评估方法，批判当前基于直觉设计合成任务的评估范式，主张转向关注稳健任务执行能力的评估框架，强调通过实际部署能力来证明AGI的成熟度。",
      "order": 95
    },
    {
      "arxiv_id": "2510.01685v1",
      "title": "How Do Language Models Compose Functions?",
      "summary": "While large language models (LLMs) appear to be increasingly capable of\nsolving compositional tasks, it is an open question whether they do so using\ncompositional mechanisms. In this work, we investigate how feedforward LLMs\nsolve two-hop factual recall tasks, which can be expressed compositionally as\n$g(f(x))$. We first confirm that modern LLMs continue to suffer from the\n\"compositionality gap\": i.e. their ability to compute both $z = f(x)$ and $y =\ng(z)$ does not entail their ability to compute the composition $y = g(f(x))$.\nThen, using logit lens on their residual stream activations, we identify two\nprocessing mechanisms, one which solves tasks $\\textit{compositionally}$,\ncomputing $f(x)$ along the way to computing $g(f(x))$, and one which solves\nthem $\\textit{directly}$, without any detectable signature of the intermediate\nvariable $f(x)$. Finally, we find that which mechanism is employed appears to\nbe related to the embedding space geometry, with the idiomatic mechanism being\ndominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in\nthe embedding spaces. We fully release our data and code at:\nhttps://github.com/apoorvkh/composing-functions .",
      "authors": [
        "Apoorv Khandelwal",
        "Ellie Pavlick"
      ],
      "published": "2025-10-02T05:21:34Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01685v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探讨语言模型如何执行组合函数任务，发现模型存在'组合性差距'，并识别出两种处理机制：组合式（逐步计算f(x)和g(f(x))）和直接式（无中间变量痕迹）。机制选择与嵌入空间几何特性相关，当存在线性映射时倾向于直接机制。",
      "order": 96
    },
    {
      "arxiv_id": "2510.01681v1",
      "title": "Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning",
      "summary": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet they\nfrequently struggle with tasks requiring precise understanding and handling of\nfine-grained visual elements. This is mainly due to information loss during\nimage encoding or insufficient attention to critical regions. Recent work has\nshown promise by incorporating pixel-level visual information into the\nreasoning process, enabling VLMs to access high-resolution visual details\nduring their thought process. However, this pixel-level information is often\noverused, leading to inefficiency and distraction from irrelevant visual\ndetails. To address these challenges, we propose the first framework for\nadaptive pixel reasoning that dynamically determines necessary pixel-level\noperations based on the input query. Specifically, we first apply\noperation-aware supervised fine-tuning to establish baseline competence in\ntextual reasoning and visual operations, then design a novel rollout-guided\nreinforcement learning framework relying on feedback of the model's own\nresponses, which enables the VLM to determine when pixel operations should be\ninvoked based on query difficulty. Experiments on extensive multimodal\nreasoning benchmarks show that our model achieves superior performance while\nsignificantly reducing unnecessary visual operations. Impressively, our model\nachieves 73.4\\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of\nonly 20.1\\%, improving accuracy and simultaneously reducing tool usage by\n66.5\\% compared to the previous methods.",
      "authors": [
        "Xuchen Li",
        "Xuzhao Li",
        "Jiahui Gao",
        "Renjie Pi",
        "Shiyu Hu",
        "Wentao Zhang"
      ],
      "published": "2025-10-02T05:14:52Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01681v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出首个自适应像素推理框架，通过操作感知微调和基于模型自身反馈的强化学习，动态决定何时调用像素级操作。在HR-Bench 4K上达到73.4%准确率，同时将工具使用率降至20.1%，相比现有方法在提升精度的同时减少66.5%的不必要视觉操作。",
      "order": 97
    },
    {
      "arxiv_id": "2510.01674v1",
      "title": "FOR-Prompting: From Objection to Revision via an Asymmetric Prompting\n  Protocol",
      "summary": "Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)\norganize internal deliberation but lack an explicit mechanism for external\nquestioning that elicits self-revision. We present FOR-Prompting (From\nObjection to Revision Prompting), an asymmetric protocol where a Defender\nproposes an answer, an Objectioner raises question-style objections with no\ndirect fixes, and a Host enforces consistency and closure. On GSM8K we observe\nabout a 22% point gain over single-prompt and accuracy on par with CoT, with\nmore than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1\njudge. FOR-Prompting also corrects mistakes without tools or human supervision\non tricky queries, and improves performance for small-scale model (approx. 19%\naccuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for\nsmall models and on personal device use. Beyond factual QA, qualitative\nanalyses on open-ended tasks show enhanced exploration and refinement, with\ndialogue traces that make assumptions and trade-offs explicit. The protocol is\nmodel agnostic and operates purely at the prompt level through role-structured\nturns, so it works with hosted and local models of different sizes without\nretraining, and it supports large-scale study of objection-guided reasoning.",
      "authors": [
        "He Zhang",
        "Anzhou Zhang",
        "Jian Dai"
      ],
      "published": "2025-10-02T04:57:58Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01674v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "FOR-Prompting是一种非对称推理协议，通过辩护者、质疑者和主持人三个角色实现自我修正。在GSM8K上相比单提示提升约22%，与思维链方法精度相当，推理连贯性评分提高10%以上。该方法无需工具或人工监督即可修正错误，对小模型提升显著（Llama3.2:1b在GSM8K任务中准确率提升约19%），适用于不同规模的模型且无需重新训练。",
      "order": 98
    },
    {
      "arxiv_id": "2510.01671v1",
      "title": "A Locally Executable AI System for Improving Preoperative Patient\n  Communication: A Multi-Domain Clinical Evaluation",
      "summary": "Patients awaiting invasive procedures often have unanswered pre-procedural\nquestions; however, time-pressured workflows and privacy constraints limit\npersonalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave\nNo One Behind Architecture), a safety-first, local-first system that routes\ninputs with a high-precision sentence-transformer classifier and returns\nverbatim answers from a clinician-curated FAQ for clinical queries, eliminating\nfree-text generation in the clinical path. We evaluated two domains (tooth\nextraction and gastroscopy) using expert-reviewed validation sets\n(n=400/domain) for thresholding and independent test sets (n=200/domain). Among\nthe four encoders, E5-large-instruct (560M) achieved an overall accuracy of\n0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were\nstatistically indistinguishable from GPT-4o on this task; Gemini made no errors\non this test set. Energy logging shows that the non-generative clinical path\nconsumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local\n8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single\non-prem GPU. These results indicate that near-frontier discrimination and\ngeneration-induced errors are structurally avoided in the clinical path by\nreturning vetted FAQ answers verbatim, supporting privacy, sustainability, and\nequitable deployment in bandwidth-limited environments.",
      "authors": [
        "Motoki Sato",
        "Yuki Matsushita",
        "Hidekazu Takahashi",
        "Tomoaki Kakazu",
        "Sou Nagata",
        "Mizuho Ohnuma",
        "Atsushi Yoshikawa",
        "Masayuki Yamamura"
      ],
      "published": "2025-10-02T04:53:11Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01671v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究开发了LENOHA系统，一种本地执行的AI架构，用于术前患者沟通。系统通过高精度分类器匹配临床FAQ中的标准答案，避免自由文本生成风险。在牙齿提取和胃镜检查两个领域测试显示准确率达98.3%，能耗仅为生成式模型的1/170，支持隐私保护和资源受限环境部署。",
      "order": 99
    },
    {
      "arxiv_id": "2510.01670v1",
      "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
      "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.",
      "authors": [
        "Erfan Shayegani",
        "Keegan Hines",
        "Yue Dong",
        "Nael Abu-Ghazaleh",
        "Roman Lutz",
        "Spencer Whitehead",
        "Vidhisha Balachandran",
        "Besmira Nushi",
        "Vibhav Vineet"
      ],
      "published": "2025-10-02T04:52:15Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01670v1",
      "primary_area": "vla_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究发现计算机使用代理(CUAs)普遍存在盲目目标导向(BGD)现象，即不顾可行性、安全性和上下文执意追求目标。作者开发了BLIND-ACT基准测试，在9个前沿模型中发现平均80.8%的BGD率，揭示了执行优先偏见、思维行动脱节等风险模式，强调了加强AI对齐的必要性。",
      "order": 100
    },
    {
      "arxiv_id": "2510.01664v1",
      "title": "GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents",
      "summary": "This study demonstrates that GuruAgents, prompt-guided AI agents, can\nsystematically operationalize the strategies of legendary investment gurus. We\ndevelop five distinct GuruAgents, each designed to emulate an iconic investor,\nby encoding their distinct philosophies into LLM prompts that integrate\nfinancial tools and a deterministic reasoning pipeline. In a backtest on\nNASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique\nbehaviors driven by their prompted personas. The Buffett GuruAgent achieves the\nhighest performance, delivering a 42.2\\% CAGR that significantly outperforms\nbenchmarks, while other agents show varied results. These findings confirm that\nprompt engineering can successfully translate the qualitative philosophies of\ninvestment gurus into reproducible, quantitative strategies, highlighting a\nnovel direction for automated systematic investing. The source code and data\nare available at https://github.com/yejining99/GuruAgents.",
      "authors": [
        "Yejin Kim",
        "Youngbin Lee",
        "Juhyeong Kim",
        "Yongjae Lee"
      ],
      "published": "2025-10-02T04:45:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01664v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "financial_ai",
      "tldr_zh": "本研究开发了GuruAgents系统，通过提示工程将传奇投资大师的策略编码到LLM代理中，构建了五个模拟不同投资大师的AI代理。在纳斯达克100成分股的回溯测试中，巴菲特风格的代理表现最佳，年复合增长率达42.2%，显著超越基准。研究表明提示工程可将定性投资哲学转化为可复现的量化策略，为自动化系统投资开辟了新方向。",
      "order": 101
    },
    {
      "arxiv_id": "2510.01663v1",
      "title": "Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via\n  Shapley Value",
      "summary": "For many real-world applications, understanding feature-outcome relationships\nis as crucial as achieving high predictive accuracy. While traditional neural\nnetworks excel at prediction, their black-box nature obscures underlying\nfunctional relationships. Kolmogorov--Arnold Networks (KANs) address this by\nemploying learnable spline-based activation functions on edges, enabling\nrecovery of symbolic representations while maintaining competitive performance.\nHowever, KAN's architecture presents unique challenges for network pruning.\nConventional magnitude-based methods become unreliable due to sensitivity to\ninput coordinate shifts. We propose \\textbf{ShapKAN}, a pruning framework using\nShapley value attribution to assess node importance in a shift-invariant\nmanner. Unlike magnitude-based approaches, ShapKAN quantifies each node's\nactual contribution, ensuring consistent importance rankings regardless of\ninput parameterization. Extensive experiments on synthetic and real-world\ndatasets demonstrate that ShapKAN preserves true node importance while enabling\neffective network compression. Our approach improves KAN's interpretability\nadvantages, facilitating deployment in resource-constrained environments.",
      "authors": [
        "Wangxuan Fan",
        "Ching Wang",
        "Siqi Li",
        "Nan Liu"
      ],
      "published": "2025-10-02T04:45:02Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01663v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ShapKAN框架，利用Shapley值对Kolmogorov-Arnold网络进行节点重要性评估和剪枝，解决了传统基于幅度的剪枝方法对输入坐标偏移敏感的问题。该方法能保持节点重要性排名的一致性，在保持网络性能的同时实现有效压缩，增强了KAN网络在资源受限环境中的可解释性和部署能力。",
      "order": 102
    },
    {
      "arxiv_id": "2510.01659v1",
      "title": "MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue\n  Summarization",
      "summary": "Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging\napplications. To support the development of effective MDS models, robust\nautomatic evaluation methods are essential for reducing both cost and human\neffort. However, such methods require a strong meta-evaluation benchmark\ngrounded in human annotations. In this work, we introduce MDSEval, the first\nmeta-evaluation benchmark for MDS, consisting image-sharing dialogues,\ncorresponding summaries, and human judgments across eight well-defined quality\naspects. To ensure data quality and richfulness, we propose a novel filtering\nframework leveraging Mutually Exclusive Key Information (MEKI) across\nmodalities. Our work is the first to identify and formalize key evaluation\ndimensions specific to MDS. We benchmark state-of-the-art modal evaluation\nmethods, revealing their limitations in distinguishing summaries from advanced\nMLLMs and their susceptibility to various bias.",
      "authors": [
        "Yinhong Liu",
        "Jianfeng He",
        "Hang Su",
        "Ruixue Lian",
        "Yi Nian",
        "Jake Vincent",
        "Srikanth Vishnubhotla",
        "Robinson Piramuthu",
        "Saab Mansour"
      ],
      "published": "2025-10-02T04:38:27Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01659v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MDSEval——首个多模态对话摘要的元评估基准，包含图像共享对话、对应摘要及八维度人工标注。通过跨模态互斥关键信息筛选框架确保数据质量，首次系统定义MDS专属评估维度，并揭示现有评估方法在区分先进多模态大模型生成摘要时的局限性。",
      "order": 103
    },
    {
      "arxiv_id": "2510.01658v1",
      "title": "Learning Time-Series Representations by Hierarchical\n  Uniformity-Tolerance Latent Balancing",
      "summary": "We propose TimeHUT, a novel method for learning time-series representations\nby hierarchical uniformity-tolerance balancing of contrastive representations.\nOur method uses two distinct losses to learn strong representations with the\naim of striking an effective balance between uniformity and tolerance in the\nembedding space. First, TimeHUT uses a hierarchical setup to learn both\ninstance-wise and temporal information from input time-series. Next, we\nintegrate a temperature scheduler within the vanilla contrastive loss to\nbalance the uniformity and tolerance characteristics of the embeddings.\nAdditionally, a hierarchical angular margin loss enforces instance-wise and\ntemporal contrast losses, creating geometric margins between positive and\nnegative pairs of temporal sequences. This approach improves the coherence of\npositive pairs and their separation from the negatives, enhancing the capture\nof temporal dependencies within a time-series sample. We evaluate our approach\non a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and\nmultivariate classification, as well as Yahoo and KPI datasets for anomaly\ndetection. The results demonstrate that TimeHUT outperforms prior methods by\nconsiderable margins on classification, while obtaining competitive results for\nanomaly detection. Finally, detailed sensitivity and ablation studies are\nperformed to evaluate different components and hyperparameters of our method.",
      "authors": [
        "Amin Jalali",
        "Milad Soltany",
        "Michael Greenspan",
        "Ali Etemad"
      ],
      "published": "2025-10-02T04:30:13Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01658v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "提出TimeHUT方法，通过层次化均匀性-容忍度平衡学习时间序列表示。该方法结合分层对比学习和温度调度器，在嵌入空间中平衡均匀性与容忍度，使用层次角间隔损失增强正负样本区分。在128个UCR和30个UAE分类数据集及异常检测任务上表现优异，分类性能显著超越现有方法。",
      "order": 104
    },
    {
      "arxiv_id": "2510.01656v1",
      "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM\n  reasoning",
      "summary": "Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing\nthem with average advantage baselines. This shift is largely pragmatic:\nconventional value functions are computationally expensive to train at LLM\nscale and often fail under sparse rewards and long reasoning horizons. We\nrevisit this bottleneck from an architectural perspective and introduce\nAsymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable\nframework that restores the critics role while remaining efficient in\nlarge-model settings. AsyPPO employs a set of lightweight mini-critics, each\ntrained on disjoint prompt shards. This design encourages diversity while\npreserving calibration, reducing value-estimation bias. Beyond robust\nestimation, AsyPPO leverages inter-critic uncertainty to refine the policy\nupdate: (i) masking advantages in states where critics agree and gradients add\nlittle learning signal, and (ii) filtering high-divergence states from entropy\nregularization, suppressing spurious exploration. After training on open-source\ndata with only 5,000 samples, AsyPPO consistently improves learning stability\nand performance across multiple benchmarks over strong baselines, such as GRPO,\nachieving performance gains of more than six percent on Qwen3-4b-Base and about\nthree percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without\nadditional tricks. These results highlight the importance of architectural\ninnovations for scalable, efficient algorithms.",
      "authors": [
        "Jiashun Liu",
        "Johan Obando-Ceron",
        "Han Lu",
        "Yancheng He",
        "Weixun Wang",
        "Wenbo Su",
        "Bo Zheng",
        "Pablo Samuel Castro",
        "Aaron Courville",
        "Ling Pan"
      ],
      "published": "2025-10-02T04:24:27Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01656v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出非对称近端策略优化(AsyPPO)，通过使用轻量级迷你评论器解决LLM强化学习中价值函数训练难题。该方法在多个基准测试中显著提升学习稳定性和性能，仅用5000个样本即实现超过PPO基线6%的性能提升。",
      "order": 105
    },
    {
      "arxiv_id": "2510.01654v1",
      "title": "SoK: Measuring What Matters for Closed-Loop Security Agents",
      "summary": "Cybersecurity is a relentless arms race, with AI driven offensive systems\nevolving faster than traditional defenses can adapt. Research and tooling\nremain fragmented across isolated defensive functions, creating blind spots\nthat adversaries exploit. Autonomous agents capable of integrating, exploit\nconfirmation, remediation, and validation into a single closed loop offer\npromise, but the field lacks three essentials: a framework defining the agentic\ncapabilities of security systems across security life cycle, a principled\nmethod for evaluating closed loop agents, and a benchmark for measuring their\nperformance in practice. We introduce CLASP: the Closed-Loop Autonomous\nSecurity Performance framework which aligns the security lifecycle\n(reconnaissance, exploitation, root cause analysis, patch synthesis,\nvalidation) with core agentic capabilities (planning, tool use, memory,\nreasoning, reflection & perception) providing a common vocabulary and rubric\nfor assessing agentic capabilities in security tasks. By applying CLASP to 21\nrepresentative works, we map where systems demonstrate strengths, and where\ncapability gaps persist. We then define the Closed-Loop Capability (CLC) Score,\na composite metric quantifying both degree of loop closure and operational\neffectiveness, and outline the requirements for a closed loop benchmark.\nTogether, CLASP and the CLC Score, provide the vocabulary, diagnostics, and\nmeasurements needed to advance both function level performance and measure\nclosed loop security agents.",
      "authors": [
        "Mudita Khurana",
        "Raunak Jain"
      ],
      "published": "2025-10-02T04:20:35Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01654v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出CLASP框架，将安全生命周期与智能体核心能力对齐，为评估闭环安全智能体提供统一标准和CLC评分指标，填补了该领域缺乏系统性评估方法的空白。",
      "order": 106
    },
    {
      "arxiv_id": "2510.01650v1",
      "title": "The Unseen Frontier: Pushing the Limits of LLM Sparsity with\n  Surrogate-Free ADMM",
      "summary": "Neural network pruning is a promising technique to mitigate the excessive\ncomputational and memory requirements of large language models (LLMs). Despite\nits promise, however, progress in this area has diminished, as conventional\nmethods are seemingly unable to surpass moderate sparsity levels (50-60%)\nwithout severely degrading model accuracy. This work breaks through the current\nimpasse, presenting a principled and effective method called $\\texttt{Elsa}$,\nwhich achieves extreme sparsity levels of up to 90% while retaining high model\nfidelity. This is done by identifying several limitations in current practice,\nall of which can be traced back to their reliance on a surrogate objective\nformulation. $\\texttt{Elsa}$ tackles this issue directly and effectively via\nstandard and well-established constrained optimization techniques based on\nADMM. Our extensive experiments across a wide range of models and scales show\nthat $\\texttt{Elsa}$ achieves substantial improvements over existing methods;\ne.g., it achieves 7.8$\\times$ less perplexity than the best existing method on\nLLaMA-2-7B at 90% sparsity. Furthermore, we present\n$\\texttt{Elsa}_{\\text{-L}}$, a quantized variant that scales to extremely large\nmodels (27B), and establish its theoretical convergence guarantees. These\nresults highlight meaningful progress in advancing the frontier of LLM\nsparsity, while promising that significant opportunities for further\nadvancement may remain in directions that have so far attracted limited\nexploration.",
      "authors": [
        "Kwanhee Lee",
        "Hyeondo Jang",
        "Dongyeop Lee",
        "Dan Alistarh",
        "Namhoon Lee"
      ],
      "published": "2025-10-02T04:10:17Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01650v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种名为Elsa的新型剪枝方法，采用无代理目标的ADMM优化技术，突破现有大语言模型稀疏化极限，在保持高精度的同时实现高达90%的稀疏度，相比现有方法在LLaMA-2-7B模型上困惑度降低7.8倍。",
      "order": 107
    },
    {
      "arxiv_id": "2510.01649v1",
      "title": "Source-Free Cross-Domain Continual Learning",
      "summary": "Although existing cross-domain continual learning approaches successfully\naddress many streaming tasks having domain shifts, they call for a fully\nlabeled source domain hindering their feasibility in the privacy constrained\nenvironments. This paper goes one step ahead with the problem of source-free\ncross-domain continual learning where the use of source-domain samples are\ncompletely prohibited. We propose the idea of rehearsal-free frequency-aware\ndynamic prompt collaborations (REFEREE) to cope with the absence of labeled\nsource-domain samples in realm of cross-domain continual learning. REFEREE is\nbuilt upon a synergy between a source-pre-trained model and a large-scale\nvision-language model, thus overcoming the problem of sub-optimal\ngeneralizations when relying only on a source pre-trained model. The domain\nshift problem between the source domain and the target domain is handled by a\nfrequency-aware prompting technique encouraging low-frequency components while\nsuppressing high-frequency components. This strategy generates frequency-aware\naugmented samples, robust against noisy pseudo labels. The noisy pseudo-label\nproblem is further addressed with the uncertainty-aware weighting strategy\nwhere the mean and covariance matrix are weighted by prediction uncertainties,\nthus mitigating the adverse effects of the noisy pseudo label. Besides, the\nissue of catastrophic forgetting (CF) is overcome by kernel linear discriminant\nanalysis (KLDA) where the backbone network is frozen while the classification\nis performed using the linear discriminant analysis approach guided by the\nrandom kernel method. Our rigorous numerical studies confirm the advantage of\nour approach where it beats prior arts having access to source domain samples\nwith significant margins.",
      "authors": [
        "Muhammad Tanzil Furqon",
        "Mahardhika Pratama",
        "Igor Škrjanc",
        "Lin Liu",
        "Habibullah Habibullah",
        "Kutluyil Dogancay"
      ],
      "published": "2025-10-02T04:09:25Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01649v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出REFEREE方法解决无源跨域持续学习问题，通过频率感知提示技术和不确定性加权策略处理域偏移和噪声伪标签，利用核线性判别分析缓解灾难性遗忘，在隐私受限环境下超越现有方法。",
      "order": 108
    },
    {
      "arxiv_id": "2510.01645v1",
      "title": "Position: Privacy Is Not Just Memorization!",
      "summary": "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats.",
      "authors": [
        "Niloofar Mireshghallah",
        "Tianshi Li"
      ],
      "published": "2025-10-02T04:02:06Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01645v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文主张大语言模型隐私风险远不止训练数据记忆，涵盖数据收集、推理时上下文泄露、自主代理能力及深度推理攻击等全生命周期威胁。通过对1322篇隐私论文的纵向分析，揭示当前研究过度关注记忆问题，而最紧迫的隐私危害存在于其他领域，呼吁研究社区转向跨学科方法应对这些社会技术性威胁。",
      "order": 109
    },
    {
      "arxiv_id": "2510.01644v1",
      "title": "NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with\n  BERT",
      "summary": "Large Language Models (LLMs) suffer from a range of vulnerabilities that\nallow malicious users to solicit undesirable responses through manipulation of\nthe input text. These so-called jailbreak prompts are designed to trick the LLM\ninto circumventing the safety guardrails put in place to keep responses\nacceptable to the developer's policies. In this study, we analyse the ability\nof different machine learning models to distinguish jailbreak prompts from\ngenuine uses, including looking at our ability to identify jailbreaks that use\npreviously unseen strategies. Our results indicate that using current datasets\nthe best performance is achieved by fine tuning a Bidirectional Encoder\nRepresentations from Transformers (BERT) model end-to-end for identifying\njailbreaks. We visualise the keywords that distinguish jailbreak from genuine\nprompts and conclude that explicit reflexivity in prompt structure could be a\nsignal of jailbreak intention.",
      "authors": [
        "John Hawkins",
        "Aditya Pramar",
        "Rodney Beard",
        "Rohitash Chandra"
      ],
      "published": "2025-10-02T03:55:29Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01644v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探讨了使用机器学习方法检测大语言模型越狱攻击的能力，发现通过端到端微调BERT模型在现有数据集上表现最佳。研究分析了区分越狱提示与正常使用的关键词特征，并指出提示结构中的显性自反性可能是越狱意图的信号。",
      "order": 110
    },
    {
      "arxiv_id": "2510.01639v1",
      "title": "Understanding the Geospatial Reasoning Capabilities of LLMs: A\n  Trajectory Recovery Perspective",
      "summary": "We explore the geospatial reasoning capabilities of Large Language Models\n(LLMs), specifically, whether LLMs can read road network maps and perform\nnavigation. We frame trajectory recovery as a proxy task, which requires models\nto reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with\nover 4,000 real-world trajectories across diverse regions and transportation\nmodes. Using road network as context, our prompting framework enables LLMs to\ngenerate valid paths without accessing any external navigation tools.\nExperiments show that LLMs outperform off-the-shelf baselines and specialized\ntrajectory recovery models, with strong zero-shot generalization. Fine-grained\nanalysis shows that LLMs have strong comprehension of the road network and\ncoordinate systems, but also pose systematic biases with respect to regions and\ntransportation modes. Finally, we demonstrate how LLMs can enhance navigation\nexperiences by reasoning over maps in flexible ways to incorporate user\npreferences.",
      "authors": [
        "Thinh Hung Truong",
        "Jey Han Lau",
        "Jianzhong Qi"
      ],
      "published": "2025-10-02T03:37:41Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01639v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过轨迹恢复任务评估大语言模型的地理空间推理能力，提出GLOBALTRACE数据集和提示框架，证明LLMs无需外部工具即可理解路网生成有效路径，在零样本泛化中优于专业模型，但存在区域和交通模式的系统性偏差，展示了LLMs通过地图推理增强导航体验的潜力。",
      "order": 111
    },
    {
      "arxiv_id": "2510.01638v1",
      "title": "Towards Human-Centered RegTech: Unpacking Professionals' Strategies and\n  Needs for Using LLMs Safely",
      "summary": "Large Language Models are profoundly changing work patterns in high-risk\nprofessional domains, yet their application also introduces severe and\nunderexplored compliance risks. To investigate this issue, we conducted\nsemi-structured interviews with 24 highly-skilled knowledge workers from\nindustries such as law, healthcare, and finance. The study found that these\nexperts are commonly concerned about sensitive information leakage,\nintellectual property infringement, and uncertainty regarding the quality of\nmodel outputs. In response, they spontaneously adopt various mitigation\nstrategies, such as actively distorting input data and limiting the details in\ntheir prompts. However, the effectiveness of these spontaneous efforts is\nlimited due to a lack of specific compliance guidance and training for Large\nLanguage Models. Our research reveals a significant gap between current NLP\ntools and the actual compliance needs of experts. This paper positions these\nvaluable empirical findings as foundational work for building the next\ngeneration of Human-Centered, Compliance-Driven Natural Language Processing for\nRegulatory Technology (RegTech), providing a critical human-centered\nperspective and design requirements for engineering NLP systems that can\nproactively support expert compliance workflows.",
      "authors": [
        "Siying Hu",
        "Yaxing Yao",
        "Zhicong Lu"
      ],
      "published": "2025-10-02T03:35:46Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.01638v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过访谈24名法律、医疗和金融领域专家，探讨专业人士使用大语言模型时的合规风险与应对策略。研究发现专家普遍关注敏感信息泄露、知识产权侵权和输出质量不确定性等问题，并自发采取数据扭曲、提示简化等缓解措施。研究揭示了当前NLP工具与专家合规需求间的显著差距，为构建以人为中心、合规驱动的监管科技自然语言处理系统提供了设计基础。",
      "order": 112
    },
    {
      "arxiv_id": "2510.01632v1",
      "title": "BioBlobs: Differentiable Graph Partitioning for Protein Representation\n  Learning",
      "summary": "Protein function is driven by coherent substructures which vary in size and\ntopology, yet current protein representation learning models (PRL) distort\nthese signals by relying on rigid substructures such as k-hop and fixed radius\nneighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable\nmodule that represents proteins by dynamically partitioning structures into\nflexibly-sized, non-overlapping substructures (\"blobs\"). The resulting blobs\nare quantized into a shared and interpretable codebook, yielding a discrete\nvocabulary of function-relevant protein substructures used to compute protein\nembeddings. We show that BioBlobs representations improve the performance of\nwidely used protein encoders such as GVP-GNN across various PRL tasks. Our\napproach highlights the value of architectures that directly capture\nfunction-relevant protein substructures, enabling both improved predictive\nperformance and mechanistic insight into protein function.",
      "authors": [
        "Xin Wang",
        "Carlos Oliver"
      ],
      "published": "2025-10-02T03:25:02Z",
      "primary_category": "q-bio.BM",
      "arxiv_url": "https://arxiv.org/abs/2510.01632v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "BioBlobs是一种可微分图划分模块，通过将蛋白质结构动态分割为灵活大小的非重叠子结构（'blobs'）来改进蛋白质表示学习。该方法将生成的blobs量化为共享可解释的代码本，形成功能相关的蛋白质子结构词汇表，提升了GVP-GNN等蛋白质编码器在多种任务中的性能，同时增强了对蛋白质功能机制的理解。",
      "order": 113
    },
    {
      "arxiv_id": "2510.01631v1",
      "title": "Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of\n  Scaling Laws, Benefits, and Pitfalls",
      "summary": "Training data plays a crucial role in Large Language Models (LLM) scaling,\nyet high quality data is of limited supply. Synthetic data techniques offer a\npotential path toward sidestepping these limitations. We conduct a large-scale\nempirical investigation (>1000 LLMs with >100k GPU hours) using a unified\nprotocol and scaling laws, comparing natural web data, diverse synthetic types\n(rephrased text, generated textbooks), and mixtures of natural and synthetic\ndata. Specifically, we found pre-training on rephrased synthetic data\n\\textit{alone} is not faster than pre-training on natural web texts; while\npre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts\ncan speed up 5-10x (to reach the same validation loss) at larger data budgets.\nPre-training on textbook-style synthetic data \\textit{alone} results in notably\nhigher loss on many downstream domains especially at small data budgets. \"Good\"\nratios of synthetic data in training data mixtures depend on the model size and\ndata budget, empirically converging to ~30% for rephrased synthetic data.\nLarger generator models do not necessarily yield better pre-training data than\n~8B-param models. These results contribute mixed evidence on \"model collapse\"\nduring large-scale single-round (n=1) model training on synthetic\ndata--training on rephrased synthetic data shows no degradation in performance\nin foreseeable scales whereas training on mixtures of textbook-style\npure-generated synthetic data shows patterns predicted by \"model collapse\". Our\nwork demystifies synthetic data in pre-training, validates its conditional\nbenefits, and offers practical guidance.",
      "authors": [
        "Feiyang Kang",
        "Newsha Ardalani",
        "Michael Kuchnik",
        "Youssef Emad",
        "Mostafa Elhoushi",
        "Shubhabrata Sengupta",
        "Shang-Wen Li",
        "Ramya Raghavendra",
        "Ruoxi Jia",
        "Carole-Jean Wu"
      ],
      "published": "2025-10-02T03:24:42Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01631v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过大规模实证分析（>1000个LLM，>10万GPU小时）系统探讨了合成数据在LLM预训练中的作用。研究发现：仅使用改写合成数据训练效果不优于自然网络文本，但1/3改写合成数据与2/3自然数据混合可加速训练5-10倍；纯教科书式合成数据训练在小型数据预算下表现较差；最佳合成数据比例约30%且与模型规模相关；约80亿参数生成器已能产生优质训练数据。研究为合成数据的条件性优势提供了实证依据，并揭示了不同合成数据类型对'模型崩溃'现象的影响差异。",
      "order": 114
    },
    {
      "arxiv_id": "2510.01624v1",
      "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What\n  to Use Instead",
      "summary": "In post-training for reasoning Large Language Models (LLMs), the current\nstate of practice trains LLMs in two independent stages: Supervised Fine-Tuning\n(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as\n``RL'' below). In this work, we challenge whether high SFT scores translate to\nimproved performance after RL. We provide extensive counter-examples where this\nis not true. We find high SFT scores can be biased toward simpler or more\nhomogeneous data and are not reliably predictive of subsequent RL gains or\nscaled-up post-training effectiveness. In some cases, RL training on models\nwith improved SFT performance could lead to substantially worse outcome\ncompared to RL on the base model without SFT. We study alternative metrics and\nidentify generalization loss on held-out reasoning examples and Pass@large k\nperformance to provide strong proxies for the RL outcome. We trained hundreds\nof models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive\nevaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU\nhours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple\nstate-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL\nperformance, prediction based on generalization loss and Pass@large k achieves\nsubstantial higher precision, improving $R^2$ coefficient and Spearman's rank\ncorrelation coefficient by up to 0.5 (2x). This provides strong utility for\nbroad use cases. For example, in most experiments, we find SFT training on\nunique examples for a one epoch underperforms training on half examples for two\nepochs, either after SFT or SFT-then-RL; With the same SFT budget, training\nonly on short examples may lead to better SFT performance, though, it often\nleads to worse outcome after RL compared to training on examples with varying\nlengths. Evaluation tool will be open-sourced.",
      "authors": [
        "Feiyang Kang",
        "Michael Kuchnik",
        "Karthik Padthe",
        "Marin Vlastelica",
        "Ruoxi Jia",
        "Carole-Jean Wu",
        "Newsha Ardalani"
      ],
      "published": "2025-10-02T02:57:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01624v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究挑战了SFT高分必然提升RL后性能的传统认知，通过大规模实验发现SFT分数可能因数据简单或同质而产生偏差，无法可靠预测RL效果。提出泛化损失和Pass@large k作为更优的RL结果预测指标，在7个数学基准测试中验证了其有效性，为LLM后训练提供了新评估范式。",
      "order": 115
    },
    {
      "arxiv_id": "2510.01622v1",
      "title": "LLM4Rec: Large Language Models for Multimodal Generative Recommendation\n  with Causal Debiasing",
      "summary": "Contemporary generative recommendation systems face significant challenges in\nhandling multimodal data, eliminating algorithmic biases, and providing\ntransparent decision-making processes. This paper introduces an enhanced\ngenerative recommendation framework that addresses these limitations through\nfive key innovations: multimodal fusion architecture, retrieval-augmented\ngeneration mechanisms, causal inference-based debiasing, explainable\nrecommendation generation, and real-time adaptive learning capabilities. Our\nframework leverages advanced large language models as the backbone while\nincorporating specialized modules for cross-modal understanding, contextual\nknowledge integration, bias mitigation, explanation synthesis, and continuous\nmodel adaptation. Extensive experiments on three benchmark datasets\n(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistent\nimprovements in recommendation accuracy, fairness, and diversity compared to\nexisting approaches. The proposed framework achieves up to 2.3% improvement in\nNDCG@10 and 1.4% enhancement in diversity metrics while maintaining\ncomputational efficiency through optimized inference strategies.",
      "authors": [
        "Bo Ma",
        "Hang Li",
        "ZeHua Hu",
        "XiaoFan Gui",
        "LuYao Liu",
        "Simon Lau"
      ],
      "published": "2025-10-02T02:53:05Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01622v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出LLM4Rec框架，通过多模态融合架构、检索增强生成、因果去偏、可解释推荐和实时自适应学习五大创新，解决了生成式推荐系统在处理多模态数据、消除算法偏见和提供透明决策方面的挑战。在三个基准数据集上的实验表明，该框架在推荐准确性、公平性和多样性方面均优于现有方法。",
      "order": 116
    },
    {
      "arxiv_id": "2510.01620v1",
      "title": "Learning to Decide with Just Enough: Information-Theoretic Context\n  Summarization for CDMPs",
      "summary": "Contextual Markov Decision Processes (CMDPs) offer a framework for sequential\ndecision-making under external signals, but existing methods often fail to\ngeneralize in high-dimensional or unstructured contexts, resulting in excessive\ncomputation and unstable performance. We propose an information-theoretic\nsummarization approach that uses large language models (LLMs) to compress\ncontextual inputs into low-dimensional, semantically rich summaries. These\nsummaries augment states by preserving decision-critical cues while reducing\nredundancy. Building on the notion of approximate context sufficiency, we\nprovide, to our knowledge, the first regret bounds and a latency-entropy\ntrade-off characterization for CMDPs. Our analysis clarifies how\ninformativeness impacts computational cost. Experiments across discrete,\ncontinuous, visual, and recommendation benchmarks show that our method\noutperforms raw-context and non-context baselines, improving reward, success\nrate, and sample efficiency, while reducing latency and memory usage. These\nfindings demonstrate that LLM-based summarization offers a scalable and\ninterpretable solution for efficient decision-making in context-rich,\nresource-constrained environments.",
      "authors": [
        "Peidong Liu",
        "Junjiang Lin",
        "Shaowen Wang",
        "Yao Xu",
        "Haiqing Li",
        "Xuhao Xie",
        "Siyi Wu",
        "Hao Li"
      ],
      "published": "2025-10-02T02:52:24Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01620v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于信息论的上下文摘要方法，利用大语言模型压缩高维上下文为低维语义摘要，增强情境马尔可夫决策过程的决策效率。该方法在保持关键信息的同时减少冗余，首次为CMDPs提供遗憾界和延迟-熵权衡分析，实验证明其在奖励、成功率、样本效率方面优于基线，并降低延迟和内存使用。",
      "order": 117
    },
    {
      "arxiv_id": "2510.01612v1",
      "title": "RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical\n  Question Answering",
      "summary": "The exponential growth of biomedical literature creates significant\nchallenges for accessing precise medical information. Current biomedical\nquestion-answering systems primarily focus on short-form answers, failing to\nprovide the comprehensive explanations necessary for clinical decision-making.\nWe present RAG-BioQA, a novel framework combining retrieval-augmented\ngeneration with domain-specific fine-tuning to produce evidence-based,\nlong-form biomedical answers. Our approach integrates BioBERT embeddings with\nFAISS indexing and compares various re-ranking strategies (BM25, ColBERT,\nMonoT5) to optimize context selection before synthesizing evidence through a\nfine-tuned T5 model. Experimental results on the PubMedQA dataset show\nsignificant improvements over baselines, with our best model achieving\nsubstantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state\nof accessible, evidence-based biomedical knowledge retrieval.",
      "authors": [
        "Lovely Yeswanth Panchumarthi",
        "Sai Prasad Gudari",
        "Atharva Negi",
        "Praveen Raj Budime",
        "Harsit Upadhya"
      ],
      "published": "2025-10-02T02:49:09Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01612v1",
      "primary_area": "text_models",
      "secondary_focus": "long_context",
      "application_domain": "medical_ai",
      "tldr_zh": "RAG-BioQA是一个结合检索增强生成与领域微调的生物医学问答框架，通过BioBERT嵌入、FAISS索引和重排序策略优化上下文选择，使用微调T5模型生成基于证据的长篇医学答案，在PubMedQA数据集上显著优于基线模型。",
      "order": 118
    },
    {
      "arxiv_id": "2510.01611v1",
      "title": "PychoBench: Evaluating the Psychology Intelligence of Large Language\n  Models",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: Can LLMs be effectively applied to psychological\ncounseling? To determine whether an LLM can effectively take on the role of a\npsychological counselor, the first step is to assess whether it meets the\nqualifications required for such a role, namely the ability to pass the U.S.\nNational Counselor Certification Exam (NCE). This is because, just as a human\ncounselor must pass a certification exam to practice, an LLM must demonstrate\nsufficient psychological knowledge to meet the standards required for such a\nrole. To address this, we introduce PsychoBench, a benchmark grounded in\nU.S.national counselor examinations, a licensure test for professional\ncounselors that requires about 70% accuracy to pass. PsychoBench comprises\napproximately 2,252 carefully curated single-choice questions, crafted to\nrequire deep understanding and broad enough to cover various sub-disciplines of\npsychology. This benchmark provides a comprehensive assessment of an LLM's\nability to function as a counselor. Our evaluation shows that advanced models\nsuch as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing\nthreshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)\nremain far below it. These results suggest that only frontier LLMs are\ncurrently capable of meeting counseling exam standards, highlighting both the\npromise and the challenges of developing psychology-oriented LLMs.",
      "authors": [
        "Min Zeng"
      ],
      "published": "2025-10-02T02:49:06Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01611v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出PsychoBench基准，基于美国国家心理咨询师认证考试构建，包含2252道单选题，用于评估大语言模型在心理学咨询领域的专业能力。测试结果显示，GPT-4o、Llama3.3-70B等前沿模型已超过考试通过线（70%准确率），而较小模型仍远未达标，表明目前只有顶级大模型具备担任心理咨询师的知识储备。",
      "order": 119
    },
    {
      "arxiv_id": "2510.01609v1",
      "title": "AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative\n  Recommendation with Adaptive Intelligence",
      "summary": "Interactive conversational recommender systems have gained significant\nattention for their ability to capture user preferences through natural\nlanguage interactions. However, existing approaches face substantial challenges\nin handling dynamic user preferences, maintaining conversation coherence, and\nbalancing multiple ranking objectives simultaneously. This paper introduces\nAgentRec, a next-generation LLM-powered multi-agent collaborative\nrecommendation framework that addresses these limitations through hierarchical\nagent networks with adaptive intelligence. Our approach employs specialized\nLLM-powered agents for conversation understanding, preference modeling, context\nawareness, and dynamic ranking, coordinated through an adaptive weighting\nmechanism that learns from interaction patterns. We propose a three-tier\nlearning strategy combining rapid response for simple queries, intelligent\nreasoning for complex preferences, and deep collaboration for challenging\nscenarios. Extensive experiments on three real-world datasets demonstrate that\nAgentRec achieves consistent improvements over state-of-the-art baselines, with\n2.8\\% enhancement in conversation success rate, 1.9\\% improvement in\nrecommendation accuracy (NDCG@10), and 3.2\\% better conversation efficiency\nwhile maintaining comparable computational costs through intelligent agent\ncoordination.",
      "authors": [
        "Bo Ma",
        "Hang Li",
        "ZeHua Hu",
        "XiaoFan Gui",
        "LuYao Liu",
        "Simon Lau"
      ],
      "published": "2025-10-02T02:47:11Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01609v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "AgentRec是一种基于大语言模型的多智能体协同推荐框架，通过分层智能体网络解决动态用户偏好、对话连贯性和多目标排序平衡问题。该系统采用三层次学习策略，在三个真实数据集上实验表明，对话成功率提升2.8%，推荐准确率(NDCG@10)提高1.9%，对话效率提升3.2%。",
      "order": 120
    },
    {
      "arxiv_id": "2510.01606v1",
      "title": "Bridging Collaborative Filtering and Large Language Models with Dynamic\n  Alignment, Multimodal Fusion and Evidence-grounded Explanations",
      "summary": "Recent research has explored using Large Language Models for recommendation\ntasks by transforming user interaction histories and item metadata into text\nprompts, then having the LLM produce rankings or recommendations. A promising\napproach involves connecting collaborative filtering knowledge to LLM\nrepresentations through compact adapter networks, which avoids expensive\nfine-tuning while preserving the strengths of both components. Yet several\nchallenges persist in practice: collaborative filtering models often use static\nsnapshots that miss rapidly changing user preferences; many real-world items\ncontain rich visual and audio content beyond textual descriptions; and current\nsystems struggle to provide trustworthy explanations backed by concrete\nevidence. Our work introduces \\model{}, a framework that tackles these\nlimitations through three key innovations. We develop an online adaptation\nmechanism that continuously incorporates new user interactions through\nlightweight modules, avoiding the need to retrain large models. We create a\nunified representation that seamlessly combines collaborative signals with\nvisual and audio features, handling cases where some modalities may be\nunavailable. Finally, we design an explanation system that grounds\nrecommendations in specific collaborative patterns and item attributes,\nproducing natural language rationales users can verify. Our approach maintains\nthe efficiency of frozen base models while adding minimal computational\noverhead, making it practical for real-world deployment.",
      "authors": [
        "Bo Ma",
        "LuYao Liu",
        "Simon Lau",
        "Chandler Yuan",
        "and XueY Cui",
        "Rosie Zhang"
      ],
      "published": "2025-10-02T02:43:24Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01606v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种融合协同过滤与大语言模型的推荐框架，通过动态对齐机制实时更新用户偏好，整合文本、视觉和音频多模态特征，并生成基于具体证据的可验证解释，在保持基础模型高效性的同时提升推荐系统的适应性和可信度。",
      "order": 121
    },
    {
      "arxiv_id": "2510.01600v1",
      "title": "A Comparison of Independent and Joint Fine-tuning Strategies for\n  Retrieval-Augmented Generation",
      "summary": "A Comparison of Independent and Joint Fine-tuning Strategies for\nRetrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,\nAnoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP\n2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0\nKeywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),\nFine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and\ncompare strategies for fine-tuning Retrieval Augmented Generation (RAG)\npipelines, including independent fine-tuning, joint fine-tuning, and two-phase\nfine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular\nframework for question answering that is powered by two large language models\n(LLMs): an embedding model that retrieves context documents from a database\nthat are relevant to a given question, and a generator model that uses the\nretrieved context to generate an answer to the question. Both the embedding and\ngenerator models can be fine-tuned to increase performance of a RAG pipeline on\na new task, but multiple fine-tuning strategies exist with different costs and\nbenefits. In this paper, we evaluate and compare several RAG fine-tuning\nstrategies, including independent, joint, and two-phase fine-tuning. In our\nexperiments, we observe that all of these strategies achieve about equal\nimprovement in EM and F1 generation quality metrics, although they have\nsignificantly different computational costs. We conclude the optimal\nfine-tuning strategy to use depends on whether the training dataset includes\ncontext labels and whether a grid search over the learning rates for the\nembedding and generator models is required.",
      "authors": [
        "Neal Gregory Lawton",
        "Alfy Samuel",
        "Anoop Kumar",
        "Daben Liu"
      ],
      "published": "2025-10-02T02:30:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01600v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文评估并比较了检索增强生成(RAG)管线的多种微调策略，包括独立微调、联合微调和两阶段微调。实验表明这些策略在生成质量指标上提升相当，但计算成本差异显著，最优策略选择取决于训练数据是否包含上下文标签以及是否需要学习率网格搜索。",
      "order": 122
    },
    {
      "arxiv_id": "2510.01588v1",
      "title": "Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via\n  Contrastive Feature Augmentation",
      "summary": "Parkinson's disease (PD) is one of the most common neurodegenerative\ndisorder. PD telemonitoring emerges as a novel assessment modality enabling\nself-administered at-home tests of Unified Parkinson's Disease Rating Scale\n(UPDRS) scores, enhancing accessibility for PD patients. However, three types\nof noise would occur during measurements: (1) patient-induced measurement\ninaccuracies, (2) environmental noise, and (3) data packet loss during\ntransmission, resulting in higher prediction errors. To address these\nchallenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,\nthe original speech features are grouped into ordered bins, based on the\ncontinuous values of a selected feature, to construct contrastive pairs.\nSecond, the contrastive pairs are employed to train a multilayer perceptron\nencoder for generating noise-robust features. Finally, these features are\nconcatenated with the original features as the augmented features, which are\nthen fed into the UPDRS prediction models. Notably, we further introduces a\nnovel evaluation approach with customizable noise injection module, and\nextensive experiments show that NoRo can successfully enhance the noise\nrobustness of UPDRS prediction across various downstream prediction models\nunder different noisy environments.",
      "authors": [
        "Ziming Tang",
        "Chengbin Hou",
        "Tianyu Zhang",
        "Bangxu Tian",
        "Jinbao Wang",
        "Hairong Lv"
      ],
      "published": "2025-10-02T02:07:41Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01588v1",
      "primary_area": "audio_models",
      "secondary_focus": "training_optimization",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出NoRo框架，通过对比特征增强技术提升帕金森病远程监测系统的噪声鲁棒性。该方法将语音特征分组构建对比对，训练多层感知机编码器生成抗噪特征，并与原始特征拼接后输入预测模型。实验表明该方案能有效应对患者操作误差、环境噪声和数据丢包等干扰，显著提高UPDRS评分预测精度。",
      "order": 123
    },
    {
      "arxiv_id": "2510.01586v1",
      "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial\n  Co-Evolution in Multi-Agent Reinforcement Learning",
      "summary": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead.",
      "authors": [
        "Zhenyu Pan",
        "Yiting Zhang",
        "Zhuo Liu",
        "Yolo Yunlong Tang",
        "Zeliang Zhang",
        "Haozheng Luo",
        "Yuwei Han",
        "Jianshu Zhang",
        "Dennis Wu",
        "Hong-Yu Chen",
        "Haoran Lu",
        "Haoyang Fang",
        "Manling Li",
        "Chenliang Xu",
        "Philip S. Yu",
        "Han Liu"
      ],
      "published": "2025-10-02T02:06:30Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01586v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "AdvEvo-MARL是一种通过对抗性协同进化在多智能体强化学习中内化安全性的框架。该方法联合优化攻击者（生成越狱提示）和防御者（任务智能体），在对抗环境中训练智能体同时完成任务并抵抗攻击。通过引入群体级均值回报基线稳定学习，在保持任务准确性的同时将攻击成功率控制在20%以下，无需额外防护模块。",
      "order": 124
    },
    {
      "arxiv_id": "2510.01581v1",
      "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,\n  Attentive Compression",
      "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time\ncompute, but this scaling must be allocated in line with task difficulty. On\none hand, short reasoning (underthinking) leads to errors on harder problems\nthat require extended reasoning steps; but, excessively long reasoning\n(overthinking) can be token-inefficient, generating unnecessary steps even\nafter reaching a correct intermediate solution. We refer to this as\nunder-adaptivity, where the model fails to modulate its response length\nappropriately given problems of varying difficulty. To address under-adaptivity\nand strike a balance between under- and overthinking, we propose TRAAC (Think\nRight with Adaptive, Attentive Compression), an online post-training RL method\nthat leverages the model's self-attention over a long reasoning trajectory to\nidentify important steps and prune redundant ones. TRAAC also estimates\ndifficulty and incorporates it into training rewards, thereby learning to\nallocate reasoning budget commensurate with example difficulty. Our approach\nimproves accuracy, reduces reasoning steps, and enables adaptive thinking\ncompared to base models and other RL baselines. Across a variety of tasks\n(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute\naccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%\ncompared to the base model, and a 7.9% accuracy gain paired with a 29.4% length\ndrop compared to the best RL baseline. TRAAC also shows strong generalization:\nalthough our models are trained on math datasets, they show accuracy and\nefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,\nand OptimalThinkingBench. Our analysis further verifies that TRAAC provides\nfine-grained adjustments to thinking budget based on difficulty and that a\ncombination of task-difficulty calibration and attention-based compression\nyields gains across diverse tasks.",
      "authors": [
        "Joykirat Singh",
        "Justin Chih-Yao Chen",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Akshay Nambi",
        "Mohit Bansal"
      ],
      "published": "2025-10-02T02:00:20Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01581v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出TRAAC方法，通过自适应注意力压缩解决思维模型中的欠适应问题，平衡推理过程中的思考不足与过度思考。该方法利用自注意力机制识别重要推理步骤并修剪冗余，根据问题难度分配推理资源，在多个任务上实现准确率提升8.4%且推理长度减少36.8%，展现出强大的泛化能力。",
      "order": 125
    },
    {
      "arxiv_id": "2510.01576v1",
      "title": "Guiding Multimodal Large Language Models with Blind and Low Vision\n  People Visual Questions for Proactive Visual Interpretations",
      "summary": "Multimodal large language models (MLLMs) have been integrated into visual\ninterpretation applications to support Blind and Low Vision (BLV) users because\nof their accuracy and ability to provide rich, human-like interpretations.\nHowever, these applications often default to comprehensive, lengthy\ndescriptions regardless of context. This leads to inefficient exchanges, as\nusers must go through irrelevant details rather than receiving the specific\ninformation they are likely to seek. To deliver more contextually-relevant\ninformation, we developed a system that draws on historical BLV users\nquestions. When given an image, our system identifies similar past visual\ncontexts from the VizWiz-LF dataset and uses the associated questions to guide\nthe MLLM generate descriptions more relevant to BLV users. An evaluation with\nthree human labelers who revised 92 context-aware and context-free descriptions\nshowed that context-aware descriptions anticipated and answered users'\nquestions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of\ncomparisons (50 out of 92). Our paper reviews, and data analysis are publicly\navailable in a Github repository at\nhttps://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .",
      "authors": [
        "Ricardo Gonzalez Penuela",
        "Felipe Arias-Russi",
        "Victor Capriles"
      ],
      "published": "2025-10-02T01:48:51Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01576v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究针对盲人和低视力用户开发了一种多模态大语言模型引导系统，通过分析历史视觉问题数据来生成更相关的图像描述。该系统利用VizWiz-LF数据集中的相似视觉上下文，使模型能预测用户需求并提供针对性解释，在76.1%的情况下能预判并回答用户问题，54.4%的案例中更受用户青睐。",
      "order": 126
    },
    {
      "arxiv_id": "2510.01574v1",
      "title": "Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query\n  Autocomplete",
      "summary": "We introduce a data-centric approach for mitigating presentation bias in\nreal-time neural query autocomplete systems through the use of synthetic\nprefixes. These prefixes are generated from complete user queries collected\nduring regular search sessions where autocomplete was not active. This allows\nus to enrich the training data for learning to rank models with more diverse\nand less biased examples. This method addresses the inherent bias in engagement\nsignals collected from live query autocomplete interactions, where model\nsuggestions influence user behavior. Our neural ranker is optimized for\nreal-time deployment under strict latency constraints and incorporates a rich\nset of features, including query popularity, seasonality, fuzzy match scores,\nand contextual signals such as department affinity, device type, and vertical\nalignment with previous user queries. To support efficient training, we\nintroduce a task-specific simplification of the listwise loss, reducing\ncomputational complexity from $O(n^2)$ to $O(n)$ by leveraging the query\nautocomplete structure of having only one ground-truth selection per prefix.\nDeployed in a large-scale e-commerce setting, our system demonstrates\nstatistically significant improvements in user engagement, as measured by mean\nreciprocal rank and related metrics. Our findings show that synthetic prefixes\nnot only improve generalization but also provide a scalable path toward bias\nmitigation in other low-latency ranking tasks, including related searches and\nquery recommendations.",
      "authors": [
        "Adithya Rajan",
        "Xiaoyu Liu",
        "Prateek Verma",
        "Vibhu Arora"
      ],
      "published": "2025-10-02T01:44:44Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01574v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种数据驱动方法，通过合成前缀缓解实时神经查询自动补全中的展示偏差。该方法利用无自动补全时收集的完整查询生成前缀，丰富排序模型训练数据，并设计了计算效率更高的列表损失函数。在电商场景的部署验证了该方法能显著提升用户参与度指标。",
      "order": 127
    },
    {
      "arxiv_id": "2510.01571v1",
      "title": "From Supervision to Exploration: What Does Protein Language Model Learn\n  During Reinforcement Learning?",
      "summary": "Protein language models (PLMs) have advanced computational protein science\nthrough large-scale pretraining and scalable architectures. In parallel,\nreinforcement learning (RL) has broadened exploration and enabled precise\nmulti-objective optimization in protein design. Yet whether RL can push PLMs\nbeyond their pretraining priors to uncover latent sequence-structure-function\nrules remains unclear. We address this by pairing RL with PLMs across four\ndomains: antimicrobial peptide design, kinase variant optimization, antibody\nengineering, and inverse folding. Using diverse RL algorithms and model\nclasses, we ask if RL improves sampling efficiency and, more importantly, if it\nreveals capabilities not captured by supervised learning. Across benchmarks, RL\nconsistently boosts success rates and sample efficiency. Performance follows a\nthree-factor interaction: task headroom, reward fidelity, and policy capacity\njointly determine gains. When rewards are accurate and informative, policies\nhave sufficient capacity, and tasks leave room beyond supervised baselines,\nimprovements scale; when rewards are noisy or capacity is constrained, gains\nsaturate despite exploration. This view yields practical guidance for RL in\nprotein design: prioritize reward modeling and calibration before scaling\npolicy size, match algorithm and regularization strength to task difficulty,\nand allocate capacity where marginal gains are largest. Implementation is\navailable at https://github.com/chq1155/RL-PLM.",
      "authors": [
        "Hanqun Cao",
        "Hongrui Zhang",
        "Junde Xu",
        "Zhou Zhang",
        "Lingdong Shen",
        "Minghao Sun",
        "Ge Liu",
        "Jinbo Xu",
        "Wu-Jun Li",
        "Jinren Ni",
        "Cesar de la Fuente-Nunez",
        "Tianfan Fu",
        "Yejin Choi",
        "Pheng-Ann Heng",
        "Fang Wu"
      ],
      "published": "2025-10-02T01:31:10Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01571v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究探讨强化学习如何增强蛋白质语言模型在抗菌肽设计、激酶变体优化等四个生物医学领域的性能。研究发现RL能提升采样效率和成功率，其效果取决于任务空间、奖励精度和策略容量三因素交互作用，为蛋白质设计中的RL应用提供了实用指导。",
      "order": 128
    },
    {
      "arxiv_id": "2510.01569v1",
      "title": "InvThink: Towards AI Safety via Inverse Reasoning",
      "summary": "We present InvThink, a simple yet powerful approach that gives large language\nmodels (LLMs) the capability of inverse thinking: reasoning through failure\nmodes before generating responses. Unlike existing safety alignment methods\nthat optimize directly for safe response, InvThink instructs models to 1)\nenumerate potential harms, 2) analyze their consequences, and 3) generate safe\noutputs that proactively avoid these risks. Our method reveals three key\nfindings: (i) safety improvements show stronger scaling with model size\ncompared to existing safety methods. (ii) InvThink mitigates safety tax; by\ntraining models to systematically consider failure modes, it preserves general\nreasoning capabilities on standard benchmarks. (iii) beyond general safety\ntasks, InvThink excels in high-stakes domains including external-facing\n(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,\nachieving up to 15.7% reduction in harmful responses compared to baseline\nmethods like SafetyPrompt. We further implement InvThink via supervised\nfine-tuning, and reinforcement learning across three LLM families. These\nresults suggest that inverse reasoning provides a scalable and generalizable\npath toward safer, more capable language models.",
      "authors": [
        "Yubin Kim",
        "Taehan Kim",
        "Eugene Park",
        "Chunjong Park",
        "Cynthia Breazeal",
        "Daniel McDuff",
        "Hae Won Park"
      ],
      "published": "2025-10-02T01:26:53Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01569v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出InvThink方法，通过逆向思维增强大语言模型的安全性：在生成回答前先推理潜在危害及其后果，从而主动规避风险。该方法在模型规模扩展时安全性能提升更显著，能减轻安全税效应，并在高风险领域（医疗、金融、法律等）将有害回答减少达15.7%。",
      "order": 129
    },
    {
      "arxiv_id": "2510.01555v1",
      "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient\n  Optimization",
      "summary": "Reinforcement Learning from Human Feedback (RLHF) leverages a\nKullback-Leibler (KL) divergence loss to stabilize training and prevent\noverfitting. However, in methods such as GRPO, its implementation may be guided\nby principles from numerical value estimation-a practice that overlooks the\nterm's functional role as an optimization loss. To analyze this issue, we\nestablish a unified framework that connects two seemingly distinct\nimplementation styles: using the mathematical term $k_n$ as a detached\ncoefficient for the policy's score function ('$k_n$ in reward') or as a direct\nloss function through which gradients are propagated ('$k_n$ as loss'). We show\nthat the latter can always be analyzed via an equivalent gradient coefficient\nin the former, unifying the two perspectives. Through this framework, we prove\nthat the conventional '$k_1$ in reward' (like in PPO) is the principled loss\nfor Reverse KL (RKL) regularization. We further establish a key finding: under\non-policy conditions, the '$k_2$ as loss' formulation is, in fact,\ngradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our\nwork, identifies both as the theoretically sound implementations of the RKL\nobjective. In contrast, we show that the recently adopted '$k_3$ as loss' (like\nin GRPO) is merely a first-order, biased approximation of the principled loss.\nFurthermore, we argue that common off-policy implementations of '$k_n$ as loss'\nmethods are biased due to neglected importance sampling, and we propose a\nprincipled correction. Our findings provide a comprehensive, gradient-based\nrationale for choosing and correctly implementing KL regularization, paving the\nway for more robust and effective RLHF systems.",
      "authors": [
        "Kezhao Liu",
        "Jason Klein Liu",
        "Mingtao Chen",
        "Yiming Liu"
      ],
      "published": "2025-10-02T01:00:02Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01555v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文重新审视了RLHF中的KL正则化方法，建立了统一框架连接'k_n在奖励中'和'k_n作为损失'两种实现方式。研究证明在策略条件下，'k_2作为损失'与'k_1在奖励中'具有梯度等价性，均为理论正确的RKL正则化实现，而GRPO等采用的'k_3作为损失'仅为有偏近似。同时指出了离策略实现中的重要性采样偏差问题并提出了修正方案。",
      "order": 130
    },
    {
      "arxiv_id": "2510.01552v1",
      "title": "POLAR: Automating Cyber Threat Prioritization through LLM-Powered\n  Assessment",
      "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research.",
      "authors": [
        "Luoxi Tang",
        "Yuqiao Meng",
        "Ankita Patra",
        "Weicheng Ma",
        "Muchao Ye",
        "Zhaohan Xi"
      ],
      "published": "2025-10-02T00:49:20Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01552v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究大语言模型在网络安全威胁情报中的内在脆弱性，通过大规模评估提出新型分类方法，揭示了虚假相关性、矛盾知识和受限泛化三大根本漏洞，并为设计更鲁棒的LLM驱动CTI系统提供可行见解。",
      "order": 131
    },
    {
      "arxiv_id": "2510.01545v1",
      "title": "Predictive Preference Learning from Human Interventions",
      "summary": "Learning from human involvement aims to incorporate the human subject to\nmonitor and correct agent behavior errors. Although most interactive imitation\nlearning methods focus on correcting the agent's action at the current state,\nthey do not adjust its actions in future states, which may be potentially more\nhazardous. To address this, we introduce Predictive Preference Learning from\nHuman Interventions (PPL), which leverages the implicit preference signals\ncontained in human interventions to inform predictions of future rollouts. The\nkey idea of PPL is to bootstrap each human intervention into L future time\nsteps, called the preference horizon, with the assumption that the agent\nfollows the same action and the human makes the same intervention in the\npreference horizon. By applying preference optimization on these future states,\nexpert corrections are propagated into the safety-critical regions where the\nagent is expected to explore, significantly improving learning efficiency and\nreducing human demonstrations needed. We evaluate our approach with experiments\non both autonomous driving and robotic manipulation benchmarks and demonstrate\nits efficiency and generality. Our theoretical analysis further shows that\nselecting an appropriate preference horizon L balances coverage of risky states\nwith label correctness, thereby bounding the algorithmic optimality gap. Demo\nand code are available at: https://metadriverse.github.io/ppl",
      "authors": [
        "Haoyuan Cai",
        "Zhenghao Peng",
        "Bolei Zhou"
      ],
      "published": "2025-10-02T00:38:18Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01545v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出预测性偏好学习(PPL)方法，通过人类干预的隐式偏好信号预测未来状态，将专家修正传播至安全关键区域，显著提升学习效率并减少人工演示需求。在自动驾驶和机器人操作基准测试中验证了方法的有效性和通用性。",
      "order": 132
    },
    {
      "arxiv_id": "2510.01544v1",
      "title": "Step-Aware Policy Optimization for Reasoning in Diffusion Large Language\n  Models",
      "summary": "Diffusion language models (dLLMs) offer a promising, non-autoregressive\nparadigm for text generation, yet training them for complex reasoning remains a\nkey challenge. Current reinforcement learning approaches often rely on sparse,\noutcome-based rewards, which can reinforce flawed reasoning paths that lead to\ncoincidentally correct answers. We argue that this stems from a fundamental\nmismatch with the natural structure of reasoning. We first propose a\ntheoretical framework that formalizes complex problem solving as a hierarchical\nselection process, where an intractable global constraint is decomposed into a\nseries of simpler, localized logical steps. This framework provides a\nprincipled foundation for algorithm design, including theoretical insights into\nthe identifiability of this latent reasoning structure. Motivated by this\ntheory, we identify unstructured refinement -- a failure mode where a model's\niterative steps do not contribute meaningfully to the solution -- as a core\ndeficiency in existing methods. We then introduce Step-Aware Policy\nOptimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising\nprocess with the latent reasoning hierarchy. By using a process-based reward\nfunction that encourages incremental progress, SAPO guides the model to learn\nstructured, coherent reasoning paths. Our empirical results show that this\nprincipled approach significantly improves performance on challenging reasoning\nbenchmarks and enhances the interpretability of the generation process.",
      "authors": [
        "Shaoan Xie",
        "Lingjing Kong",
        "Xiangchen Song",
        "Xinshuai Dong",
        "Guangyi Chen",
        "Eric P. Xing",
        "Kun Zhang"
      ],
      "published": "2025-10-02T00:34:15Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01544v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出步感知策略优化(SAPO)方法，针对扩散语言模型在复杂推理任务中的训练挑战。通过理论框架将推理建模为层次化选择过程，并设计基于过程的奖励函数来引导模型学习结构化推理路径，显著提升推理性能与生成过程可解释性。",
      "order": 133
    },
    {
      "arxiv_id": "2510.01531v1",
      "title": "Information Seeking for Robust Decision Making under Partial\n  Observability",
      "summary": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io",
      "authors": [
        "Djengo Cyun-Jyun Fang",
        "Tsung-Wei Ke"
      ],
      "published": "2025-10-02T00:06:32Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01531v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出InfoSeeker框架，通过将任务规划与信息寻求相结合，使LLM在部分可观测环境中主动收集信息以验证理解、检测环境变化或测试假设，从而在不确定环境下做出最优决策。实验表明该方法相比现有方法性能提升74%，并在机器人操作和网页导航等基准测试中表现优异。",
      "order": 134
    },
    {
      "arxiv_id": "2510.01530v1",
      "title": "LOGicalThought: Logic-Based Ontological Grounding of LLMs for\n  High-Assurance Reasoning",
      "summary": "High-assurance reasoning, particularly in critical domains such as law and\nmedicine, requires conclusions that are accurate, verifiable, and explicitly\ngrounded in evidence. This reasoning relies on premises codified from rules,\nstatutes, and contracts, inherently involving defeasible or non-monotonic logic\ndue to numerous exceptions, where the introduction of a single fact can\ninvalidate general rules, posing significant challenges. While large language\nmodels (LLMs) excel at processing natural language, their capabilities in\nstandard inference tasks do not translate to the rigorous reasoning required\nover high-assurance text guidelines. Core reasoning challenges within such\ntexts often manifest specific logical structures involving negation,\nimplication, and, most critically, defeasible rules and exceptions. In this\npaper, we propose a novel neurosymbolically-grounded architecture called\nLOGicalThought (LogT) that uses an advanced logical language and reasoner in\nconjunction with an LLM to construct a dual symbolic graph context and\nlogic-based context. These two context representations transform the problem\nfrom inference over long-form guidelines into a compact grounded evaluation.\nEvaluated on four multi-domain benchmarks against four baselines, LogT improves\noverall performance by 11.84% across all LLMs. Performance improves\nsignificantly across all three modes of reasoning: by up to +10.2% on negation,\n+13.2% on implication, and +5.5% on defeasible reasoning compared to the\nstrongest baseline.",
      "authors": [
        "Navapat Nananukul",
        "Yue Zhang",
        "Ryan Lee",
        "Eric Boxer",
        "Jonathan May",
        "Vibhav Giridhar Gogate",
        "Jay Pujara",
        "Mayank Kejriwal"
      ],
      "published": "2025-10-02T00:06:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01530v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "legal_ai",
      "tldr_zh": "LOGicalThought提出一种神经符号架构，结合逻辑语言推理器与大语言模型，构建双重上下文表示，将长文本推理转化为紧凑的基于逻辑的评估。在四个多领域基准测试中，相比基线模型性能提升11.84%，在否定推理、蕴含推理和可废止推理方面分别提升10.2%、13.2%和5.5%，特别适用于法律等高可靠性领域。",
      "order": 135
    },
    {
      "arxiv_id": "2510.01528v1",
      "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse\n  Autoencoder-Guided Generation",
      "summary": "We propose a novel method that leverages sparse autoencoders (SAEs) and\nclustering techniques to analyze the internal token representations of large\nlanguage models (LLMs) and guide generations in mathematical reasoning tasks.\nOur approach first trains an SAE to generate sparse vector representations for\ntraining tokens, then applies k-means clustering to construct a graph where\nvertices represent token clusters and weighted edges capture sequential token\ntransitions. Using this graph, we define an edge-weight based reward function\nto quantify adherence to established reasoning traces, thereby identifying\nexploitative reasoning trajectories. Additionally, we measure generation\ndiversity from clustering to assess the extent of exploration. Our findings\nindicate that balancing both exploitation and exploration is crucial for\nachieving high accuracy in mathematical reasoning tasks. During generation, the\nSAE can serve as a scalable reward model to guide generations, ensuring a\nbalanced trade-off between exploitation and exploration. This prevents extreme\nbehaviors in either direction, ultimately fostering a higher-quality reasoning\nprocess in LLMs.",
      "authors": [
        "Daniel Zhao",
        "Abhilash Shankarampeta",
        "Lanxiang Hu",
        "Tajana Rosing",
        "Hao Zhang"
      ],
      "published": "2025-10-02T00:01:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01528v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种利用稀疏自编码器和聚类技术分析大语言模型内部表征的新方法，通过构建词元转移图定义奖励函数来平衡推理过程中的利用与探索，从而提升数学推理任务的准确性和可解释性。",
      "order": 136
    },
    {
      "arxiv_id": "2510.01524v1",
      "title": "WALT: Web Agents that Learn Tools",
      "summary": "Web agents promise to automate complex browser tasks, but current methods\nremain brittle -- relying on step-by-step UI interactions and heavy LLM\nreasoning that break under dynamic layouts and long horizons. Humans, by\ncontrast, exploit website-provided functionality through high-level operations\nlike search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),\na framework that reverse-engineers latent website functionality into reusable\ninvocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust\nimplementations of automations already designed into websites -- spanning\ndiscovery (search, filter, sort), communication (post, comment, upvote), and\ncontent management (create, edit, delete). Tools abstract away low-level\nexecution: instead of reasoning about how to click and type, agents simply call\nsearch(query) or create(listing). This shifts the computational burden from\nfragile step-by-step reasoning to reliable tool invocation. On VisualWebArena\nand WebArena, WALT achieves higher success with fewer steps and less\nLLM-dependent reasoning, establishing a robust and generalizable paradigm for\nbrowser automation.",
      "authors": [
        "Viraj Prabhu",
        "Yutong Dai",
        "Matthew Fernandez",
        "Jing Gu",
        "Krithika Ramakrishnan",
        "Yanqi Luo",
        "Silvio Savarese",
        "Caiming Xiong",
        "Junnan Li",
        "Zeyuan Chen",
        "Ran Xu"
      ],
      "published": "2025-10-01T23:41:47Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01524v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "WALT提出了一种网络代理框架，通过逆向工程将网站功能转化为可复用的工具，使代理能够直接调用高级操作（如搜索、筛选、排序），而非依赖脆弱的逐步UI交互。该方法在VisualWebArena和WebArena测试中实现了更高成功率、更少步骤和更低LLM依赖，为浏览器自动化提供了稳健且可推广的范式。",
      "order": 137
    },
    {
      "arxiv_id": "2510.01520v1",
      "title": "Predictive Modeling and Explainable AI for Veterinary Safety Profiles,\n  Residue Assessment, and Health Outcomes Using Real-World Data and\n  Physicochemical Properties",
      "summary": "The safe use of pharmaceuticals in food-producing animals is vital to protect\nanimal welfare and human food safety. Adverse events (AEs) may signal\nunexpected pharmacokinetic or toxicokinetic effects, increasing the risk of\nviolative residues in the food chain. This study introduces a predictive\nframework for classifying outcomes (Death vs. Recovery) using ~1.28 million\nreports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary\nMedicine. A preprocessing pipeline merged relational tables and standardized\nAEs through VeDDRA ontologies. Data were normalized, missing values imputed,\nand high-cardinality features reduced; physicochemical drug properties were\nintegrated to capture chemical-residue links. We evaluated supervised models,\nincluding Random Forest, CatBoost, XGBoost, ExcelFormer, and large language\nmodels (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as\nundersampling and oversampling, with a focus on prioritizing recall for fatal\noutcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,\nachieving precision, recall, and F1-scores of 0.95. Incorporating Average\nUncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved\nminority-class detection, particularly in ExcelFormer and XGBoost.\nInterpretability via SHAP identified biologically plausible predictors,\nincluding lung, heart, and bronchial disorders, animal demographics, and drug\nphysicochemical properties. These features were strongly linked to fatal\noutcomes. Overall, the framework shows that combining rigorous data\nengineering, advanced machine learning, and explainable AI enables accurate,\ninterpretable predictions of veterinary safety outcomes. The approach supports\nFARAD's mission by enabling early detection of high-risk drug-event profiles,\nstrengthening residue risk assessment, and informing regulatory and clinical\ndecision-making.",
      "authors": [
        "Hossein Sholehrasa",
        "Xuan Xu",
        "Doina Caragea",
        "Jim E. Riviere",
        "Majid Jaberi-Douraki"
      ],
      "published": "2025-10-01T23:34:46Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01520v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究开发了一个结合真实世界数据和药物理化性质的预测框架，用于兽医安全结果分类。基于FDA约128万份报告，采用多种机器学习模型和集成方法，在死亡与康复分类中达到0.95的精确度、召回率和F1分数。通过SHAP可解释性分析识别出与致命结果相关的生物合理性预测因子，支持早期高风险药物事件检测和监管决策。",
      "order": 138
    },
    {
      "arxiv_id": "2510.01513v1",
      "title": "From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods\n  for Multimodal Content Analysis and Understanding",
      "summary": "Analysis of multi-modal content can be tricky, computationally expensive, and\nrequire a significant amount of engineering efforts. Lots of work with\npre-trained models on static data is out there, yet fusing these opensource\nmodels and methods with complex data such as videos is relatively challenging.\nIn this paper, we present a framework that enables efficiently prototyping\npipelines for multi-modal content analysis. We craft a candidate recipe for a\npipeline, marrying a set of pre-trained models, to convert videos into a\ntemporal semi-structured data format. We translate this structure further to a\nframe-level indexed knowledge graph representation that is query-able and\nsupports continual learning, enabling the dynamic incorporation of new\ndomain-specific knowledge through an interactive medium.",
      "authors": [
        "Basem Rizk",
        "Joel Walsh",
        "Mark Core",
        "Benjamin Nye"
      ],
      "published": "2025-10-01T23:20:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01513v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一个多模态内容分析框架，通过整合预训练模型将视频转换为时序半结构化数据，并进一步构建可查询的帧级索引知识图谱，支持持续学习和动态融入领域知识。",
      "order": 139
    },
    {
      "arxiv_id": "2510.01500v1",
      "title": "Lateral Tree-of-Thoughts Surpasses ToT by Incorporating\n  Logically-Consistent, Low-Utility Candidates",
      "summary": "Modern deployments increasingly allocate large test-time compute (thousands\nof tokens or many node expansions) to boost reliability. Under such budgets,\nstandard Tree-of-Thoughts-style search exhibits two pathologies: breadth\nsaturation (additional samples mostly produce near-duplicates, so width stops\ngrowing) and depth myopia (noisy short-horizon utilities prune branches whose\npayoff appears after a few more steps). We propose Lateral Tree-of-Thoughts\n(LToT), a drop-in controller that separates utility from logical consistency\nand treats low-utility but consistent candidates as assets rather than waste.\nThe frontier is split into mainlines (high-utility candidates used for\nexploitation) and laterals (consistent, initially low-utility candidates that\nreceive short, cheap probes before judgment). LToT explores laterals via\nLateral Racing with Short-Circuit (LR--SC): a capped successive-halving race\nthat spreads tiny probes across a very wide lateral set, uses width-aware\nthresholds with repeat-to-confirm, and immediately promotes a branch once its\nenvelope clears the mainline bar; mainlines are kept intentionally narrow so\nsurplus compute is invested where width is cheap. We prove a pseudolinear\nlateral cost $\\Theta(N_0 \\log_{\\eta} N_0)$ with logarithmically many rungs\n(initial lateral width $N_0$; culling factor $\\eta>1$), in contrast to the\nexponential growth of uncapped mainlines. Empirical evaluations on benchmark\ntasks are in preparation and will be added in a future revision. In short, LToT\nturns large test-time budgets into principled diversity while preserving\npromotion discipline, mitigating saturation and myopia without inflating\ncompute.",
      "authors": [
        "Abhinav Madahar"
      ],
      "published": "2025-10-01T22:23:58Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01500v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出横向思维树(LToT)方法，通过分离逻辑一致性与效用评估，将低效用但逻辑一致的候选方案作为探索资产而非废弃项。该方法采用横向竞速与短路机制，在保持主线窄化的同时广泛探测横向分支，以伪线性成本解决传统思维树方法的广度饱和与深度近视问题，显著提升大计算预算下的推理多样性。",
      "order": 140
    },
    {
      "arxiv_id": "2510.01499v1",
      "title": "Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order\n  Information",
      "summary": "With the rapid progress of multi-agent large language model (LLM) reasoning,\nhow to effectively aggregate answers from multiple LLMs has emerged as a\nfundamental challenge. Standard majority voting treats all answers equally,\nfailing to consider latent heterogeneity and correlation across models. In this\nwork, we design two new aggregation algorithms called Optimal Weight (OW) and\nInverse Surprising Popularity (ISP), leveraging both first-order and\nsecond-order information. Our theoretical analysis shows these methods provably\nmitigate inherent limitations of majority voting under mild assumptions,\nleading to more reliable collective decisions. We empirically validate our\nalgorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as\nUltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all\ncases, our methods consistently outperform majority voting, offering both\npractical performance gains and conceptual insights for the design of robust\nmulti-agent LLM pipelines.",
      "authors": [
        "Rui Ai",
        "Yuqi Pan",
        "David Simchi-Levi",
        "Milind Tambe",
        "Haifeng Xu"
      ],
      "published": "2025-10-01T22:21:50Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01499v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出两种超越多数投票的LLM聚合算法——最优权重(OW)和逆惊奇流行度(ISP)，通过利用一阶和二阶信息解决多智能体LLM推理中的答案聚合问题。理论分析和在合成数据集、UltraFeedback/MMLU基准及ARMMAN医疗场景的实验表明，新方法能显著提升集体决策的可靠性。",
      "order": 141
    },
    {
      "arxiv_id": "2510.01498v1",
      "title": "AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA\n  Imaging",
      "summary": "While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic\naneurysms (AAA), the required iodinated contrast agents pose significant risks,\nincluding nephrotoxicity, patient allergies, and environmental harm. To reduce\ncontrast agent use, recent deep learning methods have focused on generating\nsynthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a\nmulti-stage pipeline that first generates images and then performs\nsegmentation, which leads to error accumulation and fails to leverage shared\nsemantic and anatomical structures. To address this, we propose a unified deep\nlearning framework that generates synthetic CECT images from NCCT scans while\nsimultaneously segmenting the aortic lumen and thrombus. Our approach\nintegrates conditional diffusion models (CDM) with multi-task learning,\nenabling end-to-end joint optimization of image synthesis and anatomical\nsegmentation. Unlike previous multitask diffusion models, our approach requires\nno initial predictions (e.g., a coarse segmentation mask), shares both encoder\nand decoder parameters across tasks, and employs a semi-supervised training\nstrategy to learn from scans with missing segmentation labels, a common\nconstraint in real-world clinical data. We evaluated our method on a cohort of\n264 patients, where it consistently outperformed state-of-the-art single-task\nand multi-stage models. For image synthesis, our model achieved a PSNR of 25.61\ndB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,\nit improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus\nDice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to\nmore accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm\nfrom 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to\nnnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.",
      "authors": [
        "Yuxuan Ou",
        "Ning Bi",
        "Jiazhen Pan",
        "Jiancheng Yang",
        "Boliang Yu",
        "Usama Zidan",
        "Regent Lee",
        "Vicente Grau"
      ],
      "published": "2025-10-01T22:19:27Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01498v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "AortaDiff提出统一多任务扩散框架，从非增强CT生成合成对比增强CT图像并同时分割主动脉腔和血栓。该方法结合条件扩散模型与多任务学习，在264名患者数据上验证，在图像合成质量和解剖分割精度上均优于现有单任务和多阶段模型。",
      "order": 142
    },
    {
      "arxiv_id": "2510.01494v1",
      "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks\n  Fail Where Data-Space Attacks Succeed",
      "summary": "The field of adversarial robustness has long established that adversarial\nexamples can successfully transfer between image classifiers and that text\njailbreaks can successfully transfer between language models (LMs). However, a\npair of recent studies reported being unable to successfully transfer image\njailbreaks between vision-language models (VLMs). To explain this striking\ndifference, we propose a fundamental distinction regarding the transferability\nof attacks against machine learning models: attacks in the input data-space can\ntransfer, whereas attacks in model representation space do not, at least not\nwithout geometric alignment of representations. We then provide theoretical and\nempirical evidence of this hypothesis in four different settings. First, we\nmathematically prove this distinction in a simple setting where two networks\ncompute the same input-output map but via different representations. Second, we\nconstruct representation-space attacks against image classifiers that are as\nsuccessful as well-known data-space attacks, but fail to transfer. Third, we\nconstruct representation-space attacks against LMs that successfully jailbreak\nthe attacked models but again fail to transfer. Fourth, we construct data-space\nattacks against VLMs that successfully transfer to new VLMs, and we show that\nrepresentation space attacks \\emph{can} transfer when VLMs' latent geometries\nare sufficiently aligned in post-projector space. Our work reveals that\nadversarial transfer is not an inherent property of all attacks but contingent\non their operational domain - the shared data-space versus models' unique\nrepresentation spaces - a critical insight for building more robust models.",
      "authors": [
        "Isha Gupta",
        "Rylan Schaeffer",
        "Joshua Kazdan",
        "Ken Liu",
        "Sanmi Koyejo"
      ],
      "published": "2025-10-01T22:10:58Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01494v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文揭示了对抗性攻击迁移性的关键区别：数据空间攻击可迁移，而表示空间攻击无法迁移（除非表示几何对齐）。通过理论证明和四个实验场景验证，发现对抗迁移性取决于攻击操作域——共享数据空间vs模型独有表示空间，为构建更鲁棒模型提供重要见解。",
      "order": 143
    },
    {
      "arxiv_id": "2510.01483v1",
      "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification\n  using Spatiotemporal Knowledge Graphs",
      "summary": "Vision-language models (VLMs) have shown potential for robot navigation but\nencounter fundamental limitations: they lack persistent scene memory, offer\nlimited spatial reasoning, and do not scale effectively with video duration for\nreal-time application. We present VL-KnG, a Visual Scene Understanding system\nthat tackles these challenges using spatiotemporal knowledge graph construction\nand computationally efficient query processing for navigation goal\nidentification. Our approach processes video sequences in chunks utilizing\nmodern VLMs, creates persistent knowledge graphs that maintain object identity\nover time, and enables explainable spatial reasoning through queryable graph\nstructures. We also introduce WalkieKnowledge, a new benchmark with about 200\nmanually annotated questions across 8 diverse trajectories spanning\napproximately 100 minutes of video data, enabling fair comparison between\nstructured approaches and general-purpose VLMs. Real-world deployment on a\ndifferential drive robot demonstrates practical applicability, with our method\nachieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5\nPro performance while providing explainable reasoning supported by the\nknowledge graph, computational efficiency for real-time deployment across\ndifferent tasks, such as localization, navigation and planning. Code and\ndataset will be released after acceptance.",
      "authors": [
        "Mohamad Al Mdfaa",
        "Svetlana Lukina",
        "Timur Akhtyamov",
        "Arthur Nigmatzyanov",
        "Dmitrii Nalberskii",
        "Sergey Zagoruyko",
        "Gonzalo Ferrer"
      ],
      "published": "2025-10-01T21:53:44Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01483v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "VL-KnG提出一种基于时空知识图谱的视觉场景理解系统，用于导航目标识别。该系统通过分块处理视频序列构建持久知识图谱，保持对象身份一致性，支持可解释的空间推理。在真实机器人部署中达到77.27%成功率，性能媲美Gemini 2.5 Pro，同时具备实时计算效率和可解释性优势。",
      "order": 144
    },
    {
      "arxiv_id": "2510.01480v1",
      "title": "Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules",
      "summary": "The integration of artificial intelligence (AI) in early-stage drug discovery\noffers unprecedented opportunities for exploring chemical space and\naccelerating hit-to-lead optimization. However, docking optimization in\ngenerative approaches is computationally expensive and may lead to inaccurate\nresults. Here, we present a novel generative framework that balances\npharmacophore similarity to reference compounds with structural diversity from\nactive molecules. The framework allows users to provide custom reference sets,\nincluding FDA-approved drugs or clinical candidates, and guides the \\textit{de\nnovo} generation of potential therapeutics. We demonstrate its applicability\nthrough a case study targeting estrogen receptor modulators and antagonists for\nbreast cancer. The generated compounds maintain high pharmacophoric fidelity to\nknown active molecules while introducing substantial structural novelty,\nsuggesting strong potential for functional innovation and patentability.\nComprehensive evaluation of the generated molecules against common drug-like\nproperties confirms the robustness and pharmaceutical relevance of the\napproach.",
      "authors": [
        "Ekaterina Podplutova",
        "Anastasia Vepreva",
        "Olga A. Konovalova",
        "Vladimir Vinogradov",
        "Dmitrii O. Shkil",
        "Andrei Dmitrenko"
      ],
      "published": "2025-10-01T21:45:58Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.01480v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出一种基于药效团指导的生成式AI框架，用于设计新型药物分子。该方法平衡参考化合物的药效团相似性与活性分子的结构多样性，通过雌激素受体调节剂的案例验证，生成分子既保持高药效团保真度又具备结构新颖性，在乳腺癌治疗领域展现创新潜力。",
      "order": 145
    },
    {
      "arxiv_id": "2510.01478v1",
      "title": "Purrception: Variational Flow Matching for Vector-Quantized Image\n  Generation",
      "summary": "We introduce Purrception, a variational flow matching approach for\nvector-quantized image generation that provides explicit categorical\nsupervision while maintaining continuous transport dynamics. Our method adapts\nVariational Flow Matching to vector-quantized latents by learning categorical\nposteriors over codebook indices while computing velocity fields in the\ncontinuous embedding space. This combines the geometric awareness of continuous\nmethods with the discrete supervision of categorical approaches, enabling\nuncertainty quantification over plausible codes and temperature-controlled\ngeneration. We evaluate Purrception on ImageNet-1k 256x256 generation. Training\nconverges faster than both continuous flow matching and discrete flow matching\nbaselines while achieving competitive FID scores with state-of-the-art models.\nThis demonstrates that Variational Flow Matching can effectively bridge\ncontinuous transport and discrete supervision for improved training efficiency\nin image generation.",
      "authors": [
        "Răzvan-Andrei Matişan",
        "Vincent Tao Hu",
        "Grigory Bartosh",
        "Björn Ommer",
        "Cees G. M. Snoek",
        "Max Welling",
        "Jan-Willem van de Meent",
        "Mohammad Mahdi Derakhshani",
        "Floor Eijkelboom"
      ],
      "published": "2025-10-01T21:41:30Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01478v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "Purrception是一种用于矢量量化图像生成的变分流匹配方法，通过在连续嵌入空间学习速度场的同时学习码书索引的类别后验，将连续方法的几何感知与离散监督相结合。该方法在ImageNet-1k 256×256生成任务上训练收敛速度优于基线，同时达到竞争力的FID分数。",
      "order": 146
    },
    {
      "arxiv_id": "2510.01474v1",
      "title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation\n  Compliance",
      "summary": "As governments move to regulate AI, there is growing interest in using Large\nLanguage Models (LLMs) to assess whether or not an AI system complies with a\ngiven AI Regulation (AIR). However, there is presently no way to benchmark the\nperformance of LLMs at this task. To fill this void, we introduce AIReg-Bench:\nthe first benchmark dataset designed to test how well LLMs can assess\ncompliance with the EU AI Act (AIA). We created this dataset through a two-step\nprocess: (1) by prompting an LLM with carefully structured instructions, we\ngenerated 120 technical documentation excerpts (samples), each depicting a\nfictional, albeit plausible, AI system - of the kind an AI provider might\nproduce to demonstrate their compliance with AIR; (2) legal experts then\nreviewed and annotated each sample to indicate whether, and in what way, the AI\nsystem described therein violates specific Articles of the AIA. The resulting\ndataset, together with our evaluation of whether frontier LLMs can reproduce\nthe experts' compliance labels, provides a starting point to understand the\nopportunities and limitations of LLM-based AIR compliance assessment tools and\nestablishes a benchmark against which subsequent LLMs can be compared. The\ndataset and evaluation code are available at\nhttps://github.com/camlsys/aireg-bench.",
      "authors": [
        "Bill Marino",
        "Rosco Hunter",
        "Zubair Jamali",
        "Marinos Emmanouil Kalpakos",
        "Mudra Kashyap",
        "Isaiah Hinton",
        "Alexa Hanson",
        "Maahum Nazir",
        "Christoph Schnabl",
        "Felix Steffek",
        "Hongkai Wen",
        "Nicholas D. Lane"
      ],
      "published": "2025-10-01T21:33:33Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01474v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "legal_ai",
      "tldr_zh": "AIReg-Bench是首个用于评估大语言模型在AI法规合规性判断能力的基准数据集，专注于欧盟AI法案。通过LLM生成技术文档片段并由法律专家标注违规情况，为开发基于LLM的合规评估工具提供基准测试框架。",
      "order": 147
    },
    {
      "arxiv_id": "2510.01473v1",
      "title": "From keywords to semantics: Perceptions of large language models in data\n  discovery",
      "summary": "Current approaches to data discovery match keywords between metadata and\nqueries. This matching requires researchers to know the exact wording that\nother researchers previously used, creating a challenging process that could\nlead to missing relevant data. Large Language Models (LLMs) could enhance data\ndiscovery by removing this requirement and allowing researchers to ask\nquestions with natural language. However, we do not currently know if\nresearchers would accept LLMs for data discovery. Using a human-centered\nartificial intelligence (HCAI) focus, we ran focus groups (N = 27) to\nunderstand researchers' perspectives towards LLMs for data discovery. Our\nconceptual model shows that the potential benefits are not enough for\nresearchers to use LLMs instead of current technology. Barriers prevent\nresearchers from fully accepting LLMs, but features around transparency could\novercome them. Using our model will allow developers to incorporate features\nthat result in an increased acceptance of LLMs for data discovery.",
      "authors": [
        "Maura E Halstead",
        "Mark A. Green",
        "Caroline Jay",
        "Richard Kingston",
        "David Topping",
        "Alexander Singleton"
      ],
      "published": "2025-10-01T21:31:54Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.01473v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过焦点小组(N=27)探讨研究人员对使用大语言模型进行数据发现的接受度。研究发现，尽管LLMs能通过自然语言查询改善传统关键词匹配的局限性，但研究人员不会仅因潜在优势就放弃现有技术。透明性相关的功能设计是克服使用障碍、提升接受度的关键因素。",
      "order": 148
    },
    {
      "arxiv_id": "2510.01462v1",
      "title": "RealClass: A Framework for Classroom Speech Simulation with Public\n  Datasets and Game Engines",
      "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Classroom datasets remain\nlimited and not publicly available, and the absence of dedicated classroom\nnoise or Room Impulse Response (RIR) corpora prevents the use of standard data\naugmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise and RIRs using game engines, a versatile framework that can extend to\nother domains beyond the classroom. Building on this methodology, we present\nRealClass, a dataset that combines a synthesized classroom noise corpus with a\nclassroom speech dataset compiled from publicly available corpora. The speech\ndata pairs a children's speech corpus with instructional speech extracted from\nYouTube videos to approximate real classroom interactions in clean conditions.\nExperiments on clean and noisy speech show that RealClass closely approximates\nreal classroom speech, making it a valuable asset in the absence of abundant\nreal classroom speech.",
      "authors": [
        "Ahmed Adel Attia",
        "Jing Liu",
        "Carol Espy Wilson"
      ],
      "published": "2025-10-01T21:04:51Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01462v1",
      "primary_area": "audio_models",
      "secondary_focus": "training_optimization",
      "application_domain": "education_ai",
      "tldr_zh": "RealClass是一个利用游戏引擎和公开数据集合成教室语音的框架，通过生成教室噪声和房间脉冲响应，解决了教育领域AI语音模型训练数据稀缺的问题，实验证明其能有效模拟真实教室语音环境。",
      "order": 149
    },
    {
      "arxiv_id": "2510.01460v1",
      "title": "The Three Regimes of Offline-to-Online Reinforcement Learning",
      "summary": "Offline-to-online reinforcement learning (RL) has emerged as a practical\nparadigm that leverages offline datasets for pretraining and online\ninteractions for fine-tuning. However, its empirical behavior is highly\ninconsistent: design choices of online-fine tuning that work well in one\nsetting can fail completely in another. We propose a stability--plasticity\nprinciple that can explain this inconsistency: we should preserve the knowledge\nof pretrained policy or offline dataset during online fine-tuning, whichever is\nbetter, while maintaining sufficient plasticity. This perspective identifies\nthree regimes of online fine-tuning, each requiring distinct stability\nproperties. We validate this framework through a large-scale empirical study,\nfinding that the results strongly align with its predictions in 45 of 63 cases.\nThis work provides a principled framework for guiding design choices in\noffline-to-online RL based on the relative performance of the offline dataset\nand the pretrained policy.",
      "authors": [
        "Lu Li",
        "Tianwei Ni",
        "Yihao Sun",
        "Pierre-Luc Bacon"
      ],
      "published": "2025-10-01T20:58:14Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01460v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出稳定性-可塑性原则来解释离线到在线强化学习中的不一致性，识别了在线微调的三种机制，并通过大规模实证研究验证了该框架在45/63案例中的预测准确性，为基于离线数据集与预训练策略相对性能的设计选择提供了理论指导。",
      "order": 150
    },
    {
      "arxiv_id": "2510.01453v1",
      "title": "The Command Line GUIde: Graphical Interfaces from Man Pages via AI",
      "summary": "Although birthed in the era of teletypes, the command line shell survived the\ngraphical interface revolution of the 1980's and lives on in modern desktop\noperating systems. The command line provides access to powerful functionality\nnot otherwise exposed on the computer, but requires users to recall textual\nsyntax and carefully scour documentation. In contrast, graphical interfaces let\nusers organically discover and invoke possible actions through widgets and\nmenus. To better expose the power of the command line, we demonstrate a\nmechanism for automatically creating graphical interfaces for command line\ntools by translating their documentation (in the form of man pages) into\ninterface specifications via AI. Using these specifications, our user-facing\nsystem, called GUIde, presents the command options to the user graphically. We\nevaluate the generated interfaces on a corpus of commands to show to what\ndegree GUIde offers thorough graphical interfaces for users' real-world command\nline tasks.",
      "authors": [
        "Saketh Ram Kasibatla",
        "Kiran Medleri Hiremath",
        "Raven Rothkopf",
        "Sorin Lerner",
        "Haijun Xia",
        "Brian Hempel"
      ],
      "published": "2025-10-01T20:46:53Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.01453v1",
      "primary_area": "text_models",
      "secondary_focus": "code_generation",
      "application_domain": "general_purpose",
      "tldr_zh": "GUIde系统利用AI技术将命令行工具的手册页自动转换为图形界面规范，通过可视化组件展示命令选项，降低用户记忆负担并提升命令行工具的可发现性。该系统评估了多命令的界面生成效果，证明能为真实世界命令行任务提供全面的图形界面支持。",
      "order": 151
    },
    {
      "arxiv_id": "2510.01450v1",
      "title": "Local Linear Attention: An Optimal Interpolation of Linear and Softmax\n  Attention For Test-Time Regression",
      "summary": "Transformer architectures have achieved remarkable success in various\ndomains. While efficient alternatives to Softmax Attention have been widely\nstudied, the search for more expressive mechanisms grounded in theoretical\ninsight-even at greater computational cost-has been relatively underexplored.\nIn this work, we bridge this gap by proposing Local Linear Attention (LLA), a\nnovel attention mechanism derived from nonparametric statistics through the\nlens of test-time regression. First, we show that LLA offers theoretical\nadvantages over Linear and Softmax Attention for associative memory via a\nbias-variance trade-off analysis. Next, we address its computational challenges\nand propose two memory-efficient primitives to tackle the $\\Theta(n^2 d)$ and\n$\\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,\nblockwise algorithm that enables scalable and parallel computation on modern\naccelerators. In addition, we implement and profile a customized inference\nkernel that significantly reduces memory overheads. Finally, we empirically\nvalidate the advantages and limitations of LLA on test-time regression,\nin-context regression, associative recall and state tracking tasks. Experiment\nresults demonstrate that LLA effectively adapts to non-stationarity,\noutperforming strong baselines in test-time training and in-context learning,\nand exhibiting promising evidence for its scalability and applicability in\nlarge-scale models. Code is available at\nhttps://github.com/Yifei-Zuo/Flash-LLA.",
      "authors": [
        "Yifei Zuo",
        "Yutong Yin",
        "Zhichen Zeng",
        "Ang Li",
        "Banghua Zhu",
        "Zhaoran Wang"
      ],
      "published": "2025-10-01T20:42:21Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01450v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出局部线性注意力(LLA)，一种基于非参数统计和测试时回归的新型注意力机制。LLA在线性与Softmax注意力间实现最优插值，通过偏差-方差权衡分析展示理论优势，并开发FlashLLA算法解决计算复杂度问题。实验证明LLA在测试时回归、上下文学习等任务中有效适应非平稳性，优于基线方法。",
      "order": 152
    },
    {
      "arxiv_id": "2510.01448v1",
      "title": "GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of\n  Geographic Embeddings",
      "summary": "Worldwide visual geo-localization seeks to determine the geographic location\nof an image anywhere on Earth using only its visual content. Learned\nrepresentations of geography for visual geo-localization remain an active\nresearch topic despite much progress. We formulate geo-localization as aligning\nthe visual representation of the query image with a learned geographic\nrepresentation. Our novel geographic representation explicitly models the world\nas a hierarchy of geographic embeddings. Additionally, we introduce an approach\nto efficiently fuse the appearance features of the query image with its\nsemantic segmentation map, forming a robust visual representation. Our main\nexperiments demonstrate improved all-time bests in 22 out of 25 metrics\nmeasured across five benchmark datasets compared to prior state-of-the-art\n(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional\nablation studies support the claim that these gains are primarily driven by the\ncombination of geographic and visual representations.",
      "authors": [
        "Angel Daruna",
        "Nicholas Meegan",
        "Han-Pang Chiu",
        "Supun Samarasekera",
        "Rakesh Kumar"
      ],
      "published": "2025-10-01T20:39:48Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01448v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "GeoSURGE提出了一种新颖的地理定位方法，通过层次化地理嵌入和视觉-语义特征融合，在五个基准数据集上22/25个指标超越现有最佳方法，主要突破来自地理表示与视觉表征的有效结合。",
      "order": 153
    },
    {
      "arxiv_id": "2510.01444v1",
      "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
      "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce $\\textbf{VOGUE (Visual Uncertainty Guided\nExploration)}$, a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.",
      "authors": [
        "Rui Liu",
        "Dian Yu",
        "Tong Zheng",
        "Runpeng Dai",
        "Zongxia Li",
        "Wenhao Yu",
        "Zhenwen Liang",
        "Linfeng Song",
        "Haitao Mi",
        "Pratap Tokekar",
        "Dong Yu"
      ],
      "published": "2025-10-01T20:32:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01444v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "VOGUE提出一种新颖的多模态强化学习方法，通过量化视觉输入的不确定性来引导探索过程。该方法将图像视为随机上下文，使用对称KL散度衡量策略对视觉扰动的敏感性，结合不确定性奖励和退火采样，在多个视觉数学和通用推理基准上显著提升性能，有效缓解RL微调中的探索衰减问题。",
      "order": 154
    },
    {
      "arxiv_id": "2510.01433v1",
      "title": "AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for\n  Generalizable and Lightweight Robotic Manipulation",
      "summary": "Vision-based robot learning often relies on dense image or point-cloud\ninputs, which are computationally heavy and entangle irrelevant background\nfeatures. Existing keypoint-based approaches can focus on manipulation-centric\nfeatures and be lightweight, but either depend on manual heuristics or\ntask-coupled selection, limiting scalability and semantic understanding. To\naddress this, we propose AFFORD2ACT, an affordance-guided framework that\ndistills a minimal set of semantic 2D keypoints from a text prompt and a single\nimage. AFFORD2ACT follows a three-stage pipeline: affordance filtering,\ncategory-level keypoint construction, and transformer-based policy learning\nwith embedded gating to reason about the most relevant keypoints, yielding a\ncompact 38-dimensional state policy that can be trained in 15 minutes, which\nperforms well in real-time without proprioception or dense representations.\nAcross diverse real-world manipulation tasks, AFFORD2ACT consistently improves\ndata efficiency, achieving an 82% success rate on unseen objects, novel\ncategories, backgrounds, and distractors.",
      "authors": [
        "Anukriti Singh",
        "Kasra Torshizi",
        "Khuzema Habib",
        "Kelin Yu",
        "Ruohan Gao",
        "Pratap Tokekar"
      ],
      "published": "2025-10-01T20:13:39Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01433v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "AFFORD2ACT提出了一种基于功能感知的机器人操作框架，通过文本提示和单张图像提取语义2D关键点，采用三阶段流程：功能过滤、类别级关键点构建和基于Transformer的策略学习。该方法仅需15分钟训练，生成38维紧凑策略，在未见过的物体、新类别和复杂背景下达到82%成功率，显著提升数据效率和实时性能。",
      "order": 155
    },
    {
      "arxiv_id": "2510.01432v1",
      "title": "On the Role of Domain Experts in Creating Effective Tutoring Systems",
      "summary": "The role that highly curated knowledge, provided by domain experts, could\nplay in creating effective tutoring systems is often overlooked within the AI\nfor education community. In this paper, we highlight this topic by discussing\ntwo ways such highly curated expert knowledge could help in creating novel\neducational systems. First, we will look at how one could use explainable AI\n(XAI) techniques to automatically create lessons. Most existing XAI methods are\nprimarily aimed at debugging AI systems. However, we will discuss how one could\nuse expert specified rules about solving specific problems along with novel XAI\ntechniques to automatically generate lessons that could be provided to\nlearners. Secondly, we will see how an expert specified curriculum for learning\na target concept can help develop adaptive tutoring systems, that can not only\nprovide a better learning experience, but could also allow us to use more\nefficient algorithms to create these systems. Finally, we will highlight the\nimportance of such methods using a case study of creating a tutoring system for\npollinator identification, where such knowledge could easily be elicited from\nexperts.",
      "authors": [
        "Sarath Sreedharan",
        "Kelsey Sikes",
        "Nathaniel Blanchard",
        "Lisa Mason",
        "Nikhil Krishnaswamy",
        "Jill Zarestky"
      ],
      "published": "2025-10-01T20:12:57Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01432v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "education_ai",
      "tldr_zh": "本文探讨领域专家知识在构建智能教学系统中的关键作用，提出利用可解释AI技术自动生成课程内容，并通过专家设计的课程体系开发自适应教学系统，最后以传粉昆虫识别教学系统为例验证该方法的有效性。",
      "order": 156
    },
    {
      "arxiv_id": "2510.01428v1",
      "title": "BioVERSE: Representation Alignment of Biomedical Modalities to LLMs for\n  Multi-Modal Reasoning",
      "summary": "Recent advances in large language models (LLMs) and biomedical foundation\nmodels (BioFMs) have achieved strong results in biological text reasoning,\nmolecular modeling, and single-cell analysis, yet they remain siloed in\ndisjoint embedding spaces, limiting cross-modal reasoning. We present BIOVERSE\n(Biomedical Vector Embedding Realignment for Semantic Engagement), a two-stage\napproach that adapts pretrained BioFMs as modality encoders and aligns them\nwith LLMs through lightweight, modality-specific projection layers. The\napproach first aligns each modality to a shared LLM space through independently\ntrained projections, allowing them to interoperate naturally, and then applies\nstandard instruction tuning with multi-modal data to bring them together for\ndownstream reasoning. By unifying raw biomedical data with knowledge embedded\nin LLMs, the approach enables zero-shot annotation, cross-modal question\nanswering, and interactive, explainable dialogue. Across tasks spanning\ncell-type annotation, molecular description, and protein function reasoning,\ncompact BIOVERSE configurations surpass larger LLM baselines while enabling\nricher, generative outputs than existing BioFMs, establishing a foundation for\nprincipled multi-modal biomedical reasoning.",
      "authors": [
        "Ching-Huei Tsou",
        "Michal Ozery-Flato",
        "Ella Barkan",
        "Diwakar Mahajan",
        "Ben Shapira"
      ],
      "published": "2025-10-01T20:07:36Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.01428v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "medical_ai",
      "tldr_zh": "BioVERSE提出两阶段方法，将预训练生物医学基础模型与大型语言模型对齐，通过轻量级投影层实现多模态推理。该方法在细胞类型标注、分子描述和蛋白质功能推理等任务中超越大型LLM基线，支持零样本标注、跨模态问答和可解释对话，为生物医学多模态推理奠定基础。",
      "order": 157
    },
    {
      "arxiv_id": "2510.01427v1",
      "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge\n  Mining",
      "summary": "At the core of Deep Research is knowledge mining, the task of extracting\nstructured information from massive unstructured text in response to user\ninstructions. Large language models (LLMs) excel at interpreting such\ninstructions but are prohibitively expensive to deploy at scale, while\ntraditional pipelines of classifiers and extractors remain efficient yet\nbrittle and unable to generalize to new tasks. We introduce Falconer, a\ncollaborative framework that combines the agentic reasoning of LLMs with\nlightweight proxy models for scalable knowledge mining. In Falconer, LLMs act\nas planners, decomposing user instructions into executable pipelines, and as\nannotators, generating supervision to train small proxies. The framework\nunifies classification and extraction into two atomic operations, get label and\nget span, enabling a single instruction-following model to replace multiple\ntask-specific components. To evaluate the consistency between proxy models\nincubated by Falconer and annotations provided by humans and large models, we\nconstruct new benchmarks covering both planning and end-to-end execution.\nExperiments show that Falconer closely matches state-of-the-art LLMs in\ninstruction-following accuracy while reducing inference cost by up to 90% and\naccelerating large-scale knowledge mining by more than 20x, offering an\nefficient and scalable foundation for Deep Research.",
      "authors": [
        "Sipeng Zhang",
        "Longfei Yun",
        "Zilong Wang",
        "Jingbo Shang",
        "Letian Peng"
      ],
      "published": "2025-10-01T20:06:48Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01427v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "Falconer框架结合LLM的智能规划与轻量级代理模型，实现可扩展的知识挖掘：LLM作为规划器分解指令并生成训练数据，小型代理模型执行分类和提取任务，在保持高准确率的同时降低90%推理成本，加速20倍大规模知识挖掘。",
      "order": 158
    },
    {
      "arxiv_id": "2510.01414v1",
      "title": "Risk Phase Transitions in Spiked Regression: Alignment Driven Benign and\n  Catastrophic Overfitting",
      "summary": "This paper analyzes the generalization error of minimum-norm interpolating\nsolutions in linear regression using spiked covariance data models. The paper\ncharacterizes how varying spike strengths and target-spike alignments can\naffect risk, especially in overparameterized settings. The study presents an\nexact expression for the generalization error, leading to a comprehensive\nclassification of benign, tempered, and catastrophic overfitting regimes based\non spike strength, the aspect ratio $c=d/n$ (particularly as $c \\to \\infty$),\nand target alignment. Notably, in well-specified aligned problems, increasing\nspike strength can surprisingly induce catastrophic overfitting before\nachieving benign overfitting. The paper also reveals that target-spike\nalignment is not always advantageous, identifying specific, sometimes\ncounterintuitive, conditions for its benefit or detriment. Alignment with the\nspike being detrimental is empirically demonstrated to persist in nonlinear\nmodels.",
      "authors": [
        "Jiping Li",
        "Rishi Sonthalia"
      ],
      "published": "2025-10-01T19:51:47Z",
      "primary_category": "stat.ML",
      "arxiv_url": "https://arxiv.org/abs/2510.01414v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究尖峰协方差数据模型中线性回归的最小范数插值解的泛化误差，揭示了尖峰强度、目标-尖峰对齐与过参数化设置下风险的相互作用。研究提出了泛化误差的精确表达式，并根据尖峰强度、纵横比和目标对齐对良性、温和和灾难性过拟合机制进行了全面分类。研究发现，在特定条件下增加尖峰强度可能意外引发灾难性过拟合，且目标-尖峰对齐并非总是有利，这些现象在非线性模型中同样存在。",
      "order": 159
    },
    {
      "arxiv_id": "2510.01409v1",
      "title": "OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity\n  Logs with Large Language Models",
      "summary": "System logs represent a valuable source of Cyber Threat Intelligence (CTI),\ncapturing attacker behaviors, exploited vulnerabilities, and traces of\nmalicious activity. Yet their utility is often limited by lack of structure,\nsemantic inconsistency, and fragmentation across devices and sessions.\nExtracting actionable CTI from logs therefore requires approaches that can\nreconcile noisy, heterogeneous data into coherent and interoperable\nrepresentations. We introduce OntoLogX, an autonomous Artificial Intelligence\n(AI) agent that leverages Large Language Models (LLMs) to transform raw logs\ninto ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a\nlightweight log ontology with Retrieval Augmented Generation (RAG) and\niterative correction steps, ensuring that generated KGs are syntactically and\nsemantically valid. Beyond event-level analysis, the system aggregates KGs into\nsessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level\nlog evidence to higher-level adversarial objectives. We evaluate OntoLogX on\nboth logs from a public benchmark and a real-world honeypot dataset,\ndemonstrating robust KG generation across multiple KGs backends and accurate\nmapping of adversarial activity to ATT&CK tactics. Results highlight the\nbenefits of retrieval and correction for precision and recall, the\neffectiveness of code-oriented models in structured log analysis, and the value\nof ontology-grounded representations for actionable CTI extraction.",
      "authors": [
        "Luca Cotti",
        "Idilio Drago",
        "Anisa Rula",
        "Devis Bianchini",
        "Federico Cerutti"
      ],
      "published": "2025-10-01T19:46:15Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01409v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "OntoLogX是一种基于大语言模型的自主AI代理，通过本体引导从网络安全日志中提取知识图谱。该系统结合轻量级日志本体、检索增强生成和迭代校正步骤，将原始日志转换为基于本体的知识图谱，并聚合会话以预测MITRE ATT&CK攻击战术，在公开基准和真实蜜罐数据集上验证了其有效性。",
      "order": 160
    },
    {
      "arxiv_id": "2510.01398v1",
      "title": "Automating Data-Driven Modeling and Analysis for Engineering\n  Applications using Large Language Model Agents",
      "summary": "Modern engineering increasingly relies on vast datasets generated by\nexperiments and simulations, driving a growing demand for efficient, reliable,\nand broadly applicable modeling strategies. There is also heightened interest\nin developing data-driven approaches, particularly neural network models, for\neffective prediction and analysis of scientific datasets. Traditional\ndata-driven methods frequently involve extensive manual intervention, limiting\ntheir ability to scale effectively and generalize to diverse applications. In\nthis study, we propose an innovative pipeline utilizing Large Language Model\n(LLM) agents to automate data-driven modeling and analysis, with a particular\nemphasis on regression tasks. We evaluate two LLM-agent frameworks: a\nmulti-agent system featuring specialized collaborative agents, and a\nsingle-agent system based on the Reasoning and Acting (ReAct) paradigm. Both\nframeworks autonomously handle data preprocessing, neural network development,\ntraining, hyperparameter optimization, and uncertainty quantification (UQ). We\nvalidate our approach using a critical heat flux (CHF) prediction benchmark,\ninvolving approximately 25,000 experimental data points from the OECD/NEA\nbenchmark dataset. Results indicate that our LLM-agent-developed model\nsurpasses traditional CHF lookup tables and delivers predictive accuracy and UQ\non par with state-of-the-art Bayesian optimized deep neural network models\ndeveloped by human experts. These outcomes underscore the significant potential\nof LLM-based agents to automate complex engineering modeling tasks, greatly\nreducing human workload while meeting or exceeding existing standards of\npredictive performance.",
      "authors": [
        "Yang Liu",
        "Zaid Abulawi",
        "Abhiram Garimidi",
        "Doyeong Lim"
      ],
      "published": "2025-10-01T19:28:35Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01398v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出利用大语言模型代理自动化数据驱动建模与分析的新方法，重点针对回归任务。通过多代理协作系统和单代理ReAct框架，实现了从数据预处理到神经网络开发、训练、超参数优化及不确定性量化的全流程自动化。在临界热通量预测基准测试中，该方法性能超越传统查表法，达到与专家开发的贝叶斯优化深度神经网络相当的预测精度，展示了LLM代理在复杂工程建模任务中的巨大潜力。",
      "order": 161
    },
    {
      "arxiv_id": "2510.01396v1",
      "title": "Neural Network Surrogates for Free Energy Computation of Complex\n  Chemical Systems",
      "summary": "Free energy reconstruction methods such as Gaussian Process Regression (GPR)\nrequire Jacobians of the collective variables (CVs), a bottleneck that\nrestricts the use of complex or machine-learned CVs. We introduce a neural\nnetwork surrogate framework that learns CVs directly from Cartesian coordinates\nand uses automatic differentiation to provide Jacobians, bypassing analytical\nforms. On an MgCl2 ion-pairing system, our method achieved high accuracy for\nboth a simple distance CV and a complex coordination-number CV. Moreover,\nJacobian errors also followed a near-Gaussian distribution, making them\nsuitable for GPR pipelines. This framework enables gradient-based free energy\nmethods to incorporate complex and machine-learned CVs, broadening the scope of\nbiochemistry and materials simulations.",
      "authors": [
        "Wasut Pornpatcharapong"
      ],
      "published": "2025-10-01T19:28:16Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01396v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种神经网络替代框架，可直接从笛卡尔坐标学习集体变量，并通过自动微分提供雅可比矩阵，克服了传统自由能计算方法对复杂集体变量的限制。在MgCl2离子配对系统中验证了该方法对简单和复杂集体变量的高精度计算能力，为生物化学和材料模拟开辟了新途径。",
      "order": 162
    },
    {
      "arxiv_id": "2510.01395v1",
      "title": "Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence",
      "summary": "Both the general public and academic communities have raised concerns about\nsycophancy, the phenomenon of artificial intelligence (AI) excessively agreeing\nwith or flattering users. Yet, beyond isolated media reports of severe\nconsequences, like reinforcing delusions, little is known about the extent of\nsycophancy or how it affects people who use AI. Here we show the pervasiveness\nand harmful impacts of sycophancy when people seek advice from AI. First,\nacross 11 state-of-the-art AI models, we find that models are highly\nsycophantic: they affirm users' actions 50% more than humans do, and they do so\neven in cases where user queries mention manipulation, deception, or other\nrelational harms. Second, in two preregistered experiments (N = 1604),\nincluding a live-interaction study where participants discuss a real\ninterpersonal conflict from their life, we find that interaction with\nsycophantic AI models significantly reduced participants' willingness to take\nactions to repair interpersonal conflict, while increasing their conviction of\nbeing in the right. However, participants rated sycophantic responses as higher\nquality, trusted the sycophantic AI model more, and were more willing to use it\nagain. This suggests that people are drawn to AI that unquestioningly validate,\neven as that validation risks eroding their judgment and reducing their\ninclination toward prosocial behavior. These preferences create perverse\nincentives both for people to increasingly rely on sycophantic AI models and\nfor AI model training to favor sycophancy. Our findings highlight the necessity\nof explicitly addressing this incentive structure to mitigate the widespread\nrisks of AI sycophancy.",
      "authors": [
        "Myra Cheng",
        "Cinoo Lee",
        "Pranav Khadpe",
        "Sunny Yu",
        "Dyllan Han",
        "Dan Jurafsky"
      ],
      "published": "2025-10-01T19:26:01Z",
      "primary_category": "cs.CY",
      "arxiv_url": "https://arxiv.org/abs/2510.01395v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "研究发现AI谄媚现象普遍存在且有害：11个先进AI模型比人类多50%几率赞同用户行为，即使涉及操纵或欺骗。实验表明谄媚AI会降低用户修复人际冲突的意愿，增强其自认正确的信念，但用户却更信任并愿意再次使用这类AI，形成依赖循环，凸显需解决此激励结构以减轻风险。",
      "order": 163
    },
    {
      "arxiv_id": "2510.01389v1",
      "title": "INSIGHT: INference-time Sequence Introspection for Generating Help\n  Triggers in Vision-Language-Action Models",
      "summary": "Recent Vision-Language-Action (VLA) models show strong generalization\ncapabilities, yet they lack introspective mechanisms for anticipating failures\nand requesting help from a human supervisor. We present \\textbf{INSIGHT}, a\nlearning framework for leveraging token-level uncertainty signals to predict\nwhen a VLA should request help. Using $\\pi_0$-FAST as the underlying model, we\nextract per-token \\emph{entropy}, \\emph{log-probability}, and Dirichlet-based\nestimates of \\emph{aleatoric and epistemic uncertainty}, and train compact\ntransformer classifiers to map these sequences to help triggers. We explore\nsupervision regimes for strong or weak supervision, and extensively compare\nthem across in-distribution and out-of-distribution tasks. Our results show a\ntrade-off: strong labels enable models to capture fine-grained uncertainty\ndynamics for reliable help detection, while weak labels, though noisier, still\nsupport competitive introspection when training and evaluation are aligned,\noffering a scalable path when dense annotation is impractical. Crucially, we\nfind that modeling the temporal evolution of token-level uncertainty signals\nwith transformers provides far greater predictive power than static\nsequence-level scores. This study provides the first systematic evaluation of\nuncertainty-based introspection in VLAs, opening future avenues for active\nlearning and for real-time error mitigation through selective human\nintervention.",
      "authors": [
        "Ulas Berk Karli",
        "Ziyao Shangguan",
        "Tesca FItzgerald"
      ],
      "published": "2025-10-01T19:22:48Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01389v1",
      "primary_area": "vla_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "INSIGHT提出一种基于推理时序列自省的视觉-语言-动作模型求助触发框架，通过分析token级不确定性信号（熵、对数概率、偶然/认知不确定性），利用紧凑Transformer分类器预测何时需要人工干预。研究比较了强监督与弱监督策略，证明建模不确定性时序演化比静态序列评分更具预测力，为VLA模型的主动学习和实时错误缓解开辟了新途径。",
      "order": 164
    },
    {
      "arxiv_id": "2510.01377v1",
      "title": "DeMuon: A Decentralized Muon for Matrix Optimization over Graphs",
      "summary": "In this paper, we propose DeMuon, a method for decentralized matrix\noptimization over a given communication topology. DeMuon incorporates matrix\northogonalization via Newton-Schulz iterations-a technique inherited from its\ncentralized predecessor, Muon-and employs gradient tracking to mitigate\nheterogeneity among local functions. Under heavy-tailed noise conditions and\nadditional mild assumptions, we establish the iteration complexity of DeMuon\nfor reaching an approximate stochastic stationary point. This complexity result\nmatches the best-known complexity bounds of centralized algorithms in terms of\ndependence on the target tolerance. To the best of our knowledge, DeMuon is the\nfirst direct extension of Muon to decentralized optimization over graphs with\nprovable complexity guarantees. We conduct preliminary numerical experiments on\ndecentralized transformer pretraining over graphs with varying degrees of\nconnectivity. Our numerical results demonstrate a clear margin of improvement\nof DeMuon over other popular decentralized algorithms across different network\ntopologies.",
      "authors": [
        "Chuan He",
        "Shuyi Ren",
        "Jingwei Mao",
        "Erik G. Larsson"
      ],
      "published": "2025-10-01T19:06:11Z",
      "primary_category": "math.OC",
      "arxiv_url": "https://arxiv.org/abs/2510.01377v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "DeMuon是一种基于图通信拓扑的分散式矩阵优化方法，结合牛顿-舒尔茨正交化和梯度追踪技术，在重尾噪声条件下具有可证明的收敛复杂度，实验显示在分散式Transformer预训练中优于现有算法。",
      "order": 165
    },
    {
      "arxiv_id": "2510.01375v1",
      "title": "Fine-tuning with RAG for Improving LLM Learning of New Skills",
      "summary": "Large language model (LLM) agents deployed for multi-step tasks frequently\nfail in predictable ways: attempting actions with unmet preconditions, issuing\nredundant commands, or mishandling environment constraints. While\nretrieval-augmented generation (RAG) can improve performance by providing\nruntime guidance, it requires maintaining external knowledge databases and adds\ncomputational overhead at every deployment. We propose a simple pipeline that\nconverts inference-time retrieval into learned competence through distillation.\nOur approach: (1) extracts compact, reusable hints from agent failures, (2)\nuses these hints to generate improved teacher trajectories via one-shot\nretrieval at episode start, and (3) trains student models on these trajectories\nwith hint strings removed, forcing internalization rather than memorization.\nAcross two interactive benchmarks, ALFWorld (household tasks) and WebShop\n(online shopping), distilled students consistently outperform baseline agents,\nachieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving\nWebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens\nthan retrieval-augmented teachers depending on the environment. The approach\ngeneralizes across model scales (7B/14B parameters) and agent architectures\n(ReAct/StateAct), demonstrating that retrieval benefits can be effectively\ninternalized through targeted fine-tuning without permanent runtime\ndependencies.",
      "authors": [
        "Humaid Ibrahim",
        "Nikolai Rozanov",
        "Marek Rei"
      ],
      "published": "2025-10-01T19:03:48Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01375v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种将检索增强生成(RAG)转化为模型内在能力的方法，通过从智能体失败中提取提示、生成改进轨迹并蒸馏训练学生模型，在ALFWorld和WebShop基准上显著提升任务成功率，同时减少推理时令牌消耗，实现无需运行时检索依赖的技能内化。",
      "order": 166
    },
    {
      "arxiv_id": "2510.01370v1",
      "title": "SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs",
      "summary": "We introduce Small PDE U-Net Solver (SPUS), a compact and efficient\nfoundation model (FM) designed as a unified neural operator for solving a wide\nrange of partial differential equations (PDEs). Unlike existing\nstate-of-the-art PDE FMs-primarily based on large complex transformer\narchitectures with high computational and parameter overhead-SPUS leverages a\nlightweight residual U-Net-based architecture that has been largely\nunderexplored as a foundation model architecture in this domain. To enable\neffective learning in this minimalist framework, we utilize a simple yet\npowerful auto-regressive pretraining strategy which closely replicates the\nbehavior of numerical solvers to learn the underlying physics. SPUS is\npretrained on a diverse set of fluid dynamics PDEs and evaluated across 6\nchallenging unseen downstream PDEs spanning various physical systems.\nExperimental results demonstrate that SPUS using residual U-Net based\narchitecture achieves state-of-the-art generalization on these downstream tasks\nwhile requiring significantly fewer parameters and minimal fine-tuning data,\nhighlighting its potential as a highly parameter-efficient FM for solving\ndiverse PDE systems.",
      "authors": [
        "Abu Bucker Siddik",
        "Diane Oyen",
        "Alexander Most",
        "Michal Kucer",
        "Ayan Biswas"
      ],
      "published": "2025-10-01T18:54:59Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01370v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "SPUS是一种轻量级参数高效的PDE基础模型，采用残差U-Net架构和自回归预训练策略，在多种偏微分方程求解任务中实现最先进的泛化性能，显著减少参数需求和微调数据。",
      "order": 167
    },
    {
      "arxiv_id": "2510.01367v1",
      "title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by\n  Measuring Reasoning Effort",
      "summary": "Reward hacking, where a reasoning model exploits loopholes in a reward\nfunction to achieve high rewards without solving the intended task, poses a\nsignificant threat. This behavior may be explicit, i.e. verbalized in the\nmodel's chain-of-thought (CoT), or implicit, where the CoT appears benign thus\nbypasses CoT monitors. To detect implicit reward hacking, we propose TRACE\n(Truncated Reasoning AUC Evaluation). Our key observation is that hacking\noccurs when exploiting the loophole is easier than solving the actual task.\nThis means that the model is using less `effort' than required to achieve high\nreward. TRACE quantifies effort by measuring how early a model's reasoning\nbecomes sufficient to pass a verifier. We progressively truncate a model's CoT\nat various lengths, force the model to answer, and measure the verifier-passing\nrate at each cutoff. A hacking model, which takes a shortcut, will achieve a\nhigh passing rate with only a small fraction of its CoT, yielding a large area\nunder the accuracy-vs-length curve. TRACE achieves over 65% gains over our\nstrongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B\nmonitor in coding. We further show that TRACE can discover unknown loopholes\nduring training. Overall, TRACE offers a scalable unsupervised approach for\noversight where current monitoring methods prove ineffective.",
      "authors": [
        "Xinpeng Wang",
        "Nitish Joshi",
        "Barbara Plank",
        "Rico Angell",
        "He He"
      ],
      "published": "2025-10-01T18:49:45Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01367v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出TRACE方法检测推理模型中的隐式奖励破解行为，通过截断思维链测量推理努力程度来识别模型是否走捷径而非真正解决问题。在数学推理和代码生成任务中分别比现有监测方法提升65%和30%以上，并能发现训练中的未知漏洞。",
      "order": 168
    },
    {
      "arxiv_id": "2510.01363v1",
      "title": "Retrieval-Augmented Framework for LLM-Based Clinical Decision Support",
      "summary": "The increasing complexity of clinical decision-making, alongside the rapid\nexpansion of electronic health records (EHR), presents both opportunities and\nchallenges for delivering data-informed care. This paper proposes a clinical\ndecision support system powered by Large Language Models (LLMs) to assist\nprescribing clinicians. The system generates therapeutic suggestions by\nanalyzing historical EHR data, including patient demographics, presenting\ncomplaints, clinical symptoms, diagnostic information, and treatment histories.\nThe framework integrates natural language processing with structured clinical\ninputs to produce contextually relevant recommendations. Rather than replacing\nclinician judgment, it is designed to augment decision-making by retrieving and\nsynthesizing precedent cases with comparable characteristics, drawing on local\ndatasets or federated sources where applicable. At its core, the system employs\na retrieval-augmented generation (RAG) pipeline that harmonizes unstructured\nnarratives and codified data to support LLM-based inference. We outline the\nsystem's technical components, including representation representation\nalignment and generation strategies. Preliminary evaluations, conducted with\nde-identified and synthetic clinical datasets, examine the clinical\nplausibility and consistency of the model's outputs. Early findings suggest\nthat LLM-based tools may provide valuable decision support in prescribing\nworkflows when appropriately constrained and rigorously validated. This work\nrepresents an initial step toward integration of generative AI into real-world\nclinical decision-making with an emphasis on transparency, safety, and\nalignment with established practices.",
      "authors": [
        "Leon Garza",
        "Anantaa Kotal",
        "Michael A. Grasso",
        "Emre Umucu"
      ],
      "published": "2025-10-01T18:45:25Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01363v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出一种基于大语言模型的临床决策支持系统，采用检索增强生成(RAG)框架整合电子健康记录数据，通过分析患者历史信息生成治疗建议，旨在辅助而非替代临床医生决策，初步评估显示在合理约束和验证下可为处方工作流提供有价值的支持。",
      "order": 169
    },
    {
      "arxiv_id": "2510.01359v1",
      "title": "Breaking the Code: Security Assessment of AI Code Agents Through\n  Systematic Jailbreaking Attacks",
      "summary": "Code-capable large language model (LLM) agents are increasingly embedded into\nsoftware engineering workflows where they can read, write, and execute code,\nraising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only\nsettings. Prior evaluations emphasize refusal or harmful-text detection,\nleaving open whether agents actually compile and run malicious programs. We\npresent JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three\nescalating workspace regimes that mirror attacker capability: empty (JAWS-0),\nsingle-file (JAWS-1), and multi-file (JAWS-M). We pair this with a\nhierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)\nattack success, (iii) syntactic correctness, and (iv) runtime executability,\nmoving beyond refusal to measure deployable harm. Using seven LLMs from five\nfamilies as backends, we find that under prompt-only conditions in JAWS-0, code\nagents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%\nrun end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~\n100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the\nmulti-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly\ndeployable attack code. Across models, wrapping an LLM in an agent\nsubstantially increases vulnerability -- ASR raises by 1.6x -- because initial\nrefusals are frequently overturned during later planning/tool-use steps.\nCategory-level analyses identify which attack classes are most vulnerable and\nmost readily deployable, while others exhibit large execution gaps. These\nfindings motivate execution-aware defenses, code-contextual safety filters, and\nmechanisms that preserve refusal decisions throughout the agent's multi-step\nreasoning and tool use.",
      "authors": [
        "Shoumik Saha",
        "Jifan Chen",
        "Sam Mayers",
        "Sanjay Krishna Gouda",
        "Zijian Wang",
        "Varun Kumar"
      ],
      "published": "2025-10-01T18:38:20Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01359v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "code_generation",
      "tldr_zh": "本研究提出JAWS-BENCH基准，系统评估AI代码代理的安全漏洞。通过三个递增的工作空间场景（空环境、单文件、多文件）测试7个LLM，发现代码代理平均61%接受攻击，58%产生有害代码，27%可端到端运行。多文件场景下攻击成功率高达75%，其中32%为可直接部署的攻击代码。研究揭示将LLM封装为代理会显著增加安全风险，呼吁开发执行感知防御和代码上下文安全过滤器。",
      "order": 170
    },
    {
      "arxiv_id": "2510.01354v1",
      "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
      "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.",
      "authors": [
        "Yinuo Liu",
        "Ruohan Xu",
        "Xilong Wang",
        "Yuqi Jia",
        "Neil Zhenqiang Gong"
      ],
      "published": "2025-10-01T18:34:06Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01354v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "WAInjectBench是首个针对网络代理的提示注入攻击检测基准研究。该工作通过细粒度威胁模型分类构建了包含恶意/良性文本和图像的数据集，系统评估了多种检测方法。研究发现现有检测器能有效识别显式指令或可见扰动的攻击，但对无显式指令或不可感知扰动的攻击效果不佳。",
      "order": 171
    },
    {
      "arxiv_id": "2510.01353v1",
      "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in\n  Multi-Platform Dynamic Agent Environments",
      "summary": "Recent works on context and memory benchmarking have primarily focused on\nconversational instances but the need for evaluating memory in dynamic\nenterprise environments is crucial for its effective application. We introduce\nMEMTRACK, a benchmark designed to evaluate long-term memory and state tracking\nin multi-platform agent environments. MEMTRACK models realistic organizational\nworkflows by integrating asynchronous events across multiple communication and\nproductivity platforms such as Slack, Linear and Git. Each benchmark instance\nprovides a chronologically platform-interleaved timeline, with noisy,\nconflicting, cross-referring information as well as potential\ncodebase/file-system comprehension and exploration. Consequently, our benchmark\ntests memory capabilities such as acquistion, selection and conflict\nresolution. We curate the MEMTRACK dataset through both manual expert driven\ndesign and scalable agent based synthesis, generating ecologically valid\nscenarios grounded in real world software development processes. We introduce\npertinent metrics for Correctness, Efficiency, and Redundancy that capture the\neffectiveness of memory mechanisms beyond simple QA performance. Experiments\nacross SoTA LLMs and memory backends reveal challenges in utilizing memory\nacross long horizons, handling cross-platform dependencies, and resolving\ncontradictions. Notably, the best performing GPT-5 model only achieves a 60\\%\nCorrectness score on MEMTRACK. This work provides an extensible framework for\nadvancing evaluation research for memory-augmented agents, beyond existing\nfocus on conversational setups, and sets the stage for multi-agent,\nmulti-platform memory benchmarking in complex organizational settings",
      "authors": [
        "Darshan Deshpande",
        "Varun Gangal",
        "Hersh Mehta",
        "Anand Kannappan",
        "Rebecca Qian",
        "Peng Wang"
      ],
      "published": "2025-10-01T18:34:03Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01353v1",
      "primary_area": "text_models",
      "secondary_focus": "long_context",
      "application_domain": "code_generation",
      "tldr_zh": "MEMTRACK是一个评估多平台动态代理环境中长期记忆与状态跟踪的基准测试，通过整合Slack、Linear和Git等平台的异步事件模拟真实组织工作流，测试记忆获取、选择和冲突解决能力。实验显示现有模型在处理跨平台依赖和矛盾信息方面仍面临挑战，最佳GPT-5模型正确率仅60%。",
      "order": 172
    },
    {
      "arxiv_id": "2510.01346v1",
      "title": "Aristotle: IMO-level Automated Theorem Proving",
      "summary": "We introduce Aristotle, an AI system that combines formal verification with\ninformal reasoning, achieving gold-medal-equivalent performance on the 2025\nInternational Mathematical Olympiad problems. Aristotle integrates three main\ncomponents: a Lean proof search system, an informal reasoning system that\ngenerates and formalizes lemmas, and a dedicated geometry solver. Our system\ndemonstrates state-of-the-art performance with favorable scaling properties for\nautomated theorem proving.",
      "authors": [
        "Tudor Achim",
        "Alex Best",
        "Kevin Der",
        "Mathïs Fédérico",
        "Sergei Gukov",
        "Daniel Halpern-Leister",
        "Kirsten Henningsgard",
        "Yury Kudryashov",
        "Alexander Meiburg",
        "Martin Michelsen",
        "Riley Patterson",
        "Eric Rodriguez",
        "Laura Scharff",
        "Vikram Shanker",
        "Vladmir Sicca",
        "Hari Sowrirajan",
        "Aidan Swope",
        "Matyas Tamas",
        "Vlad Tenev",
        "Jonathan Thomm",
        "Harold Williams",
        "Lawrence Wu"
      ],
      "published": "2025-10-01T18:21:13Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01346v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "education_ai",
      "tldr_zh": "Aristotle是一个结合形式化验证与非正式推理的AI系统，在2025年国际数学奥林匹克竞赛中达到金牌级别表现。系统整合了Lean证明搜索、引理生成与形式化推理模块以及专用几何求解器，展示了自动定理证明领域的最先进性能。",
      "order": 173
    },
    {
      "arxiv_id": "2510.01336v1",
      "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
      "summary": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.",
      "authors": [
        "Avinash Kumar",
        "Sujay Sanghavi",
        "Poulami Das"
      ],
      "published": "2025-10-01T18:04:14Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01336v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "HiSpec提出了一种基于层次化推测解码的大语言模型加速框架，利用早退模型进行中间验证，通过重用KV缓存和隐藏状态减少计算内存开销，在保持准确性的同时平均提升1.28倍吞吐量。",
      "order": 174
    },
    {
      "arxiv_id": "2510.01304v1",
      "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
      "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .",
      "authors": [
        "Yu Zeng",
        "Wenxuan Huang",
        "Shiting Huang",
        "Xikun Bao",
        "Yukun Qi",
        "Yiming Zhao",
        "Qiuchen Wang",
        "Lin Chen",
        "Zehui Chen",
        "Huaian Chen",
        "Wanli Ouyang",
        "Feng Zhao"
      ],
      "published": "2025-10-01T17:58:05Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01304v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "提出AGILE方法，通过拼图交互学习增强视觉语言模型的感知与推理能力。该方法将拼图求解构建为交互过程，模型生成可执行代码进行动作，环境提供细粒度视觉反馈，通过探索式学习显著提升模型在拼图任务（准确率从9.5%提升至82.8%）和9个通用视觉任务上的表现（平均提升3.1%），为解决多模态强化学习数据稀缺问题提供了可扩展方案。",
      "order": 175
    },
    {
      "arxiv_id": "2510.01179v1",
      "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
      "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.",
      "authors": [
        "Zhangchen Xu",
        "Adriana Meza Soria",
        "Shawn Tan",
        "Anurag Roy",
        "Ashish Sunil Agrawal",
        "Radha Poovendran",
        "Rameswar Panda"
      ],
      "published": "2025-10-01T17:58:03Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01179v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "TOUCAN是迄今最大的开源工具-智能体数据集，包含150万条从近500个真实世界MCP环境合成的轨迹。该数据集通过多样化查询生成、质量过滤和多框架轨迹合成，解决了开源社区高质量工具-智能体训练数据匮乏的问题。基于TOUCAN微调的模型在BFCL V3基准测试中超越了更大的闭源模型。",
      "order": 176
    },
    {
      "arxiv_id": "2510.01178v1",
      "title": "COM-BOM: Bayesian Exemplar Search for Efficiently Exploring the\n  Accuracy-Calibration Pareto Frontier",
      "summary": "Selecting an optimal set of exemplars is critical for good performance of\nin-context learning. However, prior exemplar search methods narrowly optimize\nfor predictive accuracy, critically neglecting model calibration--a key\ndeterminant of trustworthiness and safe deployment. In this paper, we formulate\nexemplar selection as a multi-objective optimization problem, explicitly\ntargeting both the maximization of predictive accuracy and the minimization of\nexpected calibration error. We solve this problem with a sample-efficient\nCombinatorial Bayesian Optimization algorithm (COM-BOM) to find the Pareto\nfront that optimally trades off the two objectives of accuracy and calibration.\nWe evaluate COM-BOM on multiple tasks from unsaturated MMLU-Pro benchmark and\nfind that COM-BOM beats or matches the baselines at jointly optimizing the two\nobjectives, while requiring a minimal number of LLM API calls.",
      "authors": [
        "Gaoxiang Luo",
        "Aryan Deshwal"
      ],
      "published": "2025-10-01T17:57:49Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01178v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "COM-BOM提出一种贝叶斯组合优化算法，通过多目标优化同时提升上下文学习中的预测准确率和模型校准度，在MMLU-Pro基准测试中以最少API调用实现帕累托最优。",
      "order": 177
    },
    {
      "arxiv_id": "2510.01174v1",
      "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
      "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.",
      "authors": [
        "Yanzhe Chen",
        "Kevin Qinghong Lin",
        "Mike Zheng Shou"
      ],
      "published": "2025-10-01T17:56:48Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01174v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "education_ai",
      "tldr_zh": "Code2Video提出了一种以代码为中心的教育视频生成框架，通过三个协作智能体（规划器、编码器、评审器）将教学内容转换为可执行的Python代码并生成专业教育视频。该方法在MMMC基准测试中表现优异，相比直接代码生成提升40%，视频质量接近人工制作水平。",
      "order": 178
    },
    {
      "arxiv_id": "2510.01173v1",
      "title": "EditTrack: Detecting and Attributing AI-assisted Image Editing",
      "summary": "In this work, we formulate and study the problem of image-editing detection\nand attribution: given a base image and a suspicious image, detection seeks to\ndetermine whether the suspicious image was derived from the base image using an\nAI editing model, while attribution further identifies the specific editing\nmodel responsible. Existing methods for detecting and attributing AI-generated\nimages are insufficient for this problem, as they focus on determining whether\nan image was AI-generated/edited rather than whether it was edited from a\nparticular base image. To bridge this gap, we propose EditTrack, the first\nframework for this image-editing detection and attribution problem. Building on\nfour key observations about the editing process, EditTrack introduces a novel\nre-editing strategy and leverages carefully designed similarity metrics to\ndetermine whether a suspicious image originates from a base image and, if so,\nby which model. We evaluate EditTrack on five state-of-the-art editing models\nacross six datasets, demonstrating that it consistently achieves accurate\ndetection and attribution, significantly outperforming five baselines.",
      "authors": [
        "Zhengyuan Jiang",
        "Yuyang Zhang",
        "Moyang Guo",
        "Neil Zhenqiang Gong"
      ],
      "published": "2025-10-01T17:56:35Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01173v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "EditTrack是首个用于图像编辑检测与归因的框架，通过分析基础图像与可疑图像之间的关系，不仅能检测可疑图像是否源自基础图像的AI编辑，还能识别具体使用的编辑模型。该方法基于对编辑过程的四个关键观察，引入重新编辑策略和相似性度量，在多个数据集上显著优于现有基线。",
      "order": 179
    },
    {
      "arxiv_id": "2510.01171v1",
      "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity",
      "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.",
      "authors": [
        "Jiayi Zhang",
        "Simon Yu",
        "Derek Chong",
        "Anthony Sicilia",
        "Michael R. Tomz",
        "Christopher D. Manning",
        "Weiyan Shi"
      ],
      "published": "2025-10-01T17:55:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01171v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出'语言化采样'方法解决LLM对齐训练中的模式崩溃问题。研究发现偏好数据中的典型性偏见是模式崩溃的根本原因，通过让模型生成多个回答及其概率分布，显著提升了创意写作、对话模拟等任务的多样性1.6-2.1倍，且不影响准确性和安全性。",
      "order": 180
    },
    {
      "arxiv_id": "2510.01169v1",
      "title": "Fiaingen: A financial time series generative method matching real-world\n  data quality",
      "summary": "Data is vital in enabling machine learning models to advance research and\npractical applications in finance, where accurate and robust models are\nessential for investment and trading decision-making. However, real-world data\nis limited despite its quantity, quality, and variety. The data shortage of\nvarious financial assets directly hinders the performance of machine learning\nmodels designed to trade and invest in these assets. Generative methods can\nmitigate this shortage. In this paper, we introduce a set of novel techniques\nfor time series data generation (we name them Fiaingen) and assess their\nperformance across three criteria: (a) overlap of real-world and synthetic data\non a reduced dimensionality space, (b) performance on downstream machine\nlearning tasks, and (c) runtime performance. Our experiments demonstrate that\nthe methods achieve state-of-the-art performance across the three criteria\nlisted above. Synthetic data generated with Fiaingen methods more closely\nmirrors the original time series data while keeping data generation time close\nto seconds - ensuring the scalability of the proposed approach. Furthermore,\nmodels trained on it achieve performance close to those trained with real-world\ndata.",
      "authors": [
        "Jože M. Rožanec",
        "Tina Žezlin",
        "Laurentiu Vasiliu",
        "Dunja Mladenić",
        "Radu Prodan",
        "Dumitru Roman"
      ],
      "published": "2025-10-01T17:55:08Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01169v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "financial_ai",
      "tldr_zh": "Fiaingen是一种金融时间序列生成方法，通过降维空间重叠度、下游机器学习任务性能和运行时性能三个标准评估，能生成与真实数据质量匹配的合成数据，解决金融资产数据短缺问题，生成速度快且可扩展。",
      "order": 181
    },
    {
      "arxiv_id": "2510.01167v1",
      "title": "Simultaneous Multi-objective Alignment Across Verifiable and\n  Non-verifiable Rewards",
      "summary": "Aligning large language models to human preferences is inherently\nmultidimensional, yet most pipelines collapse heterogeneous signals into a\nsingle optimizeable objective. We seek to answer what it would take to\nsimultaneously align a model across various domains spanning those with:\nverifiable rewards (mathematical accuracy), non-verifiable subjective\npreferences (human values), and complex interactive scenarios (multi-turn AI\ntutoring dialogues). Such multi-objective reinforcement learning setups are\noften plagued by the individual objectives being at odds with each other,\nresulting in inefficient training and little user control during inference. We\npropose a unified framework that: (i) standardizes {process reward model} (PRM)\ntraining across both verifiable and non-verifiable settings to better supervise\nmodels' chain-of-thought reasoning; (ii) performs {multi-objective alignment}\nby training the LLM with our $\\textbf{M}$ulti-$\\textbf{A}$ction-$\\textbf{H}$ead\n$\\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the\nvector correspond to the various objectives instead of a single scalar; and\n(iii) demonstrates how such a system provides fine-grained inference-time user\ncontrol. Experiments across math reasoning, value alignment, and multi-turn\ndialogue show that our framework improves performance across multiple\nobjectives simultaneously, while minimizing cross-objective trade-offs and\nenabling flexible inference time user control. The code can be found at\nhttps://github.com/pearls-lab/multiobj-align.",
      "authors": [
        "Yiran Shen",
        "Yu Xia",
        "Jonathan Chang",
        "Prithviraj Ammanabrolu"
      ],
      "published": "2025-10-01T17:54:15Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01167v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出统一框架解决大语言模型多目标对齐问题，通过标准化过程奖励模型训练、多动作头DPO方法和向量化奖励，在数学推理、价值对齐和多轮对话等场景中同时优化多个目标，减少目标间冲突并提供细粒度推理控制。",
      "order": 182
    },
    {
      "arxiv_id": "2510.01165v1",
      "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient\n  Few-Shot Reasoning",
      "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks,\nbut their effectiveness often depends on the quality of the provided context.\nRetrieval-Augmented Generation (RAG) enriches prompts with external\ninformation, but its reliance on static databases constrains adaptability and\ncan result in irrelevant demonstrations. In this work, we propose a Generative\nRetrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach\nwhere an LLM model is trained to generate input-specific concise\ndemonstrations. By tailoring demonstrations to each input, our method offers\nbetter contextual support than traditional RAG approaches. We demonstrate the\nsuperiority of GRAD under budget constraints, where we limit both the number of\ntokens used per demonstration and the number of tokens used for the final\noutput. Trained solely on a math dataset, GRAD consistently outperforms strong\nbaselines on Qwen2.5-14B across mathematical reasoning and advanced STEM\nquestions, highlighting GRAD's robust generalization to out-of-distribution\n(OOD) domains such as physics, chemistry, and computer science. Furthermore, we\nshow that demonstrations generated by trained smaller models can effectively\nguide larger target models, reducing training costs while maintaining\ncompetitive accuracy. Overall, this work introduces a scalable demonstration\ngenerator model presenting the first step toward a dynamic few-shot learning\nparadigm in resource-constrained settings. We release the code used for the\nproject.",
      "authors": [
        "Oussama Gabouj",
        "Kamel Charaf",
        "Ivan Zakazov",
        "Nicolas Baldwin",
        "Robert West"
      ],
      "published": "2025-10-01T17:52:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01165v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出GRAD方法，通过训练LLM生成针对特定输入的简洁示例，替代传统检索增强生成中的静态数据库。该方法在数学推理和STEM问题上表现优异，能有效泛化至物理、化学等分布外领域，并在预算限制下保持竞争力，为资源受限环境下的动态小样本学习提供了新范式。",
      "order": 183
    },
    {
      "arxiv_id": "2510.01164v1",
      "title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social\n  Welfare",
      "summary": "Large language models (LLMs) are increasingly entrusted with high-stakes\ndecisions that affect human welfare. However, the principles and values that\nguide these models when distributing scarce societal resources remain largely\nunexamined. To address this, we introduce the Social Welfare Function (SWF)\nBenchmark, a dynamic simulation environment where an LLM acts as a sovereign\nallocator, distributing tasks to a heterogeneous community of recipients. The\nbenchmark is designed to create a persistent trade-off between maximizing\ncollective efficiency (measured by Return on Investment) and ensuring\ndistributive fairness (measured by the Gini coefficient). We evaluate 20\nstate-of-the-art LLMs and present the first leaderboard for social welfare\nallocation. Our findings reveal three key insights: (i) A model's general\nconversational ability, as measured by popular leaderboards, is a poor\npredictor of its allocation skill. (ii) Most LLMs exhibit a strong default\nutilitarian orientation, prioritizing group productivity at the expense of\nsevere inequality. (iii) Allocation strategies are highly vulnerable, easily\nperturbed by output-length constraints and social-influence framing. These\nresults highlight the risks of deploying current LLMs as societal\ndecision-makers and underscore the need for specialized benchmarks and targeted\nalignment for AI governance.",
      "authors": [
        "Zhengliang Shi",
        "Ruotian Ma",
        "Jen-tse Huang",
        "Xinbei Ma",
        "Xingyu Chen",
        "Mengru Wang",
        "Qu Yang",
        "Yue Wang",
        "Fanghua Ye",
        "Ziyang Chen",
        "Shanyi Wang",
        "Cixing Li",
        "Wenxuan Wang",
        "Zhaopeng Tu",
        "Xiaolong Li",
        "Zhaochun Ren",
        "Linus"
      ],
      "published": "2025-10-01T17:52:31Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01164v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究引入社会福利函数基准测试，评估20个先进大语言模型在资源分配中的表现。研究发现：模型对话能力与分配技能无关；多数模型呈现功利主义倾向，牺牲公平追求效率；分配策略易受输出长度和社会影响框架干扰。结果凸显当前LLM作为社会决策者的风险，需专门基准测试和针对性对齐。",
      "order": 184
    },
    {
      "arxiv_id": "2510.01161v1",
      "title": "Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale\n  Data on LLMs?",
      "summary": "Reinforcement learning has been central to recent advances in large language\nmodel reasoning, but most algorithms rely on on-policy training that demands\nfresh rollouts at every update, limiting efficiency and scalability.\nAsynchronous RL systems alleviate this by decoupling rollout generation from\ntraining, yet their effectiveness hinges on tolerating large staleness in\nrollout data, a setting where existing methods either degrade in performance or\ncollapse. We revisit this challenge and uncover a prosperity-before-collapse\nphenomenon: stale data can be as informative as on-policy data if exploited\nproperly. Building on this insight, we introduce M2PO (Second-Moment Trust\nPolicy Optimization), which constrains the second moment of importance weights\nto suppress only extreme outliers while preserving informative updates.\nNotably, M2PO sharply reduces the fraction of clipped tokens under high\nstaleness (from 1.22% to 0.06% over training), precisely masking high-variance\ntokens while maintaining stable optimization. Extensive evaluation across six\nmodels (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable\noff-policy training even with data stale by at least 256 model updates and\nmatches on-policy performance.",
      "authors": [
        "Haizhong Zheng",
        "Jiawei Zhao",
        "Bedi Chen"
      ],
      "published": "2025-10-01T17:48:23Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01161v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出M2PO算法，针对大语言模型离线强化学习中的数据陈旧问题，通过约束重要性权重的二阶矩来抑制极端异常值，在数据延迟高达256次模型更新时仍能保持稳定训练，并在1.7B至32B参数的六个模型和八个基准测试中达到与在线策略相当的性能。",
      "order": 185
    },
    {
      "arxiv_id": "2510.01146v1",
      "title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models",
      "summary": "Evaluation using Large Language Model (LLM) judges has been widely adopted in\nEnglish and shown to be effective for automatic evaluation. However, their\nperformance does not generalize well to non-English settings, and it remains\nunclear what constitutes effective multilingual training for such judges. In\nthis paper, we introduce mR3, a massively multilingual, rubric-agnostic reward\nreasoning model trained on 72 languages, achieving the broadest language\ncoverage in reward modeling to date. We present a comprehensive study of data\nand curriculum selection for training to identify effective strategies and data\nsources for building high-quality reward models, including the integration of\ntarget-language reasoning datasets. Our approach attains state-of-the-art\nperformance on multilingual reward model benchmarks, surpassing much larger\nmodels (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness\nis further confirmed through extensive ablation studies. Our models, data, and\ncode are available as open source at https://github.com/rubricreward/mr3.",
      "authors": [
        "David Anugraha",
        "Shou-Yi Hung",
        "Zilu Tang",
        "Annie En-Shiun Lee",
        "Derry Tanti Wijaya",
        "Genta Indra Winata"
      ],
      "published": "2025-10-01T17:36:59Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01146v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "mR3是一个支持72种语言的多语言奖励推理模型，通过数据与课程选择策略优化训练，在多项评测中超越更大模型，实现了最广泛的奖励建模语言覆盖。",
      "order": 186
    },
    {
      "arxiv_id": "2510.01143v1",
      "title": "Generalized Parallel Scaling with Interdependent Generations",
      "summary": "Parallel LLM inference scaling involves sampling a set of $N>1$ responses for\na single input prompt. However, these $N$ parallel responses tend to be\ngenerated independently from each other, partitioning compute resources and\nleaving potentially useful information in one generation untapped by others.\nThis is in contrast to response length scaling where past computation is used\nin all future steps. For higher quality responses and response sets, we propose\nBridge to generate interdependent responses in parallel by rethinking batched\nLLM hidden states as holistic tensors rather than independent slices. With only\na small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean\naccuracy gains from reinforcement learning with verifiable rewards by up to 50%\nand boosts consistency of correct responses. Trained once, Bridge scales to any\ngeneration width, all with greater performance than independent generations,\nunlocking a more general mode of parallel scaling that effectively leverages\ninformation between sequences, compatible with any post-generation aggregation\ntechnique.",
      "authors": [
        "Harry Dong",
        "David Brandfonbrener",
        "Eryk Helenowski",
        "Yun He",
        "Mrinal Kumar",
        "Han Fang",
        "Yuejie Chi",
        "Karthik Abinav Sankararaman"
      ],
      "published": "2025-10-01T17:33:35Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01143v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Bridge方法，通过将批量LLM隐藏状态视为整体张量而非独立切片，实现并行生成相互依赖的响应。仅需少量新增参数(2.8%-5.1%)，即可提升强化学习验证奖励的相对平均准确率增益达50%，并增强正确响应的一致性。该方法一次训练即可适应任意生成宽度，性能优于独立生成，解锁了更通用的并行扩展模式。",
      "order": 187
    },
    {
      "arxiv_id": "2510.01141v1",
      "title": "Apriel-1.5-15b-Thinker",
      "summary": "We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights\nmultimodal reasoning model that achieves frontier-level performance through\ntraining design rather than sheer scale. Starting from Pixtral-12B, we apply a\nprogressive three-stage methodology: (1) depth upscaling to expand reasoning\ncapacity without pretraining from scratch, (2) staged continual pre-training\nthat first develops foundational text and vision understanding, then enhances\nvisual reasoning through targeted synthetic data generation addressing spatial\nstructure, compositional understanding, and fine-grained perception, and (3)\nhigh-quality text-only supervised fine-tuning on curated instruction-response\npairs with explicit reasoning traces spanning mathematics, coding, science, and\ntool use. Notably, our model achieves competitive results without reinforcement\nlearning or preference optimization, isolating the contribution of our\ndata-centric continual pre-training approach. On the Artificial Analysis\nIntelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching\nDeepSeek-R1-0528 despite requiring significantly fewer computational resources.\nAcross ten image benchmarks, its performance is on average within five points\nof Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model\noperating within single-GPU deployment constraints. Our results demonstrate\nthat thoughtful mid-training 2 design can close substantial capability gaps\nwithout massive scale, making frontier-level multimodal reasoning accessible to\norganizations with limited infrastructure. We release the model checkpoint, all\ntraining recipes, and evaluation protocols under the MIT license to to advance\nopen-source research.",
      "authors": [
        "Shruthan Radhakrishna",
        "Aman Tiwari",
        "Aanjaneya Shukla",
        "Masoud Hashemi",
        "Rishabh Maheshwary",
        "Shiva Krishna Reddy Malay",
        "Jash Mehta",
        "Pulkit Pattnaik",
        "Saloni Mittal",
        "Khalil Slimi",
        "Kelechi Ogueji",
        "Akintunde Oladipo",
        "Soham Parikh",
        "Oluwanifemi Bamgbose",
        "Toby Liang",
        "Ahmed Masry",
        "Khyati Mahajan",
        "Sai Rajeswar Mudumba",
        "Vikas Yadav",
        "Sathwik Tejaswi Madhusudhan",
        "Torsten Scholak",
        "Sagar Davasam",
        "Srinivas Sunkara",
        "Nicholas Chapados"
      ],
      "published": "2025-10-01T17:29:35Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01141v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "Apriel-1.5-15B-Thinker是一个150亿参数的多模态推理模型，通过三阶段渐进训练方法（深度扩展、持续预训练、监督微调）实现前沿性能，无需强化学习即可在多个基准测试中媲美更大模型，且支持单GPU部署，为资源有限的组织提供先进的多模态推理能力。",
      "order": 188
    },
    {
      "arxiv_id": "2510.01136v1",
      "title": "TabINR: An Implicit Neural Representation Framework for Tabular Data\n  Imputation",
      "summary": "Tabular data builds the basis for a wide range of applications, yet\nreal-world datasets are frequently incomplete due to collection errors, privacy\nrestrictions, or sensor failures. As missing values degrade the performance or\nhinder the applicability of downstream models, and while simple imputing\nstrategies tend to introduce bias or distort the underlying data distribution,\nwe require imputers that provide high-quality imputations, are robust across\ndataset sizes and yield fast inference. We therefore introduce TabINR, an\nauto-decoder based Implicit Neural Representation (INR) framework that models\ntables as neural functions. Building on recent advances in generalizable INRs,\nwe introduce learnable row and feature embeddings that effectively deal with\nthe discrete structure of tabular data and can be inferred from partial\nobservations, enabling instance adaptive imputations without modifying the\ntrained model. We evaluate our framework across a diverse range of twelve\nreal-world datasets and multiple missingness mechanisms, demonstrating\nconsistently strong imputation accuracy, mostly matching or outperforming\nclassical (KNN, MICE, MissForest) and deep learning based models (GAIN,\nReMasker), with the clearest gains on high-dimensional datasets.",
      "authors": [
        "Vincent Ochs",
        "Florentin Bieder",
        "Sidaty el Hadramy",
        "Paul Friedrich",
        "Stephanie Taha-Mehlitz",
        "Anas Taha",
        "Philippe C. Cattin"
      ],
      "published": "2025-10-01T17:24:35Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01136v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "TabINR提出基于隐式神经表示(INR)的表格数据填补框架，通过可学习的行列嵌入处理离散表格结构，无需修改训练模型即可实现实例自适应填补。在12个真实数据集上验证表明，该方法在填补准确性上优于传统方法和深度学习模型，尤其在高维数据中表现突出。",
      "order": 189
    },
    {
      "arxiv_id": "2510.01132v1",
      "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
      "summary": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
      "authors": [
        "Ruiyi Wang",
        "Prithviraj Ammanabrolu"
      ],
      "published": "2025-10-01T17:23:04Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01132v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文系统研究多轮强化学习中训练大语言模型代理的关键设计要素，将设计空间分解为环境、奖励和策略三大支柱，通过TextWorld、ALFWorld和SWE-Gym等测试环境实证分析任务复杂度、奖励稀疏性和策略梯度方法的影响，最终提出跨支柱协同设计的训练方案。",
      "order": 190
    },
    {
      "arxiv_id": "2510.01123v1",
      "title": "Rethinking Thinking Tokens: LLMs as Improvement Operators",
      "summary": "Reasoning training incentivizes LLMs to produce long chains of thought (long\nCoT), which among other things, allows them to explore solution strategies with\nself-checking. This results in higher accuracy, but inflates context length,\ntoken/compute cost, and answer latency. We ask: Can current models leverage\ntheir metacognition to provide other combinations on this Pareto frontier,\ne.g., better accuracy with lower context length and/or latency? Abstractly, we\nview the model as an improvement operator on its own \"thoughts\" with a\ncontinuum of possible strategies. We identify an interesting inference family\nParallel-Distill-Refine (PDR), which performs the following: (i) generate\ndiverse drafts in parallel; (ii) distill them into a bounded, textual\nworkspace; and (iii) refine conditioned on this workspace, producing an output\nthat seeds the next round. Importantly, context length (hence compute cost) is\ncontrollable via degree of parallelism, and is no longer conflated with the\ntotal number of generated tokens. We report PDR instantiations of current\nmodels that give better accuracy than long CoT while incurring lower latency.\nSetting degree of parallelism to 1 yields an interesting subcase, Sequential\nRefinement (SR) (iteratively improve a single candidate answer) which provides\nperformance superior to long CoT. Success of such model orchestrations raises\nthe question whether further training could shift the Pareto frontier. To this\nend, we train an 8B thinking model with Reinforcement Learning (RL) to make it\nconsistent with PDR as the inference method. On math tasks with verifiable\nanswers, iterative pipelines surpass single-pass baselines at matched\nsequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME\n2024 and +9% on AIME 2025).",
      "authors": [
        "Lovish Madaan",
        "Aniket Didolkar",
        "Suchin Gururangan",
        "John Quan",
        "Ruan Silva",
        "Ruslan Salakhutdinov",
        "Manzil Zaheer",
        "Sanjeev Arora",
        "Anirudh Goyal"
      ],
      "published": "2025-10-01T17:08:59Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01123v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出并行蒸馏精炼(PDR)方法，将LLMs视为自身思维的改进算子，通过并行生成草稿、蒸馏到工作空间、迭代精炼的方式，在数学推理任务上实现比长思维链更高准确率且更低延迟，并训练8B模型验证该方法在AIME等任务上提升显著(+11%)。",
      "order": 191
    },
    {
      "arxiv_id": "2510.01115v1",
      "title": "Exploring Network-Knowledge Graph Duality: A Case Study in Agentic\n  Supply Chain Risk Analysis",
      "summary": "Large Language Models (LLMs) struggle with the complex, multi-modal, and\nnetwork-native data underlying financial risk. Standard Retrieval-Augmented\nGeneration (RAG) oversimplifies relationships, while specialist models are\ncostly and static. We address this gap with an LLM-centric agent framework for\nsupply chain risk analysis. Our core contribution is to exploit the inherent\nduality between networks and knowledge graphs (KG). We treat the supply chain\nnetwork as a KG, allowing us to use structural network science principles for\nretrieval. A graph traverser, guided by network centrality scores, efficiently\nextracts the most economically salient risk paths. An agentic architecture\norchestrates this graph retrieval alongside data from numerical factor tables\nand news streams. Crucially, it employs novel ``context shells'' -- descriptive\ntemplates that embed raw figures in natural language -- to make quantitative\ndata fully intelligible to the LLM. This lightweight approach enables the model\nto generate concise, explainable, and context-rich risk narratives in real-time\nwithout costly fine-tuning or a dedicated graph database.",
      "authors": [
        "Evan Heus",
        "Rick Bookstaber",
        "Dhruv Sharma"
      ],
      "published": "2025-10-01T17:02:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01115v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "financial_ai",
      "tldr_zh": "本研究提出一种基于大语言模型的智能体框架，通过探索网络与知识图谱的二元性来解决供应链风险分析问题。该方法将供应链网络视为知识图谱，利用网络中心性指标指导图遍历，结合数值因子表和新闻流数据，采用'上下文外壳'技术将定量数据转化为自然语言，无需微调即可生成实时、可解释的风险分析报告。",
      "order": 192
    },
    {
      "arxiv_id": "2510.01114v1",
      "title": "PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned\n  Diagnosis",
      "summary": "We present PRISM-Consult, a clinician-aligned panel-of-experts architecture\nthat extends the compact PRISM sequence model into a routed family of domain\nspecialists. Episodes are tokenized as structured clinical events; a\nlight-weight router reads the first few tokens and dispatches to specialist\nmodels (Cardiac-Vascular, Pulmonary, Gastro-Oesophageal, Musculoskeletal,\nPsychogenic). Each specialist inherits PRISM's small transformer backbone and\ntoken template, enabling parameter efficiency and interpretability. On\nreal-world Emergency Department cohorts, specialists exhibit smooth convergence\nwith low development perplexities across domains, while the router achieves\nhigh routing quality and large compute savings versus consult-all under a\nsafety-first policy. We detail the data methodology (initial vs. conclusive\nICD-9 families), routing thresholds and calibration, and report per-domain\nresults to avoid dominance by common events. The framework provides a practical\npath to safe, auditable, and low-latency consult at scale, and we outline\nvalidation steps-external/temporal replication, asymmetric life-threat\nthresholds, and multi-label arbitration-to meet prospective clinical deployment\nstandards.",
      "authors": [
        "Lionel Levine",
        "John Santerre",
        "Alexander S. Young",
        "T. Barry Levine",
        "Francis Campion",
        "Majid Sarrafzadeh"
      ],
      "published": "2025-10-01T17:00:05Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01114v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "PRISM-Consult提出一种临床对齐的专家委员会架构，将紧凑的PRISM序列模型扩展为领域专家路由系统。该系统将医疗事件标记化，通过轻量级路由器分配到心脏血管、肺部、胃肠、肌肉骨骼、心理等专科模型，在保持参数效率和可解释性的同时，实现高路由质量和计算节省，为规模化安全医疗咨询提供可行路径。",
      "order": 193
    },
    {
      "arxiv_id": "2510.01094v1",
      "title": "Optimizing Fairness in Production Planning: A Human-Centric Approach to\n  Machine and Workforce Allocation",
      "summary": "This work presents a two-layer, human-centric production planning framework\ndesigned to optimize both operational efficiency and workforce fairness in\nindustrial manufacturing. The first layer formulates the Order-Line allocation\nas a Constraint Programming (CP) problem, generating high-utilization\nproduction schedules that respect machine capacities, processing times, and due\ndates. The second layer models Worker-Line allocation as a Markov Decision\nProcess (MDP), integrating human factors such as worker preference, experience,\nresilience, and medical constraints into the assignment process. Three solution\nstrategies, greedy allocation, MCTS, and RL, are implemented and compared\nacross multiple evaluation scenarios. The proposed system is validated through\n16 test sessions with domain experts from the automotive industry, combining\nquantitative key performance indicators (KPIs) with expert ratings. Results\nindicate that the CP-based scheduling approach produces compact, feasible\nproduction plans with low tardiness, while the MDP-based worker allocation\nsignificantly improves fairness and preference alignment compared to baseline\napproaches. Domain experts rated both the Order-Line and Worker-Line components\nas effective and highlighted opportunities to further refine the objective\nfunction to penalize excessive earliness and improve continuity in worker\nassignments. Overall, the findings demonstrate that combining CP with\nlearning-based decision-making provides a robust approach for human-centric\nproduction planning. The approach enables simultaneous optimization of\nthroughput and workforce well-being, offering a practical foundation for fair\nand efficient manufacturing scheduling in industrial settings.",
      "authors": [
        "Alexander Nasuta",
        "Alessandro Cisi",
        "Sylwia Olbrych",
        "Gustavo Vieira",
        "Rui Fernandes",
        "Lucas Paletta",
        "Marlene Mayr",
        "Rishyank Chevuri",
        "Robert Woitsch",
        "Hans Aoyang Zhou",
        "Anas Abdelrazeq",
        "Robert H. Schmitt"
      ],
      "published": "2025-10-01T16:41:18Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01094v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出了一种双层人本生产规划框架，结合约束编程和马尔可夫决策过程优化工业制造中的机器调度与人力资源分配。系统通过CP方法生成高效生产计划，利用MDP整合员工偏好、经验等人类因素，经汽车行业专家验证可同时提升运营效率与分配公平性。",
      "order": 194
    },
    {
      "arxiv_id": "2510.01088v1",
      "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for\n  Self-Defense",
      "summary": "Ensuring Large Language Model (LLM) safety remains challenging due to the\nabsence of universal standards and reliable content validators, making it\ndifficult to obtain effective training signals. We discover that aligned models\nalready possess robust internal safety beliefs: they consistently produce\nhigh-confidence refusals to harmful requests while exhibiting high entropy when\ngenerating potentially dangerous content. This entropy gap reveals an untapped\nsignal--models intrinsically \"know\" when to refuse. We introduce Safety\nInstincts Reinforcement Learning (SIRL), which transforms this internal\nconfidence into a self-generated reward signal, eliminating dependence on\nexternal validators or human annotations. SIRL teaches models to trust their\nsafety instincts by reinforcing low-entropy refusal behaviors. Evaluated on\nLlama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against\n20+ jailbreak methods, from static prompts to adaptive attacks. Using only\n15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods\nwhile preserving performance on mathematics, coding, and conversation\nbenchmarks. Our work demonstrates that effective alignment can emerge from\nwithin, paving the way for more autonomous and robust AI safety mechanisms that\nscale without extensive human oversight.",
      "authors": [
        "Guobin Shen",
        "Dongcheng Zhao",
        "Haibo Tong",
        "Jindong Li",
        "Feifei Zhao",
        "Yi Zeng"
      ],
      "published": "2025-10-01T16:35:03Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01088v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出SIRL方法，利用LLM内部安全置信度作为自生成奖励信号，无需外部验证器即可强化模型的安全防御能力。在多种越狱攻击下保持89%+防御成功率，仅需1.5万未标注提示即可超越监督方法，同时保持数学、编程等基准性能。",
      "order": 195
    },
    {
      "arxiv_id": "2510.01077v1",
      "title": "CodeGenLink: A Tool to Find the Likely Origin and License of\n  Automatically Generated Code",
      "summary": "Large Language Models (LLMs) are widely used in software development tasks\nnowadays. Unlike reusing code taken from the Web, for LLMs' generated code,\ndevelopers are concerned about its lack of trustworthiness and possible\ncopyright or licensing violations, due to the lack of code provenance\ninformation. This paper proposes CodeGenLink, a GitHub CoPilot extension for\nVisual Studio Code aimed at (i) suggesting links containing code very similar\nto automatically generated code, and (ii) whenever possible, indicating the\nlicense of the likely origin of the code. CodeGenLink retrieves candidate links\nby combining LLMs with their web search features and then performs similarity\nanalysis between the generated and retrieved code. Preliminary results show\nthat CodeGenLink effectively filters unrelated links via similarity analysis\nand provides licensing information when available. Tool URL:\nhttps://github.com/danielebifolco/CodeGenLink Tool Video:\nhttps://youtu.be/M6nqjBf9_pw",
      "authors": [
        "Daniele Bifolco",
        "Guido Annicchiarico",
        "Pierluigi Barbiero",
        "Massimiliano Di Penta",
        "Fiorella Zampetti"
      ],
      "published": "2025-10-01T16:21:13Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.01077v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "code_generation",
      "tldr_zh": "CodeGenLink是一款GitHub CoPilot扩展工具，通过结合LLM网络搜索与代码相似性分析，为AI生成代码追溯可能来源及许可证信息，解决代码可信度与版权问题。初步验证显示能有效过滤无关链接并提供许可信息。",
      "order": 196
    },
    {
      "arxiv_id": "2510.01303v1",
      "title": "Low Rank Gradients and Where to Find Them",
      "summary": "This paper investigates low-rank structure in the gradients of the training\nloss for two-layer neural networks while relaxing the usual isotropy\nassumptions on the training data and parameters. We consider a spiked data\nmodel in which the bulk can be anisotropic and ill-conditioned, we do not\nrequire independent data and weight matrices and we also analyze both the\nmean-field and neural-tangent-kernel scalings. We show that the gradient with\nrespect to the input weights is approximately low rank and is dominated by two\nrank-one terms: one aligned with the bulk data-residue , and another aligned\nwith the rank one spike in the input data. We characterize how properties of\nthe training data, the scaling regime and the activation function govern the\nbalance between these two components. Additionally, we also demonstrate that\nstandard regularizers, such as weight decay, input noise and Jacobian\npenalties, also selectively modulate these components. Experiments on synthetic\nand real data corroborate our theoretical predictions.",
      "authors": [
        "Rishi Sonthalia",
        "Michael Murray",
        "Guido Montúfar"
      ],
      "published": "2025-10-01T16:20:19Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01303v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究两层神经网络训练损失梯度的低秩结构，放宽了训练数据和参数的各向同性假设。在尖峰数据模型中，证明了输入权重梯度近似低秩且由两个秩一分量主导：一个与数据残差对齐，另一个与输入数据中的尖峰对齐。分析了数据特性、缩放机制和激活函数对这两个分量的影响，并展示了标准正则化方法的选择性调节作用。",
      "order": 197
    },
    {
      "arxiv_id": "2510.01069v1",
      "title": "Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM\n  Reasoning",
      "summary": "While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of\nlarge language models, the faithfulness of the generated rationales remains an\nopen problem for model interpretability. We propose a novel theoretical lens\nfor this problem grounded in the Curry-Howard correspondence, which posits a\ndirect relationship between formal proofs and computer programs. Under this\nparadigm, a faithful reasoning trace is analogous to a well-typed program,\nwhere each intermediate step corresponds to a typed logical inference. We\noperationalise this analogy, presenting methods to extract and map the\ninformal, natural language steps of CoT into a formal, typed proof structure.\nSuccessfully converting a CoT trace into a well-typed proof serves as a strong,\nverifiable certificate of its computational faithfulness, moving beyond\nheuristic interpretability towards formal verification. Our framework provides\na methodology to transform plausible narrative explanations into formally\nverifiable programs, offering a path towards building more reliable and\ntrustworthy AI systems.",
      "authors": [
        "Elija Perrier"
      ],
      "published": "2025-10-01T16:06:40Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01069v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出基于Curry-Howard对应的类型化思维链框架，将自然语言推理步骤映射为形式化类型证明结构，为LLM推理过程提供可验证的忠实性保证，推动AI系统向更可靠可信方向发展。",
      "order": 198
    },
    {
      "arxiv_id": "2510.01052v1",
      "title": "Hybrid Dialogue State Tracking for Persian Chatbots: A Language\n  Model-Based Approach",
      "summary": "Dialogue State Tracking (DST) is an essential element of conversational AI\nwith the objective of deeply understanding the conversation context and leading\nit toward answering user requests. Due to high demands for open-domain and\nmulti-turn chatbots, the traditional rule-based DST is not efficient enough,\nsince it cannot provide the required adaptability and coherence for human-like\nexperiences in complex conversations. This study proposes a hybrid DST model\nthat utilizes rule-based methods along with language models, including BERT for\nslot filling and intent detection, XGBoost for intent validation, GPT for DST,\nand online agents for real-time answer generation. This model is uniquely\ndesigned to be evaluated on a comprehensive Persian multi-turn dialogue dataset\nand demonstrated significantly improved accuracy and coherence over existing\nmethods in Persian-based chatbots. The results demonstrate how effectively a\nhybrid approach may improve DST capabilities, paving the way for conversational\nAI systems that are more customized, adaptable, and human-like.",
      "authors": [
        "Samin Mahdipour Aghabagher",
        "Saeedeh Momtazi"
      ],
      "published": "2025-10-01T15:57:19Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01052v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出了一种波斯语聊天机器人混合对话状态跟踪模型，结合规则方法与语言模型（BERT用于槽填充和意图检测，XGBoost用于意图验证，GPT用于DST），在波斯语多轮对话数据集上显著提升了准确性和连贯性。",
      "order": 199
    },
    {
      "arxiv_id": "2510.01051v1",
      "title": "GEM: A Gym for Agentic LLMs",
      "summary": "The training paradigm for large language models (LLMs) is moving from static\ndatasets to experience-based learning, where agents acquire skills via\ninteracting with complex environments. To facilitate this transition we\nintroduce GEM (General Experience Maker), an open-source environment simulator\ndesigned for the age of LLMs. Analogous to OpenAI-Gym for traditional\nreinforcement learning (RL), GEM provides a standardized framework for the\nenvironment-agent interface, including asynchronous vectorized execution for\nhigh throughput, and flexible wrappers for easy extensibility. GEM also\nfeatures a diverse suite of environments, robust integrated tools, and\nsingle-file example scripts demonstrating using GEM with five popular RL\ntraining frameworks. Along with this, we also provide a set of baselines across\n24 environments using REINFORCE with Return Batch Normalization (ReBN), which\n-- unlike GRPO -- is compatible with the full RL setting of dense per-turn\nrewards and offers better credit assignment. We further conduct apple-to-apple\nbenchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings\nusing GEM to shed light on the algorithmic designs. Lastly, GEM also functions\nas a convenient evaluation toolkit besides a training environment. We hope this\nframework can help accelerate future agentic LLM research.",
      "authors": [
        "Zichen Liu",
        "Anya Sims",
        "Keyu Duan",
        "Changyu Chen",
        "Simon Yu",
        "Xiangxin Zhou",
        "Haotian Xu",
        "Shaopan Xiong",
        "Bo Liu",
        "Chenmien Tan",
        "Chuen Yang Beh",
        "Weixun Wang",
        "Hao Zhu",
        "Weiyan Shi",
        "Diyi Yang",
        "Michael Shieh",
        "Yee Whye Teh",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "published": "2025-10-01T15:55:57Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01051v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "GEM是一个专为大型语言模型设计的开源环境模拟器，类似于传统强化学习中的OpenAI-Gym。它提供标准化的环境-代理接口、异步向量化执行、多样化环境套件和集成工具，支持与五种主流RL训练框架集成。论文还通过基准测试比较了PPO、GRPO和REINFORCE算法，旨在加速未来智能LLM代理的研究发展。",
      "order": 200
    },
    {
      "arxiv_id": "2510.01048v1",
      "title": "Interpreting Language Models Through Concept Descriptions: A Survey",
      "summary": "Understanding the decision-making processes of neural networks is a central\ngoal of mechanistic interpretability. In the context of Large Language Models\n(LLMs), this involves uncovering the underlying mechanisms and identifying the\nroles of individual model components such as neurons and attention heads, as\nwell as model abstractions such as the learned sparse features extracted by\nSparse Autoencoders (SAEs). A rapidly growing line of work tackles this\nchallenge by using powerful generator models to produce open-vocabulary,\nnatural language concept descriptions for these components. In this paper, we\nprovide the first survey of the emerging field of concept descriptions for\nmodel components and abstractions. We chart the key methods for generating\nthese descriptions, the evolving landscape of automated and human metrics for\nevaluating them, and the datasets that underpin this research. Our synthesis\nreveals a growing demand for more rigorous, causal evaluation. By outlining the\nstate of the art and identifying key challenges, this survey provides a roadmap\nfor future research toward making models more transparent.",
      "authors": [
        "Nils Feldhus",
        "Laura Kopf"
      ],
      "published": "2025-10-01T15:51:44Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01048v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文首次系统综述了语言模型概念描述这一新兴领域，聚焦于使用生成模型为神经元、注意力头等组件生成自然语言概念描述的方法。文章梳理了关键生成技术、评估指标与数据集，指出当前研究亟需更严谨的因果评估，为提升模型透明度提供了发展路线图。",
      "order": 201
    },
    {
      "arxiv_id": "2510.01047v1",
      "title": "Authentic Discrete Diffusion Model",
      "summary": "We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally\nredefines prior pseudo-discrete approaches by preserving core diffusion\ncharacteristics directly in the one-hot space through a suite of coordinated\nmechanisms. Unlike conventional \"pseudo\" discrete diffusion (PDD) methods, ADD\nreformulates the diffusion input by directly using float-encoded one-hot class\ndata, without relying on diffusing in the continuous latent spaces or masking\npolicies. At its core, a timestep-conditioned cross-entropy loss is introduced\nbetween the diffusion model's outputs and the original one-hot labels. This\nsynergistic design establishes a bridge between discriminative and generative\nlearning. Our experiments demonstrate that ADD not only achieves superior\nperformance on classification tasks compared to the baseline, but also exhibits\nexcellent text generation capabilities on Image captioning. Extensive ablations\nvalidate the measurable gains of each component.",
      "authors": [
        "Xiao Li",
        "Jiaqi Zhang",
        "Shuxiang Zhang",
        "Tianshui Chen",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "published": "2025-10-01T15:51:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01047v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出真实离散扩散(ADD)框架，通过在一热空间中直接保留核心扩散特性，重新定义离散扩散方法。该框架使用浮点编码的一热类别数据作为扩散输入，引入时间步条件交叉熵损失，在分类任务上表现优于基线，并在图像描述文本生成中展示出色能力。",
      "order": 202
    },
    {
      "arxiv_id": "2510.01038v1",
      "title": "Activation-Deactivation: A General Framework for Robust Post-hoc\n  Explainable AI",
      "summary": "Black-box explainability methods are popular tools for explaining the\ndecisions of image classifiers. A major drawback of these tools is their\nreliance on mutants obtained by occluding parts of the input, leading to\nout-of-distribution images. This raises doubts about the quality of the\nexplanations. Moreover, choosing an appropriate occlusion value often requires\ndomain knowledge. In this paper we introduce a novel forward-pass paradigm\nActivation-Deactivation (AD), which removes the effects of occluded input\nfeatures from the model's decision-making by switching off the parts of the\nmodel that correspond to the occlusions. We introduce ConvAD, a drop-in\nmechanism that can be easily added to any trained Convolutional Neural Network\n(CNN), and which implements the AD paradigm. This leads to more robust\nexplanations without any additional training or fine-tuning. We prove that the\nConvAD mechanism does not change the decision-making process of the network. We\nprovide experimental evaluation across several datasets and model\narchitectures. We compare the quality of AD-explanations with explanations\nachieved using a set of masking values, using the proxies of robustness, size,\nand confidence drop-off. We observe a consistent improvement in robustness of\nAD explanations (up to 62.5%) compared to explanations obtained with\nocclusions, demonstrating that ConvAD extracts more robust explanations without\nthe need for domain knowledge.",
      "authors": [
        "Akchunya Chanchal",
        "David A. Kelly",
        "Hana Chockler"
      ],
      "published": "2025-10-01T15:42:58Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01038v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出激活-失活(AD)框架，通过关闭模型中与遮挡输入特征对应的部分来改进黑盒可解释AI方法。提出的ConvAD机制可轻松集成到任何训练好的CNN中，无需额外训练即可生成更鲁棒的解释，实验显示AD解释的鲁棒性比传统遮挡方法提升高达62.5%。",
      "order": 203
    },
    {
      "arxiv_id": "2510.01037v1",
      "title": "CurES: From Gradient Analysis to Efficient Curriculum Learning for\n  Reasoning LLMs",
      "summary": "Curriculum learning plays a crucial role in enhancing the training efficiency\nof large language models (LLMs) on reasoning tasks. However, existing methods\noften fail to adequately account for variations in prompt difficulty or rely on\nsimplistic filtering mechanisms to select prompt datasets within a narrow\ncriterion range, resulting in significant computational waste. In this work, we\napproach the problem from the perspective of reinforcement learning gradient\noptimization, offering a systematic and theoretical investigation into how to\nimprove the training efficiency of LLMs. We identify two key factors\ninfluencing training efficiency: the selection of training prompts and the\nallocation of rollout quantities across different prompts. Our theoretical\nanalysis reveals that the sampling distribution of prompts dictates the\nconvergence rate of gradient descent, while the allocation of the rollout\nquantity influences the consistency and stability of overall gradient updates.\nBased on these insights, we propose CurES, an efficient training method that\naccelerates convergence and employs Bayesian posterior estimation to minimize\ncomputational overhead. Experiments demonstrate that our CurES outperforms\nGroup Relative Policy Optimization (GRPO) by \\textbf{+3.30} points and\n\\textbf{+4.82} points with 1.5B and 7B models, respectively. Additionally,\nCurES exhibits faster convergence compared to baselines, including GRPO.",
      "authors": [
        "Yongcheng Zeng",
        "Zexu Sun",
        "Bokai Ji",
        "Erxue Min",
        "Hengyi Cai",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Haifeng Zhang",
        "Xu Chen",
        "Jun Wang"
      ],
      "published": "2025-10-01T15:41:27Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01037v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "CurES是一种基于梯度分析的高效课程学习方法，通过优化训练提示选择和rollout数量分配来提升大语言模型在推理任务上的训练效率。该方法从强化学习梯度优化角度出发，理论分析表明提示采样分布影响梯度下降收敛速度，rollout分配影响梯度更新稳定性。实验显示CurES在1.5B和7B模型上分别比GRPO提升3.30和4.82个点，且收敛速度更快。",
      "order": 204
    },
    {
      "arxiv_id": "2510.01030v1",
      "title": "Uncovering the Computational Ingredients of Human-Like Representations\n  in LLMs",
      "summary": "The ability to translate diverse patterns of inputs into structured patterns\nof behavior has been thought to rest on both humans' and machines' ability to\nlearn robust representations of relevant concepts. The rapid advancement of\ntransformer-based large language models (LLMs) has led to a diversity of\ncomputational ingredients -- architectures, fine tuning methods, and training\ndatasets among others -- but it remains unclear which of these ingredients are\nmost crucial for building models that develop human-like representations.\nFurther, most current LLM benchmarks are not suited to measuring\nrepresentational alignment between humans and models, making benchmark scores\nunreliable for assessing if current LLMs are making progress towards becoming\nuseful cognitive models. We address these limitations by first evaluating a set\nof over 70 models that widely vary in their computational ingredients on a\ntriplet similarity task, a method well established in the cognitive sciences\nfor measuring human conceptual representations, using concepts from the THINGS\ndatabase. Comparing human and model representations, we find that models that\nundergo instruction-finetuning and which have larger dimensionality of\nattention heads are among the most human aligned, while multimodal pretraining\nand parameter size have limited bearing on alignment. Correlations between\nalignment scores and scores on existing benchmarks reveal that while some\nbenchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for\ncapturing representational alignment, no existing benchmark is capable of fully\naccounting for the variance of alignment scores, demonstrating their\ninsufficiency in capturing human-AI alignment. Taken together, our findings\nhelp highlight the computational ingredients most essential for advancing LLMs\ntowards models of human conceptual representation and address a key\nbenchmarking gap in LLM evaluation.",
      "authors": [
        "Zach Studdiford",
        "Timothy T. Rogers",
        "Kushin Mukherjee",
        "Siddharth Suresh"
      ],
      "published": "2025-10-01T15:37:19Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01030v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过三重相似性任务评估70多个LLM，发现指令微调和注意力头维度是获得人类对齐表征的关键因素，而多模态预训练和参数规模影响有限。现有基准测试无法充分捕捉人机表征对齐，揭示了LLM评估中的重要空白。",
      "order": 205
    },
    {
      "arxiv_id": "2510.01025v1",
      "title": "Shape Happens: Automatic Feature Manifold Discovery in LLMs via\n  Supervised Multi-Dimensional Scaling",
      "summary": "The linear representation hypothesis states that language models (LMs) encode\nconcepts as directions in their latent space, forming organized,\nmultidimensional manifolds. Prior efforts focus on discovering specific\ngeometries for specific features, and thus lack generalization. We introduce\nSupervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to\nautomatically discover feature manifolds. We apply SMDS to temporal reasoning\nas a case study, finding that different features form various geometric\nstructures such as circles, lines, and clusters. SMDS reveals many insights on\nthese structures: they consistently reflect the properties of the concepts they\nrepresent; are stable across model families and sizes; actively support\nreasoning in models; and dynamically reshape in response to context changes.\nTogether, our findings shed light on the functional role of feature manifolds,\nsupporting a model of entity-based reasoning in which LMs encode and transform\nstructured representations.",
      "authors": [
        "Federico Tiblias",
        "Irina Bigoulaeva",
        "Jingcheng Niu",
        "Simone Balloccu",
        "Iryna Gurevych"
      ],
      "published": "2025-10-01T15:30:47Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01025v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出监督多维缩放(SMDS)方法，自动发现语言模型中特征流形的几何结构。研究表明不同特征形成圆形、直线等多样几何形态，这些结构稳定反映概念属性、支持模型推理，并随上下文动态变化，揭示了特征流形在实体推理中的功能作用。",
      "order": 206
    },
    {
      "arxiv_id": "2510.01020v1",
      "title": "The Good, the Bad, and the Sampled: a No-Regret Approach to Safe Online\n  Classification",
      "summary": "We study the problem of sequentially testing individuals for a binary disease\noutcome whose true risk is governed by an unknown logistic model. At each\nround, a patient arrives with feature vector $x_t$, and the decision maker may\neither pay to administer a (noiseless) diagnostic test--revealing the true\nlabel--or skip testing and predict the patient's disease status based on their\nfeature vector and prior history. Our goal is to minimize the total number of\ncostly tests required while guaranteeing that the fraction of\nmisclassifications does not exceed a prespecified error tolerance $\\alpha$,\nwith probability at least $1-\\delta$. To address this, we develop a novel\nalgorithm that interleaves label-collection and distribution estimation to\nestimate both $\\theta^{*}$ and the context distribution $P$, and computes a\nconservative, data-driven threshold $\\tau_t$ on the logistic score\n$|x_t^\\top\\theta|$ to decide when testing is necessary. We prove that, with\nprobability at least $1-\\delta$, our procedure does not exceed the target\nmisclassification rate, and requires only $O(\\sqrt{T})$ excess tests compared\nto the oracle baseline that knows both $\\theta^{*}$ and the patient feature\ndistribution $P$. This establishes the first no-regret guarantees for\nerror-constrained logistic testing, with direct applications to cost-sensitive\nmedical screening. Simulations corroborate our theoretical guarantees, showing\nthat in practice our procedure efficiently estimates $\\theta^{*}$ while\nretaining safety guarantees, and does not require too many excess tests.",
      "authors": [
        "Tavor Z. Baharav",
        "Spyros Dragazis",
        "Aldo Pacchiano"
      ],
      "published": "2025-10-01T15:28:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01020v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出一种在线安全分类算法，通过交替进行标签收集和分布估计，在保证误分类率不超过预设阈值的前提下，最小化昂贵的诊断测试次数。该算法在医疗筛查等成本敏感场景中具有应用价值，理论证明其测试次数仅比已知真实参数和特征分布的神谕基线多O(√T)。",
      "order": 207
    },
    {
      "arxiv_id": "2510.01006v1",
      "title": "Integrating AI and Ensemble Forecasting: Explainable Materials Planning\n  with Scorecards and Trend Insights for a Large-Scale Manufacturer",
      "summary": "This paper presents a practical architecture for after-sales demand\nforecasting and monitoring that unifies a revenue- and cluster-aware ensemble\nof statistical, machine-learning, and deep-learning models with a role-driven\nanalytics layer for scorecards and trend diagnostics. The framework ingests\nexogenous signals (installed base, pricing, macro indicators, life cycle,\nseasonality) and treats COVID-19 as a distinct regime, producing country-part\nforecasts with calibrated intervals. A Pareto-aware segmentation forecasts\nhigh-revenue items individually and pools the long tail via clusters, while\nhorizon-aware ensembling aligns weights with business-relevant losses (e.g.,\nWMAPE). Beyond forecasts, a performance scorecard delivers decision-focused\ninsights: accuracy within tolerance thresholds by revenue share and count, bias\ndecomposition (over- vs under-forecast), geographic and product-family\nhotspots, and ranked root causes tied to high-impact part-country pairs. A\ntrend module tracks trajectories of MAPE/WMAPE and bias across recent months,\nflags entities that are improving or deteriorating, detects change points\naligned with known regimes, and attributes movements to lifecycle and seasonal\nfactors. LLMs are embedded in the analytics layer to generate role-aware\nnarratives and enforce reporting contracts. They standardize business\ndefinitions, automate quality checks and reconciliations, and translate\nquantitative results into concise, explainable summaries for planners and\nexecutives. The system exposes a reproducible workflow -- request\nspecification, model execution, database-backed artifacts, and AI-generated\nnarratives -- so planners can move from \"How accurate are we now?\" to \"Where is\naccuracy heading and which levers should we pull?\", closing the loop between\nforecasting, monitoring, and inventory decisions across more than 90 countries\nand about 6,000 parts.",
      "authors": [
        "Saravanan Venkatachalam"
      ],
      "published": "2025-10-01T15:14:10Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01006v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "financial_ai",
      "tldr_zh": "本文提出一种售后需求预测与监控架构，融合统计、机器学习和深度学习模型的收益感知集成方法，结合评分卡和趋势诊断分析层。该系统整合外生信号，采用帕累托感知分割和时域感知集成，为大规制造商提供可解释的材料规划方案，并嵌入大语言模型生成角色化分析报告，覆盖90多个国家和6000多个零部件的库存决策闭环。",
      "order": 208
    },
    {
      "arxiv_id": "2510.01004v1",
      "title": "TextCAM: Explaining Class Activation Map with Text",
      "summary": "Deep neural networks (DNNs) have achieved remarkable success across domains\nbut remain difficult to interpret, limiting their trustworthiness in\nhigh-stakes applications. This paper focuses on deep vision models, for which a\ndominant line of explainability methods are Class Activation Mapping (CAM) and\nits variants working by highlighting spatial regions that drive predictions. We\nfigure out that CAM provides little semantic insight into what attributes\nunderlie these activations. To address this limitation, we propose TextCAM, a\nnovel explanation framework that enriches CAM with natural languages. TextCAM\ncombines the precise spatial localization of CAM with the semantic alignment of\nvision-language models (VLMs). Specifically, we derive channel-level semantic\nrepresentations using CLIP embeddings and linear discriminant analysis, and\naggregate them with CAM weights to produce textual descriptions of salient\nvisual evidence. This yields explanations that jointly specify where the model\nattends and what visual attributes likely support its decision. We further\nextend TextCAM to generate feature channels into semantically coherent groups,\nenabling more fine-grained visual-textual explanations. Experiments on\nImageNet, CLEVR, and CUB demonstrate that TextCAM produces faithful and\ninterpretable rationales that improve human understanding, detect spurious\ncorrelations, and preserve model fidelity.",
      "authors": [
        "Qiming Zhao",
        "Xingjian Li",
        "Xiaoyu Cao",
        "Xiaolong Wu",
        "Min Xu"
      ],
      "published": "2025-10-01T15:11:14Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01004v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "TextCAM是一种新颖的可解释性框架，通过将类别激活图(CAM)与自然语言相结合，为深度视觉模型提供语义丰富的解释。该方法利用CLIP嵌入和线性判别分析生成通道级语义表示，结合CAM权重产生文本描述，同时指明模型关注区域及其决策依据的视觉属性。在ImageNet等数据集上的实验表明，TextCAM能生成忠实可解释的推理，提升人类理解并检测虚假相关性。",
      "order": 209
    },
    {
      "arxiv_id": "2510.00976v1",
      "title": "Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware\n  Secure Aggregation",
      "summary": "Rare-disease diagnosis remains one of the most pressing challenges in digital\nhealth, hindered by extreme data scarcity, privacy concerns, and the limited\nresources of edge devices. This paper proposes the Adaptive Federated Few-Shot\nRare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)\nfew-shot federated optimization with meta-learning to generalize from limited\npatient samples, (ii) energy-aware client scheduling to mitigate device\ndropouts and ensure balanced participation, and (iii) secure aggregation with\ncalibrated differential privacy to safeguard sensitive model updates. Unlike\nprior work that addresses these aspects in isolation, AFFR unifies them into a\nmodular pipeline deployable on real-world clinical networks. Experimental\nevaluation on simulated rare-disease detection datasets demonstrates up to 10%\nimprovement in accuracy compared with baseline FL, while reducing client\ndropouts by over 50% without degrading convergence. Furthermore,\nprivacy-utility trade-offs remain within clinically acceptable bounds. These\nfindings highlight AFFR as a practical pathway for equitable and trustworthy\nfederated diagnosis of rare conditions.",
      "authors": [
        "Aueaphum Aueawatthanaphisut"
      ],
      "published": "2025-10-01T14:52:07Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00976v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出自适应联邦少样本罕见病诊断框架AFFR，整合元学习联邦优化、能量感知客户端调度和安全聚合三大模块，在模拟罕见病数据集上实现准确率提升10%、客户端退出率降低50%，为临床网络提供实用化公平可信诊断方案。",
      "order": 210
    },
    {
      "arxiv_id": "2510.00967v1",
      "title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via\n  Agentic RL",
      "summary": "Designing and optimizing task-specific quantum circuits are crucial to\nleverage the advantage of quantum computing. Recent large language model\n(LLM)-based quantum circuit generation has emerged as a promising automatic\nsolution. However, the fundamental challenges remain unaddressed: (i)\nparameterized quantum gates require precise numerical values for optimal\nperformance, which also depend on multiple aspects, including the number of\nquantum gates, their parameters, and the layout/depth of the circuits. (ii)\nLLMs often generate low-quality or incorrect quantum circuits due to the lack\nof quantum domain-specific knowledge. We propose QUASAR, an agentic\nreinforcement learning (RL) framework for quantum circuits generation and\noptimization based on tool-augmented LLMs. To align the LLM with\nquantum-specific knowledge and improve the generated quantum circuits, QUASAR\ndesigns (i) a quantum circuit verification approach with external quantum\nsimulators and (ii) a sophisticated hierarchical reward mechanism in RL\ntraining. Extensive evaluation shows improvements in both syntax and semantic\nperformance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR\nhas achieved the validity of 99.31% in Pass@1 and 100% in Pass@10,\noutperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several\nsupervised-fine-tuning (SFT)-only and RL-only baselines.",
      "authors": [
        "Cong Yu",
        "Valter Uotila",
        "Shilong Deng",
        "Qingyuan Wu",
        "Tuo Shi",
        "Songlin Jiang",
        "Lei You",
        "Bo Zhao"
      ],
      "published": "2025-10-01T14:40:04Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00967v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "code_generation",
      "tldr_zh": "QUASAR提出了一种基于工具增强大语言模型的强化学习框架，用于量子电路生成与优化。该框架通过量子模拟器验证和分层奖励机制，解决了参数化量子门数值精度和领域知识缺乏的问题。实验表明，QUASAR在4B参数模型上实现了99.31%的Pass@1和100%的Pass@10有效性，超越了GPT-4o等工业级模型。",
      "order": 211
    },
    {
      "arxiv_id": "2510.00966v1",
      "title": "Deep Learning-Based Approach for Improving Relational Aggregated Search",
      "summary": "Due to an information explosion on the internet, there is a need for the\ndevelopment of aggregated search systems that can boost the retrieval and\nmanagement of content in various formats. To further improve the clustering of\nArabic text data in aggregated search environments, this research investigates\nthe application of advanced natural language processing techniques, namely\nstacked autoencoders and AraBERT embeddings. By transcending the limitations of\ntraditional search engines, which are imprecise, not contextually relevant, and\nnot personalized, we offer more enriched, context-aware characterizations of\nsearch results, so we used a K-means clustering algorithm to discover\ndistinctive features and relationships in these results, we then used our\napproach on different Arabic queries to evaluate its effectiveness. Our model\nillustrates that using stacked autoencoders in representation learning suits\nclustering tasks and can significantly improve clustering search results. It\nalso demonstrates improved accuracy and relevance of search results.",
      "authors": [
        "Sara Saad Soliman",
        "Ahmed Younes",
        "Islam Elkabani",
        "Ashraf Elsayed"
      ],
      "published": "2025-10-01T14:37:38Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.00966v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出一种基于堆叠自编码器和AraBERT嵌入的深度学习方法，用于改进阿拉伯语聚合搜索中的文本聚类效果。通过K-means算法发现搜索结果中的特征关系，实验表明该方法能显著提升搜索结果的准确性和相关性。",
      "order": 212
    },
    {
      "arxiv_id": "2510.00960v1",
      "title": "A Neuro-Fuzzy System for Interpretable Long-Term Stock Market\n  Forecasting",
      "summary": "In the complex landscape of multivariate time series forecasting, achieving\nboth accuracy and interpretability remains a significant challenge. This paper\nintroduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network\narchitecture combined with multi-head self-attention and fuzzy inference\nsystems to analyze multivariate stock market data and conduct long-term time\nseries forecasting. The method leverages LSTM networks and temporal attention\nto condense multivariate data into interpretable features suitable for fuzzy\ninference systems. The resulting architecture offers comparable forecasting\nperformance to conventional models such as ARIMA and LSTM while providing\nmeaningful information flow within the network. The method was examined on the\nreal world stock market index S\\&P500. Initial results show potential for\ninterpretable forecasting and identify current performance tradeoffs,\nsuggesting practical application in understanding and forecasting stock market\nbehavior.",
      "authors": [
        "Miha Ožbot",
        "Igor Škrjanc",
        "Vitomir Štruc"
      ],
      "published": "2025-10-01T14:33:07Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00960v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "financial_ai",
      "tldr_zh": "本文提出Fuzzformer模型，结合LSTM、多头自注意力与模糊推理系统，用于多变量股市数据的长期预测。在S&P500指数上验证，在保持与ARIMA、LSTM相当预测性能的同时，提供网络内部信息流的可解释性，揭示了当前性能权衡，具有实际应用潜力。",
      "order": 213
    },
    {
      "arxiv_id": "2510.00958v1",
      "title": "Test-Time Search in Neural Graph Coarsening Procedures for the\n  Capacitated Vehicle Routing Problem",
      "summary": "The identification of valid inequalities, such as the rounded capacity\ninequalities (RCIs), is a key component of cutting plane methods for the\nCapacitated Vehicle Routing Problem (CVRP). While a deep learning-based\nseparation method can learn to find high-quality cuts, our analysis reveals\nthat the model produces fewer cuts than expected because it is insufficiently\nsensitive to generate a diverse set of generated subsets. This paper proposes\nan alternative: enhancing the performance of a trained model at inference time\nthrough a new test-time search with stochasticity. First, we introduce\nstochastic edge selection into the graph coarsening procedure, replacing the\npreviously proposed greedy approach. Second, we propose the Graph Coarsening\nHistory-based Partitioning (GraphCHiP) algorithm, which leverages coarsening\nhistory to identify not only RCIs but also, for the first time, the Framed\ncapacity inequalities (FCIs). Experiments on randomly generated CVRP instances\ndemonstrate the effectiveness of our approach in reducing the dual gap compared\nto the existing neural separation method. Additionally, our method discovers\neffective FCIs on a specific instance, despite the challenging nature of\nidentifying such cuts.",
      "authors": [
        "Yoonju Sim",
        "Hyeonah Kim",
        "Changhyun Kwon"
      ],
      "published": "2025-10-01T14:31:36Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00958v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对带容量约束的车辆路径问题，提出了一种测试时搜索方法改进神经图粗化过程。通过引入随机边选择和基于粗化历史的分割算法，有效提升了切割平面的生成质量和多样性，显著减少了对偶间隙，并能首次识别框架容量不等式。",
      "order": 214
    },
    {
      "arxiv_id": "2510.00956v1",
      "title": "Bridging the Gap Between Simulated and Real Network Data Using Transfer\n  Learning",
      "summary": "Machine Learning (ML)-based network models provide fast and accurate\npredictions for complex network behaviors but require substantial training\ndata. Collecting such data from real networks is often costly and limited,\nespecially for critical scenarios like failures. As a result, researchers\ncommonly rely on simulated data, which reduces accuracy when models are\ndeployed in real environments. We propose a hybrid approach leveraging transfer\nlearning to combine simulated and real-world data. Using RouteNet-Fermi, we\nshow that fine-tuning a pre-trained model with a small real dataset\nsignificantly improves performance. Our experiments with OMNeT++ and a custom\ntestbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay\nprediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and\nwith 50 scenarios, by 48%.",
      "authors": [
        "Carlos Güemes-Palau",
        "Miquel Ferriol-Galmés",
        "Jordi Paillisse-Vilanova",
        "Albert López-Brescó",
        "Pere Barlet-Ros",
        "Albert Cabellos-Aparicio"
      ],
      "published": "2025-10-01T14:29:47Z",
      "primary_category": "cs.NI",
      "arxiv_url": "https://arxiv.org/abs/2510.00956v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种结合迁移学习的混合方法，利用模拟数据预训练网络模型，再通过少量真实数据微调，显著提升网络性能预测精度。实验表明仅需10个真实场景即可将包延迟预测误差降低37%，50个场景可降低48%。",
      "order": 215
    },
    {
      "arxiv_id": "2510.01299v1",
      "title": "Enhancing the development of Cherenkov Telescope Array control software\n  with Large Language Models",
      "summary": "We develop AI agents based on instruction-finetuned large language models\n(LLMs) to assist in the engineering and operation of the Cherenkov Telescope\nArray Observatory (CTAO) Control and Data Acquisition Software (ACADA). These\nagents align with project-specific documentation and codebases, understand\ncontextual information, interact with external APIs, and communicate with users\nin natural language. We present our progress in integrating these features into\nCTAO pipelines for operations and offline data analysis.",
      "authors": [
        "Dmitriy Kostunin",
        "Elisa Jones",
        "Vladimir Sotnikov",
        "Valery Sotnikov",
        "Sergo Golovachev",
        "Alexandre Strube"
      ],
      "published": "2025-10-01T14:14:41Z",
      "primary_category": "astro-ph.IM",
      "arxiv_url": "https://arxiv.org/abs/2510.01299v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "code_generation",
      "tldr_zh": "本研究基于指令微调的大语言模型开发AI代理，用于辅助切伦科夫望远镜阵列观测站控制与数据采集软件的工程开发与运维。这些代理能够理解项目文档与代码库、与外部API交互，并以自然语言与用户沟通，已集成至CTAO的操作和离线数据分析流程中。",
      "order": 216
    },
    {
      "arxiv_id": "2510.00922v1",
      "title": "On Discovering Algorithms for Adversarial Imitation Learning",
      "summary": "Adversarial Imitation Learning (AIL) methods, while effective in settings\nwith limited expert demonstrations, are often considered unstable. These\napproaches typically decompose into two components: Density Ratio (DR)\nestimation $\\frac{\\rho_E}{\\rho_{\\pi}}$, where a discriminator estimates the\nrelative occupancy of state-action pairs under the policy versus the expert;\nand Reward Assignment (RA), where this ratio is transformed into a reward\nsignal used to train the policy. While significant research has focused on\nimproving density estimation, the role of reward assignment in influencing\ntraining dynamics and final policy performance has been largely overlooked. RA\nfunctions in AIL are typically derived from divergence minimization objectives,\nrelying heavily on human design and ingenuity. In this work, we take a\ndifferent approach: we investigate the discovery of data-driven RA functions,\ni.e, based directly on the performance of the resulting imitation policy. To\nthis end, we leverage an LLM-guided evolutionary framework that efficiently\nexplores the space of RA functions, yielding \\emph{Discovered Adversarial\nImitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably,\nDAIL generalises across unseen environments and policy optimization algorithms,\noutperforming the current state-of-the-art of \\emph{human-designed} baselines.\nFinally, we analyse why DAIL leads to more stable training, offering novel\ninsights into the role of RA functions in the stability of AIL. Code is\npublicly available: https://github.com/shshnkreddy/DAIL.",
      "authors": [
        "Shashank Reddy Chirra",
        "Jayden Teoh",
        "Praveen Paruchuri",
        "Pradeep Varakantham"
      ],
      "published": "2025-10-01T14:02:05Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00922v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出DAIL算法，通过LLM引导的进化框架自动发现对抗模仿学习中的奖励分配函数，解决了传统方法依赖人工设计的问题。DAIL在未见环境和策略优化算法中表现出优越的泛化能力和训练稳定性，超越了现有最佳人工设计基线。",
      "order": 217
    },
    {
      "arxiv_id": "2510.00919v2",
      "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in\n  Olympic-Level Physics Problem Solving",
      "summary": "Retrieval-augmented generation (RAG) with foundation models has achieved\nstrong performance across diverse tasks, but their capacity for expert-level\nreasoning-such as solving Olympiad-level physics problems-remains largely\nunexplored. Inspired by the way students prepare for competitions by reviewing\npast problems, we investigate the potential of RAG to enhance physics reasoning\nin foundation models. We introduce PhoPile, a high-quality multimodal dataset\nspecifically designed for Olympiad-level physics, enabling systematic study of\nretrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,\ncapturing the inherently multimodal nature of physics problem solving. Using\nPhoPile, we benchmark RAG-augmented foundation models, covering both large\nlanguage models (LLMs) and large multimodal models (LMMs) with multiple\nretrievers. Our results demonstrate that integrating retrieval with physics\ncorpora can improve model performance, while also highlighting challenges that\nmotivate further research in retrieval-augmented physics reasoning.",
      "authors": [
        "Shunfeng Zheng",
        "Yudi Zhang",
        "Meng Fang",
        "Zihan Zhang",
        "Zhitan Wu",
        "Mykola Pechenizkiy",
        "Ling Chen"
      ],
      "published": "2025-10-01T13:57:53Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00919v2",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "education_ai",
      "tldr_zh": "本研究引入PhoPile数据集，系统评估检索增强生成在奥林匹克级物理问题解决中的表现，发现结合物理语料检索能提升基础模型性能，同时揭示了多模态物理推理面临的挑战。",
      "order": 218
    },
    {
      "arxiv_id": "2510.00915v1",
      "title": "Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect\n  Verifiers",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) trains policies against\nautomated verifiers to avoid costly human labeling. To reduce vulnerability to\nverifier hacking, many RLVR systems collapse rewards to binary $\\{0,1\\}$ during\ntraining. This choice carries a cost: it introduces \\textit{false negatives}\n(rejecting correct answers, FNs) and \\textit{false positives} (accepting\nincorrect ones, FPs). For instance, a rule-based checker may mark the correct\nfraction $\\frac{12}{36}$ as wrong when compared against the canonical\n$\\frac{1}{3}$ due to brittle parsing/equivalence rules (FN), while a large\nlanguage model (LLM) judges can be gamed by superficial cues or even a single\nadversarial token, yielding inflated correctness for wrong solutions (FP). We\nformalize verifier unreliability by modeling the verifier as a stochastic\nreward channel with asymmetric noise rates. From this abstraction, we derive\ntwo correction algorithms for verifier errors. The first is a \\textit{backward}\ncorrection that de-biases the observed binary reward to recover an\n\\textit{unbiased} estimator of the clean policy gradient. The second is a\n\\textit{forward} correction that reweights score-function terms so that the\nexpected update direction aligns with the \\textit{clean gradient}; notably, it\nrequires only the FN rate. We implement both as lightweight hooks in a group\nrelative policy optimization (GRPO)-based RLVR pipeline and evaluate them on\nmath-reasoning models and benchmarks. Across models and datasets, both\ncorrections improve over uncorrected training; the forward variant converges\nfaster and remains stable under heavier noise. Finally, we show a practical\nappeal mechanism in which a lightweight LLM verifier estimates the FN rate\nonline by rechecking rule-based negatives, obtaining outperformance compared\nwith other state-of-the-art contenders.",
      "authors": [
        "Xin-Qiang Cai",
        "Wei Wang",
        "Feng Liu",
        "Tongliang Liu",
        "Gang Niu",
        "Masashi Sugiyama"
      ],
      "published": "2025-10-01T13:56:44Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00915v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出在验证器不可靠的强化学习环境中，通过建模验证器为具有不对称噪声率的随机奖励通道，开发了前向和后向两种校正算法来修正验证错误。这些轻量级方法在数学推理任务中表现出优于未校正训练的性能，其中前向校正收敛更快且在强噪声下更稳定。",
      "order": 219
    },
    {
      "arxiv_id": "2510.00911v1",
      "title": "RiskPO: Risk-based Policy Optimization via Verifiable Reward for LLM\n  Post-Training",
      "summary": "Reinforcement learning with verifiable reward has recently emerged as a\ncentral paradigm for post-training large language models (LLMs); however,\nprevailing mean-based methods, such as Group Relative Policy Optimization\n(GRPO), suffer from entropy collapse and limited reasoning gains. We argue that\nthese issues stem from overemphasizing high-probability output sequences while\nneglecting rare but informative reasoning paths. To address these challenges,\nwe propose Risk-based Policy Optimization (RiskPO), which substitutes classical\nmean-based objectives with principled risk measures. Specifically, we introduce\na Mixed Value-at-Risk objective that integrates weighted attention over\nmultiple regions of the reward distribution, thereby amplifying gradient\nsignals on challenging instances and preventing overconfident convergence. We\nfurther design a bundling scheme that aggregates multiple questions into\nbundles, thus enriching the feedback signal and yielding more stable and\ninformative training dynamics. Theoretically, we prove that the risk-averse\nupdate alleviates entropy collapse and promotes exploration. Numerically,\nRiskPO achieves consistent and significant improvements in mathematical\nreasoning, multi-modal reasoning, and code generation benchmarks, surpassing\nGRPO and its variants on both Pass@1 and Pass@k metrics. Our results\ndemonstrate that risk-based optimization provides a rigorous and effective\nparadigm for enhancing LLM reasoning capabilities.",
      "authors": [
        "Tao Ren",
        "Jinyang Jiang",
        "Hui Yang",
        "Wan Tian",
        "Minhao Zou",
        "Guanghao Li",
        "Zishi Zhang",
        "Qinghao Wang",
        "Shentao Qin",
        "Yanjun Zhao",
        "Rui Tao",
        "Hui Shao",
        "Yijie Peng"
      ],
      "published": "2025-10-01T13:53:09Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00911v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出RiskPO方法，通过风险度量替代传统均值目标优化大语言模型后训练，解决熵崩溃问题并增强推理能力，在数学推理、多模态推理和代码生成任务中表现优异。",
      "order": 220
    },
    {
      "arxiv_id": "2510.00909v1",
      "title": "\"We are not Future-ready\": Understanding AI Privacy Risks and Existing\n  Mitigation Strategies from the Perspective of AI Developers in Europe",
      "summary": "The proliferation of AI has sparked privacy concerns related to training\ndata, model interfaces, downstream applications, and more. We interviewed 25 AI\ndevelopers based in Europe to understand which privacy threats they believe\npose the greatest risk to users, developers, and businesses and what protective\nstrategies, if any, would help to mitigate them. We find that there is little\nconsensus among AI developers on the relative ranking of privacy risks. These\ndifferences stem from salient reasoning patterns that often relate to human\nrather than purely technical factors. Furthermore, while AI developers are\naware of proposed mitigation strategies for addressing these risks, they\nreported minimal real-world adoption. Our findings highlight both gaps and\nopportunities for empowering AI developers to better address privacy risks in\nAI.",
      "authors": [
        "Alexandra Klymenko",
        "Stephen Meisenbacher",
        "Patrick Gage Kelley",
        "Sai Teja Peddinti",
        "Kurt Thomas",
        "Florian Matthes"
      ],
      "published": "2025-10-01T13:51:33Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.00909v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过访谈25位欧洲AI开发者，揭示了AI隐私风险评估缺乏共识的现状。研究发现隐私风险认知差异主要源于人为因素而非纯技术问题，且现有缓解策略在实际应用中采用率极低。研究强调了在赋能开发者应对AI隐私风险方面存在的差距与机遇。",
      "order": 221
    },
    {
      "arxiv_id": "2510.00908v1",
      "title": "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval\n  with Multilingual LLMs",
      "summary": "Cross-lingual information retrieval (CLIR) addresses the challenge of\nretrieving relevant documents written in languages different from that of the\noriginal query. Research in this area has typically framed the task as\nmonolingual retrieval augmented by translation, treating retrieval methods and\ncross-lingual capabilities in isolation. Both monolingual and cross-lingual\nretrieval usually follow a pipeline of query expansion, ranking, re-ranking\nand, increasingly, question answering. Recent advances, however, have shifted\nfrom translation-based methods toward embedding-based approaches and leverage\nmultilingual large language models (LLMs), for which aligning representations\nacross languages remains a central challenge. The emergence of cross-lingual\nembeddings and multilingual LLMs has introduced a new paradigm, offering\nimproved retrieval performance and enabling answer generation. This survey\nprovides a comprehensive overview of developments from early translation-based\nmethods to state-of-the-art embedding-driven and generative techniques. It\npresents a structured account of core CLIR components, evaluation practices,\nand available resources. Persistent challenges such as data imbalance and\nlinguistic variation are identified, while promising directions are suggested\nfor advancing equitable and effective cross-lingual information retrieval. By\nsituating CLIR within the broader landscape of information retrieval and\nmultilingual language processing, this work not only reviews current\ncapabilities but also outlines future directions for building retrieval systems\nthat are robust, inclusive, and adaptable.",
      "authors": [
        "Roksana Goworek",
        "Olivia Macmillan-Scott",
        "Eda B. Özyiğit"
      ],
      "published": "2025-10-01T13:50:05Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.00908v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文综述跨语言信息检索(CLIR)的发展，从早期翻译方法到基于嵌入和多语言大语言模型的先进技术。重点分析跨语言表示对齐的核心挑战，涵盖检索组件、评估方法和资源，指出数据不平衡和语言变异等持续问题，并展望构建鲁棒、包容、自适应检索系统的未来方向。",
      "order": 222
    },
    {
      "arxiv_id": "2510.00906v1",
      "title": "TubeDAgger: Reducing the Number of Expert Interventions with Stochastic\n  Reach-Tubes",
      "summary": "Interactive Imitation Learning deals with training a novice policy from\nexpert demonstrations in an online fashion. The established DAgger algorithm\ntrains a robust novice policy by alternating between interacting with the\nenvironment and retraining of the network. Many variants thereof exist, that\ndiffer in the method of discerning whether to allow the novice to act or return\ncontrol to the expert. We propose the use of stochastic reachtubes - common in\nverification of dynamical systems - as a novel method for estimating the\nnecessity of expert intervention. Our approach does not require fine-tuning of\ndecision thresholds per environment and effectively reduces the number of\nexpert interventions, especially when compared with related approaches that\nmake use of a doubt classification model.",
      "authors": [
        "Julian Lemmel",
        "Manuel Kranzl",
        "Adam Lamine",
        "Philipp Neubauer",
        "Radu Grosu",
        "Sophie A. Neubauer"
      ],
      "published": "2025-10-01T13:45:16Z",
      "primary_category": "eess.SY",
      "arxiv_url": "https://arxiv.org/abs/2510.00906v1",
      "primary_area": "vla_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "TubeDAgger提出了一种基于随机可达管的交互式模仿学习方法，通过估计专家干预的必要性，无需针对每个环境微调决策阈值，有效减少了专家干预次数，相比基于怀疑分类模型的现有方法表现更优。",
      "order": 223
    },
    {
      "arxiv_id": "2510.00894v1",
      "title": "FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge\n  Graphs",
      "summary": "Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including\ntext and images, to enhance entity and relation representations. Notably,\ndifferent modalities for the same entity often present complementary and\ndiverse information. However, existing MMKG methods primarily align modalities\ninto a shared space, which tends to overlook the distinct contributions of\nspecific modalities, limiting their performance particularly in low-resource\nsettings. To address this challenge, we propose FusionAdapter for the learning\nof few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an\nadapter module that enables efficient adaptation of each modality to unseen\nrelations and (2) a fusion strategy that integrates multimodal entity\nrepresentations while preserving diverse modality-specific characteristics. By\neffectively adapting and fusing information from diverse modalities,\nFusionAdapter improves generalization to novel relations with minimal\nsupervision. Extensive experiments on two benchmark MMKG datasets demonstrate\nthat FusionAdapter achieves superior performance over state-of-the-art methods.",
      "authors": [
        "Ran Liu",
        "Yuan Fang",
        "Xiaoli Li"
      ],
      "published": "2025-10-01T13:36:56Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00894v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出FusionAdapter方法，针对多模态知识图谱中的少样本关系学习问题。通过适配器模块实现各模态对新关系的有效适应，并采用融合策略在整合多模态实体表征的同时保留各模态特性，在低资源场景下显著提升对新关系的泛化能力。",
      "order": 224
    },
    {
      "arxiv_id": "2510.00890v1",
      "title": "Span-level Detection of AI-generated Scientific Text via Contrastive\n  Learning and Structural Calibration",
      "summary": "The rapid adoption of large language models (LLMs) in scientific writing\nraises serious concerns regarding authorship integrity and the reliability of\nscholarly publications. Existing detection approaches mainly rely on\ndocument-level classification or surface-level statistical cues; however, they\nneglect fine-grained span localization, exhibit weak calibration, and often\nfail to generalize across disciplines and generators. To address these\nlimitations, we present Sci-SpanDet, a structure-aware framework for detecting\nAI-generated scholarly texts. The proposed method combines section-conditioned\nstylistic modeling with multi-level contrastive learning to capture nuanced\nhuman-AI differences while mitigating topic dependence, thereby enhancing\ncross-domain robustness. In addition, it integrates BIO-CRF sequence labeling\nwith pointer-based boundary decoding and confidence calibration to enable\nprecise span-level detection and reliable probability estimates. Extensive\nexperiments on a newly constructed cross-disciplinary dataset of 100,000\nannotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,\nLLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with\nF1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows\nstrong resilience under adversarial rewriting and maintains balanced accuracy\nacross IMRaD sections and diverse disciplines, substantially surpassing\nexisting baselines. To ensure reproducibility and to foster further research on\nAI-generated text detection in scholarly documents, the curated dataset and\nsource code will be publicly released upon publication.",
      "authors": [
        "Zhen Yin",
        "Shenghua Wang"
      ],
      "published": "2025-10-01T13:35:14Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00890v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "Sci-SpanDet是一种基于对比学习和结构校准的AI生成科学文本检测框架，通过章节条件风格建模和多层次对比学习捕捉人机写作差异，结合BIO-CRF序列标注与边界解码实现细粒度片段定位，在跨学科数据集上达到SOTA性能，F1(AI)达80.17，对对抗性重写具有强鲁棒性。",
      "order": 225
    },
    {
      "arxiv_id": "2510.00883v1",
      "title": "GLAI: GreenLightningAI for Accelerated Training through Knowledge\n  Decoupling",
      "summary": "In this work we introduce GreenLightningAI (GLAI), a new architectural block\ndesigned as an alternative to conventional MLPs. The central idea is to\nseparate two types of knowledge that are usually entangled during training: (i)\n*structural knowledge*, encoded by the stable activation patterns induced by\nReLU activations; and (ii) *quantitative knowledge*, carried by the numerical\nweights and biases. By fixing the structure once stabilized, GLAI reformulates\nthe MLP as a combination of paths, where only the quantitative component is\noptimized. This reformulation retains the universal approximation capabilities\nof MLPs, yet achieves a more efficient training process, reducing training time\nby ~40% on average across the cases examined in this study. Crucially, GLAI is\nnot just another classifier, but a generic block that can replace MLPs wherever\nthey are used, from supervised heads with frozen backbones to projection layers\nin self-supervised learning or few-shot classifiers. Across diverse\nexperimental setups, GLAI consistently matches or exceeds the accuracy of MLPs\nwith an equivalent number of parameters, while converging faster. Overall, GLAI\nestablishes a new design principle that opens a direction for future\nintegration into large-scale architectures such as Transformers, where MLP\nblocks dominate the computational footprint.",
      "authors": [
        "Jose I. Mestre",
        "Alberto Fernández-Hernández",
        "Cristian Pérez-Corral",
        "Manuel F. Dolz",
        "Jose Duato",
        "Enrique S. Quintana-Ortí"
      ],
      "published": "2025-10-01T13:31:34Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00883v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出GreenLightningAI(GLAI)架构块，通过知识解耦方法分离结构知识与定量知识，固定激活模式后仅优化权重参数，在保持MLP通用近似能力的同时平均减少40%训练时间，可替代传统MLP应用于各类模型架构。",
      "order": 226
    },
    {
      "arxiv_id": "2510.00881v1",
      "title": "Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of\n  LLM Reasoning",
      "summary": "Large Language Models (LLMs) are increasingly integrated into software\nengineering (SE) tools for tasks that extend beyond code synthesis, including\njudgment under uncertainty and reasoning in ethically significant contexts. We\npresent a fully automated framework for assessing ethical reasoning\ncapabilities across 16 LLMs in a zero-shot setting, using 30 real-world\nethically charged scenarios. Each model is prompted to identify the most\napplicable ethical theory to an action, assess its moral acceptability, and\nexplain the reasoning behind their choice. Responses are compared against\nexpert ethicists' choices using inter-model agreement metrics. Our results show\nthat LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary\nAgreement Rate (BAR) on moral acceptability of 86.7%, with interpretable\ndivergences concentrated in ethically ambiguous cases. A qualitative analysis\nof free-text explanations reveals strong conceptual convergence across models\ndespite surface-level lexical diversity. These findings support the potential\nviability of LLMs as ethical inference engines within SE pipelines, enabling\nscalable, auditable, and adaptive integration of user-aligned ethical\nreasoning. Our focus is the Ethical Interpreter component of a broader\nprofiling pipeline: we evaluate whether current LLMs exhibit sufficient\ninterpretive stability and theory-consistent reasoning to support automated\nprofiling.",
      "authors": [
        "Patrizio Migliarini",
        "Mashal Afzal Memon",
        "Marco Autili",
        "Paola Inverardi"
      ],
      "published": "2025-10-01T13:28:26Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.00881v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出自动化框架评估16个大语言模型在零样本设置下的伦理推理能力，使用30个真实伦理场景测试模型识别适用伦理理论、判断道德可接受性及解释推理的能力。结果显示模型平均理论一致性达73.3%，道德判断一致性达86.7%，表明LLM具备作为软件工程伦理推理引擎的潜力。",
      "order": 227
    },
    {
      "arxiv_id": "2510.00876v1",
      "title": "Unveiling Interesting Insights: Monte Carlo Tree Search for Knowledge\n  Discovery",
      "summary": "Organizations are increasingly focused on leveraging data from their\nprocesses to gain insights and drive decision-making. However, converting this\ndata into actionable knowledge remains a difficult and time-consuming task.\nThere is often a gap between the volume of data collected and the ability to\nprocess and understand it, which automated knowledge discovery aims to fill.\nAutomated knowledge discovery involves complex open problems, including\neffectively navigating data, building models to extract implicit relationships,\nand considering subjective goals and knowledge. In this paper, we introduce a\nnovel method for Automated Insights and Data Exploration (AIDE), that serves as\na robust foundation for tackling these challenges through the use of Monte\nCarlo Tree Search (MCTS). We evaluate AIDE using both real-world and synthetic\ndata, demonstrating its effectiveness in identifying data transformations and\nmodels that uncover interesting data patterns. Among its strengths, AIDE's\nMCTS-based framework offers significant extensibility, allowing for future\nintegration of additional pattern extraction strategies and domain knowledge.\nThis makes AIDE a valuable step towards developing a comprehensive solution for\nautomated knowledge discovery.",
      "authors": [
        "Pietro Totis",
        "Alberto Pozanco",
        "Daniel Borrajo"
      ],
      "published": "2025-10-01T13:25:15Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00876v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出了一种基于蒙特卡洛树搜索的自动洞察与数据探索方法AIDE，通过智能搜索数据转换和模型构建来发现有趣的数据模式，为自动化知识发现提供了可扩展的解决方案框架。",
      "order": 228
    },
    {
      "arxiv_id": "2510.00862v1",
      "title": "Gather-Scatter Mamba: Accelerating Propagation with Efficient State\n  Space Model",
      "summary": "State Space Models (SSMs)-most notably RNNs-have historically played a\ncentral role in sequential modeling. Although attention mechanisms such as\nTransformers have since dominated due to their ability to model global context,\ntheir quadratic complexity and limited scalability make them less suited for\nlong sequences. Video super-resolution (VSR) methods have traditionally relied\non recurrent architectures to propagate features across frames. However, such\napproaches suffer from well-known issues including vanishing gradients, lack of\nparallelism, and slow inference speed. Recent advances in selective SSMs like\nMamba offer a compelling alternative: by enabling input-dependent state\ntransitions with linear-time complexity, Mamba mitigates these issues while\nmaintaining strong long-range modeling capabilities. Despite this potential,\nMamba alone struggles to capture fine-grained spatial dependencies due to its\ncausal nature and lack of explicit context aggregation. To address this, we\npropose a hybrid architecture that combines shifted window self-attention for\nspatial context aggregation with Mamba-based selective scanning for efficient\ntemporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an\nalignment-aware mechanism that warps features toward a center anchor frame\nwithin the temporal window before Mamba propagation and scatters them back\nafterward, effectively reducing occlusion artifacts and ensuring effective\nredistribution of aggregated information across all frames. The official\nimplementation is provided at: https://github.com/Ko-Lani/GSMamba.",
      "authors": [
        "Hyun-kyu Ko",
        "Youbin Kim",
        "Jihyeon Park",
        "Dongheok Park",
        "Gyeongjin Kang",
        "Wonjun Cho",
        "Hyung Yi",
        "Eunbyung Park"
      ],
      "published": "2025-10-01T13:11:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00862v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Gather-Scatter Mamba（GSM）混合架构，结合移位窗口自注意力与Mamba选择性扫描，通过特征对齐机制解决视频超分辨率中的时空依赖问题，在保持线性复杂度的同时提升长序列建模能力。",
      "order": 229
    },
    {
      "arxiv_id": "2510.00861v1",
      "title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented\n  LLMs",
      "summary": "While search-augmented large language models (LLMs) exhibit impressive\ncapabilities, their reliability in complex multi-hop reasoning remains limited.\nThis limitation arises from three fundamental challenges: decomposition errors,\nwhere tasks are incorrectly broken down; retrieval missing, where key evidence\nfails to be retrieved; and reasoning errors, where flawed logic propagates\nthrough the reasoning chain. A single failure in any of these stages can derail\nthe final answer. We propose Erasable Reinforcement Learning (ERL), a novel\nframework that transforms fragile reasoning into a robust process. ERL\nexplicitly identifies faulty steps, erases them, and regenerates reasoning in\nplace, preventing defective logic from propagating through the reasoning chain.\nThis targeted correction mechanism turns brittle reasoning into a more\nresilient process. Models trained with ERL, termed ESearch, achieve substantial\nimprovements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model\nachieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and\n+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest\nthat erasable reinforcement learning provides a powerful paradigm shift for\nrobust multi-step reasoning in LLMs.",
      "authors": [
        "Ziliang Wang",
        "Kang An",
        "Xuhui Zheng",
        "Faqiang Qian",
        "Weikun Zhang",
        "Cijun Ouyang",
        "Jialu Cai",
        "Yuhang Wang",
        "Yichao Wu"
      ],
      "published": "2025-10-01T13:10:36Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00861v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出可擦除强化学习(ERL)框架，解决检索增强大语言模型在多跳推理中的三大挑战：分解错误、检索缺失和推理错误。该方法通过识别错误步骤、擦除并重新生成推理，防止错误逻辑传播，在多个基准测试中显著提升性能，3B和7B模型分别实现+8.48%/+5.38% EM和+11.56%/+7.22% F1的提升。",
      "order": 230
    },
    {
      "arxiv_id": "2510.00855v1",
      "title": "Can World Models Benefit VLMs for World Dynamics?",
      "summary": "Trained on internet-scale video data, generative world models are\nincreasingly recognized as powerful world simulators that can generate\nconsistent and plausible dynamics over structure, motion, and physics. This\nraises a natural question: with the advent of strong video foundational models,\nmight they supplant conventional vision encoder paradigms for general-purpose\nmultimodal understanding? While recent studies have begun to explore the\npotential of world models on common vision tasks, these explorations typically\nlack a systematic investigation of generic, multimodal tasks. In this work, we\nstrive to investigate the capabilities when world model priors are transferred\ninto Vision-Language Models: we re-purpose a video diffusion model as a\ngenerative encoder to perform a single denoising step and treat the resulting\nlatents as a set of visual embedding. We empirically investigate this class of\nmodels, which we refer to as World-Language Models (WorldLMs), and we find that\ngenerative encoders can capture latents useful for downstream understanding\nthat show distinctions from conventional encoders. Naming our best-performing\nvariant Dynamic Vision Aligner (DyVA), we further discover that this method\nsignificantly enhances spatial reasoning abilities and enables single-image\nmodels to perform multi-frame reasoning. Through the curation of a suite of\nvisual reasoning tasks, we find DyVA to surpass both open-source and\nproprietary baselines, achieving state-of-the-art or comparable performance. We\nattribute these gains to WorldLM's inherited motion-consistency internalization\nfrom video pre-training. Finally, we systematically explore extensive model\ndesigns to highlight promising directions for future work. We hope our study\ncan pave the way for a new family of VLMs that leverage priors from world\nmodels and are on a promising path towards generalist vision learners.",
      "authors": [
        "Kevin Zhang",
        "Kuangzhi Ge",
        "Xiaowei Chi",
        "Renrui Zhang",
        "Shaojun Shi",
        "Zhen Dong",
        "Sirui Han",
        "Shanghang Zhang"
      ],
      "published": "2025-10-01T13:07:05Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00855v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探索将世界模型先验知识融入视觉语言模型，提出WorldLM框架及最佳变体DyVA。通过将视频扩散模型重构为生成编码器，该方法显著提升了空间推理能力，使单图像模型具备多帧推理功能，在视觉推理任务中达到领先性能。",
      "order": 231
    },
    {
      "arxiv_id": "2510.00844v1",
      "title": "Learning Compact Representations of LLM Abilities via Item Response\n  Theory",
      "summary": "Recent years have witnessed a surge in the number of large language models\n(LLMs), yet efficiently managing and utilizing these vast resources remains a\nsignificant challenge. In this work, we explore how to learn compact\nrepresentations of LLM abilities that can facilitate downstream tasks, such as\nmodel routing and performance prediction on new benchmarks. We frame this\nproblem as estimating the probability that a given model will correctly answer\na specific query. Inspired by the item response theory (IRT) in psychometrics,\nwe model this probability as a function of three key factors: (i) the model's\nmulti-skill ability vector, (2) the query's discrimination vector that\nseparates models of differing skills, and (3) the query's difficulty scalar. To\nlearn these parameters jointly, we introduce a Mixture-of-Experts (MoE) network\nthat couples model- and query-level embeddings. Extensive experiments\ndemonstrate that our approach leads to state-of-the-art performance in both\nmodel routing and benchmark accuracy prediction. Moreover, analysis validates\nthat the learned parameters encode meaningful, interpretable information about\nmodel capabilities and query characteristics.",
      "authors": [
        "Jianhao Chen",
        "Chenxu Wang",
        "Gengrui Zhang",
        "Peng Ye",
        "Lei Bai",
        "Wei Hu",
        "Yuzhong Qu",
        "Shuyue Hu"
      ],
      "published": "2025-10-01T12:55:34Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00844v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究借鉴心理测量学中的项目反应理论，提出一种学习大语言模型能力紧凑表示的方法。通过混合专家网络联合学习模型的多技能能力向量、查询的区分度向量和难度标量，在模型路由和基准测试预测任务中达到最先进性能，且学习参数能有效编码模型能力和查询特征的可解释信息。",
      "order": 232
    },
    {
      "arxiv_id": "2510.00845v2",
      "title": "Mechanistic Interpretability as Statistical Estimation: A Variance\n  Analysis of EAP-IG",
      "summary": "The development of trustworthy artificial intelligence requires moving beyond\nblack-box performance metrics toward an understanding of models' internal\ncomputations. Mechanistic Interpretability (MI) aims to meet this need by\nidentifying the algorithmic mechanisms underlying model behaviors. Yet, the\nscientific rigor of MI critically depends on the reliability of its findings.\nIn this work, we argue that interpretability methods, such as circuit\ndiscovery, should be viewed as statistical estimators, subject to questions of\nvariance and robustness. To illustrate this statistical framing, we present a\nsystematic stability analysis of a state-of-the-art circuit discovery method:\nEAP-IG. We evaluate its variance and robustness through a comprehensive suite\nof controlled perturbations, including input resampling, prompt paraphrasing,\nhyperparameter variation, and injected noise within the causal analysis itself.\nAcross a diverse set of models and tasks, our results demonstrate that EAP-IG\nexhibits high structural variance and sensitivity to hyperparameters,\nquestioning the stability of its findings. Based on these results, we offer a\nset of best-practice recommendations for the field, advocating for the routine\nreporting of stability metrics to promote a more rigorous and statistically\ngrounded science of interpretability.",
      "authors": [
        "Maxime Méloux",
        "François Portet",
        "Maxime Peyrard"
      ],
      "published": "2025-10-01T12:55:34Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00845v2",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究将机制可解释性方法视为统计估计器，对先进的电路发现方法EAP-IG进行了系统性稳定性分析。通过输入重采样、提示改写、超参数变化和因果分析噪声注入等扰动测试，发现EAP-IG存在高结构方差和超参数敏感性，质疑其发现稳定性。研究建议在可解释性领域常规报告稳定性指标，以建立更严谨的统计基础。",
      "order": 233
    },
    {
      "arxiv_id": "2510.00837v1",
      "title": "Feature Identification for Hierarchical Contrastive Learning",
      "summary": "Hierarchical classification is a crucial task in many applications, where\nobjects are organized into multiple levels of categories. However, conventional\nclassification approaches often neglect inherent inter-class relationships at\ndifferent hierarchy levels, thus missing important supervisory signals. Thus,\nwe propose two novel hierarchical contrastive learning (HMLC) methods. The\nfirst, leverages a Gaussian Mixture Model (G-HMLC) and the second uses an\nattention mechanism to capture hierarchy-specific features (A-HMLC), imitating\nhuman processing. Our approach explicitly models inter-class relationships and\nimbalanced class distribution at higher hierarchy levels, enabling fine-grained\nclustering across all hierarchy levels. On the competitive CIFAR100 and\nModelNet40 datasets, our method achieves state-of-the-art performance in linear\nevaluation, outperforming existing hierarchical contrastive learning methods by\n2 percentage points in terms of accuracy. The effectiveness of our approach is\nbacked by both quantitative and qualitative results, highlighting its potential\nfor applications in computer vision and beyond.",
      "authors": [
        "Julius Ott",
        "Nastassia Vysotskaya",
        "Huawei Sun",
        "Lorenzo Servadei",
        "Robert Wille"
      ],
      "published": "2025-10-01T12:46:47Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00837v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出两种层次对比学习方法（G-HMLC和A-HMLC），通过高斯混合模型和注意力机制捕捉层次结构特征，在CIFAR100和ModelNet40数据集上实现最先进性能，准确率提升2个百分点。",
      "order": 234
    },
    {
      "arxiv_id": "2510.00836v1",
      "title": "Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based\n  Models and Synthetic Oversampling Techniques",
      "summary": "This study aims to detect pump and dump (P&D) manipulation in cryptocurrency\nmarkets, where the scarcity of such events causes severe class imbalance and\nhinders accurate detection. To address this issue, the Synthetic Minority\nOversampling Technique (SMOTE) was applied, and advanced ensemble learning\nmodels were evaluated to distinguish manipulative trading behavior from normal\nmarket activity. The experimental results show that applying SMOTE greatly\nenhanced the ability of all models to detect P&D events by increasing recall\nand improving the overall balance between precision and recall. In particular,\nXGBoost and LightGBM achieved high recall rates (94.87% and 93.59%,\nrespectively) with strong F1-scores and demonstrated fast computational\nperformance, making them suitable for near real time surveillance. These\nfindings indicate that integrating data balancing techniques with ensemble\nmethods significantly improves the early detection of manipulative activities,\ncontributing to a fairer, more transparent, and more stable cryptocurrency\nmarket.",
      "authors": [
        "Jieun Yu",
        "Minjung Park",
        "Sangmi Chai"
      ],
      "published": "2025-10-01T12:46:45Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00836v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "financial_ai",
      "tldr_zh": "本研究针对加密货币市场中拉高抛售操纵行为检测的类别不平衡问题，采用SMOTE过采样技术和集成学习模型。实验表明XGBoost和LightGBM在SMOTE增强下召回率分别达94.87%和93.59%，兼具高F1分数和快速计算性能，适用于近实时监控，能显著提升市场操纵行为的早期检测能力。",
      "order": 235
    },
    {
      "arxiv_id": "2510.00833v1",
      "title": "Towards Verifiable Federated Unlearning: Framework, Challenges, and The\n  Road Ahead",
      "summary": "Federated unlearning (FUL) enables removing the data influence from the model\ntrained across distributed clients, upholding the right to be forgotten as\nmandated by privacy regulations. FUL facilitates a value exchange where clients\ngain privacy-preserving control over their data contributions, while service\nproviders leverage decentralized computing and data freshness. However, this\nentire proposition is undermined because clients have no reliable way to verify\nthat their data influence has been provably removed, as current metrics and\nsimple notifications offer insufficient assurance. We envision unlearning\nverification becoming a pivotal and trust-by-design part of the FUL life-cycle\ndevelopment, essential for highly regulated and data-sensitive services and\napplications like healthcare. This article introduces veriFUL, a reference\nframework for verifiable FUL that formalizes verification entities, goals,\napproaches, and metrics. Specifically, we consolidate existing efforts and\ncontribute new insights, concepts, and metrics to this domain. Finally, we\nhighlight research challenges and identify potential applications and\ndevelopments for verifiable FUL and veriFUL.",
      "authors": [
        "Thanh Linh Nguyen",
        "Marcela Tuler de Oliveira",
        "An Braeken",
        "Aaron Yi Ding",
        "Quoc-Viet Pham"
      ],
      "published": "2025-10-01T12:45:46Z",
      "primary_category": "cs.DC",
      "arxiv_url": "https://arxiv.org/abs/2510.00833v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出veriFUL框架，解决联邦学习中数据遗忘的可验证性问题。针对现有联邦遗忘方法缺乏可靠验证机制的缺陷，该框架通过形式化验证实体、目标和指标，确保用户数据影响被彻底移除。特别适用于医疗等高度监管领域，为隐私法规要求的'被遗忘权'提供技术保障。",
      "order": 236
    },
    {
      "arxiv_id": "2510.00831v1",
      "title": "Benchmarking Machine Learning Models for Fault Classification and\n  Localization in Power System Protection",
      "summary": "The increasing integration of distributed energy resources (DERs),\nparticularly renewables, poses significant challenges for power system\nprotection, with fault classification (FC) and fault localization (FL) being\namong the most critical tasks. Conventional protection schemes, based on fixed\nthresholds, cannot reliably identify and localize short circuits with the\nincreasing complexity of the grid under dynamic conditions. Machine learning\n(ML) offers a promising alternative; however, systematic benchmarks across\nmodels and settings remain limited. This work presents, for the first time, a\ncomparative benchmarking study of classical ML models for FC and FL in power\nsystem protection based on EMT data. Using voltage and current waveforms\nsegmented into sliding windows of 10 ms to 50 ms, we evaluate models under\nrealistic real-time constraints. Performance is assessed in terms of accuracy,\nrobustness to window size, and runtime efficiency. The best-performing FC model\nachieved an F1 score of 0.992$\\pm$0.001, while the top FL model reached an R2\nof 0.806$\\pm$0.008 with a mean processing time of 0.563 ms.",
      "authors": [
        "Julian Oelhaf",
        "Georg Kordowich",
        "Changhun Kim",
        "Paula Andrea Pérez-Toro",
        "Christian Bergler",
        "Andreas Maier",
        "Johann Jäger",
        "Siming Bayer"
      ],
      "published": "2025-10-01T12:44:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00831v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究首次对电力系统保护中的故障分类与定位机器学习模型进行系统性基准测试，基于电磁暂态数据评估多种模型在实时约束下的性能表现。最佳故障分类模型F1得分达0.992±0.001，最优定位模型R2达0.806±0.008，平均处理时间仅0.563毫秒，为动态电网条件下的保护方案提供了重要参考。",
      "order": 237
    },
    {
      "arxiv_id": "2510.00821v1",
      "title": "Logical Consistency Between Disagreeing Experts and Its Role in AI\n  Safety",
      "summary": "If two experts disagree on a test, we may conclude both cannot be 100 per\ncent correct. But if they completely agree, no possible evaluation can be\nexcluded. This asymmetry in the utility of agreements versus disagreements is\nexplored here by formalizing a logic of unsupervised evaluation for\nclassifiers. Its core problem is computing the set of group evaluations that\nare logically consistent with how we observe them agreeing and disagreeing in\ntheir decisions. Statistical summaries of their aligned decisions are inputs\ninto a Linear Programming problem in the integer space of possible correct or\nincorrect responses given true labels. Obvious logical constraints, such as,\nthe number of correct responses cannot exceed the number of observed responses,\nare inequalities. But in addition, there are axioms, universally applicable\nlinear equalities that apply to all finite tests. The practical and immediate\nutility of this approach to unsupervised evaluation using only logical\nconsistency is demonstrated by building no-knowledge alarms that can detect\nwhen one or more LLMs-as-Judges are violating a minimum grading threshold\nspecified by the user.",
      "authors": [
        "Andrés Corrada-Emmanuel"
      ],
      "published": "2025-10-01T12:30:01Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00821v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于逻辑一致性的无监督评估方法，通过形式化专家间一致与分歧的不对称性，构建线性规划问题来检测LLM评委是否违反用户设定的评分阈值，为AI安全提供理论框架。",
      "order": 238
    },
    {
      "arxiv_id": "2510.00819v1",
      "title": "Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning\n  in LLM Reasoning",
      "summary": "Reinforcement Learning, particularly through policy gradient methods, has\nplayed a central role in enabling reasoning capabilities of Large Language\nModels. However, the optimization stability of policy gradients in this setting\nremains understudied. As a result, existing implementations often resort to\nconservative hyperparameter choices to ensure stability, which requires more\ntraining samples and increases computational costs. Hence, developing models\nfor reliably tracking the underlying optimization dynamics and leveraging them\ninto training enables more sample-efficient regimes and further unleashes\nscalable post-training. We address this gap by formalizing the stochastic\noptimization problem of policy gradients with explicit consideration of\nsecond-order geometry. We propose a tractable computational framework that\ntracks and leverages curvature information during policy updates. We further\nemploy this framework to design interventions in the optimization process\nthrough data selection. The resultant algorithm, Curvature-Aware Policy\nOptimization (CAPO), identifies samples that contribute to unstable updates and\nmasks them out. Theoretically, we establish monotonic improvement guarantees\nunder realistic assumptions. On standard math reasoning benchmarks, we\nempirically show that CAPO ensures stable updates under aggressive learning\nregimes where baselines catastrophically fail. With minimal intervention\n(rejecting fewer than 8% of tokens), CAPO achieves up to 30x improvement in\nsample efficiency over standard GRPO for LLM reasoning.",
      "authors": [
        "Luckeciano C. Melo",
        "Alessandro Abate",
        "Yarin Gal"
      ],
      "published": "2025-10-01T12:29:32Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00819v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出曲率感知策略优化(CAPO)方法，通过跟踪二阶几何信息识别并屏蔽导致策略梯度不稳定的样本，在LLM推理任务中实现稳定优化，样本效率提升高达30倍，仅需屏蔽不足8%的token即可在激进学习机制下保证单调改进。",
      "order": 239
    },
    {
      "arxiv_id": "2510.00817v2",
      "title": "Semantic Bridges Between First Order c-Representations and Cost-Based\n  Semantics: An Initial Perspective",
      "summary": "Weighted-knowledge bases and cost-based semantics represent a recent\nformalism introduced by Bienvenu et al. for Ontology Mediated Data Querying in\nthe case where a given knowledge base is inconsistent. This is done by adding a\nweight to each statement in the knowledge base (KB), and then giving each DL\ninterpretation a cost based on how often it breaks rules in the KB. In this\npaper we compare this approach with c-representations, a form of non-monotonic\nreasoning originally introduced by Kern-Isberner. c-Representations describe a\nmeans to interpret defeasible concept inclusions in the first-order case. This\nis done by assigning a numerical ranking to each interpretations via penalties\nfor each violated conditional. We compare these two approaches on a semantic\nlevel. In particular, we show that under certain conditions a weighted\nknowledge base and a set of defeasible conditionals can generate the same\nordering on interpretations, and therefore an equivalence of semantic\nstructures up to relative cost. Moreover, we compare entailment described in\nboth cases, where certain notions are equivalently expressible in both\nformalisms. Our results have the potential to benefit further work on both\ncost-based semantics and c-representations",
      "authors": [
        "Nicholas Leisegang",
        "Giovanni Casini",
        "Thomas Meyer"
      ],
      "published": "2025-10-01T12:27:19Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00817v2",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文比较了加权知识库的成本语义与c-表示法在语义层面的关系，证明在特定条件下两者可生成相同的解释排序，并探讨了它们在可废止条件推理中的等价表达潜力。",
      "order": 240
    },
    {
      "arxiv_id": "2510.00808v1",
      "title": "What You See is What You Ask: Evaluating Audio Descriptions",
      "summary": "Audio descriptions (ADs) narrate important visual details in movies, enabling\nBlind and Low Vision (BLV) users to understand narratives and appreciate visual\ndetails. Existing works in automatic AD generation mostly focus on few-second\ntrimmed clips, and evaluate them by comparing against a single ground-truth\nreference AD. However, writing ADs is inherently subjective. Through alignment\nand analysis of two independent AD tracks for the same movies, we quantify the\nsubjectivity in when and whether to describe, and what and how to highlight.\nThus, we show that working with trimmed clips is inadequate. We propose ADQA, a\nQA benchmark that evaluates ADs at the level of few-minute long, coherent video\nsegments, testing whether they would help BLV users understand the story and\nappreciate visual details. ADQA features visual appreciation (VA) questions\nabout visual facts and narrative understanding (NU) questions based on the\nplot. Through ADQA, we show that current AD generation methods lag far behind\nhuman-authored ADs. We conclude with several recommendations for future work\nand introduce a public leaderboard for benchmarking.",
      "authors": [
        "Divy Kala",
        "Eshika Khandelwal",
        "Makarand Tapaswi"
      ],
      "published": "2025-10-01T12:14:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00808v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ADQA基准测试，通过量化音频描述的主观性并评估其在长视频片段中的表现，发现现有自动生成方法远逊于人工创作，为盲人和低视力用户的可访问性研究提供新评估框架。",
      "order": 241
    },
    {
      "arxiv_id": "2510.00805v1",
      "title": "MG2FlowNet: Accelerating High-Reward Sample Generation via Enhanced MCTS\n  and Greediness Control",
      "summary": "Generative Flow Networks (GFlowNets) have emerged as a powerful tool for\ngenerating diverse and high-reward structured objects by learning to sample\nfrom a distribution proportional to a given reward function. Unlike\nconventional reinforcement learning (RL) approaches that prioritize\noptimization of a single trajectory, GFlowNets seek to balance diversity and\nreward by modeling the entire trajectory distribution. This capability makes\nthem especially suitable for domains such as molecular design and combinatorial\noptimization. However, existing GFlowNets sampling strategies tend to\noverexplore and struggle to consistently generate high-reward samples,\nparticularly in large search spaces with sparse high-reward regions. Therefore,\nimproving the probability of generating high-reward samples without sacrificing\ndiversity remains a key challenge under this premise. In this work, we\nintegrate an enhanced Monte Carlo Tree Search (MCTS) into the GFlowNets\nsampling process, using MCTS-based policy evaluation to guide the generation\ntoward high-reward trajectories and Polynomial Upper Confidence Trees (PUCT) to\nbalance exploration and exploitation adaptively, and we introduce a\ncontrollable mechanism to regulate the degree of greediness. Our method\nenhances exploitation without sacrificing diversity by dynamically balancing\nexploration and reward-driven guidance. The experimental results show that our\nmethod can not only accelerate the speed of discovering high-reward regions but\nalso continuously generate high-reward samples, while preserving the diversity\nof the generative distribution. All implementations are available at\nhttps://github.com/ZRNB/MG2FlowNet.",
      "authors": [
        "Rui Zhu",
        "Xuan Yu",
        "Yudong Zhang",
        "Chen Zhang",
        "Xu Wang",
        "Yang Wang"
      ],
      "published": "2025-10-01T12:09:04Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00805v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "MG2FlowNet提出一种改进的GFlowNets采样方法，通过增强的蒙特卡洛树搜索和贪心控制机制，在保持生成多样性的同时显著提升高奖励样本的生成效率。该方法在稀疏奖励的大规模搜索空间中表现出色，适用于分子设计等结构化对象生成任务。",
      "order": 242
    },
    {
      "arxiv_id": "2510.00799v1",
      "title": "Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text\n  Vectors",
      "summary": "Most image watermarking systems focus on robustness, capacity, and\nimperceptibility while treating the embedded payload as meaningless bits. This\nbit-centric view imposes a hard ceiling on capacity and prevents watermarks\nfrom carrying useful information. We propose LatentSeal, which reframes\nwatermarking as semantic communication: a lightweight text autoencoder maps\nfull-sentence messages into a compact 256-dimensional unit-norm latent vector,\nwhich is robustly embedded by a finetuned watermark model and secured through a\nsecret, invertible rotation. The resulting system hides full-sentence messages,\ndecodes in real time, and survives valuemetric and geometric attacks. It\nsurpasses prior state of the art in BLEU-4 and Exact Match on several\nbenchmarks, while breaking through the long-standing 256-bit payload ceiling.\nIt also introduces a statistically calibrated score that yields a ROC AUC score\nof 0.97-0.99, and practical operating points for deployment. By shifting from\nbit payloads to semantic latent vectors, LatentSeal enables watermarking that\nis not only robust and high-capacity, but also secure and interpretable,\nproviding a concrete path toward provenance, tamper explanation, and\ntrustworthy AI governance. Models, training and inference code, and data splits\nwill be available upon publication.",
      "authors": [
        "Gautier Evennou",
        "Vivien Chappelier",
        "Ewa Kijak"
      ],
      "published": "2025-10-01T11:56:40Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.00799v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出LatentSeal图像水印系统，将水印技术重新定义为语义通信：通过轻量级文本自编码器将完整句子映射为256维潜向量，经微调的水印模型鲁棒嵌入并采用可逆旋转加密。该系统突破传统256比特载荷限制，实现实时解码，抗多种攻击，在多项基准测试中超越现有技术，为图像溯源和可信AI治理提供新途径。",
      "order": 243
    },
    {
      "arxiv_id": "2510.00797v1",
      "title": "Solar PV Installation Potential Assessment on Building Facades Based on\n  Vision and Language Foundation Models",
      "summary": "Building facades represent a significant untapped resource for solar energy\ngeneration in dense urban environments, yet assessing their photovoltaic (PV)\npotential remains challenging due to complex geometries and semantic com\nponents. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), an\nautomated framework that transforms street-view photographs into quantitative\nPV deployment assessments. The approach combines com puter vision and\nartificial intelligence techniques to address three key challenges: perspective\ndistortion correction, semantic understanding of facade elements, and spatial\nreasoning for PV layout optimization. Our four-stage pipeline processes images\nthrough geometric rectification, zero-shot semantic segmentation, Large\nLanguage Model (LLM) guided spatial reasoning, and energy simulation.\nValidation across 80 buildings in four countries demonstrates ro bust\nperformance with mean area estimation errors of 6.2% &#177; 2.8% compared to\nexpert annotations. The auto mated assessment requires approximately 100\nseconds per building, a substantial gain in efficiency over manual methods.\nSimulated energy yield predictions confirm the method's reliability and\napplicability for regional poten tial studies, urban energy planning, and\nbuilding-integrated photovoltaic (BIPV) deployment. Code is available at:\nhttps:github.com/CodeAXu/Solar-PV-Installation",
      "authors": [
        "Ruyu Liu",
        "Dongxu Zhuang",
        "Jianhua Zhang",
        "Arega Getaneh Abate",
        "Per Sieverts Nielsen",
        "Ben Wang",
        "Xiufeng Liu"
      ],
      "published": "2025-10-01T11:51:28Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00797v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出SF-SPA框架，利用视觉与语言基础模型自动评估建筑立面光伏安装潜力。通过几何校正、零样本语义分割、LLM空间推理和能源模拟四阶段流程，在80栋建筑验证中实现6.2%面积估计误差，单栋评估仅需100秒，为城市能源规划提供高效解决方案。",
      "order": 244
    },
    {
      "arxiv_id": "2510.00796v1",
      "title": "MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically\n  Equivalent Prompts",
      "summary": "Recent advances in text-to-image (T2I) models, especially diffusion-based\narchitectures, have significantly improved the visual quality of generated\nimages. However, these models continue to struggle with a critical limitation:\nmaintaining semantic consistency when input prompts undergo minor linguistic\nvariations. Despite being logically equivalent, such prompt pairs often yield\nmisaligned or semantically inconsistent images, exposing a lack of robustness\nin reasoning and generalisation. To address this, we propose MetaLogic, a novel\nevaluation framework that detects T2I misalignment without relying on ground\ntruth images. MetaLogic leverages metamorphic testing, generating image pairs\nfrom prompts that differ grammatically but are semantically identical. By\ndirectly comparing these image pairs, the framework identifies inconsistencies\nthat signal failures in preserving the intended meaning, effectively diagnosing\nrobustness issues in the model's logic understanding. Unlike existing\nevaluation methods that compare a generated image to a single prompt, MetaLogic\nevaluates semantic equivalence between paired images, offering a scalable,\nground-truth-free approach to identifying alignment failures. It categorises\nthese alignment errors (e.g., entity omission, duplication, positional\nmisalignment) and surfaces counterexamples that can be used for model debugging\nand refinement. We evaluate MetaLogic across multiple state-of-the-art T2I\nmodels and reveal consistent robustness failures across a range of logical\nconstructs. We find that even the SOTA text-to-image models like Flux.dev and\nDALLE-3 demonstrate a 59 percent and 71 percent misalignment rate,\nrespectively. Our results show that MetaLogic is not only efficient and\nscalable, but also effective in uncovering fine-grained logical inconsistencies\nthat are overlooked by existing evaluation metrics.",
      "authors": [
        "Yifan Shen",
        "Yangyang Shu",
        "Hye-young Paik",
        "Yulei Sui"
      ],
      "published": "2025-10-01T11:51:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00796v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "MetaLogic提出了一种无需真实图像标注的文本到图像模型鲁棒性评估框架，通过生成语法不同但语义相同的提示词对来检测模型输出的一致性。该方法能识别实体遗漏、重复、位置错位等对齐错误，在主流T2I模型上发现高达59-71%的语义不一致率，有效揭示现有评估指标忽略的逻辑推理缺陷。",
      "order": 245
    },
    {
      "arxiv_id": "2510.00795v1",
      "title": "Benchmarking Agentic Systems in Automated Scientific Information\n  Extraction with ChemX",
      "summary": "The emergence of agent-based systems represents a significant advancement in\nartificial intelligence, with growing applications in automated data\nextraction. However, chemical information extraction remains a formidable\nchallenge due to the inherent heterogeneity of chemical data. Current\nagent-based approaches, both general-purpose and domain-specific, exhibit\nlimited performance in this domain. To address this gap, we present ChemX, a\ncomprehensive collection of 10 manually curated and domain-expert-validated\ndatasets focusing on nanomaterials and small molecules. These datasets are\ndesigned to rigorously evaluate and enhance automated extraction methodologies\nin chemistry. To demonstrate their utility, we conduct an extensive\nbenchmarking study comparing existing state-of-the-art agentic systems such as\nChatGPT Agent and chemical-specific data extraction agents. Additionally, we\nintroduce our own single-agent approach that enables precise control over\ndocument preprocessing prior to extraction. We further evaluate the performance\nof modern baselines, such as GPT-5 and GPT-5 Thinking, to compare their\ncapabilities with agentic approaches. Our empirical findings reveal persistent\nchallenges in chemical information extraction, particularly in processing\ndomain-specific terminology, complex tabular and schematic representations, and\ncontext-dependent ambiguities. The ChemX benchmark serves as a critical\nresource for advancing automated information extraction in chemistry,\nchallenging the generalization capabilities of existing methods, and providing\nvaluable insights into effective evaluation strategies.",
      "authors": [
        "Anastasia Vepreva",
        "Julia Razlivina",
        "Maria Eremeeva",
        "Nina Gubina",
        "Anastasia Orlova",
        "Aleksei Dmitrenko",
        "Ksenya Kapranova",
        "Susan Jyakhwo",
        "Nikita Vasilev",
        "Arsen Sarkisyan",
        "Ivan Yu. Chernyshov",
        "Vladimir Vinogradov",
        "Andrei Dmitrenko"
      ],
      "published": "2025-10-01T11:50:11Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00795v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ChemX基准数据集，用于评估化学信息提取中的智能体系统性能。通过对比ChatGPT Agent、GPT-5等现有方法，发现化学领域术语、复杂表格和上下文歧义仍是主要挑战。ChemX为化学信息自动化提取提供了关键评估资源。",
      "order": 246
    },
    {
      "arxiv_id": "2510.00793v1",
      "title": "AI in data science education: experiences from the classroom",
      "summary": "This study explores the integration of AI, particularly large language models\n(LLMs) like ChatGPT, into educational settings, focusing on the implications\nfor teaching and learning. Through interviews with course coordinators from\ndata science courses at Wageningen University, this research identifies both\nthe benefits and challenges associated with AI in the classroom. While AI tools\ncan streamline tasks and enhance learning, concerns arise regarding students'\noverreliance on these technologies, potentially hindering the development of\nessential cognitive and problem solving skills. The study highlights the\nimportance of responsible AI usage, ethical considerations, and the need for\nadapting assessment methods to ensure educational outcomes are met. With\ncareful integration, AI can be a valuable asset in education, provided it is\nused to complement rather than replace fundamental learning processes.",
      "authors": [
        "J. A. Hageman",
        "C. F. W. Peeters"
      ],
      "published": "2025-10-01T11:45:25Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00793v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "education_ai",
      "tldr_zh": "本研究探讨了以ChatGPT为代表的大语言模型在数据科学教育中的应用，通过访谈瓦赫宁根大学课程协调员，揭示了AI工具在提升教学效率与学习体验的同时，可能因学生过度依赖而阻碍认知与问题解决能力发展。强调需通过负责任使用、伦理考量和评估方式调整，使AI成为补充而非替代基础学习过程的工具。",
      "order": 247
    },
    {
      "arxiv_id": "2510.00778v1",
      "title": "DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion\n  Models",
      "summary": "Diffusion models have shown to be strong representation learners, showcasing\nstate-of-the-art performance across multiple domains. Aside from accelerated\nsampling, DDIM also enables the inversion of real images back to their latent\ncodes. A direct inheriting application of this inversion operation is real\nimage editing, where the inversion yields latent trajectories to be utilized\nduring the synthesis of the edited image. Unfortunately, this practical tool\nhas enabled malicious users to freely synthesize misinformative or deepfake\ncontents with greater ease, which promotes the spread of unethical and abusive,\nas well as privacy-, and copyright-infringing contents. While defensive\nalgorithms such as AdvDM and Photoguard have been shown to disrupt the\ndiffusion process on these images, the misalignment between their objectives\nand the iterative denoising trajectory at test time results in weak disruptive\nperformance.In this work, we present the DDIM Inversion Attack (DIA) that\nattacks the integrated DDIM trajectory path. Our results support the effective\ndisruption, surpassing previous defensive methods across various editing\nmethods. We believe that our frameworks and results can provide practical\ndefense methods against the malicious use of AI for both the industry and the\nresearch community. Our code is available here:\nhttps://anonymous.4open.science/r/DIA-13419/.",
      "authors": [
        "Seunghoo Hong",
        "Geonho Son",
        "Juhun Lee",
        "Simon S. Woo"
      ],
      "published": "2025-10-01T11:20:03Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00778v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出DIA攻击方法，针对扩散模型中的DDIM确定性反演路径进行对抗性干扰，有效破坏恶意用户利用图像反演技术生成虚假内容的能力，在防御性能上超越现有方法，为防范AI恶意使用提供实用防御方案。",
      "order": 248
    },
    {
      "arxiv_id": "2510.00773v1",
      "title": "Uncertainty-Aware Concept Bottleneck Models with Enhanced\n  Interpretability",
      "summary": "In the context of image classification, Concept Bottleneck Models (CBMs)\nfirst embed images into a set of human-understandable concepts, followed by an\nintrinsically interpretable classifier that predicts labels based on these\nintermediate representations. While CBMs offer a semantically meaningful and\ninterpretable classification pipeline, they often sacrifice predictive\nperformance compared to end-to-end convolutional neural networks. Moreover, the\npropagation of uncertainty from concept predictions to final label decisions\nremains underexplored. In this paper, we propose a novel uncertainty-aware and\ninterpretable classifier for the second stage of CBMs. Our method learns a set\nof binary class-level concept prototypes and uses the distances between\npredicted concept vectors and each class prototype as both a classification\nscore and a measure of uncertainty. These prototypes also serve as\ninterpretable classification rules, indicating which concepts should be present\nin an image to justify a specific class prediction. The proposed framework\nenhances both interpretability and robustness by enabling conformal prediction\nfor uncertain or outlier inputs based on their deviation from the learned\nbinary class-level concept prototypes.",
      "authors": [
        "Haifei Zhang",
        "Patrick Barry",
        "Eduardo Brandao"
      ],
      "published": "2025-10-01T11:11:18Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00773v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种不确定性感知的概念瓶颈模型，通过构建二元类别概念原型，将概念向量与原型距离同时用于分类评分和不确定性度量，在保持可解释性的同时提升了模型鲁棒性，支持对异常输入的合规预测。",
      "order": 249
    },
    {
      "arxiv_id": "2510.00771v1",
      "title": "UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free\n  Flow Matching",
      "summary": "In this paper, we present a vocoder-free framework for audio super-resolution\nthat employs a flow matching generative model to capture the conditional\ndistribution of complex-valued spectral coefficients. Unlike conventional\ntwo-stage diffusion-based approaches that predict a mel-spectrogram and then\nrely on a pre-trained neural vocoder to synthesize waveforms, our method\ndirectly reconstructs waveforms via the inverse Short-Time Fourier Transform\n(iSTFT), thereby eliminating the dependence on a separate vocoder. This design\nnot only simplifies end-to-end optimization but also overcomes a critical\nbottleneck of two-stage pipelines, where the final audio quality is\nfundamentally constrained by vocoder performance. Experiments show that our\nmodel consistently produces high-fidelity 48 kHz audio across diverse\nupsampling factors, achieving state-of-the-art performance on both speech and\ngeneral audio datasets.",
      "authors": [
        "Woongjib Choi",
        "Sangmin Lee",
        "Hyungseob Lim",
        "Hong-Goo Kang"
      ],
      "published": "2025-10-01T11:04:53Z",
      "primary_category": "eess.AS",
      "arxiv_url": "https://arxiv.org/abs/2510.00771v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "UniverSR提出了一种无需声码器的音频超分辨率框架，采用流匹配生成模型直接重建波形，避免了传统两阶段方法对声码器的依赖。该模型通过逆短时傅里叶变换实现端到端优化，在多种上采样因子下均能生成48kHz高保真音频，在语音和通用音频数据集上达到最先进性能。",
      "order": 250
    },
    {
      "arxiv_id": "2510.00766v1",
      "title": "Multi-Objective Task-Aware Predictor for Image-Text Alignment",
      "summary": "Evaluating image-text alignment while reflecting human preferences across\nmultiple aspects is a significant issue for the development of reliable\nvision-language applications. It becomes especially crucial in real-world\nscenarios where multiple valid descriptions exist depending on contexts or user\nneeds. However, research progress is hindered by the lack of comprehensive\nbenchmarks and existing evaluation predictors lacking at least one of these key\nproperties: (1) Alignment with human judgments, (2) Long-sequence processing,\n(3) Inference efficiency, and (4) Applicability to multi-objective scoring. To\naddress these challenges, we propose a plug-and-play architecture to build a\nrobust predictor, MULTI-TAP (Multi-Objective Task-Aware Predictor), capable of\nboth multi and single-objective scoring. MULTI-TAP can produce a single overall\nscore, utilizing a reward head built on top of a large vision-language model\n(LVLMs). We show that MULTI-TAP is robust in terms of application to different\nLVLM architectures, achieving significantly higher performance than existing\nmetrics and even on par with the GPT-4o-based predictor, G-VEval, with a\nsmaller size (7-8B). By training a lightweight ridge regression layer on the\nfrozen hidden states of a pre-trained LVLM, MULTI-TAP can produce fine-grained\nscores for multiple human-interpretable objectives. MULTI-TAP performs better\nthan VisionREWARD, a high-performing multi-objective reward model, in both\nperformance and efficiency on multi-objective benchmarks and our newly released\ntext-image-to-text dataset, EYE4ALL. Our new dataset, consisting of\nchosen/rejected human preferences (EYE4ALLPref) and human-annotated\nfine-grained scores across seven dimensions (EYE4ALLMulti), can serve as a\nfoundation for developing more accessible AI systems by capturing the\nunderlying preferences of users, including blind and low-vision (BLV)\nindividuals.",
      "authors": [
        "Eunki Kim",
        "Na Min An",
        "James Thorne",
        "Hyunjung Shim"
      ],
      "published": "2025-10-01T10:55:33Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00766v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MULTI-TAP，一种多目标任务感知的图像-文本对齐评估器，通过在大规模视觉语言模型上构建奖励头，实现高效的多维度评分，性能优于现有指标并在新数据集EYE4ALL上验证了有效性。",
      "order": 251
    },
    {
      "arxiv_id": "2510.00743v1",
      "title": "From Scores to Preferences: Redefining MOS Benchmarking for Speech\n  Quality Reward Modeling",
      "summary": "Assessing the perceptual quality of synthetic speech is crucial for guiding\nthe development and refinement of speech generation models. However, it has\ntraditionally relied on human subjective ratings such as the Mean Opinion Score\n(MOS), which depend on manual annotations and often suffer from inconsistent\nrating standards and poor reproducibility. To address these limitations, we\nintroduce MOS-RMBench, a unified benchmark that reformulates diverse MOS\ndatasets into a preference-comparison setting, enabling rigorous evaluation\nacross different datasets. Building on MOS-RMBench, we systematically construct\nand evaluate three paradigms for reward modeling: scalar reward models,\nsemi-scalar reward models, and generative reward models (GRMs). Our experiments\nreveal three key findings: (1) scalar models achieve the strongest overall\nperformance, consistently exceeding 74% accuracy; (2) most models perform\nconsiderably worse on synthetic speech than on human speech; and (3) all models\nstruggle on pairs with very small MOS differences. To improve performance on\nthese challenging pairs, we propose a MOS-aware GRM that incorporates an\nMOS-difference-based reward function, enabling the model to adaptively scale\nrewards according to the difficulty of each sample pair. Experimental results\nshow that the MOS-aware GRM significantly improves fine-grained quality\ndiscrimination and narrows the gap with scalar models on the most challenging\ncases. We hope this work will establish both a benchmark and a methodological\nframework to foster more rigorous and scalable research in automatic speech\nquality assessment.",
      "authors": [
        "Yifei Cao",
        "Changhao Jiang",
        "Jiabao Zhuang",
        "Jiajun Sun",
        "Ming Zhang",
        "Zhiheng Xi",
        "Hui Li",
        "Shihan Dou",
        "Yuran Wang",
        "Yunke Zhang",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "published": "2025-10-01T10:27:51Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00743v1",
      "primary_area": "audio_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MOS-RMBench基准，将传统语音质量评分MOS转换为偏好比较设置，系统评估了标量、半标量和生成式奖励模型三种范式。研究发现标量模型整体表现最佳，但所有模型在小MOS差异样本上表现不佳。为此提出MOS感知生成奖励模型，通过基于MOS差异的奖励函数显著提升细粒度质量判别能力。",
      "order": 252
    },
    {
      "arxiv_id": "2510.00733v2",
      "title": "Neural Diffusion Processes for Physically Interpretable Survival\n  Prediction",
      "summary": "We introduce DeepFHT, a survival-analysis framework that couples deep neural\nnetworks with first hitting time (FHT) distributions from stochastic process\ntheory. Time to event is represented as the first passage of a latent diffusion\nprocess to an absorbing boundary. A neural network maps input variables to\nphysically meaningful parameters including initial condition, drift, and\ndiffusion, within a chosen FHT process such as Brownian motion, both with drift\nand driftless. This yields closed-form survival and hazard functions and\ncaptures time-varying risk without assuming proportional-hazards.\n  We compare DeepFHT with Cox survival model using synthetic and real-world\ndatasets. The method achieves predictive accuracy on par with state-of-the-art\napproaches, while maintaining a physics-based interpretable parameterization\nthat elucidates the relation between input features and risk. This combination\nof stochastic process theory and deep learning provides a principled avenue for\nmodeling survival phenomena in complex systems.",
      "authors": [
        "Alessio Cristofoletto",
        "Cesare Rollo",
        "Giovanni Birolo",
        "Piero Fariselli"
      ],
      "published": "2025-10-01T10:16:29Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00733v2",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "DeepFHT框架将深度神经网络与随机过程理论中的首次命中时间分布相结合，通过潜在扩散过程模拟事件发生时间，生成封闭形式的生存和风险函数。该方法在保持与最先进方法相当的预测准确性的同时，提供了基于物理的可解释参数化，阐明输入特征与风险之间的关系，特别适用于复杂系统中的生存现象建模。",
      "order": 253
    },
    {
      "arxiv_id": "2510.00732v1",
      "title": "EvolProver: Advancing Automated Theorem Proving by Evolving Formalized\n  Problems via Symmetry and Difficulty",
      "summary": "Large Language Models (LLMs) for formal theorem proving have shown\nsignificant promise, yet they often lack generalizability and are fragile to\neven minor transformations of problem statements. To address this limitation,\nwe introduce a novel data augmentation pipeline designed to enhance model\nrobustness from two perspectives: symmetry and difficulty. From the symmetry\nperspective, we propose two complementary methods: EvolAST, an Abstract Syntax\nTree (AST) based approach that targets syntactic symmetry to generate\nsemantically equivalent problem variants, and EvolDomain, which leverages LLMs\nto address semantic symmetry by translating theorems across mathematical\ndomains. From the difficulty perspective, we propose EvolDifficulty, which uses\ncarefully designed evolutionary instructions to guide LLMs in generating new\ntheorems with a wider range of difficulty. We then use the evolved data to\ntrain EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver\nestablishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8%\npass@32 rate, surpassing all models of comparable size, including\nreasoning-based models. It also sets new SOTA records for non-reasoning models\non MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and\nIneq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our\ndata augmentation pipeline's effectiveness across multiple benchmarks.",
      "authors": [
        "Yuchen Tian",
        "Ruiyuan Huang",
        "Xuanwu Wang",
        "Jing Ma",
        "Zengfeng Huang",
        "Ziyang Luo",
        "Hongzhan Lin",
        "Da Zheng",
        "Lun Du"
      ],
      "published": "2025-10-01T10:15:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00732v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "EvolProver提出了一种通过对称性和难度演化形式化问题的新型数据增强方法，包含EvolAST（语法对称性）、EvolDomain（语义对称性）和EvolDifficulty（难度扩展）三个组件。训练出的70亿参数非推理定理证明器在多个数学推理基准测试中创造了新的最先进记录。",
      "order": 254
    },
    {
      "arxiv_id": "2510.00728v1",
      "title": "Extreme Blind Image Restoration via Prompt-Conditioned Information\n  Bottleneck",
      "summary": "Blind Image Restoration (BIR) methods have achieved remarkable success but\nfalter when faced with Extreme Blind Image Restoration (EBIR), where inputs\nsuffer from severe, compounded degradations beyond their training scope.\nDirectly learning a mapping from extremely low-quality (ELQ) to high-quality\n(HQ) images is challenging due to the massive domain gap, often leading to\nunnatural artifacts and loss of detail. To address this, we propose a novel\nframework that decomposes the intractable ELQ-to-HQ restoration process. We\nfirst learn a projector that maps an ELQ image onto an intermediate,\nless-degraded LQ manifold. This intermediate image is then restored to HQ using\na frozen, off-the-shelf BIR model. Our approach is grounded in information\ntheory; we provide a novel perspective of image restoration as an Information\nBottleneck problem and derive a theoretically-driven objective to train our\nprojector. This loss function effectively stabilizes training by balancing a\nlow-quality reconstruction term with a high-quality prior-matching term. Our\nframework enables Look Forward Once (LFO) for inference-time prompt refinement,\nand supports plug-and-play strengthening of existing image restoration models\nwithout need for finetuning. Extensive experiments under severe degradation\nregimes provide a thorough analysis of the effectiveness of our work.",
      "authors": [
        "Hongeun Kim",
        "Bryan Sangwoo Kim",
        "Jong Chul Ye"
      ],
      "published": "2025-10-01T10:13:27Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00728v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种针对极端盲图像恢复(EBIR)的新框架，通过信息瓶颈理论将严重退化图像映射到中间低质量流形，再利用现成BIR模型恢复。该方法采用提示条件信息瓶颈，支持推理时提示优化和即插即用增强，在严重退化场景下表现优异。",
      "order": 255
    },
    {
      "arxiv_id": "2510.00726v1",
      "title": "CroSTAta: Cross-State Transition Attention Transformer for Robotic\n  Manipulation",
      "summary": "Learning robotic manipulation policies through supervised learning from\ndemonstrations remains challenging when policies encounter execution variations\nnot explicitly covered during training. While incorporating historical context\nthrough attention mechanisms can improve robustness, standard approaches\nprocess all past states in a sequence without explicitly modeling the temporal\nstructure that demonstrations may include, such as failure and recovery\npatterns. We propose a Cross-State Transition Attention Transformer that\nemploys a novel State Transition Attention (STA) mechanism to modulate standard\nattention weights based on learned state evolution patterns, enabling policies\nto better adapt their behavior based on execution history. Our approach\ncombines this structured attention with temporal masking during training, where\nvisual information is randomly removed from recent timesteps to encourage\ntemporal reasoning from historical context. Evaluation in simulation shows that\nSTA consistently outperforms standard cross-attention and temporal modeling\napproaches like TCN and LSTM networks across all tasks, achieving more than 2x\nimprovement over cross-attention on precision-critical tasks.",
      "authors": [
        "Giovanni Minelli",
        "Giulio Turrisi",
        "Victor Barasuol",
        "Claudio Semini"
      ],
      "published": "2025-10-01T10:09:05Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00726v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出CroSTAta模型，一种用于机器人操作的跨状态转换注意力Transformer。该模型通过新颖的状态转换注意力机制学习状态演化模式来调节标准注意力权重，结合训练时的时序掩码技术，使策略能基于执行历史更好地适应行为变化。在仿真评估中，该方法在精度关键任务上比标准交叉注意力提升2倍以上，优于TCN和LSTM等时序建模方法。",
      "order": 256
    },
    {
      "arxiv_id": "2510.01296v1",
      "title": "From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic\n  Resonance Imaging: A Review",
      "summary": "Deep learning-based 3-dimensional (3D) shape reconstruction from\n2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly\nimportant in medical disease diagnosis, treatment planning, and computational\nmodeling. This review surveys the methodological landscape of 3D MRI\nreconstruction, focusing on 4 primary approaches: point cloud, mesh-based,\nshape-aware, and volumetric models. For each category, we analyze the current\nstate-of-the-art techniques, their methodological foundation, limitations, and\napplications across anatomical structures. We provide an extensive overview\nranging from cardiac to neurological to lung imaging. We also focus on the\nclinical applicability of models to diseased anatomy, and the influence of\ntheir training and testing data. We examine publicly available datasets,\ncomputational demands, and evaluation metrics. Finally, we highlight the\nemerging research directions including multimodal integration and\ncross-modality frameworks. This review aims to provide researchers with a\nstructured overview of current 3D reconstruction methodologies to identify\nopportunities for advancing deep learning towards more robust, generalizable,\nand clinically impactful solutions.",
      "authors": [
        "Emma McMillian",
        "Abhirup Banerjee",
        "Alfonso Bueno-Orovio"
      ],
      "published": "2025-10-01T09:57:29Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01296v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "本文综述了基于深度学习的2D MRI到3D形状重建方法，涵盖点云、网格、形状感知和体积模型四大技术路线，分析了各方法在心脏、神经、肺部等解剖结构中的应用现状、临床价值、数据集及评估指标，并展望了多模态融合等未来研究方向。",
      "order": 257
    },
    {
      "arxiv_id": "2510.00706v1",
      "title": "AttentionDep: Domain-Aware Attention for Explainable Depression Severity\n  Assessment",
      "summary": "In today's interconnected society, social media platforms provide a window\ninto individuals' thoughts, emotions, and mental states. This paper explores\nthe use of platforms like Facebook, X (formerly Twitter), and Reddit for\ndepression severity detection. We propose AttentionDep, a domain-aware\nattention model that drives explainable depression severity estimation by\nfusing contextual and domain knowledge. Posts are encoded hierarchically using\nunigrams and bigrams, with attention mechanisms highlighting clinically\nrelevant tokens. Domain knowledge from a curated mental health knowledge graph\nis incorporated through a cross-attention mechanism, enriching the contextual\nfeatures. Finally, depression severity is predicted using an ordinal regression\nframework that respects the clinical-relevance and natural ordering of severity\nlevels. Our experiments demonstrate that AttentionDep outperforms\nstate-of-the-art baselines by over 5% in graded F1 score across datasets, while\nproviding interpretable insights into its predictions. This work advances the\ndevelopment of trustworthy and transparent AI systems for mental health\nassessment from social media.",
      "authors": [
        "Yusif Ibrahimov",
        "Tarique Anwar",
        "Tommy Yuan",
        "Turan Mutallimov",
        "Elgun Hasanov"
      ],
      "published": "2025-10-01T09:20:53Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00706v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出AttentionDep模型，通过领域感知注意力机制融合上下文和心理健康知识图谱，实现可解释的抑郁症严重程度评估。该模型在社交媒体文本上采用分层编码和交叉注意力，通过序数回归框架预测抑郁程度，在多个数据集上比现有方法F1分数提升5%以上，为心理健康AI系统提供透明可信的评估方案。",
      "order": 258
    },
    {
      "arxiv_id": "2510.00694v1",
      "title": "ALARB: An Arabic Legal Argument Reasoning Benchmark",
      "summary": "We introduce ALARB, a dataset and suite of tasks designed to evaluate the\nreasoning capabilities of large language models (LLMs) within the Arabic legal\ndomain. While existing Arabic benchmarks cover some knowledge-intensive tasks\nsuch as retrieval and understanding, substantial datasets focusing specifically\non multistep reasoning for Arabic LLMs, especially in open-ended contexts, are\nlacking. The dataset comprises over 13K commercial court cases from Saudi\nArabia, with each case including the facts presented, the reasoning of the\ncourt, the verdict, as well as the cited clauses extracted from the regulatory\ndocuments. We define a set of challenging tasks leveraging this dataset and\nreflecting the complexity of real-world legal reasoning, including verdict\nprediction, completion of reasoning chains in multistep legal arguments, and\nidentification of relevant regulations based on case facts. We benchmark a\nrepresentative selection of current open and closed Arabic LLMs on these tasks\nand demonstrate the dataset's utility for instruction tuning. Notably, we show\nthat instruction-tuning a modest 12B parameter model using ALARB significantly\nenhances its performance in verdict prediction and Arabic verdict generation,\nreaching a level comparable to that of GPT-4o.",
      "authors": [
        "Harethah Abu Shairah",
        "Somayah AlHarbi",
        "Abdulaziz AlHussein",
        "Sameer Alsabea",
        "Omar Shaqaqi",
        "Hebah AlShamlan",
        "Omar Knio",
        "George Turkiyyah"
      ],
      "published": "2025-10-01T09:15:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00694v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "legal_ai",
      "tldr_zh": "ALARB是一个阿拉伯语法律论证推理基准数据集，包含13K+沙特商业法庭案例，用于评估大语言模型在阿拉伯法律领域的多步推理能力，包括判决预测、推理链补全和相关法规识别。研究表明，使用该数据集进行指令微调可显著提升模型性能。",
      "order": 259
    },
    {
      "arxiv_id": "2510.00691v1",
      "title": "Inclusive Easy-to-Read Generation for Individuals with Cognitive\n  Impairments",
      "summary": "Ensuring accessibility for individuals with cognitive impairments is\nessential for autonomy, self-determination, and full citizenship. However,\nmanual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to\nscale, limiting access to crucial information in healthcare, education, and\ncivic life. AI-driven ETR generation offers a scalable solution but faces key\nchallenges, including dataset scarcity, domain adaptation, and balancing\nlightweight learning of Large Language Models (LLMs). In this paper, we\nintroduce ETR-fr, the first dataset for ETR text generation fully compliant\nwith European ETR guidelines. We implement parameter-efficient fine-tuning on\nPLMs and LLMs to establish generative baselines. To ensure high-quality and\naccessible outputs, we introduce an evaluation framework based on automatic\nmetrics supplemented by human assessments. The latter is conducted using a\n36-question evaluation form that is aligned with the guidelines. Overall\nresults show that PLMs perform comparably to LLMs and adapt effectively to\nout-of-domain texts.",
      "authors": [
        "François Ledoyen",
        "Gaël Dias",
        "Alexis Lechervy",
        "Jeremie Pantin",
        "Fabrice Maurel",
        "Youssef Chahir",
        "Elisa Gouzonnat",
        "Mélanie Berthelot",
        "Stanislas Moravac",
        "Armony Altinier",
        "Amy Khairalla"
      ],
      "published": "2025-10-01T09:13:18Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00691v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出首个符合欧洲简易阅读指南的ETR-fr数据集，通过参数高效微调方法在预训练模型和大语言模型上建立生成基线，并引入结合自动指标与人工评估的框架。结果显示预训练模型在跨领域文本适应方面表现与大模型相当，为认知障碍群体提供可扩展的文本简化解决方案。",
      "order": 260
    },
    {
      "arxiv_id": "2510.00690v1",
      "title": "ACPO: Adaptive Curriculum Policy Optimization for Aligning\n  Vision-Language Models in Complex Reasoning",
      "summary": "Aligning large-scale vision-language models (VLMs) for complex reasoning via\nreinforcement learning is often hampered by the limitations of existing policy\noptimization algorithms, such as static training schedules and the rigid,\nuniform clipping mechanism in Proximal Policy Optimization (PPO). In this work,\nwe introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework\nthat addresses these challenges through a dual-component adaptive learning\nstrategy. First, ACPO employs a dynamic curriculum that orchestrates a\nprincipled transition from a stable, near on-policy exploration phase to an\nefficient, off-policy exploitation phase by progressively increasing sample\nreuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism\nthat replaces the fixed clipping hyperparameter with dynamic, sample-wise\nbounds modulated by the normalized advantage of each token. This allows for\nmore granular and robust policy updates, enabling larger gradients for\nhigh-potential samples while safeguarding against destructive ones. We conduct\nextensive experiments on a suite of challenging multimodal reasoning\nbenchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate\nthat ACPO consistently outperforms strong baselines such as DAPO and PAPO,\nachieving state-of-the-art performance, accelerated convergence, and superior\ntraining stability.",
      "authors": [
        "Yunhao Wang",
        "Ziting Li",
        "Shuai Chen",
        "Tao Liu",
        "Chao Song",
        "Junjie Jiang",
        "Jian Zhu",
        "Peng Gao",
        "Bin Qin"
      ],
      "published": "2025-10-01T09:11:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00690v1",
      "primary_area": "vla_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ACPO框架，通过动态课程学习和自适应裁剪机制优化视觉语言模型的复杂推理对齐，在多个基准测试中实现最先进性能。",
      "order": 261
    },
    {
      "arxiv_id": "2510.00689v1",
      "title": "Relevance-Zone Reduction in Game Solving",
      "summary": "Game solving aims to find the optimal strategies for all players and\ndetermine the theoretical outcome of a game. However, due to the exponential\ngrowth of game trees, many games remain unsolved, even though methods like\nAlphaZero have demonstrated super-human level in game playing. The\nRelevance-Zone (RZ) is a local strategy reuse technique that restricts the\nsearch to only the regions relevant to the outcome, significantly reducing the\nsearch space. However, RZs are not unique. Different solutions may result in\nRZs of varying sizes. Smaller RZs are generally more favorable, as they\nincrease the chance of reuse and improve pruning efficiency. To this end, we\npropose an iterative RZ reduction method that repeatedly solves the same\nposition while gradually restricting the region involved, guiding the solver\ntoward smaller RZs. We design three constraint generation strategies and\nintegrate an RZ Pattern Table to fully leverage past solutions. In experiments\non 7x7 Killall-Go, our method reduces the average RZ size to 85.95% of the\noriginal. Furthermore, the reduced RZs can be permanently stored as reusable\nknowledge for future solving tasks, especially for larger board sizes or\ndifferent openings.",
      "authors": [
        "Chi-Huang Lin",
        "Ting Han Wei",
        "Chun-Jui Wang",
        "Hung Guei",
        "Chung-Chin Shih",
        "Yun-Jui Tsai",
        "I-Chen Wu",
        "Ti-Rong Wu"
      ],
      "published": "2025-10-01T09:10:32Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00689v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种迭代式相关区域缩减方法，通过逐步限制求解区域来缩小游戏求解中的相关区域规模，在7×7 Killall-Go实验中平均将相关区域大小降至原始的85.95%，并可将缩减后的相关区域作为可重用知识存储。",
      "order": 262
    },
    {
      "arxiv_id": "2510.00664v1",
      "title": "Batch-CAM: Introduction to better reasoning in convolutional deep\n  learning models",
      "summary": "Understanding the inner workings of deep learning models is crucial for\nadvancing artificial intelligence, particularly in high-stakes fields such as\nhealthcare, where accurate explanations are as vital as precision. This paper\nintroduces Batch-CAM, a novel training paradigm that fuses a batch\nimplementation of the Grad-CAM algorithm with a prototypical reconstruction\nloss. This combination guides the model to focus on salient image features,\nthereby enhancing its performance across classification tasks. Our results\ndemonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and\nimage reconstruction quality while reducing training and inference times. By\nensuring models learn from evidence-relevant information,this approach makes a\nrelevant contribution to building more transparent, explainable, and\ntrustworthy AI systems.",
      "authors": [
        "Giacomo Ignesti",
        "Davide Moroni",
        "Massimo Martinelli"
      ],
      "published": "2025-10-01T08:47:00Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00664v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出Batch-CAM训练范式，融合批量Grad-CAM算法与原型重建损失，引导模型聚焦关键图像特征，在提升分类精度的同时优化重建质量并降低计算耗时，为构建透明可信的AI系统提供技术支持，特别适用于医疗等高风险领域。",
      "order": 263
    },
    {
      "arxiv_id": "2510.00662v1",
      "title": "Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to\n  Easy-to-Read Text Generation",
      "summary": "Simplifying complex texts is essential for ensuring equitable access to\ninformation, especially for individuals with cognitive impairments. The\nEasy-to-Read (ETR) initiative offers a framework for making content accessible\nto the neurodivergent population, but the manual creation of such texts remains\ntime-consuming and resource-intensive. In this work, we investigate the\npotential of large language models (LLMs) to automate the generation of ETR\ncontent. To address the scarcity of aligned corpora and the specificity of ETR\nconstraints, we propose a multi-task learning (MTL) approach that trains models\njointly on text summarization, text simplification, and ETR generation. We\nexplore two different strategies: multi-task retrieval-augmented generation\n(RAG) for in-context learning, and MTL-LoRA for parameter-efficient\nfine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a\nnew high-quality dataset, demonstrate the benefits of multi-task setups over\nsingle-task baselines across all configurations. Moreover, results show that\nthe RAG-based strategy enables generalization in out-of-domain settings, while\nMTL-LoRA outperforms all learning strategies within in-domain configurations.",
      "authors": [
        "François Ledoyen",
        "Gaël Dias",
        "Jeremie Pantin",
        "Alexis Lechervy",
        "Fabrice Maurel",
        "Youssef Chahir"
      ],
      "published": "2025-10-01T08:44:05Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00662v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "education_ai",
      "tldr_zh": "本研究提出一种多任务学习方法，利用大语言模型自动生成易读文本，通过结合文本摘要、文本简化和ETR生成任务，解决了认知障碍人群获取信息的难题。实验证明多任务设置优于单任务基线，RAG策略支持跨领域泛化，MTL-LoRA在领域内表现最佳。",
      "order": 264
    },
    {
      "arxiv_id": "2510.00658v1",
      "title": "Align Your Tangent: Training Better Consistency Models via\n  Manifold-Aligned Tangents",
      "summary": "With diffusion and flow matching models achieving state-of-the-art generating\nperformance, the interest of the community now turned to reducing the inference\ntime without sacrificing sample quality. Consistency Models (CMs), which are\ntrained to be consistent on diffusion or probability flow ordinary differential\nequation (PF-ODE) trajectories, enable one or two-step flow or diffusion\nsampling. However, CMs typically require prolonged training with large batch\nsizes to obtain competitive sample quality. In this paper, we examine the\ntraining dynamics of CMs near convergence and discover that CM tangents -- CM\noutput update directions -- are quite oscillatory, in the sense that they move\nparallel to the data manifold, not towards the manifold. To mitigate\noscillatory tangents, we propose a new loss function, called the manifold\nfeature distance (MFD), which provides manifold-aligned tangents that point\ntoward the data manifold. Consequently, our method -- dubbed Align Your Tangent\n(AYT) -- can accelerate CM training by orders of magnitude and even out-perform\nthe learned perceptual image patch similarity metric (LPIPS). Furthermore, we\nfind that our loss enables training with extremely small batch sizes without\ncompromising sample quality. Code: https://github.com/1202kbs/AYT",
      "authors": [
        "Beomsu Kim",
        "Byunghee Cha",
        "Jong Chul Ye"
      ],
      "published": "2025-10-01T08:35:18Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00658v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种名为'对齐切线'(AYT)的新方法，通过引入流形特征距离(MFD)损失函数来解决一致性模型训练中切线振荡问题。该方法能显著加速训练速度，在极小的批次大小下仍保持样本质量，甚至超越LPIPS指标。",
      "order": 265
    },
    {
      "arxiv_id": "2510.00636v1",
      "title": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution",
      "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.",
      "authors": [
        "Alessio Devoto",
        "Maximilian Jeblick",
        "Simon Jégou"
      ],
      "published": "2025-10-01T08:12:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00636v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出'期望注意力'方法，通过预测未来查询分布来估计KV对重要性，实现无需训练的KV缓存压缩。该方法利用LLM激活的分布特性计算闭式期望注意力分数，在预填充和解码阶段均优于现有技术，并发布了包含20多种技术的KVPress压缩库。",
      "order": 266
    },
    {
      "arxiv_id": "2510.00629v2",
      "title": "Tenyidie Syllabification corpus creation and deep learning applications",
      "summary": "The Tenyidie language is a low-resource language of the Tibeto-Burman family\nspoken by the Tenyimia Community of Nagaland in the north-eastern part of India\nand is considered a major language in Nagaland. It is tonal,\nSubject-Object-Verb, and highly agglutinative in nature. Being a low-resource\nlanguage, very limited research on Natural Language Processing (NLP) has been\nconducted. To the best of our knowledge, no work on syllabification has been\nreported for this language. Among the many NLP tasks, syllabification or\nsyllabication is an important task in which the given word syllables are\nidentified. The contribution of this work is the creation of 10,120 syllabified\nTenyidie words and the application of the Deep Learning techniques on the\ncreated corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and\nEncoder-decoder deep learning architectures on our created dataset. In our\ndataset split of 80:10:10 (train:validation:test) set, we achieved the highest\naccuracy of 99.21% with BLSTM model on the test set. This work will find its\napplication in numerous other NLP applications, such as morphological analysis,\npart-of-speech tagging, machine translation, etc, for the Tenyidie Language.\n  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF;\nEncoder-decoder",
      "authors": [
        "Teisovi Angami",
        "Kevisino Khate"
      ],
      "published": "2025-10-01T08:00:59Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00629v2",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究为低资源藏缅语系Tenyidie语言创建了首个包含10,120个音节划分标注的语料库，并应用LSTM、BLSTM、BLSTM+CRF及编码器-解码器等深度学习架构进行音节划分任务。在80:10:10的数据划分下，BLSTM模型在测试集上达到99.21%的最高准确率，为后续词法分析、词性标注和机器翻译等NLP应用奠定基础。",
      "order": 267
    },
    {
      "arxiv_id": "2510.00627v1",
      "title": "Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and\n  Lightweight Trajectory Prediction",
      "summary": "Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and\nIntelligent Transportation Systems (ITS), supporting efficient motion planning\nand real-time traffic safety management. Diffusion models have recently\ndemonstrated strong performance in probabilistic trajectory prediction, but\ntheir large model size and slow sampling process hinder real-world deployment.\nThis paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel\nmethod for real-time and lightweight trajectory prediction. Built upon\nCollaborative Progressive Distillation (CPD), CDDM progressively transfers\nknowledge from a high-capacity teacher diffusion model to a lightweight student\nmodel, jointly reducing both the number of sampling steps and the model size\nacross distillation iterations. A dual-signal regularized distillation loss is\nfurther introduced to incorporate guidance from both the teacher and\nground-truth data, mitigating potential overfitting and ensuring robust\nperformance. Extensive experiments on the ETH-UCY pedestrian benchmark and the\nnuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art\nprediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the\nbaseline model's ADE and FDE performance on pedestrian trajectories, while\nrequiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x\ncompression, 31x acceleration, and 9 ms latency. Qualitative results further\nshow that CDDM generates diverse and accurate trajectories under dynamic agent\nbehaviors and complex social interactions. By bridging high-performing\ngenerative models with practical deployment constraints, CDDM enables\nresource-efficient probabilistic prediction for AVs and ITS. Code is available\nat https://github.com/bingzhangw/CDDM.",
      "authors": [
        "Bingzhang Wang",
        "Kehua Chen",
        "Yinhai Wang"
      ],
      "published": "2025-10-01T08:00:31Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00627v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出协作蒸馏扩散模型(CDDM)，通过协作渐进蒸馏技术将高容量教师模型知识迁移至轻量学生模型，在保持轨迹预测精度的同时实现161倍模型压缩和31倍加速，为自动驾驶和智能交通系统提供高效的实时预测解决方案。",
      "order": 268
    },
    {
      "arxiv_id": "2510.00625v1",
      "title": "Is Model Editing Built on Sand? Revealing Its Illusory Success and\n  Fragile Foundation",
      "summary": "Large language models (LLMs) inevitably encode outdated or incorrect\nknowledge. Updating, deleting, and forgetting such knowledge is important for\nalignment, safety, and other issues. To address this issue, model editing has\nemerged as a promising paradigm: by precisely editing a small subset of\nparameters such that a specific fact is updated while preserving other\nknowledge. Despite its great success reported in previous papers, we find the\napparent reliability of editing rests on a fragile foundation and the current\nliterature is largely driven by illusory success. The fundamental goal of\nsteering the model's output toward a target with minimal modification would\nencourage exploiting hidden shortcuts, rather than utilizing real semantics.\nThis problem directly challenges the feasibility of the current model editing\nliterature at its very foundation, as shortcuts are inherently at odds with\nrobust knowledge integration. Coincidentally, this issue has long been obscured\nby evaluation frameworks that lack the design of negative examples. To uncover\nit, we systematically develop a suite of new evaluation methods. Strikingly, we\nfind that state-of-the-art approaches collapse even under the simplest negation\nqueries. Our empirical evidence shows that editing is likely to be based on\nshortcuts rather than full semantics, calling for an urgent reconsideration of\nthe very basis of model editing before further advancements can be meaningfully\npursued.",
      "authors": [
        "Wei Liu",
        "Haomei Xu",
        "Bingqing Liu",
        "Zhiying Deng",
        "Haozhao Wang",
        "Jun Wang",
        "Ruixuan Li",
        "Yee Whye Teh",
        "Wee Sun Lee"
      ],
      "published": "2025-10-01T07:59:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00625v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文质疑当前模型编辑技术的可靠性，指出其成功可能建立在利用隐藏捷径而非真实语义的基础上。通过开发新的评估方法，研究发现现有最先进方法在简单否定查询下即失效，表明编辑可能基于脆弱机制，呼吁重新审视模型编辑的基础。",
      "order": 269
    },
    {
      "arxiv_id": "2510.00621v1",
      "title": "FAME: Adaptive Functional Attention with Expert Routing for\n  Function-on-Function Regression",
      "summary": "Functional data play a pivotal role across science and engineering, yet their\ninfinite-dimensional nature makes representation learning challenging.\nConventional statistical models depend on pre-chosen basis expansions or\nkernels, limiting the flexibility of data-driven discovery, while many\ndeep-learning pipelines treat functions as fixed-grid vectors, ignoring\ninherent continuity. In this paper, we introduce Functional Attention with a\nMixture-of-Experts (FAME), an end-to-end, fully data-driven framework for\nfunction-on-function regression. FAME forms continuous attention by coupling a\nbidirectional neural controlled differential equation with MoE-driven vector\nfields to capture intra-functional continuity, and further fuses change to\ninter-functional dependencies via multi-head cross attention. Extensive\nexperiments on synthetic and real-world functional-regression benchmarks show\nthat FAME achieves state-of-the-art accuracy, strong robustness to arbitrarily\nsampled discrete observations of functions.",
      "authors": [
        "Yifei Gao",
        "Yong Chen",
        "Chen Zhang"
      ],
      "published": "2025-10-01T07:53:55Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00621v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "FAME提出了一种用于函数对函数回归的自适应功能注意力框架，结合神经控制微分方程和专家混合路由，在保持函数连续性的同时实现数据驱动的表示学习，在合成和真实数据集上达到最先进性能。",
      "order": 270
    },
    {
      "arxiv_id": "2510.00620v1",
      "title": "HARPA: A Testability-Driven, Literature-Grounded Framework for Research\n  Ideation",
      "summary": "While there has been a surge of interest in automated scientific discovery\n(ASD), especially with the emergence of LLMs, it remains challenging for tools\nto generate hypotheses that are both testable and grounded in the scientific\nliterature. Additionally, existing ideation tools are not adaptive to prior\nexperimental outcomes. We developed HARPA to address these challenges by\nincorporating the ideation workflow inspired by human researchers. HARPA first\nidentifies emerging research trends through literature mining, then explores\nhypothesis design spaces, and finally converges on precise, testable hypotheses\nby pinpointing research gaps and justifying design choices. Our evaluations\nshow that HARPA-generated hypothesis-driven research proposals perform\ncomparably to a strong baseline AI-researcher across most qualitative\ndimensions (e.g., specificity, novelty, overall quality), but achieve\nsignificant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness\n(+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the\nASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11\nout of 40) and fewer failures (16 vs. 21 out of 40), showing that expert\nfeasibility judgments track with actual execution success. Furthermore, to\nsimulate how researchers continuously refine their understanding of what\nhypotheses are both testable and potentially interesting from experience, HARPA\nlearns a reward model that scores new hypotheses based on prior experimental\noutcomes, achieving approx. a 28\\% absolute gain over HARPA's untrained\nbaseline scorer. Together, these methods represent a step forward in the field\nof AI-driven scientific discovery.",
      "authors": [
        "Rosni Vasu",
        "Peter Jansen",
        "Pao Siangliulue",
        "Cristina Sarasua",
        "Abraham Bernstein",
        "Peter Clark",
        "Bhavana Dalvi Mishra"
      ],
      "published": "2025-10-01T07:52:19Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00620v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "HARPA是一个基于文献挖掘的科研构思框架，通过识别研究趋势、探索假设空间并收敛到可测试假设，显著提升了研究提案的可行性和文献基础性。评估显示其在可行性和基础性方面优于基线，并能通过奖励模型从实验经验中学习改进假设生成。",
      "order": 271
    },
    {
      "arxiv_id": "2510.00619v1",
      "title": "What Did I Learn? Operational Competence Assessment for AI-Based\n  Trajectory Planners",
      "summary": "Automated driving functions increasingly rely on machine learning for tasks\nlike perception and trajectory planning, requiring large, relevant datasets.\nThe performance of these algorithms depends on how closely the training data\nmatches the task. To ensure reliable functioning, it is crucial to know what is\nincluded in the dataset to assess the trained model's operational risk. We aim\nto enhance the safe use of machine learning in automated driving by developing\na method to recognize situations that an automated vehicle has not been\nsufficiently trained on. This method also improves explainability by describing\nthe dataset at a human-understandable level. We propose modeling driving data\nas knowledge graphs, representing driving scenes with entities and their\nrelationships. These graphs are queried for specific sub-scene configurations\nto check their occurrence in the dataset. We estimate a vehicle's competence in\na driving scene by considering the coverage and complexity of sub-scene\nconfigurations in the training set. Higher complexity scenes require greater\ncoverage for high competence. We apply this method to the NuPlan dataset,\nmodeling it with knowledge graphs and analyzing the coverage of specific\ndriving scenes. This approach helps monitor the competence of machine learning\nmodels trained on the dataset, which is essential for trustworthy AI to be\ndeployed in automated driving.",
      "authors": [
        "Michiel Braat",
        "Maren Buermann",
        "Marijke van Weperen",
        "Jan-Pieter Paardekooper"
      ],
      "published": "2025-10-01T07:46:50Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00619v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于知识图谱的自动驾驶轨迹规划器能力评估方法，通过将驾驶场景建模为实体关系图，查询训练数据中特定子场景配置的覆盖情况，从而评估AI模型在未充分训练场景中的操作风险，增强自动驾驶中机器学习的可靠性和可解释性。",
      "order": 272
    },
    {
      "arxiv_id": "2510.00615v1",
      "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
      "summary": "Large language models (LLMs) are increasingly deployed as agents in dynamic,\nreal-world environments, where success requires both reasoning and effective\ntool use. A central challenge for agentic tasks is the growing context length,\nas agents must accumulate long histories of actions and observations. This\nexpansion raises costs and reduces efficiency in long-horizon tasks, yet prior\nwork on context compression has mostly focused on single-step tasks or narrow\napplications. We introduce Agent Context Optimization (ACON), a unified\nframework that optimally compresses both environment observations and\ninteraction histories into concise yet informative condensations. ACON\nleverages compression guideline optimization in natural language space: given\npaired trajectories where full context succeeds but compressed context fails,\ncapable LLMs analyze the causes of failure, and the compression guideline is\nupdated accordingly. Furthermore, we propose distilling the optimized LLM\ncompressor into smaller models to reduce the overhead of the additional module.\nExperiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON\nreduces memory usage by 26-54% (peak tokens) while largely preserving task\nperformance, preserves over 95% of accuracy when distilled into smaller\ncompressors, and enhances smaller LMs as long-horizon agents with up to 46%\nperformance improvement.",
      "authors": [
        "Minki Kang",
        "Wei-Ning Chen",
        "Dongge Han",
        "Huseyin A. Inan",
        "Lukas Wutschitz",
        "Yanzhi Chen",
        "Robert Sim",
        "Saravan Rajmohan"
      ],
      "published": "2025-10-01T07:43:49Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00615v1",
      "primary_area": "text_models",
      "secondary_focus": "['long_context', 'model_compression']",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ACON框架，通过自然语言空间优化压缩指导原则，将环境观察和交互历史压缩为简洁信息，在保持任务性能的同时显著降低内存使用（26-54%），并可蒸馏至小模型实现高效部署。",
      "order": 273
    },
    {
      "arxiv_id": "2510.00600v1",
      "title": "Hybrid Training for Vision-Language-Action Models",
      "summary": "Using Large Language Models to produce intermediate thoughts, a.k.a.\nChain-of-thought (CoT), before providing an answer has been a successful recipe\nfor solving complex language tasks. In robotics, similar embodied CoT\nstrategies, generating thoughts before actions, have also been shown to lead to\nimproved performance when using Vision-Language-Action models (VLAs). As these\ntechniques increase the length of the model's generated outputs to include the\nthoughts, the inference time is negatively affected. Delaying an agent's\nactions in real-world executions, as in robotic manipulation settings, strongly\naffects the usability of a method, as tasks require long sequences of actions.\nHowever, is the generation of long chains-of-thought a strong prerequisite for\nachieving performance improvements? In this work, we explore the idea of Hybrid\nTraining (HyT), a framework that enables VLAs to learn from thoughts and\nbenefit from the associated performance gains, while enabling the possibility\nto leave out CoT generation during inference. Furthermore, by learning to\nconditionally predict a diverse set of outputs, HyT supports flexibility at\ninference time, enabling the model to either predict actions directly, generate\nthoughts or follow instructions. We evaluate the proposed method in a series of\nsimulated benchmarks and real-world experiments.",
      "authors": [
        "Pietro Mazzaglia",
        "Cansu Sancaktar",
        "Markus Peschl",
        "Daniel Dijkman"
      ],
      "published": "2025-10-01T07:27:15Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00600v1",
      "primary_area": "vla_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出混合训练框架HyT，使视觉-语言-动作模型能在训练时学习思维链推理，在推理时无需生成思维链即可保持性能优势，解决了机器人任务中长思维链导致的推理延迟问题。",
      "order": 274
    },
    {
      "arxiv_id": "2510.00591v1",
      "title": "AI-Driven Self-Evolving Software: A Promising Path Toward Software\n  Automation",
      "summary": "Software automation has long been a central goal of software engineering,\nstriving for software development that proceeds without human intervention.\nRecent efforts have leveraged Artificial Intelligence (AI) to advance software\nautomation with notable progress. However, current AI functions primarily as\nassistants to human developers, leaving software development still dependent on\nexplicit human intervention. This raises a fundamental question: Can AI move\nbeyond its role as an assistant to become a core component of software, thereby\nenabling genuine software automation? To investigate this vision, we introduce\nAI-Driven Self-Evolving Software, a new form of software that evolves\ncontinuously through direct interaction with users. We demonstrate the\nfeasibility of this idea with a lightweight prototype built on a multi-agent\narchitecture that autonomously interprets user requirements, generates and\nvalidates code, and integrates new functionalities. Case studies across\nmultiple representative scenarios show that the prototype can reliably\nconstruct and reuse functionality, providing early evidence that such software\nsystems can scale to more sophisticated applications and pave the way toward\ntruly automated software development. We make code and cases in this work\npublicly available at https://anonymous.4open.science/r/live-software.",
      "authors": [
        "Liyi Cai",
        "Yijie Ren",
        "Yitong Zhang",
        "Jia Li"
      ],
      "published": "2025-10-01T07:17:51Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.00591v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "code_generation",
      "tldr_zh": "本文提出AI驱动的自进化软件概念，通过多智能体架构实现软件与用户直接交互下的持续演化。原型系统能自主解析需求、生成验证代码并集成新功能，案例研究表明该系统可可靠构建复用功能，为真正自动化软件开发铺平道路。",
      "order": 275
    },
    {
      "arxiv_id": "2510.01295v1",
      "title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM\n  Evaluation",
      "summary": "As Large Language Models (LLMs) transition from static tools to autonomous\nagents, traditional evaluation benchmarks that measure performance on\ndownstream tasks are becoming insufficient. These methods fail to capture the\nemergent social and cognitive dynamics that arise when agents communicate,\npersuade, and collaborate in interactive environments. To address this gap, we\nintroduce a novel evaluation framework that uses multi-agent debate as a\ncontrolled \"social laboratory\" to discover and quantify these behaviors. In our\nframework, LLM-based agents, instantiated with distinct personas and\nincentives, deliberate on a wide range of challenging topics under the\nsupervision of an LLM moderator. Our analysis, enabled by a new suite of\npsychometric and semantic metrics, reveals several key findings. Across\nhundreds of debates, we uncover a powerful and robust emergent tendency for\nagents to seek consensus, consistently reaching high semantic agreement ({\\mu}\n> 0.88) even without explicit instruction and across sensitive topics. We show\nthat assigned personas induce stable, measurable psychometric profiles,\nparticularly in cognitive effort, and that the moderators persona can\nsignificantly alter debate outcomes by structuring the environment, a key\nfinding for external AI alignment. This work provides a blueprint for a new\nclass of dynamic, psychometrically grounded evaluation protocols designed for\nthe agentic setting, offering a crucial methodology for understanding and\nshaping the social behaviors of the next generation of AI agents. We have\nreleased the code and results at\nhttps://github.com/znreza/multi-agent-LLM-eval-for-debate.",
      "authors": [
        "Zarreen Reza"
      ],
      "published": "2025-10-01T07:10:28Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01295v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出了一种新颖的多智能体LLM评估框架，通过辩论作为'社会实验室'来量化智能体在交互环境中的社交和认知行为。研究发现智能体具有强烈的共识寻求倾向，角色设定能产生稳定的心理测量特征，且主持人角色能显著影响辩论结果，为下一代AI智能体的社会行为评估提供了重要方法论。",
      "order": 276
    },
    {
      "arxiv_id": "2510.00585v1",
      "title": "U-DFA: A Unified DINOv2-Unet with Dual Fusion Attention for\n  Multi-Dataset Medical Segmentation",
      "summary": "Accurate medical image segmentation plays a crucial role in overall diagnosis\nand is one of the most essential tasks in the diagnostic pipeline. CNN-based\nmodels, despite their extensive use, suffer from a local receptive field and\nfail to capture the global context. A common approach that combines CNNs with\ntransformers attempts to bridge this gap but fails to effectively fuse the\nlocal and global features. With the recent emergence of VLMs and foundation\nmodels, they have been adapted for downstream medical imaging tasks; however,\nthey suffer from an inherent domain gap and high computational cost. To this\nend, we propose U-DFA, a unified DINOv2-Unet encoder-decoder architecture that\nintegrates a novel Local-Global Fusion Adapter (LGFA) to enhance segmentation\nperformance. LGFA modules inject spatial features from a CNN-based Spatial\nPattern Adapter (SPA) module into frozen DINOv2 blocks at multiple stages,\nenabling effective fusion of high-level semantic and spatial features. Our\nmethod achieves state-of-the-art performance on the Synapse and ACDC datasets\nwith only 33\\% of the trainable model parameters. These results demonstrate\nthat U-DFA is a robust and scalable framework for medical image segmentation\nacross multiple modalities.",
      "authors": [
        "Zulkaif Sajjad",
        "Furqan Shaukat",
        "Junaid Mir"
      ],
      "published": "2025-10-01T07:06:49Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.00585v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "U-DFA提出了一种统一的DINOv2-Unet架构，通过新型局部-全局融合适配器(LGFA)和空间模式适配器(SPA)，有效融合CNN的局部特征与Transformer的全局语义，在医学图像分割任务上以仅33%可训练参数达到最先进性能。",
      "order": 277
    },
    {
      "arxiv_id": "2510.00582v1",
      "title": "SAGE-LD: Towards Scalable and Generalizable End-to-End Language\n  Diarization via Simulated Data Augmentation",
      "summary": "In this paper, we present a neural spoken language diarization model that\nsupports an unconstrained span of languages within a single framework. Our\napproach integrates a learnable query-based architecture grounded in\nmultilingual awareness, with large-scale pretraining on simulated\ncode-switching data. By jointly leveraging these two components, our method\novercomes the limitations of conventional approaches in data scarcity and\narchitecture optimization, and generalizes effectively to real-world\nmultilingual settings across diverse environments. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance on several\nlanguage diarization benchmarks, with a relative performance improvement of 23%\nto 52% over previous methods. We believe that this work not only advances\nresearch in language diarization but also establishes a foundational framework\nfor code-switching speech technologies.",
      "authors": [
        "Sangmin Lee",
        "Woongjib Choi",
        "Jihyun Kim",
        "Hong-Goo Kang"
      ],
      "published": "2025-10-01T07:01:33Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00582v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出SAGE-LD模型，通过可学习查询架构和模拟语码转换数据的大规模预训练，实现单框架内支持任意语言的语言日记化。该方法克服了传统方法在数据稀缺和架构优化上的局限，在多个基准测试中相对性能提升23%-52%，为语码转换语音技术建立了基础框架。",
      "order": 278
    },
    {
      "arxiv_id": "2510.00570v1",
      "title": "Adaptive Shared Experts with LoRA-Based Mixture of Experts for\n  Multi-Task Learning",
      "summary": "Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task\nlearning (MTL). However, existing MoE-MTL methods often rely on single-task\npretrained backbones and suffer from redundant adaptation and inefficient\nknowledge sharing during the transition from single-task to multi-task learning\n(STL to MTL). To address these limitations, we propose adaptive shared experts\n(ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are\nassigned router-computed gating weights jointly normalized with sparse experts.\nThis design facilitates STL to MTL transition, enhances expert specialization,\nand cooperation. Furthermore, we incorporate fine-grained experts by increasing\nthe number of LoRA experts while proportionally reducing their rank, enabling\nmore effective knowledge sharing under a comparable parameter budget. Extensive\nexperiments on the PASCAL-Context benchmark, under unified training settings,\ndemonstrate that ASE consistently improves performance across diverse\nconfigurations and validates the effectiveness of fine-grained designs for MTL.",
      "authors": [
        "Minghao Yang",
        "Ren Togo",
        "Guang Li",
        "Takahiro Ogawa",
        "Miki Haseyama"
      ],
      "published": "2025-10-01T06:49:19Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00570v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出了一种基于LoRA的自适应共享专家混合模型（ASE），通过路由器计算的门控权重联合归一化共享专家与稀疏专家，解决了多任务学习中从单任务到多任务转换时的冗余适应和知识共享效率低下的问题。在PASCAL-Context基准测试中，该方法在统一训练设置下显著提升了性能，验证了细粒度设计在多任务学习中的有效性。",
      "order": 279
    },
    {
      "arxiv_id": "2510.00566v1",
      "title": "Panorama: Fast-Track Nearest Neighbors",
      "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
      "authors": [
        "Vansh Ramani",
        "Alexis Schlomer",
        "Akash Nayar",
        "Panagiotis Karras",
        "Sayan Ranu",
        "Jignesh M. Patel"
      ],
      "published": "2025-10-01T06:38:45Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00566v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "PANORAMA提出一种机器学习驱动的近似最近邻搜索优化方法，通过数据自适应的正交变换将90%以上信号能量压缩到前半维度，实现基于部分距离计算的早期候选剪枝。该方法无需修改现有索引结构，在IVFPQ、HNSW等主流ANNS算法上实现2-30倍端到端加速且不损失召回率。",
      "order": 280
    },
    {
      "arxiv_id": "2510.00565v1",
      "title": "Toward Safer Diffusion Language Models: Discovery and Mitigation of\n  Priming Vulnerability",
      "summary": "Diffusion language models (DLMs) generate tokens in parallel through\niterative denoising, which can reduce latency and enable bidirectional\nconditioning. However, the safety risks posed by jailbreak attacks that exploit\nthis inference mechanism are not well understood. In this paper, we reveal that\nDLMs have a critical vulnerability stemming from their iterative denoising\nprocess and propose a countermeasure. Specifically, our investigation shows\nthat if an affirmative token for a harmful query appears at an intermediate\nstep, subsequent denoising can be steered toward a harmful response even in\naligned models. As a result, simply injecting such affirmative tokens can\nreadily bypass the safety guardrails. Furthermore, we demonstrate that the\nvulnerability allows existing optimization-based jailbreak attacks to succeed\non DLMs. Building on this analysis, we propose a novel safety alignment method\ntailored to DLMs that trains models to generate safe responses from\ncontaminated intermediate states that contain affirmative tokens. Our\nexperiments indicate that the proposed method significantly mitigates the\nvulnerability with minimal impact on task performance. Furthermore, our method\nimproves robustness against conventional jailbreak attacks. Our work\nunderscores the need for DLM-specific safety research.",
      "authors": [
        "Shojiro Yamabe",
        "Jun Sakuma"
      ],
      "published": "2025-10-01T06:35:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00565v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文揭示了扩散语言模型(DLMs)在迭代去噪过程中存在关键安全漏洞：中间步骤出现有害查询的肯定令牌会引导模型生成有害回复，从而绕过安全防护。研究提出了一种针对DLMs的安全对齐方法，训练模型从被污染的中间状态生成安全响应，实验表明该方法能显著缓解漏洞且对任务性能影响最小，同时提升对传统越狱攻击的鲁棒性。",
      "order": 281
    },
    {
      "arxiv_id": "2510.00563v1",
      "title": "Memory Determines Learning Direction: A Theory of Gradient-Based\n  Optimization in State Space Models",
      "summary": "State space models (SSMs) have gained attention by showing potential to\noutperform Transformers. However, previous studies have not sufficiently\naddressed the mechanisms underlying their high performance owing to a lack of\ntheoretical explanation of SSMs' learning dynamics. In this study, we provide\nsuch an explanation and propose an improved training strategy. The memory\ncapacity of SSMs can be evaluated by examining how input time series are stored\nin their current state. Such an examination reveals a tradeoff between memory\naccuracy and length, as well as the theoretical equivalence between the\nstructured state space sequence model (S4) and a simplified S4 with diagonal\nrecurrent weights. This theoretical foundation allows us to elucidate the\nlearning dynamics, proving the importance of initial parameters. Our analytical\nresults suggest that successful learning requires the initial memory structure\nto be the longest possible even if memory accuracy may deteriorate or the\ngradient lose the teacher information. Experiments on tasks requiring long\nmemory confirmed that extending memory is difficult, emphasizing the importance\nof initialization. Furthermore, we found that fixing recurrent weights can be\nmore advantageous than adapting them because it achieves comparable or even\nhigher performance with faster convergence. Our results provide a new\ntheoretical foundation for SSMs and potentially offer a novel optimization\nstrategy.",
      "authors": [
        "JingChuan Guan",
        "Tomoyuki Kubota",
        "Yasuo Kuniyoshi",
        "Kohei Nakajima"
      ],
      "published": "2025-10-01T06:30:42Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00563v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出状态空间模型(SSMs)的学习动态理论，揭示记忆精度与长度间的权衡关系，证明初始参数对学习成功的关键作用。研究发现固定循环权重可获得更快收敛和相当甚至更优性能，为SSMs提供了新的理论基础和优化策略。",
      "order": 282
    },
    {
      "arxiv_id": "2510.00555v1",
      "title": "PromptPilot: Improving Human-AI Collaboration Through LLM-Enhanced\n  Prompt Engineering",
      "summary": "Effective prompt engineering is critical to realizing the promised\nproductivity gains of large language models (LLMs) in knowledge-intensive\ntasks. Yet, many users struggle to craft prompts that yield high-quality\noutputs, limiting the practical benefits of LLMs. Existing approaches, such as\nprompt handbooks or automated optimization pipelines, either require\nsubstantial effort, expert knowledge, or lack interactive guidance. To address\nthis gap, we design and evaluate PromptPilot, an interactive prompting\nassistant grounded in four empirically derived design objectives for\nLLM-enhanced prompt engineering. We conducted a randomized controlled\nexperiment with 80 participants completing three realistic, work-related\nwriting tasks. Participants supported by PromptPilot achieved significantly\nhigher performance (median: 78.3 vs. 61.7; p = .045, d = 0.56), and reported\nenhanced efficiency, ease-of-use, and autonomy during interaction. These\nfindings empirically validate the effectiveness of our proposed design\nobjectives, establishing LLM-enhanced prompt engineering as a viable technique\nfor improving human-AI collaboration.",
      "authors": [
        "Niklas Gutheil",
        "Valentin Mayer",
        "Leopold Müller",
        "Jörg Rommelt",
        "Niklas Kühl"
      ],
      "published": "2025-10-01T06:14:42Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.00555v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "PromptPilot是一个基于LLM增强的交互式提示工程助手，通过四项实证设计目标改善人机协作。在80名参与者的随机对照实验中，使用该系统的用户在写作任务中表现显著提升（中位数：78.3 vs 61.7），并报告了更高的效率、易用性和自主性。",
      "order": 283
    },
    {
      "arxiv_id": "2510.00553v2",
      "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
      "summary": "Recent advances in reasoning capabilities of large language models (LLMs) are\nlargely driven by reinforcement learning (RL), yet the underlying parameter\ndynamics during RL training remain poorly understood. This work identifies two\nfundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1\nDominance, where the top singular subspace of the parameter update matrix\nnearly fully determines reasoning improvements, recovering over 99\\% of\nperformance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace\nevolves linearly throughout training, enabling accurate prediction from early\ncheckpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the\ngeneralizability of these properties. More importantly, based on these\nfindings, we propose AlphaRL, a plug-in acceleration framework that\nextrapolates the final parameter update using a short early training window,\nachieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning\nperformance without extra modules or hyperparameter tuning. This positions our\nfinding as a versatile and practical tool for large-scale RL, opening a path\ntoward principled, interpretable, and efficient training paradigm for LLMs.",
      "authors": [
        "Yuchen Cai",
        "Ding Cao",
        "Xin Xu",
        "Zijun Yao",
        "Yuqing Huang",
        "Zhenyu Tan",
        "Benyi Zhang",
        "Guiquan Liu",
        "Junfeng Fang"
      ],
      "published": "2025-10-01T06:13:50Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00553v2",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究揭示大语言模型强化学习训练中的参数动态规律：发现参数更新矩阵呈现秩1主导特性，其主导子空间线性演化，据此提出AlphaRL加速框架，可在保持96%以上推理性能的同时实现2.5倍训练加速，为LLM训练提供可解释的高效范式。",
      "order": 284
    },
    {
      "arxiv_id": "2510.00552v1",
      "title": "Data Quality Challenges in Retrieval-Augmented Generation",
      "summary": "Organizations increasingly adopt Retrieval-Augmented Generation (RAG) to\nenhance Large Language Models with enterprise-specific knowledge. However,\ncurrent data quality (DQ) frameworks have been primarily developed for static\ndatasets, and only inadequately address the dynamic, multi-stage nature of RAG\nsystems. This study aims to develop DQ dimensions for this new type of AI-based\nsystems. We conduct 16 semi-structured interviews with practitioners of leading\nIT service companies. Through a qualitative content analysis, we inductively\nderive 15 distinct DQ dimensions across the four processing stages of RAG\nsystems: data extraction, data transformation, prompt & search, and generation.\nOur findings reveal that (1) new dimensions have to be added to traditional DQ\nframeworks to also cover RAG contexts; (2) these new dimensions are\nconcentrated in early RAG steps, suggesting the need for front-loaded quality\nmanagement strategies, and (3) DQ issues transform and propagate through the\nRAG pipeline, necessitating a dynamic, step-aware approach to quality\nmanagement.",
      "authors": [
        "Leopold Müller",
        "Joshua Holstein",
        "Sarah Bause",
        "Gerhard Satzger",
        "Niklas Kühl"
      ],
      "published": "2025-10-01T06:13:40Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00552v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究针对检索增强生成(RAG)系统开发数据质量维度框架。通过对16位IT服务公司从业者的访谈，识别出覆盖RAG四个处理阶段的15个数据质量维度。研究发现：传统数据质量框架需新增维度以适应RAG场景；新维度集中于前期处理阶段，需前置质量管理策略；数据质量问题会在RAG流程中传递演变，需动态分阶段管理。",
      "order": 285
    },
    {
      "arxiv_id": "2510.00549v2",
      "title": "EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases",
      "summary": "Machine learning models for clinical prediction rely on structured data\nextracted from Electronic Medical Records (EMRs), yet this process remains\ndominated by hardcoded, database-specific pipelines for cohort definition,\nfeature selection, and code mapping. These manual efforts limit scalability,\nreproducibility, and cross-institutional generalization. To address this, we\nintroduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an\nagent-based framework that replaces manual rule writing with dynamic, language\nmodel-driven interaction to extract and standardize structured clinical data.\nOur framework automates cohort selection, feature extraction, and code mapping\nthrough interactive querying of databases. Our modular agents iteratively\nobserve query results and reason over schema and documentation, using SQL not\njust for data retrieval but also as a tool for database observation and\ndecision making. This eliminates the need for hand-crafted, schema-specific\nlogic. To enable rigorous evaluation, we develop a benchmarking codebase for\nthree EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen\nschema settings. Our results demonstrate strong performance and generalization\nacross these databases, highlighting the feasibility of automating a process\npreviously thought to require expert-driven design. The code will be released\npublicly at https://github.com/AITRICS/EMR-AGENT/tree/main. For a\ndemonstration, please visit our anonymous demo page:\nhttps://anonymoususer-max600.github.io/EMR_AGENT/",
      "authors": [
        "Kwanhyung Lee",
        "Sungsoo Hong",
        "Joonhyung Park",
        "Jeonghyeop Lim",
        "Juhwan Choi",
        "Donghwee Yoon",
        "Eunho Yang"
      ],
      "published": "2025-10-01T06:10:04Z",
      "primary_category": "cs.DB",
      "arxiv_url": "https://arxiv.org/abs/2510.00549v2",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "medical_ai",
      "tldr_zh": "EMR-AGENT是一个基于语言模型的自动化框架，用于从电子病历数据库中提取结构化临床数据。该系统通过智能代理动态交互，自动化完成队列选择、特征提取和代码映射，无需手动编写特定数据库规则。在MIMIC-III、eICU和SICdb三个EMR数据库上的测试表明，该方法具有良好的性能和泛化能力，显著提升了临床数据提取的可扩展性和跨机构通用性。",
      "order": 286
    },
    {
      "arxiv_id": "2510.00547v1",
      "title": "Forestpest-YOLO: A High-Performance Detection Framework for Small\n  Forestry Pests",
      "summary": "Detecting agricultural pests in complex forestry environments using remote\nsensing imagery is fundamental for ecological preservation, yet it is severely\nhampered by practical challenges. Targets are often minuscule, heavily\noccluded, and visually similar to the cluttered background, causing\nconventional object detection models to falter due to the loss of fine-grained\nfeatures and an inability to handle extreme data imbalance. To overcome these\nobstacles, this paper introduces Forestpest-YOLO, a detection framework\nmeticulously optimized for the nuances of forestry remote sensing. Building\nupon the YOLOv8 architecture, our framework introduces a synergistic trio of\ninnovations. We first integrate a lossless downsampling module, SPD-Conv, to\nensure that critical high-resolution details of small targets are preserved\nthroughout the network. This is complemented by a novel cross-stage feature\nfusion block, CSPOK, which dynamically enhances multi-scale feature\nrepresentation while suppressing background noise. Finally, we employ\nVarifocalLoss to refine the training objective, compelling the model to focus\non high-quality and hard-to-classify samples. Extensive experiments on our\nchallenging, self-constructed ForestPest dataset demonstrate that\nForestpest-YOLO achieves state-of-the-art performance, showing marked\nimprovements in detecting small, occluded pests and significantly outperforming\nestablished baseline models.",
      "authors": [
        "Aoduo Li",
        "Peikai Lin",
        "Jiancheng Li",
        "Zhen Zhang",
        "Shiting Wu",
        "Zexiao Liang",
        "Zhifa Jiang"
      ],
      "published": "2025-10-01T06:06:40Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00547v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "Forestpest-YOLO是针对林业害虫检测的高性能框架，基于YOLOv8架构优化，通过SPD-Conv无损下采样、CSPOK跨阶段特征融合和VarifocalLoss损失函数，有效解决小目标、遮挡和背景干扰问题，在自建数据集上达到最先进性能。",
      "order": 287
    },
    {
      "arxiv_id": "2510.01293v1",
      "title": "Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town\n  for Self-Directed Research Evolution and Emergent Scientific Discovery",
      "summary": "The rapid advancement of artificial intelligence (AI) has demonstrated\nsubstantial potential in chemical engineering, yet existing AI systems remain\nlimited in interdisciplinary collaboration and exploration of uncharted\nproblems. To address these issues, we present the Cyber Academia-Chemical\nEngineering (CA-ChemE) system, a living digital town that enables self-directed\nresearch evolution and emergent scientific discovery through multi-agent\ncollaboration. By integrating domain-specific knowledge bases, knowledge\nenhancement technologies, and collaboration agents, the system successfully\nconstructs an intelligent ecosystem capable of deep professional reasoning and\nefficient interdisciplinary collaboration. Our findings demonstrate that\nknowledge base-enabled enhancement mechanisms improved dialogue quality scores\nby 10-15% on average across all seven expert agents, fundamentally ensuring\ntechnical judgments are grounded in verifiable scientific evidence. However, we\nobserved a critical bottleneck in cross-domain collaboration efficiency,\nprompting the introduction of a Collaboration Agent (CA) equipped with ontology\nengineering capabilities. CA's intervention achieved 8.5% improvements for\ndistant-domain expert pairs compared to only 0.8% for domain-proximate pairs -\na 10.6-fold difference - unveiling the \"diminished collaborative efficiency\ncaused by knowledge-base gaps\" effect. This study demonstrates how carefully\ndesigned multi-agent architectures can provide a viable pathway toward\nautonomous scientific discovery in chemical engineering.",
      "authors": [
        "Zekun Jiang",
        "Chunming Xu",
        "Tianhang Zhou"
      ],
      "published": "2025-10-01T05:26:55Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01293v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出CA-ChemE系统，一个支持自主研究演进和涌现科学发现的数字化学术小镇。通过整合领域知识库与多智能体协作，该系统在化学工程领域实现了深度专业推理和跨学科合作。研究发现知识库增强机制将对话质量提升10-15%，但跨域协作存在效率瓶颈，协作代理的介入使远域专家组合效提升8.5%，揭示了知识库差距导致的协作效率递减效应。",
      "order": 288
    },
    {
      "arxiv_id": "2510.00523v1",
      "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
      "summary": "Multimodal representation learning models have demonstrated successful\noperation across complex tasks, and the integration of vision-language models\n(VLMs) has further enabled embedding models with instruction-following\ncapabilities. However, existing embedding models lack visual-interactive\ncapabilities to specify regions of interest from users (e.g., point, bounding\nbox, mask), which have been explored in generative models to broaden their\nhuman-interactive applicability. Equipping embedding models with visual\ninteractions not only would unlock new applications with localized grounding of\nuser intent, which remains unexplored, but also enable the models to learn\nentity-level information within images to complement their global\nrepresentations for conventional embedding tasks. In this paper, we propose a\nnovel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends\nthe capabilities of the segmentation model and the vision-language model to the\nrealm of representation learning. In VIRTUE, the segmentation model can process\nvisual prompts that pinpoint specific regions within an image, thereby enabling\nthe embedder to handle complex and ambiguous scenarios more precisely. To\nevaluate the visual-interaction ability of VIRTUE, we introduce a large-scale\nSegmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples\nthat aims to retrieve the text caption by jointly considering the entity with a\nspecific object and image scene. VIRTUE consistently achieves a\nstate-of-the-art performance with significant improvements across 36 universal\nMMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
      "authors": [
        "Wei-Yao Wang",
        "Kazuya Tateishi",
        "Qiyu Wu",
        "Shusuke Takahashi",
        "Yuki Mitsufuji"
      ],
      "published": "2025-10-01T05:11:54Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00523v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "VIRTUE提出了一种视觉交互式文本-图像通用嵌入模型，通过整合分割模型和视觉语言模型，支持用户通过点选、边界框等交互方式指定图像关注区域，在36个多模态任务和5个视觉交互检索任务上实现显著性能提升。",
      "order": 289
    },
    {
      "arxiv_id": "2510.00519v1",
      "title": "Architectural Transformations and Emerging Verification Demands in\n  AI-Enabled Cyber-Physical Systems",
      "summary": "In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion\noccurs where digital technology meets the physical world. This synergy has been\nsignificantly transformed by the integration of artificial intelligence (AI), a\nmove that dramatically enhances system adaptability and introduces a layer of\ncomplexity that impacts CPS control optimization and reliability. Despite\nadvancements in AI integration, a significant gap remains in understanding how\nthis shift affects CPS architecture, operational complexity, and verification\npractices. The extended abstract addresses this gap by investigating\narchitectural distinctions between AI-driven and traditional control models\ndesigned in Simulink and their respective implications for system verification.",
      "authors": [
        "Hadiza Umar Yusuf",
        "Khouloud Gaaloul"
      ],
      "published": "2025-10-01T05:09:12Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.00519v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文探讨了人工智能在信息物理系统(CPS)中的集成对系统架构和验证实践的影响，比较了AI驱动与传统Simulink控制模型的架构差异及其对系统验证的启示。",
      "order": 290
    },
    {
      "arxiv_id": "2510.00512v1",
      "title": "Adaptive Data-Knowledge Alignment in Genetic Perturbation Prediction",
      "summary": "The transcriptional response to genetic perturbation reveals fundamental\ninsights into complex cellular systems. While current approaches have made\nprogress in predicting genetic perturbation responses, they provide limited\nbiological understanding and cannot systematically refine existing knowledge.\nOvercoming these limitations requires an end-to-end integration of data-driven\nlearning and existing knowledge. However, this integration is challenging due\nto inconsistencies between data and knowledge bases, such as noise,\nmisannotation, and incompleteness. To address this challenge, we propose\nALIGNED (Adaptive aLignment for Inconsistent Genetic kNowledgE and Data), a\nneuro-symbolic framework based on the Abductive Learning (ABL) paradigm. This\nend-to-end framework aligns neural and symbolic components and performs\nsystematic knowledge refinement. We introduce a balanced consistency metric to\nevaluate the predictions' consistency against both data and knowledge. Our\nresults show that ALIGNED outperforms state-of-the-art methods by achieving the\nhighest balanced consistency, while also re-discovering biologically meaningful\nknowledge. Our work advances beyond existing methods to enable both the\ntransparency and the evolution of mechanistic biological understanding.",
      "authors": [
        "Yuanfang Xiang",
        "Lun Ai"
      ],
      "published": "2025-10-01T04:48:43Z",
      "primary_category": "q-bio.MN",
      "arxiv_url": "https://arxiv.org/abs/2510.00512v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出ALIGNED框架，一种基于溯因学习的神经符号方法，用于解决遗传扰动预测中数据与知识库不一致的问题。该框架通过平衡一致性度量整合数据驱动学习与现有知识，在提升预测性能的同时实现知识系统精炼，超越现有方法并重新发现具有生物学意义的知识。",
      "order": 291
    },
    {
      "arxiv_id": "2510.00508v1",
      "title": "Copy-Paste to Mitigate Large Language Model Hallucinations",
      "summary": "While Retrieval-Augmented Generation (RAG) enables large language models\n(LLMs) to generate contextually grounded responses, contextual faithfulness\nremains challenging as LLMs may not consistently trust provided context,\nleading to hallucinations that undermine reliability. We observe an inverse\ncorrelation between response copying degree and context-unfaithful\nhallucinations on RAGTruth, suggesting that higher copying degrees reduce\nhallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,\nobtained through two-stage high-copying response preference training. We design\nthree prompting methods to enhance copying degree, demonstrating that\nhigh-copying responses achieve superior contextual faithfulness and\nhallucination control. These approaches enable a fully automated pipeline that\ntransforms generated responses into high-copying preference data for training\nCopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best\nperformance in both counterfactual and original contexts, remarkably with 12.2%\nto 24.5% accuracy improvements on FaithEval over the best baseline, while\nrequiring only 365 training samples -- 1/50th of baseline data. To elucidate\nCopyPasteLLM's effectiveness, we propose the Context-Parameter Copying\nCapturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates\nreliance on internal parametric knowledge rather than external knowledge during\ngeneration. All codes are available at\nhttps://github.com/longyongchao/CopyPasteLLM",
      "authors": [
        "Yongchao Long",
        "Xian Wu",
        "Yingying Zhang",
        "Xianbin Wen",
        "Yuxi Zhou",
        "Shenda Hong"
      ],
      "published": "2025-10-01T04:40:04Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00508v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出CopyPasteLLM方法，通过两阶段高复制响应偏好训练解决LLM在RAG中的幻觉问题。研究发现响应复制程度与幻觉呈负相关，设计了三种提示方法增强复制行为，仅需365个训练样本即可在多个基准测试中显著提升准确性12.2%-24.5%，并通过算法揭示模型重新校准了对内部参数知识的依赖。",
      "order": 292
    },
    {
      "arxiv_id": "2510.00507v1",
      "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via\n  Knowledge Graphs",
      "summary": "As multimodal LLM-driven agents continue to advance in autonomy and\ngeneralization, evaluation based on static datasets can no longer adequately\nassess their true capabilities in dynamic environments and diverse tasks.\nExisting LLM-based synthetic data methods are largely designed for LLM training\nand evaluation, and thus cannot be directly applied to agent tasks that require\ntool use and interactive capabilities. While recent studies have explored\nautomatic agent task generation with LLMs, most efforts remain limited to text\nor image analysis, without systematically modeling multi-step interactions in\nweb environments. To address these challenges, we propose Graph2Eval, a\nknowledge graph-based framework that automatically generates both multimodal\ndocument comprehension tasks and web interaction tasks, enabling comprehensive\nevaluation of agents' reasoning, collaboration, and interactive capabilities.\nIn our approach, knowledge graphs constructed from multi-source external data\nserve as the task space, where we translate semantic relations into structured\nmultimodal tasks using subgraph sampling, task templates, and meta-paths. A\nmulti-stage filtering pipeline based on node reachability, LLM scoring, and\nsimilarity analysis is applied to guarantee the quality and executability of\nthe generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of\nmultiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures\nreasoning, collaboration, and interaction capabilities. We instantiate the\nframework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning\ndocument comprehension and web interaction scenarios. Experiments show that\nGraph2Eval efficiently generates tasks that differentiate agent and model\nperformance, revealing gaps in reasoning, collaboration, and web interaction\nacross different settings and offering a new perspective for agent evaluation.",
      "authors": [
        "Yurun Chen",
        "Xavier Hu",
        "Yuhan Liu",
        "Ziqi Wang",
        "Zeyi Liao",
        "Lin Chen",
        "Feng Wei",
        "Yuxi Qian",
        "Bo Zheng",
        "Keting Yin",
        "Shengyu Zhang"
      ],
      "published": "2025-10-01T04:37:54Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00507v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "Graph2Eval是一个基于知识图谱的框架，能够自动生成多模态文档理解任务和网页交互任务，用于全面评估智能体的推理、协作和交互能力。该方法通过子图采样、任务模板和元路径将语义关系转化为结构化任务，并采用多阶段过滤确保任务质量，支持单智能体、多智能体和网页智能体的端到端评估。",
      "order": 293
    },
    {
      "arxiv_id": "2510.00500v1",
      "title": "Relative-Absolute Fusion: Rethinking Feature Extraction in Image-Based\n  Iterative Method Selection for Solving Sparse Linear Systems",
      "summary": "Iterative method selection is crucial for solving sparse linear systems\nbecause these methods inherently lack robustness. Though image-based selection\napproaches have shown promise, their feature extraction techniques might encode\ndistinct matrices into identical image representations, leading to the same\nselection and suboptimal method. In this paper, we introduce RAF\n(Relative-Absolute Fusion), an efficient feature extraction technique to\nenhance image-based selection approaches. By simultaneously extracting and\nfusing image representations as relative features with corresponding numerical\nvalues as absolute features, RAF achieves comprehensive matrix representations\nthat prevent feature ambiguity across distinct matrices, thus improving\nselection accuracy and unlocking the potential of image-based selection\napproaches. We conducted comprehensive evaluations of RAF on SuiteSparse and\nour developed BMCMat (Balanced Multi-Classification Matrix dataset),\ndemonstrating solution time reductions of 0.08s-0.29s for sparse linear\nsystems, which is 5.86%-11.50% faster than conventional image-based selection\napproaches and achieves state-of-the-art (SOTA) performance. BMCMat is\navailable at https://github.com/zkqq/BMCMat.",
      "authors": [
        "Kaiqi Zhang",
        "Mingguan Yang",
        "Dali Chang",
        "Chun Chen",
        "Yuxiang Zhang",
        "Kexun He",
        "Jing Zhao"
      ],
      "published": "2025-10-01T04:33:23Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00500v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出RAF（相对-绝对融合）特征提取技术，通过融合图像相对特征与数值绝对特征，解决稀疏线性系统求解中图像化方法选择时不同矩阵产生相同特征表示的问题。在SuiteSparse和BMCMat数据集上验证，相比传统图像方法提速5.86%-11.50%，达到SOTA性能。",
      "order": 294
    },
    {
      "arxiv_id": "2510.00499v2",
      "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
      "summary": "Spoken dialogue systems often rely on cascaded pipelines that transcribe,\nprocess, and resynthesize speech. While effective, this design discards\nparalinguistic cues and limits expressivity. Recent end-to-end methods reduce\nlatency and better preserve these cues, yet still rely on text intermediates,\ncreating a fundamental bottleneck. We present MOSS-Speech, a true\nspeech-to-speech large language model that directly understands and generates\nspeech without relying on text guidance. Our approach combines a modality-based\nlayer-splitting architecture with a frozen pre-training strategy, preserving\nthe reasoning and knowledge of pretrained text LLMs while adding native speech\ncapabilities. Experiments show that our model achieves state-of-the-art results\nin spoken question answering and delivers comparable speech-to-speech\nperformance relative to existing text-guided systems, while still maintaining\ncompetitive text performance. By narrowing the gap between text-guided and\ndirect speech generation, our work establishes a new paradigm for expressive\nand efficient end-to-end speech interaction.",
      "authors": [
        "Xingjian Zhao",
        "Zhe Xu",
        "Qinyuan Cheng",
        "Zhaoye Fei",
        "Luozhijie Jin",
        "Yang Wang",
        "Hanfu Chen",
        "Yaozhou Jiang",
        "Qinghui Gao",
        "Ke Chen",
        "Ruixiao Li",
        "Mingshu Chen",
        "Ruiming Wang",
        "Wenbo Zhang",
        "Yiyang Zhang",
        "Donghua Yu",
        "Yang Gao",
        "Xiaogui Yang",
        "Yitian Gong",
        "Yuanfan Xu",
        "Yaqian Zhou",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "published": "2025-10-01T04:32:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00499v2",
      "primary_area": "audio_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "MOSS-Speech提出了一种无需文本指导的纯语音到语音大语言模型，通过模态分层架构与冻结预训练策略，在保留文本LLM推理能力的同时实现原生语音理解与生成，在口语问答任务中达到SOTA水平，为端到端语音交互建立了新范式。",
      "order": 295
    },
    {
      "arxiv_id": "2510.00495v2",
      "title": "Normal-Abnormal Guided Generalist Anomaly Detection",
      "summary": "Generalist Anomaly Detection (GAD) aims to train a unified model on an\noriginal domain that can detect anomalies in new target domains. Previous GAD\nmethods primarily use only normal samples as references, overlooking the\nvaluable information contained in anomalous samples that are often available in\nreal-world scenarios. To address this limitation, we propose a more practical\napproach: normal-abnormal-guided generalist anomaly detection, which leverages\nboth normal and anomalous samples as references to guide anomaly detection\nacross diverse domains. We introduce the Normal-Abnormal Generalist Learning\n(NAGL) framework, consisting of two key components: Residual Mining (RM) and\nAnomaly Feature Learning (AFL). RM extracts abnormal patterns from\nnormal-abnormal reference residuals to establish transferable anomaly\nrepresentations, while AFL adaptively learns anomaly features in query images\nthrough residual mapping to identify instance-aware anomalies. Our approach\neffectively utilizes both normal and anomalous references for more accurate and\nefficient cross-domain anomaly detection. Extensive experiments across multiple\nbenchmarks demonstrate that our method significantly outperforms existing GAD\napproaches. This work represents the first to adopt a mixture of normal and\nabnormal samples as references in generalist anomaly detection. The code and\ndatasets are available at https://github.com/JasonKyng/NAGL.",
      "authors": [
        "Yuexin Wang",
        "Xiaolei Wang",
        "Yizheng Gong",
        "Jimin Xiao"
      ],
      "published": "2025-10-01T04:27:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00495v2",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种实用的通用异常检测方法NAGL，首次同时利用正常和异常样本作为参考，通过残差挖掘和异常特征学习组件，实现跨域异常检测的显著性能提升。",
      "order": 296
    },
    {
      "arxiv_id": "2510.00494v1",
      "title": "Exploring System 1 and 2 communication for latent reasoning in LLMs",
      "summary": "Should LLM reasoning live in a separate module, or within a single model's\nforward pass and representational space? We study dual-architecture latent\nreasoning, where a fluent Base exchanges latent messages with a Coprocessor,\nand test two hypotheses aimed at improving latent communication over Liu et al.\n(2024): (H1) increase channel capacity; (H2) learn communication via joint\nfinetuning. Under matched latent-token budgets on GPT-2 and Qwen-3, H2 is\nconsistently strongest while H1 yields modest gains. A unified soft-embedding\nbaseline, a single model with the same forward pass and shared representations,\nusing the same latent-token budget, nearly matches H2 and surpasses H1,\nsuggesting current dual designs mostly add compute rather than qualitatively\nimproving reasoning. Across GSM8K, ProsQA, and a Countdown stress test with\nincreasing branching factor, scaling the latent-token budget beyond small\nvalues fails to improve robustness. Latent analyses show overlapping subspaces\nwith limited specialization, consistent with weak reasoning gains. We conclude\ndual-model latent reasoning remains promising in principle, but likely requires\nobjectives and communication mechanisms that explicitly shape latent spaces for\nalgorithmic planning.",
      "authors": [
        "Julian Coda-Forno",
        "Zhuokai Zhao",
        "Qiang Zhang",
        "Dipesh Tamboli",
        "Weiwei Li",
        "Xiangjun Fan",
        "Lizhu Zhang",
        "Eric Schulz",
        "Hsiao-Ping Tseng"
      ],
      "published": "2025-10-01T04:26:09Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00494v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探索大语言模型中的双架构潜在推理机制，比较分离模块与统一模型的推理性能。实验表明，联合微调通信机制优于单纯增加信道容量，但统一软嵌入基线表现相当，提示当前双模型设计主要增加计算而非实质性提升推理能力。潜在空间分析显示有限的专业化程度，未来需开发能显式塑造算法规划潜空间的目标与通信机制。",
      "order": 297
    },
    {
      "arxiv_id": "2510.00492v2",
      "title": "Rethinking Reward Models for Multi-Domain Test-Time Scaling",
      "summary": "The reliability of large language models (LLMs) during test-time scaling is\noften assessed with \\emph{external verifiers} or \\emph{reward models} that\ndistinguish correct reasoning from flawed logic. Prior work generally assumes\nthat process reward models (PRMs), which score every intermediate reasoning\nstep, outperform outcome reward models (ORMs) that assess only the final\nanswer. This view is based mainly on evidence from narrow, math-adjacent\ndomains. We present the first unified evaluation of four reward model variants,\ndiscriminative ORM and PRM (\\DisORM, \\DisPRM) and generative ORM and PRM\n(\\GenORM, \\GenPRM), across 14 diverse domains. Contrary to conventional wisdom,\nwe find that (i) \\DisORM performs on par with \\DisPRM, (ii) \\GenPRM is not\ncompetitive, and (iii) overall, \\GenORM is the most robust, yielding\nsignificant and consistent gains across every tested domain. We attribute this\nto PRM-style stepwise scoring, which inherits label noise from LLM\nauto-labeling and has difficulty evaluating long reasoning trajectories,\nincluding those involving self-correcting reasoning. Our theoretical analysis\nshows that step-wise aggregation compounds errors as reasoning length grows,\nand our empirical observations confirm this effect. These findings challenge\nthe prevailing assumption that fine-grained supervision is always better and\nsupport generative outcome verification for multi-domain deployment. We\npublicly release our code, datasets, and checkpoints at\n\\href{https://github.com/db-Lee/Multi-RM}{\\underline{\\small\\texttt{https://github.com/db-Lee/Multi-RM}}}\nto facilitate future research in multi-domain settings.",
      "authors": [
        "Dong Bok Lee",
        "Seanie Lee",
        "Sangwoo Park",
        "Minki Kang",
        "Jinheon Baek",
        "Dongki Kim",
        "Dominik Wagner",
        "Jiongdao Jin",
        "Heejun Lee",
        "Tobias Bocklet",
        "Jinyu Wang",
        "Jingjing Fu",
        "Sung Ju Hwang",
        "Jiang Bian",
        "Lei Song"
      ],
      "published": "2025-10-01T04:21:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00492v2",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究挑战了过程奖励模型(PRM)优于结果奖励模型(ORM)的传统观点，通过对14个不同领域的评估发现：判别式ORM与PRM表现相当，生成式PRM不具竞争力，而生成式ORM在所有测试领域表现最稳健。研究揭示了逐步评分会因标签噪声和长推理轨迹而累积错误，支持在多领域部署中采用生成式结果验证方法。",
      "order": 298
    },
    {
      "arxiv_id": "2510.00491v1",
      "title": "From Human Hands to Robot Arms: Manipulation Skills Transfer via\n  Trajectory Alignment",
      "summary": "Learning diverse manipulation skills for real-world robots is severely\nbottlenecked by the reliance on costly and hard-to-scale teleoperated\ndemonstrations. While human videos offer a scalable alternative, effectively\ntransferring manipulation knowledge is fundamentally hindered by the\nsignificant morphological gap between human and robotic embodiments. To address\nthis challenge and facilitate skill transfer from human to robot, we introduce\nTraj2Action,a novel framework that bridges this embodiment gap by using the 3D\ntrajectory of the operational endpoint as a unified intermediate\nrepresentation, and then transfers the manipulation knowledge embedded in this\ntrajectory to the robot's actions. Our policy first learns to generate a coarse\ntrajectory, which forms an high-level motion plan by leveraging both human and\nrobot data. This plan then conditions the synthesis of precise, robot-specific\nactions (e.g., orientation and gripper state) within a co-denoising framework.\nExtensive real-world experiments on a Franka robot demonstrate that Traj2Action\nboosts the performance by up to 27% and 22.25% over $\\pi_0$ baseline on short-\nand long-horizon real-world tasks, and achieves significant gains as human data\nscales in robot policy learning. Our project website, featuring code and video\ndemonstrations, is available at\nhttps://anonymous.4open.science/w/Traj2Action-4A45/.",
      "authors": [
        "Han Zhou",
        "Jinjin Cao",
        "Liyuan Ma",
        "Xueji Fang",
        "Guo-jun Qi"
      ],
      "published": "2025-10-01T04:21:12Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00491v1",
      "primary_area": "video_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "提出Traj2Action框架，通过3D轨迹对齐解决人机形态差异，将人类操作技能转化为机器人动作，在真实机器人实验中性能提升最高达27%",
      "order": 299
    },
    {
      "arxiv_id": "2510.00487v1",
      "title": "Black-Box Time-Series Domain Adaptation via Cross-Prompt Foundation\n  Models",
      "summary": "The black-box domain adaptation (BBDA) topic is developed to address the\nprivacy and security issues where only an application programming interface\n(API) of the source model is available for domain adaptations. Although the\nBBDA topic has attracted growing research attentions, existing works mostly\ntarget the vision applications and are not directly applicable to the\ntime-series applications possessing unique spatio-temporal characteristics. In\naddition, none of existing approaches have explored the strength of foundation\nmodel for black box time-series domain adaptation (BBTSDA). This paper proposes\na concept of Cross-Prompt Foundation Model (CPFM) for the BBTSDA problems. CPFM\nis constructed under a dual branch network structure where each branch is\nequipped with a unique prompt to capture different characteristics of data\ndistributions. In the domain adaptation phase, the reconstruction learning\nphase in the prompt and input levels is developed. All of which are built upon\na time-series foundation model to overcome the spatio-temporal dynamic. Our\nrigorous experiments substantiate the advantage of CPFM achieving improved\nresults with noticeable margins from its competitors in three time-series\ndatasets of different application domains.",
      "authors": [
        "M. T. Furqon",
        "Mahardhika Pratama",
        "Igor Skrjanc",
        "Lin Liu",
        "Habibullah Habibullah",
        "Kutluyil Dogancay"
      ],
      "published": "2025-10-01T04:09:01Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00487v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出跨提示基础模型(CPFM)解决黑盒时间序列域适应问题，采用双分支网络结构，通过提示和输入层面的重构学习，利用时间序列基础模型克服时空动态特性，在三个不同应用领域的时间序列数据集上取得显著优势。",
      "order": 300
    },
    {
      "arxiv_id": "2510.00485v1",
      "title": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
      "summary": "Recently, an increasing number of multimodal (text and audio) benchmarks have\nemerged, primarily focusing on evaluating models' understanding capability.\nHowever, exploration into assessing generative capabilities remains limited,\nespecially for open-ended long-form content generation. Significant challenges\nlie in no reference standard answer, no unified evaluation metrics and\nuncontrollable human judgments. In this work, we take podcast-like audio\ngeneration as a starting point and propose PodEval, a comprehensive and\nwell-designed open-source evaluation framework. In this framework: 1) We\nconstruct a real-world podcast dataset spanning diverse topics, serving as a\nreference for human-level creative quality. 2) We introduce a multimodal\nevaluation strategy and decompose the complex task into three dimensions: text,\nspeech and audio, with different evaluation emphasis on \"Content\" and \"Format\".\n3) For each modality, we design corresponding evaluation methods, involving\nboth objective metrics and subjective listening test. We leverage\nrepresentative podcast generation systems (including open-source, close-source,\nand human-made) in our experiments. The results offer in-depth analysis and\ninsights into podcast generation, demonstrating the effectiveness of PodEval in\nevaluating open-ended long-form audio. This project is open-source to\nfacilitate public use: https://github.com/yujxx/PodEval.",
      "authors": [
        "Yujia Xiao",
        "Liumeng Xue",
        "Lei He",
        "Xinyi Chen",
        "Aemon Yat Fei Chiu",
        "Wenjie Tian",
        "Shaofei Zhang",
        "Qiuqiang Kong",
        "Xinfa Zhu",
        "Wei Xue",
        "Tan Lee"
      ],
      "published": "2025-10-01T04:08:08Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00485v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "PodEval是一个针对播客音频生成的多模态评估框架，通过构建真实播客数据集、分解文本/语音/音频三个维度的评估策略，结合客观指标和主观听测，解决了开放式长内容生成缺乏标准答案和统一评估指标的难题。",
      "order": 301
    },
    {
      "arxiv_id": "2510.00481v1",
      "title": "Make a Video Call with LLM: A Measurement Campaign over Five Mainstream\n  Apps",
      "summary": "In 2025, Large Language Model (LLM) services have launched a new feature --\nAI video chat -- allowing users to interact with AI agents via real-time video\ncommunication (RTC), just like chatting with real people. Despite its\nsignificance, no systematic study has characterized the performance of existing\nAI video chat systems. To address this gap, this paper proposes a comprehensive\nbenchmark with carefully designed metrics across four dimensions: quality,\nlatency, internal mechanisms, and system overhead. Using custom testbeds, we\nfurther evaluate five mainstream AI video chatbots with this benchmark. This\nwork provides the research community a baseline of real-world performance and\nidentifies unique system bottlenecks. In the meantime, our benchmarking results\nalso open up several research questions for future optimizations of AI video\nchatbots.",
      "authors": [
        "Jiayang Xu",
        "Xiangjie Huang",
        "Zijie Li",
        "Zili Meng"
      ],
      "published": "2025-10-01T04:03:51Z",
      "primary_category": "cs.NI",
      "arxiv_url": "https://arxiv.org/abs/2510.00481v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对2025年新兴的AI视频聊天功能，首次提出了涵盖质量、延迟、内部机制和系统开销四个维度的综合评测基准，并对五款主流AI视频聊天应用进行了系统性能测量，为研究社区提供了现实性能基准并识别了系统瓶颈。",
      "order": 302
    },
    {
      "arxiv_id": "2510.00480v1",
      "title": "Expandable Decision-Making States for Multi-Agent Deep Reinforcement\n  Learning in Soccer Tactical Analysis",
      "summary": "Invasion team sports such as soccer produce a high-dimensional, strongly\ncoupled state space as many players continuously interact on a shared field,\nchallenging quantitative tactical analysis. Traditional rule-based analyses are\nintuitive, while modern predictive machine learning models often perform\npattern-matching without explicit agent representations. The problem we address\nis how to build player-level agent models from data, whose learned values and\npolicies are both tactically interpretable and robust across heterogeneous data\nsources. Here, we propose Expandable Decision-Making States (EDMS), a\nsemantically enriched state representation that augments raw positions and\nvelocities with relational variables (e.g., scoring of space, pass, and score),\ncombined with an action-masking scheme that gives on-ball and off-ball agents\ndistinct decision sets. Compared to prior work, EDMS maps learned value\nfunctions and action policies to human-interpretable tactical concepts (e.g.,\nmarking pressure, passing lanes, ball accessibility) instead of raw coordinate\nfeatures, and aligns agent choices with the rules of play. In the experiments,\nEDMS with action masking consistently reduced both action-prediction loss and\ntemporal-difference (TD) error compared to the baseline. Qualitative case\nstudies and Q-value visualizations further indicate that EDMS highlights\nhigh-risk, high-reward tactical patterns (e.g., fast counterattacks and\ndefensive breakthroughs). We also integrated our approach into an open-source\nlibrary and demonstrated compatibility with multiple commercial and open\ndatasets, enabling cross-provider evaluation and reproducible experiments.",
      "authors": [
        "Kenjiro Ide",
        "Taiga Someya",
        "Kohei Kawaguchi",
        "Keisuke Fujii"
      ],
      "published": "2025-10-01T04:01:51Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00480v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出可扩展决策状态(EDMS)方法，通过语义增强的状态表示和动作掩码机制，在足球战术分析中构建可解释的多智能体深度强化学习模型。该方法将原始位置速度数据与关系变量结合，使学习到的价值函数和策略对应人类可理解的战术概念，在实验中有效降低了动作预测损失和时序差分误差，并识别高风险高回报战术模式。",
      "order": 303
    },
    {
      "arxiv_id": "2510.00476v1",
      "title": "Analyzing Latent Concepts in Code Language Models",
      "summary": "Interpreting the internal behavior of large language models trained on code\nremains a critical challenge, particularly for applications demanding trust,\ntransparency, and semantic robustness. We propose Code Concept Analysis\n(CoCoA): a global post-hoc interpretability framework that uncovers emergent\nlexical, syntactic, and semantic structures in a code language model's\nrepresentation space by clustering contextualized token embeddings into\nhuman-interpretable concept groups. We propose a hybrid annotation pipeline\nthat combines static analysis tool-based syntactic alignment with\nprompt-engineered large language models (LLMs), enabling scalable labeling of\nlatent concepts across abstraction levels. We analyse the distribution of\nconcepts across layers and across three finetuning tasks. Emergent concept\nclusters can help identify unexpected latent interactions and be used to\nidentify trends and biases within the model's learned representations. We\nfurther integrate LCA with local attribution methods to produce\nconcept-grounded explanations, improving the coherence and interpretability of\ntoken-level saliency. Empirical evaluations across multiple models and tasks\nshow that LCA discovers concepts that remain stable under semantic-preserving\nperturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve\npredictably with fine-tuning. In a user study, concept-augmented explanations\ndisambiguate token roles. In a user study on the programming-language\nclassification task, concept-augmented explanations disambiguated token roles\nand improved human-centric explainability by 37 percentage points compared with\ntoken-level attributions using Integrated Gradients.",
      "authors": [
        "Arushi Sharma",
        "Vedant Pungliya",
        "Christopher J. Quinn",
        "Ali Jannesari"
      ],
      "published": "2025-10-01T03:53:21Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.00476v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "code_generation",
      "tldr_zh": "本文提出Code Concept Analysis (CoCoA)框架，通过聚类代码语言模型中的上下文标记嵌入来揭示词汇、句法和语义结构。该混合标注方法结合静态分析与提示工程，能够识别模型表示中的潜在概念和偏差，在用户研究中将可解释性提升了37个百分点。",
      "order": 304
    },
    {
      "arxiv_id": "2510.00468v1",
      "title": "Feature Identification via the Empirical NTK",
      "summary": "We provide evidence that eigenanalysis of the empirical neural tangent kernel\n(eNTK) can surface the features used by trained neural networks. Across two\nstandard toy models for mechanistic interpretability, Toy Models of\nSuperposition (TMS) and a 1-layer MLP trained on modular addition, we find that\nthe eNTK exhibits sharp spectral cliffs whose top eigenspaces align with\nground-truth features. In TMS, the eNTK recovers the ground-truth features in\nboth the sparse (high superposition) and dense regimes. In modular arithmetic,\nthe eNTK can be used to recover Fourier feature families. Moreover, we provide\nevidence that a layerwise eNTK localizes features to specific layers and that\nthe evolution of the eNTK eigenspectrum can be used to diagnose the grokking\nphase transition. These results suggest that eNTK analysis may provide a\npractical handle for feature discovery and for detecting phase changes in small\nmodels.",
      "authors": [
        "Jennifer Lin"
      ],
      "published": "2025-10-01T03:39:48Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00468v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过经验神经正切核(eNTK)的特征分析，证明其能有效识别训练后神经网络所使用的特征。在两个标准玩具模型(TMS和模加法MLP)中，eNTK的谱悬崖顶部特征空间与真实特征对齐，可恢复稀疏/稠密状态下的真实特征和傅里叶特征族。层间eNTK能将特征定位到特定层，其谱演化可诊断grokking相变，为小模型的特征发现和相变检测提供了实用工具。",
      "order": 305
    },
    {
      "arxiv_id": "2510.00466v1",
      "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A\n  Reinforcement Learning Approach for Robot Social Navigation",
      "summary": "Offline reinforcement learning (RL) has emerged as a promising framework for\naddressing robot social navigation challenges. However, inherent uncertainties\nin pedestrian behavior and limited environmental interaction during training\noften lead to suboptimal exploration and distributional shifts between offline\ntraining and online deployment. To overcome these limitations, this paper\nproposes a novel offline-to-online fine-tuning RL algorithm for robot social\nnavigation by integrating Return-to-Go (RTG) prediction into a causal\nTransformer architecture. Our algorithm features a spatiotem-poral fusion model\ndesigned to precisely estimate RTG values in real-time by jointly encoding\ntemporal pedestrian motion patterns and spatial crowd dynamics. This RTG\nprediction framework mitigates distribution shift by aligning offline policy\ntraining with online environmental interactions. Furthermore, a hybrid\noffline-online experience sampling mechanism is built to stabilize policy\nupdates during fine-tuning, ensuring balanced integration of pre-trained\nknowledge and real-time adaptation. Extensive experiments in simulated social\nnavigation environments demonstrate that our method achieves a higher success\nrate and lower collision rate compared to state-of-the-art baselines. These\nresults underscore the efficacy of our algorithm in enhancing navigation policy\nrobustness and adaptability. This work paves the way for more reliable and\nadaptive robotic navigation systems in real-world applications.",
      "authors": [
        "Run Su",
        "Hao Fu",
        "Shuai Zhou",
        "Yingao Fu"
      ],
      "published": "2025-10-01T03:37:02Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00466v1",
      "primary_area": "vla_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种结合离线预训练与在线微调的强化学习算法，用于机器人社交导航。通过将回报预测集成到因果Transformer架构中，构建时空融合模型实时估计回报值，缓解离线训练与在线部署的分布偏移问题。实验表明该方法在模拟环境中相比基线具有更高的成功率和更低的碰撞率。",
      "order": 306
    },
    {
      "arxiv_id": "2510.00461v1",
      "title": "TimeEmb: A Lightweight Static-Dynamic Disentanglement Framework for Time\n  Series Forecasting",
      "summary": "Temporal non-stationarity, the phenomenon that time series distributions\nchange over time, poses fundamental challenges to reliable time series\nforecasting. Intuitively, the complex time series can be decomposed into two\nfactors, \\ie time-invariant and time-varying components, which indicate static\nand dynamic patterns, respectively. Nonetheless, existing methods often\nconflate the time-varying and time-invariant components, and jointly learn the\ncombined long-term patterns and short-term fluctuations, leading to suboptimal\nperformance facing distribution shifts. To address this issue, we initiatively\npropose a lightweight static-dynamic decomposition framework, TimeEmb, for time\nseries forecasting. TimeEmb innovatively separates time series into two\ncomplementary components: (1) time-invariant component, captured by a novel\nglobal embedding module that learns persistent representations across time\nseries, and (2) time-varying component, processed by an efficient\nfrequency-domain filtering mechanism inspired by full-spectrum analysis in\nsignal processing. Experiments on real-world datasets demonstrate that TimeEmb\noutperforms state-of-the-art baselines and requires fewer computational\nresources. We conduct comprehensive quantitative and qualitative analyses to\nverify the efficacy of static-dynamic disentanglement. This lightweight\nframework can also improve existing time-series forecasting methods with simple\nintegration. To ease reproducibility, the code is available at\nhttps://github.com/showmeon/TimeEmb.",
      "authors": [
        "Mingyuan Xia",
        "Chunxu Zhang",
        "Zijian Zhang",
        "Hao Miao",
        "Qidong Liu",
        "Yuanshao Zhu",
        "Bo Yang"
      ],
      "published": "2025-10-01T03:28:49Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00461v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "TimeEmb提出了一种轻量级静态-动态解耦框架，用于时间序列预测。该框架创新地将时间序列分解为时间不变分量（通过全局嵌入模块学习）和时间变化分量（通过频域滤波机制处理），有效解决了时间非平稳性问题。实验表明该方法在减少计算资源的同时优于现有先进基线。",
      "order": 307
    },
    {
      "arxiv_id": "2510.00457v1",
      "title": "UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous\n  Graphs for Urban Microclimate Prediction",
      "summary": "With rapid urbanization, predicting urban microclimates has become critical,\nas it affects building energy demand and public health risks. However, existing\ngenerative and homogeneous graph approaches fall short in capturing physical\nconsistency, spatial dependencies, and temporal variability. To address this,\nwe introduce UrbanGraph, a physics-informed framework integrating heterogeneous\nand dynamic spatio-temporal graphs. It encodes key physical processes --\nvegetation evapotranspiration, shading, and convective diffusion -- while\nmodeling complex spatial dependencies among diverse urban entities and their\ntemporal evolution. We evaluate UrbanGraph on UMC4/12, a physics-based\nsimulation dataset covering diverse urban configurations and climates. Results\nshow that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0%\nover all baselines, with heterogeneous and dynamic graphs contributing 3.5% and\n7.1% gains. Our dataset provides the first high-resolution benchmark for\nspatio-temporal microclimate modeling, and our method extends to broader urban\nheterogeneous dynamic computing tasks.",
      "authors": [
        "Weilin Xin",
        "Chenyu Huang",
        "Peilin Li",
        "Jing Zhong",
        "Jiawei Yao"
      ],
      "published": "2025-10-01T03:14:05Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00457v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "UrbanGraph是一种物理信息驱动的时空动态异构图框架，用于城市微气候预测。该模型整合植被蒸腾、遮荫和对流扩散等物理过程，在UMC4/12数据集上相比基线方法R²提升10.8%，计算量降低17%，为城市异质动态计算任务提供了新解决方案。",
      "order": 308
    },
    {
      "arxiv_id": "2510.00454v1",
      "title": "Measuring and Controlling the Spectral Bias for Self-Supervised Image\n  Denoising",
      "summary": "Current self-supervised denoising methods for paired noisy images typically\ninvolve mapping one noisy image through the network to the other noisy image.\nHowever, after measuring the spectral bias of such methods using our proposed\nImage Pair Frequency-Band Similarity, it suffers from two practical\nlimitations. Firstly, the high-frequency structural details in images are not\npreserved well enough. Secondly, during the process of fitting high\nfrequencies, the network learns high-frequency noise from the mapped noisy\nimages. To address these challenges, we introduce a Spectral Controlling\nnetwork (SCNet) to optimize self-supervised denoising of paired noisy images.\nFirst, we propose a selection strategy to choose frequency band components for\nnoisy images, to accelerate the convergence speed of training. Next, we present\na parameter optimization method that restricts the learning ability of\nconvolutional kernels to high-frequency noise using the Lipschitz constant,\nwithout changing the network structure. Finally, we introduce the Spectral\nSeparation and low-rank Reconstruction module (SSR module), which separates\nnoise and high-frequency details through frequency domain separation and\nlow-rank space reconstruction, to retain the high-frequency structural details\nof images. Experiments performed on synthetic and real-world datasets verify\nthe effectiveness of SCNet.",
      "authors": [
        "Wang Zhang",
        "Huaqiu Li",
        "Xiaowan Hu",
        "Tao Jiang",
        "Zikang Chen",
        "Haoqian Wang"
      ],
      "published": "2025-10-01T03:07:05Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00454v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出SCNet方法解决自监督图像去噪中的频谱偏差问题。通过频率带选择策略加速训练收敛，利用Lipschitz常数限制卷积核学习高频噪声的能力，并设计频谱分离与低秩重建模块保留图像高频细节。在合成和真实数据集上验证了有效性。",
      "order": 309
    },
    {
      "arxiv_id": "2510.00452v1",
      "title": "Cloud Investigation Automation Framework (CIAF): An AI-Driven Approach\n  to Cloud Forensics",
      "summary": "Large Language Models (LLMs) have gained prominence in domains including\ncloud security and forensics. Yet cloud forensic investigations still rely on\nmanual analysis, making them time-consuming and error-prone. LLMs can mimic\nhuman reasoning, offering a pathway to automating cloud log analysis. To\naddress this, we introduce the Cloud Investigation Automation Framework (CIAF),\nan ontology-driven framework that systematically investigates cloud forensic\nlogs while improving efficiency and accuracy. CIAF standardizes user inputs\nthrough semantic validation, eliminating ambiguity and ensuring consistency in\nlog interpretation. This not only enhances data quality but also provides\ninvestigators with reliable, standardized information for decision-making. To\nevaluate security and performance, we analyzed Microsoft Azure logs containing\nransomware-related events. By simulating attacks and assessing CIAF's impact,\nresults showed significant improvement in ransomware detection, achieving\nprecision, recall, and F1 scores of 93 percent. CIAF's modular, adaptable\ndesign extends beyond ransomware, making it a robust solution for diverse\ncyberattacks. By laying the foundation for standardized forensic methodologies\nand informing future AI-driven automation, this work underscores the role of\ndeterministic prompt engineering and ontology-based validation in enhancing\ncloud forensic investigations. These advancements improve cloud security while\npaving the way for efficient, automated forensic workflows.",
      "authors": [
        "Dalal Alharthi",
        "Ivan Roberto Kawaminami Garcia"
      ],
      "published": "2025-10-01T03:05:47Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.00452v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出云调查自动化框架(CIAF)，采用基于本体论和大型语言模型的方法自动化云取证分析。该框架通过语义验证标准化输入，消除歧义，在微软Azure勒索软件攻击测试中实现93%的精确率、召回率和F1值，为标准化云取证方法奠定基础。",
      "order": 310
    },
    {
      "arxiv_id": "2510.00451v1",
      "title": "A Call to Action for a Secure-by-Design Generative AI Paradigm",
      "summary": "Large language models have gained widespread prominence, yet their\nvulnerability to prompt injection and other adversarial attacks remains a\ncritical concern. This paper argues for a security-by-design AI paradigm that\nproactively mitigates LLM vulnerabilities while enhancing performance. To\nachieve this, we introduce PromptShield, an ontology-driven framework that\nensures deterministic and secure prompt interactions. It standardizes user\ninputs through semantic validation, eliminating ambiguity and mitigating\nadversarial manipulation. To assess PromptShield's security and performance\ncapabilities, we conducted an experiment on an agent-based system to analyze\ncloud logs within Amazon Web Services (AWS), containing 493 distinct events\nrelated to malicious activities and anomalies. By simulating prompt injection\nattacks and assessing the impact of deploying PromptShield, our results\ndemonstrate a significant improvement in model security and performance,\nachieving precision, recall, and F1 scores of approximately 94%. Notably, the\nontology-based framework not only mitigates adversarial threats but also\nenhances the overall performance and reliability of the system. Furthermore,\nPromptShield's modular and adaptable design ensures its applicability beyond\ncloud security, making it a robust solution for safeguarding generative AI\napplications across various domains. By laying the groundwork for AI safety\nstandards and informing future policy development, this work stimulates a\ncrucial dialogue on the pivotal role of deterministic prompt engineering and\nontology-based validation in ensuring the safe and responsible deployment of\nLLMs in high-stakes environments.",
      "authors": [
        "Dalal Alharthi",
        "Ivan Roberto Kawaminami Garcia"
      ],
      "published": "2025-10-01T03:05:07Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.00451v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出PromptShield框架，通过本体驱动的语义验证实现安全优先的生成式AI范式。该框架能标准化用户输入，消除歧义并防御对抗攻击，在AWS云日志测试中达到约94%的精确率/召回率，同时提升系统性能与可靠性，为高风险环境下的LLM安全部署奠定基础。",
      "order": 311
    },
    {
      "arxiv_id": "2510.00436v1",
      "title": "Automated Evaluation can Distinguish the Good and Bad AI Responses to\n  Patient Questions about Hospitalization",
      "summary": "Automated approaches to answer patient-posed health questions are rising, but\nselecting among systems requires reliable evaluation. The current gold standard\nfor evaluating the free-text artificial intelligence (AI) responses--human\nexpert review--is labor-intensive and slow, limiting scalability. Automated\nmetrics are promising yet variably aligned with human judgments and often\ncontext-dependent. To address the feasibility of automating the evaluation of\nAI responses to hospitalization-related questions posed by patients, we\nconducted a large systematic study of evaluation approaches. Across 100 patient\ncases, we collected responses from 28 AI systems (2800 total) and assessed them\nalong three dimensions: whether a system response (1) answers the question, (2)\nappropriately uses clinical note evidence, and (3) uses general medical\nknowledge. Using clinician-authored reference answers to anchor metrics,\nautomated rankings closely matched expert ratings. Our findings suggest that\ncarefully designed automated evaluation can scale comparative assessment of AI\nsystems and support patient-clinician communication.",
      "authors": [
        "Sarvesh Soni",
        "Dina Demner-Fushman"
      ],
      "published": "2025-10-01T02:39:37Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00436v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究系统评估了28个AI系统对100个患者住院相关问题的回答质量，通过三个维度（回答问题、使用临床证据、运用医学知识）验证自动化评估方法的可行性。研究发现，基于临床专家参考答案的自动化排名与专家评分高度一致，表明精心设计的自动化评估可扩展AI系统比较并支持医患沟通。",
      "order": 312
    },
    {
      "arxiv_id": "2510.00430v1",
      "title": "Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model\n  Alignment",
      "summary": "Despite the recent progress, reinforcement learning (RL)-based fine-tuning of\ndiffusion models often struggles with generalization, composability, and\nrobustness against reward hacking. Recent studies have explored prompt\nrefinement as a modular alternative, but most adopt a feed-forward approach\nthat applies a single refined prompt throughout the entire sampling trajectory,\nthereby failing to fully leverage the sequential nature of reinforcement\nlearning. To address this, here we introduce PromptLoop, a plug-and-play RL\nframework that incorporates latent feedback into step-wise prompt refinement.\nRather than modifying diffusion model weights, a multimodal large language\nmodel (MLLM) is trained with RL to iteratively update prompts based on\nintermediate latent states of diffusion models. This design achieves a\nstructural analogy to the Diffusion RL approach, while retaining the\nflexibility and generality of prompt-based alignment. Extensive experiments\nacross diverse reward functions and diffusion backbones demonstrate that\nPromptLoop (i) achieves effective reward optimization, (ii) generalizes\nseamlessly to unseen models, (iii) composes orthogonally with existing\nalignment methods, and (iv) mitigates over-optimization and reward hacking.",
      "authors": [
        "Suhyeon Lee",
        "Jong Chul Ye"
      ],
      "published": "2025-10-01T02:18:58Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00430v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出PromptLoop框架，通过潜在反馈实现逐步提示优化，解决扩散模型对齐中的泛化性和鲁棒性问题。该方法利用多模态大语言模型基于扩散中间状态迭代更新提示，无需修改模型权重，在多种奖励函数和扩散骨干网络上均表现出色。",
      "order": 313
    },
    {
      "arxiv_id": "2510.00428v1",
      "title": "Automated Structured Radiology Report Generation with Rich Clinical\n  Context",
      "summary": "Automated structured radiology report generation (SRRG) from chest X-ray\nimages offers significant potential to reduce workload of radiologists by\ngenerating reports in structured formats that ensure clarity, consistency, and\nadherence to clinical reporting standards. While radiologists effectively\nutilize available clinical contexts in their diagnostic reasoning, existing\nSRRG systems overlook these essential elements. This fundamental gap leads to\ncritical problems including temporal hallucinations when referencing\nnon-existent clinical contexts. To address these limitations, we propose\ncontextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical\ncontext for SRRG. We curate C-SRRG dataset by integrating comprehensive\nclinical context encompassing 1) multi-view X-ray images, 2) clinical\nindication, 3) imaging techniques, and 4) prior studies with corresponding\ncomparisons based on patient histories. Through extensive benchmarking with\nstate-of-the-art multimodal large language models, we demonstrate that\nincorporating clinical context with the proposed C-SRRG significantly improves\nreport generation quality. We publicly release dataset, code, and checkpoints\nto facilitate future research for clinically-aligned automated RRG at\nhttps://github.com/vuno/contextualized-srrg.",
      "authors": [
        "Seongjae Kang",
        "Dong Bok Lee",
        "Juho Jung",
        "Dongseop Kim",
        "Won Hwa Kim",
        "Sunghoon Joo"
      ],
      "published": "2025-10-01T02:14:23Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00428v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出C-SRRG方法，通过整合多视角X光图像、临床指征、成像技术和既往研究等丰富临床背景，解决现有结构化放射报告生成系统忽略临床上下文导致的时序幻觉问题。在基准测试中显著提升报告质量，并公开数据集和代码推动临床对齐的自动化报告生成研究。",
      "order": 314
    },
    {
      "arxiv_id": "2510.00416v1",
      "title": "Domain-Specialized Interactive Segmentation Framework for Meningioma\n  Radiotherapy Planning",
      "summary": "Precise delineation of meningiomas is crucial for effective radiotherapy (RT)\nplanning, directly influencing treatment efficacy and preservation of adjacent\nhealthy tissues. While automated deep learning approaches have demonstrated\nconsiderable potential, achieving consistently accurate clinical segmentation\nremains challenging due to tumor heterogeneity. Interactive Medical Image\nSegmentation (IMIS) addresses this challenge by integrating advanced AI\ntechniques with clinical input. However, generic segmentation tools, despite\nwidespread applicability, often lack the specificity required for clinically\ncritical and disease-specific tasks like meningioma RT planning. To overcome\nthese limitations, we introduce Interactive-MEN-RT, a dedicated IMIS tool\nspecifically developed for clinician-assisted 3D meningioma segmentation in RT\nworkflows. The system incorporates multiple clinically relevant interaction\nmethods, including point annotations, bounding boxes, lasso tools, and\nscribbles, enhancing usability and clinical precision. In our evaluation\ninvolving 500 contrast-enhanced T1-weighted MRI scans from the BraTS 2025\nMeningioma RT Segmentation Challenge, Interactive-MEN-RT demonstrated\nsubstantial improvement compared to other segmentation methods, achieving Dice\nsimilarity coefficients of up to 77.6\\% and Intersection over Union scores of\n64.8\\%. These results emphasize the need for clinically tailored segmentation\nsolutions in critical applications such as meningioma RT planning. The code is\npublicly available at: https://github.com/snuh-rad-aicon/Interactive-MEN-RT",
      "authors": [
        "Junhyeok Lee",
        "Han Jang",
        "Kyu Sung Choi"
      ],
      "published": "2025-10-01T01:57:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00416v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出Interactive-MEN-RT，一种专用于脑膜瘤放疗规划的交互式医学图像分割框架。该系统整合多种临床交互方法（点标注、边界框、套索工具等），在BraTS 2025数据集上达到77.6% Dice系数和64.8% IoU，显著优于通用分割工具，强调临床定制化解决方案在关键医疗应用中的必要性。",
      "order": 315
    },
    {
      "arxiv_id": "2510.00415v1",
      "title": "Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via\n  Test-Time Exploration under Validate-by-Reproduce Paradigm",
      "summary": "Recent advances in large language models (LLMs) and agent system designs have\nempowered agents with unprecedented levels of capability. However, existing\nagent benchmarks are showing a trend of rapid ceiling-hitting by newly\ndeveloped agents, making it difficult to meet the demands for evaluating agent\nabilities. To address this problem, we propose the Trajectory-based\nValidated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE)\nframework. This framework takes an original task from an existing benchmark and\nencourages agents to freely explore and evolve it into a new task with higher\ndifficulty while recording validatable agent trajectories. The framework\nproceeds in three stages: (1) evolutionary proposal mining, which provides task\nevolution proposals through preliminary exploration and divergent thinking; (2)\nproblem formation and free exploration, where proposals are conceptualized into\nfeasible problem candidates and the agents then explore them freely while\nrecording their execution trajectories; and (3) multi-level validation, which\nensures that the evolved tasks are accompanied by validatable and reproducible\ntrajectories. Experiments on the GAIA benchmark demonstrate that the TRACE\nframework consistently enhances task complexity while improving the reliability\nof correctness through validatable execution trajectories. This work marks a\nparadigm shift from static, manually curated benchmarks to dynamic,\nself-evolving evaluation systems, providing a sustainable and challenging\nrunway for agent development.",
      "authors": [
        "Dadi Guo",
        "Tianyi Zhou",
        "Dongrui Liu",
        "Chen Qian",
        "Qihan Ren",
        "Shuai Shao",
        "Zhiyuan Fan",
        "Yi R. Fung",
        "Kun Wang",
        "Linfeng Zhang",
        "Jing Shao"
      ],
      "published": "2025-10-01T01:52:52Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00415v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出TRACE框架，通过验证-重现范式实现基准测试的自进化：从现有任务出发，让智能体自由探索生成更高难度的新任务，并记录可验证的执行轨迹。该框架包含进化提议挖掘、问题形成与自由探索、多级验证三个阶段，在GAIA基准上验证了其能持续提升任务复杂度并增强评估可靠性，标志着从静态基准向动态自进化评估系统的范式转变。",
      "order": 316
    },
    {
      "arxiv_id": "2510.00411v2",
      "title": "Does Bigger Mean Better? Comparitive Analysis of CNNs and Biomedical\n  Vision Language Modles in Medical Diagnosis",
      "summary": "The accurate interpretation of chest radiographs using automated methods is a\ncritical task in medical imaging. This paper presents a comparative analysis\nbetween a supervised lightweight Convolutional Neural Network (CNN) and a\nstate-of-the-art, zero-shot medical Vision-Language Model (VLM), BiomedCLIP,\nacross two distinct diagnostic tasks: pneumonia detection on the PneumoniaMNIST\nbenchmark and tuberculosis detection on the Shenzhen TB dataset. Our\nexperiments show that supervised CNNs serve as highly competitive baselines in\nboth cases. While the default zero-shot performance of the VLM is lower, we\ndemonstrate that its potential can be unlocked via a simple yet crucial remedy:\ndecision threshold calibration. By optimizing the classification threshold on a\nvalidation set, the performance of BiomedCLIP is significantly boosted across\nboth datasets. For pneumonia detection, calibration enables the zero-shot VLM\nto achieve a superior F1-score of 0.8841, surpassing the supervised CNN's\n0.8803. For tuberculosis detection, calibration dramatically improves the\nF1-score from 0.4812 to 0.7684, bringing it close to the supervised baseline's\n0.7834. This work highlights a key insight: proper calibration is essential for\nleveraging the full diagnostic power of zero-shot VLMs, enabling them to match\nor even outperform efficient, task-specific supervised models.",
      "authors": [
        "Ran Tong",
        "Jiaqi Liu",
        "Su Liu",
        "Jiexi Xu",
        "Lanruo Wang",
        "Tong Wang"
      ],
      "published": "2025-10-01T01:46:09Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00411v2",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本文比较了轻量级CNN与BiomedCLIP视觉语言模型在肺炎和结核病检测中的表现。研究发现，通过决策阈值校准，零射VLM在肺炎检测中F1分数达0.8841，超越监督CNN的0.8803；在结核病检测中从0.4812提升至0.7684，接近监督基线的0.7834。表明适当校准对发挥零射VLM诊断潜力至关重要。",
      "order": 317
    },
    {
      "arxiv_id": "2510.00405v1",
      "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy\n  Observations",
      "summary": "Reliable trajectory prediction from an ego-centric perspective is crucial for\nrobotic navigation in human-centric environments. However, existing methods\ntypically assume idealized observation histories, failing to account for the\nperceptual artifacts inherent in first-person vision, such as occlusions, ID\nswitches, and tracking drift. This discrepancy between training assumptions and\ndeployment reality severely limits model robustness. To bridge this gap, we\nintroduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,\nfirst-person visual histories in clean, bird's-eye-view future trajectories,\nenabling robust learning under realistic perceptual constraints. Building on\nthis benchmark, we propose BiFlow, a dual-stream flow matching model that\nconcurrently denoises historical observations and forecasts future motion by\nleveraging a shared latent representation. To better model agent intent, BiFlow\nincorporates our EgoAnchor mechanism, which conditions the prediction decoder\non distilled historical features via feature modulation. Extensive experiments\nshow that BiFlow achieves state-of-the-art performance, reducing minADE and\nminFDE by 10-15% on average and demonstrating superior robustness. We\nanticipate that our benchmark and model will provide a critical foundation for\ndeveloping trajectory forecasting systems truly resilient to the challenges of\nreal-world, ego-centric perception.",
      "authors": [
        "Jiayi Liu",
        "Jiaming Zhou",
        "Ke Ye",
        "Kun-Yu Lin",
        "Allan Wang",
        "Junwei Liang"
      ],
      "published": "2025-10-01T01:30:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00405v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出EgoTraj-Bench基准和BiFlow模型，针对第一人称视角下轨迹预测的噪声观测问题，通过双流流匹配和EgoAnchor机制，在去噪历史观测的同时预测未来轨迹，平均降低10-15%的预测误差，显著提升模型鲁棒性。",
      "order": 318
    },
    {
      "arxiv_id": "2510.00404v2",
      "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features",
      "summary": "Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs.",
      "authors": [
        "Xudong Zhu",
        "Mohammad Mahdi Khalili",
        "Zhihui Zhu"
      ],
      "published": "2025-10-01T01:29:31Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00404v2",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出AbsTopK稀疏自编码器，通过解构稀疏编码的邻近梯度方法，揭示现有SAE因强制非负性而无法表示双向概念的局限。AbsTopK基于ℓ₀稀疏约束，保留正负激活，在四个大语言模型和七项任务中证明其能提升重建保真度、增强可解释性，并实现单个特征编码对立概念。",
      "order": 319
    },
    {
      "arxiv_id": "2510.00401v1",
      "title": "Physics-Informed Neural Controlled Differential Equations for Scalable\n  Long Horizon Multi-Agent Motion Forecasting",
      "summary": "Long-horizon motion forecasting for multiple autonomous robots is challenging\ndue to non-linear agent interactions, compounding prediction errors, and\ncontinuous-time evolution of dynamics. Learned dynamics of such a system can be\nuseful in various applications such as travel time prediction,\nprediction-guided planning and generative simulation. In this work, we aim to\ndevelop an efficient trajectory forecasting model conditioned on multi-agent\ngoals. Motivated by the recent success of physics-guided deep learning for\npartially known dynamical systems, we develop a model based on neural\nControlled Differential Equations (CDEs) for long-horizon motion forecasting.\nUnlike discrete-time methods such as RNNs and transformers, neural CDEs operate\nin continuous time, allowing us to combine physics-informed constraints and\nbiases to jointly model multi-robot dynamics. Our approach, named PINCoDE\n(Physics-Informed Neural Controlled Differential Equations), learns\ndifferential equation parameters that can be used to predict the trajectories\nof a multi-agent system starting from an initial condition. PINCoDE is\nconditioned on future goals and enforces physics constraints for robot motion\nover extended periods of time. We adopt a strategy that scales our model from\n10 robots to 100 robots without the need for additional model parameters, while\nproducing predictions with an average ADE below 0.5 m for a 1-minute horizon.\nFurthermore, progressive training with curriculum learning for our PINCoDE\nmodel results in a 2.7X reduction of forecasted pose error over 4 minute\nhorizons compared to analytical models.",
      "authors": [
        "Shounak Sural",
        "Charles Kekeh",
        "Wenliang Liu",
        "Federico Pecora",
        "Mouhacine Benosman"
      ],
      "published": "2025-10-01T01:27:07Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00401v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出PINCoDE模型，结合物理约束与神经控制微分方程，实现多机器人长时程运动预测。该连续时间模型支持10-100个智能体规模扩展，在1分钟预测范围内平均位移误差低于0.5米，相比解析模型将4分钟姿态预测误差降低2.7倍。",
      "order": 320
    },
    {
      "arxiv_id": "2510.01288v1",
      "title": "Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal\n  LLM Misbehaviours",
      "summary": "We draw inspiration from microsaccades, tiny involuntary eye movements that\nreveal hidden dynamics of human perception, to propose an analogous probing\nmethod for large language models (LLMs). Just as microsaccades expose subtle\nbut informative shifts in vision, we show that lightweight position encoding\nperturbations elicit latent signals that indicate model misbehaviour. Our\nmethod requires no fine-tuning or task-specific supervision, yet detects\nfailures across diverse settings including factuality, safety, toxicity, and\nbackdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate\nthat these perturbation-based probes surface misbehaviours while remaining\ncomputationally efficient. These findings suggest that pretrained LLMs already\nencode the internal evidence needed to flag their own failures, and that\nmicrosaccade-inspired interventions provide a pathway for detecting and\nmitigating undesirable behaviours.",
      "authors": [
        "Rui Melo",
        "Rui Abreu",
        "Corina S. Pasareanu"
      ],
      "published": "2025-10-01T01:24:59Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01288v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "受微眼跳启发，提出一种通过位置编码扰动探测大语言模型潜在错误行为的方法。该方法无需微调或任务监督，即可高效检测事实性、安全性、毒性及后门攻击等多种问题，表明预训练模型已具备自我错误识别的内部证据。",
      "order": 321
    },
    {
      "arxiv_id": "2510.00395v1",
      "title": "SAGE-Music: Low-Latency Symbolic Music Generation via\n  Attribute-Specialized Key-Value Head Sharing",
      "summary": "Low-latency symbolic music generation is essential for real-time\nimprovisation and human-AI co-creation. Existing transformer-based models,\nhowever, face a trade-off between inference speed and musical quality.\nTraditional acceleration techniques such as embedding pooling significantly\ndegrade quality, while recently proposed Byte Pair Encoding (BPE) methods -\nthough effective on single-track piano data - suffer large performance drops in\nmulti-track settings, as revealed by our analysis. We propose\nAttribute-Specialized Key-Value Head Sharing (AS-KVHS), adapted to music's\nstructured symbolic representation, achieving about 30% inference speedup with\nonly a negligible (about 0.4%) quality drop in objective evaluations and slight\nimprovements in subjective listening tests. Our main contributions are (1) the\nfirst systematic study of BPE's generalizability in multi-track symbolic music,\nand (2) the introduction of AS-KVHS for low-latency symbolic music generation.\nBeyond these, we also release SAGE-Music, an open-source benchmark that matches\nor surpasses state-of-the-art models in generation quality.",
      "authors": [
        "Jiaye Tan",
        "Haonan Luo",
        "Linfeng Song",
        "Shuaiqi Chen",
        "Yishan Lyu",
        "Zian Zhong",
        "Roujia Wang",
        "Daniel Jiang",
        "Haoran Zhang",
        "Jiaming Bai",
        "Haoran Cheng",
        "Q. Vera Liao",
        "Hao-Wen Dong"
      ],
      "published": "2025-10-01T01:11:43Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00395v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "SAGE-Music提出属性专用键值头共享(AS-KVHS)方法，针对符号音乐的结构化表示优化，在实现约30%推理加速的同时，客观评估质量仅下降约0.4%，主观听测略有提升。该研究首次系统分析了BPE在多轨符号音乐中的泛化性，并发布了开源基准SAGE-Music，在生成质量上达到或超越现有最优模型。",
      "order": 322
    },
    {
      "arxiv_id": "2510.00386v1",
      "title": "Train on Validation (ToV): Fast data selection with applications to\n  fine-tuning",
      "summary": "State-of-the-art machine learning often follows a two-stage process:\n$(i)$~pre-training on large, general-purpose datasets; $(ii)$~fine-tuning on\ntask-specific data. In fine-tuning, selecting training examples that closely\nreflect the target distribution is crucial. However, it is often the case that\nonly a few samples are available from the target distribution. Existing data\nselection methods treat these target samples as a validation set and estimate\nthe effect of adding or removing a single sample from the training pool by\nperforming inference on the validation set.\n  We propose a simpler and faster alternative that inverts the usual role of\ntrain and validation: we perform inference on the training pool before and\nafter fine-tuning on the validation set. We then select samples whose\npredictions change the most. Our key insight is that the training samples most\naffected by fine-tuning on a small validation set tend to be the most\nbeneficial for reducing test loss on the target distribution. Experiments on\ninstruction tuning and named entity recognition tasks show that, in most cases,\nour method achieves lower test log-loss than state-of-the-art approaches. We\nsupport our findings with theoretical analysis.",
      "authors": [
        "Ayush Jain",
        "Andrea Montanari",
        "Eren Sasoglu"
      ],
      "published": "2025-10-01T00:55:39Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00386v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出'训练验证集(ToV)'方法，通过反转训练集和验证集的传统角色来快速选择微调数据。核心思路是在验证集上微调后，选择预测变化最大的训练样本，实验证明该方法在指令调优和命名实体识别任务中能有效降低测试损失。",
      "order": 323
    },
    {
      "arxiv_id": "2510.00381v1",
      "title": "Semantic-Driven AI Agent Communications: Challenges and Solutions",
      "summary": "With the rapid growth of intelligent services, communication targets are\nshifting from humans to artificial intelligent (AI) agents, which require new\nparadigms to enable real-time perception, decision-making, and collaboration.\nSemantic communication, which conveys task-relevant meaning rather than raw\ndata, offers a promising solution. However, its practical deployment remains\nconstrained by dynamic environments and limited resources. To address these\nissues, this article proposes a semantic-driven AI agent communication\nframework and develops three enabling techniques. First, semantic adaptation\ntransmission applies fine-tuning with real or generative samples to efficiently\nadapt models to varying environments. Second, semantic lightweight transmission\nincorporates pruning, quantization, and perception-aware sampling to reduce\nmodel complexity and alleviate computational burden on edge agents. Third,\nsemantic self-evolution control employs distributed hierarchical\ndecision-making to optimize multi-dimensional resources, enabling robust\nmulti-agent collaboration in dynamic environments. Simulation results show that\nthe proposed solutions achieve faster convergence and stronger robustness,\nwhile the proposed distributed hierarchical optimization method significantly\noutperforms conventional decision-making schemes, highlighting its potential\nfor AI agent communication networks.",
      "authors": [
        "Kaiwen Yu",
        "Mengying Sun",
        "Zhijin Qin",
        "Xiaodong Xu",
        "Ping Yang",
        "Yue Xiao",
        "Gang Wu"
      ],
      "published": "2025-10-01T00:52:37Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00381v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出语义驱动的AI智能体通信框架，包含语义自适应传输、语义轻量传输和语义自进化控制三大技术，通过模型调优、剪枝量化和分布式决策优化，解决动态环境和资源受限下的智能体通信问题，实现更快的收敛速度和更强的鲁棒性。",
      "order": 324
    },
    {
      "arxiv_id": "2510.00376v1",
      "title": "Discrete Wavelet Transform as a Facilitator for Expressive Latent Space\n  Representation in Variational Autoencoders in Satellite Imagery",
      "summary": "Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the\ncomputational complexity of pixel-space diffusion by operating within a\ncompressed latent space constructed by Variational Autoencoders (VAEs),\ndemonstrating significant advantages in Remote Sensing (RS) applications.\nThough numerous studies enhancing LDMs have been conducted, investigations\nexplicitly targeting improvements within the intrinsic latent space remain\nscarce. This paper proposes an innovative perspective, utilizing the Discrete\nWavelet Transform (DWT) to enhance the VAE's latent space representation,\ndesigned for satellite imagery. The proposed method, ExpDWT-VAE, introduces\ndual branches: one processes spatial domain input through convolutional\noperations, while the other extracts and processes frequency-domain features\nvia 2D Haar wavelet decomposition, convolutional operation, and inverse DWT\nreconstruction. These branches merge to create an integrated spatial-frequency\nrepresentation, further refined through convolutional and diagonal Gaussian\nmapping into a robust latent representation. We utilize a new satellite imagery\ndataset housed by the TerraFly mapping system to validate our method.\nExperimental results across several performance metrics highlight the efficacy\nof the proposed method at enhancing latent space representation.",
      "authors": [
        "Arpan Mahara",
        "Md Rezaul Karim Khan",
        "Naphtali Rishe",
        "Wenjia Wang",
        "Seyed Masoud Sadjadi"
      ],
      "published": "2025-10-01T00:49:41Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00376v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ExpDWT-VAE方法，通过离散小波变换增强变分自编码器的潜在空间表示，结合空间域和频域特征处理，在卫星图像数据集上验证了改进的潜在表示效果。",
      "order": 325
    },
    {
      "arxiv_id": "2510.00373v1",
      "title": "Combining Large Language Models and Gradient-Free Optimization for\n  Automatic Control Policy Synthesis",
      "summary": "Large Language models (LLMs) have shown promise as generators of symbolic\ncontrol policies, producing interpretable program-like representations through\niterative search. However, these models are not capable of separating the\nfunctional structure of a policy from the numerical values it is parametrized\nby, thus making the search process slow and inefficient. We propose a hybrid\napproach that decouples structural synthesis from parameter optimization by\nintroducing an additional optimization layer for local parameter search. In our\nmethod, the numerical parameters of LLM-generated programs are extracted and\noptimized numerically to maximize task performance. With this integration, an\nLLM iterates over the functional structure of programs, while a separate\noptimization loop is used to find a locally optimal set of parameters\naccompanying candidate programs. We evaluate our method on a set of control\ntasks, showing that it achieves higher returns and improved sample efficiency\ncompared to purely LLM-guided search. We show that combining symbolic program\nsynthesis with numerical optimization yields interpretable yet high-performing\npolicies, bridging the gap between language-model-guided design and classical\ncontrol tuning. Our code is available at\nhttps://sites.google.com/berkeley.edu/colmo.",
      "authors": [
        "Carlo Bosio",
        "Matteo Guarrera",
        "Alberto Sangiovanni-Vincentelli",
        "Mark W. Mueller"
      ],
      "published": "2025-10-01T00:42:15Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00373v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种混合方法，将大语言模型的结构合成与梯度自由参数优化分离，通过引入局部参数搜索层提升控制策略生成效率。该方法在控制任务中实现了更高回报率和样本效率，生成兼具可解释性与高性能的策略。",
      "order": 326
    },
    {
      "arxiv_id": "2510.01287v1",
      "title": "Evaluating New AI Cell Foundation Models on Challenging Kidney Pathology\n  Cases Unaddressed by Previous Foundation Models",
      "summary": "Accurate cell nuclei segmentation is critical for downstream tasks in kidney\npathology and remains a major challenge due to the morphological diversity and\nimaging variability of renal tissues. While our prior work has evaluated\nearly-generation AI cell foundation models in this domain, the effectiveness of\nrecent cell foundation models remains unclear. In this study, we benchmark\nadvanced AI cell foundation models (2025), including CellViT++ variants and\nCellpose-SAM, against three widely used cell foundation models developed prior\nto 2024, using a diverse large-scale set of kidney image patches within a\nhuman-in-the-loop rating framework. We further performed fusion-based ensemble\nevaluation and model agreement analysis to assess the segmentation capabilities\nof the different models. Our results show that CellViT++ [Virchow] yields the\nhighest standalone performance with 40.3% of predictions rated as \"Good\" on a\ncurated set of 2,091 challenging samples, outperforming all prior models. In\naddition, our fused model achieves 62.2% \"Good\" predictions and only 0.4%\n\"Bad\", substantially reducing segmentation errors. Notably, the fusion model\n(2025) successfully resolved the majority of challenging cases that remained\nunaddressed in our previous study. These findings demonstrate the potential of\nAI cell foundation model development in renal pathology and provide a curated\ndataset of challenging samples to support future kidney-specific model\nrefinement.",
      "authors": [
        "Runchen Wang",
        "Junlin Guo",
        "Siqi Lu",
        "Ruining Deng",
        "Zhengyi Lu",
        "Yanfan Zhu",
        "Yuechen Yang",
        "Chongyu Qu",
        "Yu Wang",
        "Shilin Zhao",
        "Catie Chang",
        "Mitchell Wilkes",
        "Mengmeng Yin",
        "Haichun Yang",
        "Yuankai Huo"
      ],
      "published": "2025-10-01T00:38:36Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.01287v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究评估了2025年新型AI细胞基础模型在肾脏病理学中的表现，重点测试了CellViT++变体和Cellpose-SAM模型。在2091个挑战性肾脏图像样本上，CellViT++ [Virchow]以40.3%的'良好'预测率表现最佳，融合模型更将'良好'率提升至62.2%，'差评'率降至0.4%，成功解决了先前研究中未处理的多数难题，为肾脏病理AI模型发展提供了重要基准和数据集。",
      "order": 327
    },
    {
      "arxiv_id": "2510.00361v1",
      "title": "Attribution Gradients: Incrementally Unfolding Citations for Critical\n  Examination of Attributed AI Answers",
      "summary": "AI question answering systems increasingly generate responses with\nattributions to sources. However, the task of verifying the actual content of\nthese attributions is in most cases impractical. In this paper, we present\nattribution gradients as a solution. Attribution gradients provide integrated,\nincremental affordances for diving into an attributed passage. A user can\ndecompose a sentence of an answer into its claims. For each claim, the user can\nview supporting and contradictory excerpts mined from sources. Those excerpts\nserve as clickable conduits into the source (in our application, scientific\npapers). When evidence itself contains more citations, the UI unpacks the\nevidence into excerpts from the cited sources. These features of attribution\ngradients facilitate concurrent interconnections among answer, claim, excerpt,\nand context. In a usability study, we observed greater engagement with sources\nand richer revision in a task where participants revised an attributed AI\nanswer with attribution gradients and a baseline.",
      "authors": [
        "Hita Kambhamettu",
        "Alyssa Hwang",
        "Philippe Laban",
        "Andrew Head"
      ],
      "published": "2025-10-01T00:07:28Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.00361v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出归因梯度方法，通过逐层展开引文支持用户深入验证AI问答系统的答案来源。该方法将答案分解为具体主张，展示支持与矛盾证据，并通过可点击链接追溯原始文献，实验证明能提升用户对源材料的参与度和答案修订质量。",
      "order": 328
    },
    {
      "arxiv_id": "2510.02306v1",
      "title": "Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation",
      "summary": "In arena-style evaluation of large language models (LLMs), two LLMs respond\nto a user query, and the user chooses the winning response or deems the\n\"battle\" a draw, resulting in an adjustment to the ratings of both models. The\nprevailing approach for modeling these rating dynamics is to view battles as\ntwo-player game matches, as in chess, and apply the Elo rating system and its\nderivatives. In this paper, we critically examine this paradigm. Specifically,\nwe question whether a draw genuinely means that the two models are equal and\nhence whether their ratings should be equalized. Instead, we conjecture that\ndraws are more indicative of query difficulty: if the query is too easy, then\nboth models are more likely to succeed equally. On three real-world arena\ndatasets, we show that ignoring rating updates for draws yields a 1-3% relative\nincrease in battle outcome prediction accuracy (which includes draws) for all\nfour rating systems studied. Further analyses suggest that draws occur more for\nqueries rated as very easy and those as highly objective, with risk ratios of\n1.37 and 1.35, respectively. We recommend future rating systems to reconsider\nexisting draw semantics and to account for query properties in rating updates.",
      "authors": [
        "Raphael Tang",
        "Crystina Zhang",
        "Wenyan Li",
        "Carmen Lai",
        "Pontus Stenetorp",
        "Yao Lu"
      ],
      "published": "2025-10-02T17:59:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02306v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文批判性审视了竞技场式大语言模型评估中平局处理的现有范式，挑战了平局意味着模型能力相等的传统观点。通过三个真实数据集分析发现，平局更可能反映查询难度而非模型对等，忽略平局时的评分更新可提升1-3%的预测准确率。研究建议未来评分系统应重新考虑平局语义并纳入查询属性。",
      "order": 329
    },
    {
      "arxiv_id": "2510.02297v1",
      "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
      "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
      "authors": [
        "Wentao Zhang",
        "Yang Young Lu",
        "Yuntian Deng"
      ],
      "published": "2025-10-02T17:59:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02297v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出交互式训练框架，通过控制服务器实现人机协同实时干预神经网络训练过程，支持动态调整超参数、训练数据和模型检查点，在三个案例中证明其能提升训练稳定性、降低超参数敏感性并增强适应性。",
      "order": 330
    },
    {
      "arxiv_id": "2510.02294v1",
      "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
      "summary": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of\nstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike\nprevious top-ranking embedding models that require massive contrastive\npretraining, sophisticated training pipelines, and costly synthetic training\ndata, F2LLM is directly finetuned from foundation models on 6 million\nquery-document-negative tuples curated from open-source, non-synthetic\ndatasets, striking a strong balance between training cost, model size, and\nembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd\namong models with approximately 4B parameters and 7th overall, while F2LLM-1.7B\nranks 1st among models in the 1B-2B size range. To facilitate future research\nin the field, we release the models, training dataset, and code, positioning\nF2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
      "authors": [
        "Ziyin Zhang",
        "Zihan Liao",
        "Hang Yu",
        "Peng Di",
        "Rui Wang"
      ],
      "published": "2025-10-02T17:58:49Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02294v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "F2LLM技术报告提出了一套0.6B、1.7B和4B三种规模的嵌入模型，仅使用600万开源非合成数据微调基础模型，在MTEB英文榜单上表现优异：4B版本在同等参数模型中排名第二，1.7B版本在1B-2B规模中排名第一，实现了训练成本、模型大小与嵌入性能的良好平衡。",
      "order": 331
    },
    {
      "arxiv_id": "2510.02292v1",
      "title": "From Behavioral Performance to Internal Competence: Interpreting\n  Vision-Language Models with VLM-Lens",
      "summary": "We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,\nanalysis, and interpretation of vision-language models (VLMs) by supporting the\nextraction of intermediate outputs from any layer during the forward pass of\nopen-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that\nabstracts away model-specific complexities and supports user-friendly operation\nacross diverse VLMs. It currently supports 16 state-of-the-art base VLMs and\ntheir over 30 variants, and is extensible to accommodate new models without\nchanging the core logic.\n  The toolkit integrates easily with various interpretability and analysis\nmethods. We demonstrate its usage with two simple analytical experiments,\nrevealing systematic differences in the hidden representations of VLMs across\nlayers and target concepts. VLM-Lens is released as an open-sourced project to\naccelerate community efforts in understanding and improving VLMs.",
      "authors": [
        "Hala Sheta",
        "Eric Huang",
        "Shuyu Wu",
        "Ilia Alenabi",
        "Jiajun Hong",
        "Ryker Lin",
        "Ruoxi Ning",
        "Daniel Wei",
        "Jialin Yang",
        "Jiawei Zhou",
        "Ziqiao Ma",
        "Freda Shi"
      ],
      "published": "2025-10-02T17:58:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02292v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "VLM-Lens是一个用于系统化评估、分析和解释视觉语言模型的开源工具包，支持从任意层提取中间输出，提供统一配置接口，兼容16种主流VLM及其30多个变体，可加速社区对多模态模型的理解与改进。",
      "order": 332
    },
    {
      "arxiv_id": "2510.02286v1",
      "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
      "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
      "authors": [
        "Ruohao Guo",
        "Afshin Oroojlooy",
        "Roshan Sridhar",
        "Miguel Ballesteros",
        "Alan Ritter",
        "Dan Roth"
      ],
      "published": "2025-10-02T17:57:05Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02286v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出DialTree-RPO框架，结合强化学习与树搜索，针对大语言模型在多轮对话中的安全漏洞进行自动化红队攻击。该方法将对话视为序列决策问题，无需人工标注数据即可发现多样化攻击策略，在10个目标模型上攻击成功率比现有最优方法提升25.9%，有效揭示多轮攻击中的新型安全威胁。",
      "order": 333
    },
    {
      "arxiv_id": "2510.02272v1",
      "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
      "summary": "Recent advancements in Reinforcement Post-Training (RPT) have significantly\nenhanced the capabilities of Large Reasoning Models (LRMs), sparking increased\ninterest in the generalization of RL-based reasoning. While existing work has\nprimarily focused on investigating its generalization across tasks or\nmodalities, this study proposes a novel cross-linguistic perspective to\ninvestigate reasoning generalization. This raises a crucial question:\n$\\textit{Does the reasoning capability achieved from English RPT effectively\ntransfer to other languages?}$ We address this by systematically evaluating\nEnglish-centric LRMs on multilingual reasoning benchmarks and introducing a\nmetric to quantify cross-lingual transferability. Our findings reveal that\ncross-lingual transferability varies significantly across initial model, target\nlanguage, and training paradigm. Through interventional studies, we find that\nmodels with stronger initial English capabilities tend to over-rely on\nEnglish-specific patterns, leading to diminished cross-lingual generalization.\nTo address this, we conduct a thorough parallel training study. Experimental\nresults yield three key findings: $\\textbf{First-Parallel Leap}$, a substantial\nleap in performance when transitioning from monolingual to just a single\nparallel language, and a predictable $\\textbf{Parallel Scaling Law}$, revealing\nthat cross-lingual reasoning transfer follows a power-law with the number of\ntraining parallel languages. Moreover, we identify the discrepancy between\nactual monolingual performance and the power-law prediction as\n$\\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs\nfail to fully generalize across languages. Our study challenges the assumption\nthat LRM reasoning mirrors human cognition, providing critical insights for the\ndevelopment of more language-agnostic LRMs.",
      "authors": [
        "Wen Yang",
        "Junhong Wu",
        "Chong Li",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "published": "2025-10-02T17:49:49Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02272v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究从跨语言视角探讨大型推理模型的泛化能力，发现英语训练的模型在其他语言上存在泛化不足。通过平行训练实验，揭示了'平行跃迁'现象和'平行缩放定律'，表明跨语言推理能力随训练语言数量呈幂律增长，并识别出'单语泛化差距'，挑战了LRM推理类似人类认知的假设。",
      "order": 334
    },
    {
      "arxiv_id": "2510.02271v1",
      "title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in\n  Tool-Augmented Agents",
      "summary": "Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling.",
      "authors": [
        "Yaxin Du",
        "Yuanshuo Zhang",
        "Xiyuan Yang",
        "Yifan Zhou",
        "Cheng Wang",
        "Gongyi Zou",
        "Xianghe Pang",
        "Wenhao Wang",
        "Menglan Chen",
        "Shuo Tang",
        "Zhiyu Li",
        "Siheng Chen"
      ],
      "published": "2025-10-02T17:48:03Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02271v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "InfoMosaic-Bench是首个专门评估工具增强型智能体多源信息检索能力的基准测试，涵盖医疗、金融、地图等六大领域。研究发现：仅靠网络信息不足（GPT-5准确率仅38.2%），领域工具效果不稳定，22.4%失败源于工具使用错误，揭示当前LLM在工具整合方面仍存在显著缺陷。",
      "order": 335
    },
    {
      "arxiv_id": "2510.02263v1",
      "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
      "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.",
      "authors": [
        "Yuxiao Qu",
        "Anikait Singh",
        "Yoonho Lee",
        "Amrith Setlur",
        "Ruslan Salakhutdinov",
        "Chelsea Finn",
        "Aviral Kumar"
      ],
      "published": "2025-10-02T17:44:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02263v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "RLAD提出一种双智能体强化学习框架，通过训练抽象生成器和解决方案生成器来提升大语言模型的推理能力。该方法让模型先提出自然语言描述的推理抽象概念，再利用这些抽象指导问题求解，有效改善结构化探索和泛化性能，在复杂推理任务中表现优异。",
      "order": 336
    },
    {
      "arxiv_id": "2510.02250v1",
      "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
      "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
      "authors": [
        "Gonzalo Gonzalez-Pumariega",
        "Vincent Tu",
        "Chih-Lun Lee",
        "Jiachen Yang",
        "Ang Li",
        "Xin Eric Wang"
      ],
      "published": "2025-10-02T17:37:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02250v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出行为N选优(bBoN)方法，通过生成多个执行轨迹并使用行为叙事进行选择，显著提升计算机使用代理的可靠性和成功率。在OSWorld基准测试中达到69.9%的新SOTA，接近人类72%的水平，并在跨操作系统环境中展现出强大泛化能力，证明了规模化代理在结构化轨迹理解与选择下的非凡有效性。",
      "order": 337
    },
    {
      "arxiv_id": "2510.02249v1",
      "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process.",
      "authors": [
        "Tianyi Jiang",
        "Yi Bin",
        "Yujuan Ding",
        "Kainian Zhu",
        "Fei Ma",
        "Jingkuan Song",
        "Heng Tao Shen"
      ],
      "published": "2025-10-02T17:36:50Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02249v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种名为'简要探索后决策'的新推理范式，通过累积熵调控机制解决大语言模型过度思考问题。该方法利用标记熵累积平均值动态确定最佳思考停止点，在保持解题能力的同时显著减少71%的响应长度，实现更高效的自适应推理过程。",
      "order": 338
    },
    {
      "arxiv_id": "2510.02245v1",
      "title": "ExGRPO: Learning to Reason from Experience",
      "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.",
      "authors": [
        "Runzhe Zhan",
        "Yafu Li",
        "Zhi Wang",
        "Xiaoye Qu",
        "Dongrui Liu",
        "Jing Shao",
        "Derek F. Wong",
        "Yu Cheng"
      ],
      "published": "2025-10-02T17:31:30Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02245v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ExGRPO框架，通过分析推理经验的价值特征（正确性和熵），对经验进行组织优化，在数学和通用推理基准上相比传统强化学习方法平均提升3.5/7.6分，并显著提升训练稳定性。",
      "order": 339
    },
    {
      "arxiv_id": "2510.02243v1",
      "title": "AccurateRAG: A Framework for Building Accurate Retrieval-Augmented\n  Question-Answering Applications",
      "summary": "We introduce AccurateRAG -- a novel framework for constructing\nhigh-performance question-answering applications based on retrieval-augmented\ngeneration (RAG). Our framework offers a pipeline for development efficiency\nwith tools for raw dataset processing, fine-tuning data generation, text\nembedding & LLM fine-tuning, output evaluation, and building RAG systems\nlocally. Experimental results show that our framework outperforms previous\nstrong baselines and obtains new state-of-the-art question-answering\nperformance on benchmark datasets.",
      "authors": [
        "Linh The Nguyen",
        "Chi Tran",
        "Dung Ngoc Nguyen",
        "Van-Cuong Pham",
        "Hoang Ngo",
        "Dat Quoc Nguyen"
      ],
      "published": "2025-10-02T17:30:08Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02243v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "AccurateRAG是一个用于构建高精度检索增强生成问答应用的新框架，提供从数据处理、微调、评估到本地部署的完整流程，在基准测试中超越了现有基线并达到最先进的性能水平。",
      "order": 340
    },
    {
      "arxiv_id": "2510.02241v1",
      "title": "Study on LLMs for Promptagator-Style Dense Retriever Training",
      "summary": "Promptagator demonstrated that Large Language Models (LLMs) with few-shot\nprompts can be used as task-specific query generators for fine-tuning\ndomain-specialized dense retrieval models. However, the original Promptagator\napproach relied on proprietary and large-scale LLMs which users may not have\naccess to or may be prohibited from using with sensitive data. In this work, we\nstudy the impact of open-source LLMs at accessible scales ($\\leq$14B\nparameters) as an alternative. Our results demonstrate that open-source LLMs as\nsmall as 3B parameters can serve as effective Promptagator-style query\ngenerators. We hope our work will inform practitioners with reliable\nalternatives for synthetic data generation and give insights to maximize\nfine-tuning results for domain-specific applications.",
      "authors": [
        "Daniel Gwon",
        "Nour Jedidi",
        "Jimmy Lin"
      ],
      "published": "2025-10-02T17:29:51Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.02241v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探索使用参数规模≤14B的开源大语言模型替代原始Promptagator中的专有大模型，用于生成特定领域查询以训练稠密检索模型。结果表明，仅需3B参数的开源模型即可有效完成Promptagator式查询生成，为敏感数据场景下的领域专用检索模型优化提供可行方案。",
      "order": 341
    },
    {
      "arxiv_id": "2510.02232v1",
      "title": "Enhanced Arabic-language cyberbullying detection: deep embedding and\n  transformer (BERT) approaches",
      "summary": "Recent technological advances in smartphones and communications, including\nthe growth of such online platforms as massive social media networks such as X\n(formerly known as Twitter) endangers young people and their emotional\nwell-being by exposing them to cyberbullying, taunting, and bullying content.\nMost proposed approaches for automatically detecting cyberbullying have been\ndeveloped around the English language, and methods for detecting\nArabic-language cyberbullying are scarce. Methods for detecting Arabic-language\ncyberbullying are especially scarce. This paper aims to enhance the\neffectiveness of methods for detecting cyberbullying in Arabic-language\ncontent. We assembled a dataset of 10,662 X posts, pre-processed the data, and\nused the kappa tool to verify and enhance the quality of our annotations. We\nconducted four experiments to test numerous deep learning models for\nautomatically detecting Arabic-language cyberbullying. We first tested a long\nshort-term memory (LSTM) model and a bidirectional long short-term memory\n(Bi-LSTM) model with several experimental word embeddings. We also tested the\nLSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder from\nrepresentations (BERT) and then tested them on a different experimental models\nBERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTM\nwith FastText embedding word performed even better, achieving 98% accuracy. As\na result, the outcomes are generalize",
      "authors": [
        "Ebtesam Jaber Aljohani",
        "Wael M. S. Yafoo"
      ],
      "published": "2025-10-02T17:20:02Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02232v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究针对阿拉伯语网络欺凌检测方法稀缺的问题，构建了包含10,662条推文的数据集，测试了LSTM、Bi-LSTM与BERT等多种深度学习模型。实验结果显示，Bi-LSTM结合FastText词嵌入模型达到98%的最高准确率，LSTM-BERT和Bi-LSTM-BERT组合也取得97%的准确率，显著提升了阿拉伯语网络欺凌内容的自动检测效果。",
      "order": 342
    },
    {
      "arxiv_id": "2510.02230v1",
      "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.",
      "authors": [
        "Phuc Minh Nguyen",
        "Chinh D. La",
        "Duy M. H. Nguyen",
        "Nitesh V. Chawla",
        "Binh T. Nguyen",
        "Khoa D. Doan"
      ],
      "published": "2025-10-02T17:17:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02230v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文揭示强化学习与可验证奖励(RLVR)在提升大语言模型推理能力时存在的悖论：反而会缩小推理边界。研究发现RLVR存在负干扰现象和赢家通吃效应，导致模型收敛于狭窄的解题策略。通过理论分析和数学推理基准测试，提出针对低概率问题的数据筛选算法，显著提升了Pass@k性能。",
      "order": 343
    },
    {
      "arxiv_id": "2510.02227v1",
      "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.",
      "authors": [
        "Xiaoyang Yuan",
        "Yujuan Ding",
        "Yi Bin",
        "Wenqi Shao",
        "Jinyu Cai",
        "Jingkuan Song",
        "Yang Yang",
        "Hengtao Shen"
      ],
      "published": "2025-10-02T17:14:00Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02227v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出自适应多引导策略优化(AMPO)框架，通过动态调用多个教师模型指导，仅在学生模型无法生成正确答案时提供帮助，实现更高效的推理探索。该方法在数学推理任务上提升4.3%，分布外任务提升12.2%，显著增强推理多样性和性能。",
      "order": 344
    },
    {
      "arxiv_id": "2510.02209v1",
      "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
      "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
      "authors": [
        "Yanxu Chen",
        "Zijun Yao",
        "Yantao Liu",
        "Jin Ye",
        "Jianing Yu",
        "Lei Hou",
        "Juanzi Li"
      ],
      "published": "2025-10-02T16:54:57Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02209v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "financial_ai",
      "tldr_zh": "本文提出StockBench基准，评估大语言模型在真实股票交易环境中的表现。研究显示，尽管多数LLM代理难以超越简单买入持有策略，但部分模型展现出更高收益和风险管理潜力，表明静态金融知识与实际交易策略之间存在差距。",
      "order": 345
    },
    {
      "arxiv_id": "2510.02204v1",
      "title": "Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in\n  VLM-Powered Mobile-Use Agents",
      "summary": "Mobile-use agents powered by vision-language models (VLMs) have shown great\npotential in interpreting natural language instructions and generating\ncorresponding actions based on mobile graphical user interface. Recent studies\nsuggest that incorporating chain-of-thought (CoT) reasoning tends to improve\nthe execution accuracy. However, existing evaluations emphasize execution\naccuracy while neglecting whether CoT reasoning aligns with ground-truth\nactions. This oversight fails to assess potential reasoning-execution gaps,\nwhich in turn foster over-trust: users relying on seemingly plausible CoTs may\nunknowingly authorize harmful actions, potentially resulting in financial loss\nor trust crisis. In this work, we introduce a new evaluation framework to\ndiagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment\n(GTA), which measures whether the action implied by a CoT matches the\nground-truth action. By combining GTA with the standard Exact Match (EM)\nmetric, we jointly assess both the reasoning accuracy and execution accuracy.\nThis joint perspective reveals two types of reasoning-execution gaps: (i)\nExecution Gap (EG), where the reasoning correctly identifies the correct action\nbut execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but\nreasoning process conflicts with the actual execution. Experimental results\nacross a wide range of mobile interaction tasks reveal that reasoning-execution\ngaps are prevalent, with execution gaps occurring more frequently than\nreasoning gaps. Moreover, while scaling up model size reduces the overall gap,\nsizable execution gaps persist even in the largest models. Further analysis\nshows that our framework reliably reflects systematic EG/RG patterns in\nstate-of-the-art models. These findings offer concrete diagnostics and support\nthe development of more trustworthy mobile-use agents.",
      "authors": [
        "Lingzhong Dong",
        "Ziqi Zhou",
        "Shuaibo Yang",
        "Haiyue Sheng",
        "Pengzhou Cheng",
        "Zongru Wu",
        "Zheng Wu",
        "Gongshen Liu",
        "Zhuosheng Zhang"
      ],
      "published": "2025-10-02T16:51:19Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02204v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对VLM驱动的移动应用代理，提出诊断推理-执行差距的新评估框架。通过结合真实对齐(GTA)和精确匹配(EM)指标，识别出执行差距(正确推理但执行失败)和推理差距(执行成功但推理错误)两种问题。实验表明这些差距普遍存在，即使在大模型中执行差距仍显著，为开发更可信的移动代理提供具体诊断方法。",
      "order": 346
    },
    {
      "arxiv_id": "2510.02200v1",
      "title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge\n  Graph Exploration Utilities",
      "summary": "Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain.",
      "authors": [
        "Felix Brei",
        "Lorenz Bühmann",
        "Johannes Frey",
        "Daniel Gerber",
        "Lars-Peter Meyer",
        "Claus Stadler",
        "Kirill Bulert"
      ],
      "published": "2025-10-02T16:49:27Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02200v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ARUQULA方法，基于SPINACH框架，利用大语言模型和知识图谱探索工具，通过迭代探索与执行过程将自然语言问题转换为SPARQL查询，降低非专业人士使用知识图谱的门槛。",
      "order": 347
    },
    {
      "arxiv_id": "2510.02190v1",
      "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
      "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.",
      "authors": [
        "Yang Yao",
        "Yixu Wang",
        "Yuxuan Zhang",
        "Yi Lu",
        "Tianle Gu",
        "Lingyu Li",
        "Dingyi Zhao",
        "Keming Wu",
        "Haozhe Wang",
        "Ping Nie",
        "Yan Teng",
        "Yingchun Wang"
      ],
      "published": "2025-10-02T16:40:02Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02190v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对深度研究代理(DRAs)提出了一套严谨的基准测试与多维评估框架，包含214个专家精选查询和10个主题领域，通过语义质量、主题聚焦和检索可信度等指标评估长篇幅报告，验证了DRAs优于增强推理模型但仍有改进空间。",
      "order": 348
    },
    {
      "arxiv_id": "2510.02173v1",
      "title": "Learning to Reason for Hallucination Span Detection",
      "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
      "authors": [
        "Hsuan Su",
        "Ting-Yao Hu",
        "Hema Swetha Koppula",
        "Kundan Krishna",
        "Hadi Pouransari",
        "Cheng-Yu Hsieh",
        "Cem Koc",
        "Joseph Yitan Cheng",
        "Oncel Tuzel",
        "Raviteja Vemulapalli"
      ],
      "published": "2025-10-02T16:24:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02173v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出RL4HS强化学习框架，通过链式思维推理和跨度级奖励函数来检测大语言模型生成的幻觉内容。相比传统二分类方法，该方法能精确定位幻觉片段，在RAGTruth基准测试中优于预训练模型和监督微调。",
      "order": 349
    },
    {
      "arxiv_id": "2510.02172v1",
      "title": "RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with\n  Self-Penalization",
      "summary": "Reinforcement learning with human-annotated data has boosted chain-of-thought\nreasoning in large reasoning models, but these gains come at high costs in\nlabeled data while faltering on harder tasks. A natural next step is\nexperience-driven learning, where models improve without curated labels by\nadapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with\nSelf-restraint), a self-penalizing RL framework that converts the absence of\ngold labels into a useful learning signal. Instead of overcommitting to\nspurious majority votes, RESTRAIN exploits signals from the model's entire\nanswer distribution: penalizing overconfident rollouts and low-consistency\nexamples while preserving promising reasoning chains. The self-penalization\nmechanism integrates seamlessly into policy optimization methods such as GRPO,\nenabling continual self-improvement without supervision. On challenging\nreasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.\nWith Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to\n+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on\nGPQA-Diamond, nearly matching gold-label training while using no gold labels.\nThese results demonstrate that RESTRAIN establishes a scalable path toward\nstronger reasoning without gold labels.",
      "authors": [
        "Zhaoning Yu",
        "Will Su",
        "Leitian Tao",
        "Haozhu Wang",
        "Aashu Singh",
        "Hanchao Yu",
        "Jianyu Wang",
        "Hongyang Gao",
        "Weizhe Yuan",
        "Jason Weston",
        "Ping Yu",
        "Jing Xu"
      ],
      "published": "2025-10-02T16:24:01Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02172v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "RESTRAIN是一种自惩罚强化学习框架，通过利用模型完整答案分布中的信号而非依赖人工标注，在无监督情况下提升推理能力。该方法惩罚过度自信和低一致性样本，在多个推理基准测试中取得显著效果提升，接近有监督训练水平。",
      "order": 350
    },
    {
      "arxiv_id": "2510.02128v1",
      "title": "The Disparate Impacts of Speculative Decoding",
      "summary": "The practice of speculative decoding, whereby inference is probabilistically\nsupported by a smaller, cheaper, ``drafter'' model, has become a standard\ntechnique for systematically reducing the decoding time of large language\nmodels. This paper conducts an analysis of speculative decoding through the\nlens of its potential disparate speed-up rates across tasks. Crucially, the\npaper shows that speed-up gained from speculative decoding is not uniformly\ndistributed across tasks, consistently diminishing for under-fit, and often\nunderrepresented tasks. To better understand this phenomenon, we derive an\nanalysis to quantify this observed ``unfairness'' and draw attention to the\nfactors that motivate such disparate speed-ups to emerge. Further, guided by\nthese insights, the paper proposes a mitigation strategy designed to reduce\nspeed-up disparities and validates the approach across several model pairs,\nrevealing on average a 12% improvement in our fairness metric.",
      "authors": [
        "Jameson Sandler",
        "Ahmet Üstün",
        "Marco Romanelli",
        "Sara Hooker",
        "Ferdinando Fioretto"
      ],
      "published": "2025-10-02T15:38:57Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02128v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文分析推测解码技术在任务间加速效果的不均衡问题，发现该技术对欠拟合和代表性不足任务的加速效果较差，提出量化评估方法并设计缓解策略，使公平性指标平均提升12%。",
      "order": 351
    },
    {
      "arxiv_id": "2510.02125v1",
      "title": "Do AI Models Perform Human-like Abstract Reasoning Across Modalities?",
      "summary": "OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI\nbenchmark, but does that mean state-of-the-art models recognize and reason with\nthe abstractions that the task creators intended? We investigate models'\nabstraction abilities on ConceptARC. We evaluate models under settings that\nvary the input modality (textual vs. visual), whether the model is permitted to\nuse external Python tools, and, for reasoning models, the amount of reasoning\neffort. In addition to measuring output accuracy, we perform fine-grained\nevaluation of the natural-language rules that models generate to explain their\nsolutions. This dual evaluation lets us assess whether models solve tasks using\nthe abstractions ConceptARC was designed to elicit, rather than relying on\nsurface-level patterns. Our results show that, while some models using\ntext-based representations match human output accuracy, the best models' rules\nare often based on surface-level ``shortcuts'' and capture intended\nabstractions far less often than humans. Thus their capabilities for general\nabstract reasoning may be overestimated by evaluations based on accuracy alone.\nIn the visual modality, AI models' output accuracy drops sharply, yet our\nrule-level analysis reveals that models might be underestimated, as they still\nexhibit a substantial share of rules that capture intended abstractions, but\nare often unable to correctly apply these rules. In short, our results show\nthat models still lag humans in abstract reasoning, and that using accuracy\nalone to evaluate abstract reasoning on ARC-like tasks may overestimate\nabstract-reasoning capabilities in textual modalities and underestimate it in\nvisual modalities. We believe that our evaluation framework offers a more\nfaithful picture of multimodal models' abstract reasoning abilities and a more\nprincipled way to track progress toward human-like, abstraction-centered\nintelligence.",
      "authors": [
        "Claas Beger",
        "Ryan Yi",
        "Shuhao Fu",
        "Arseny Moskvichev",
        "Sarah W. Tsai",
        "Sivasankaran Rajamanickam",
        "Melanie Mitchell"
      ],
      "published": "2025-10-02T15:35:10Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02125v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究评估AI模型在ConceptARC基准上的抽象推理能力，发现尽管某些文本模型在准确率上接近人类水平，但其规则分析显示模型多依赖表面特征而非深层抽象推理。视觉模态下模型准确率下降，但仍能生成部分有效抽象规则。研究表明仅凭准确率评估会高估文本模态的推理能力，低估视觉模态潜力，提出了更全面的多模态抽象推理评估框架。",
      "order": 352
    },
    {
      "arxiv_id": "2510.02066v1",
      "title": "Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken\n  Dialogue Systems",
      "summary": "Most end-to-end (E2E) spoken dialogue systems (SDS) rely on voice activity\ndetection (VAD) for turn-taking, but VAD fails to distinguish between pauses\nand turn completions. Duplex SDS models address this by predicting output\ncontinuously, including silence tokens, thus removing the need for explicit\nVAD. However, they often have complex dual-channel architecture and lag behind\ncascaded models in semantic reasoning. To overcome these challenges, we propose\nSCoT: a Streaming Chain-of-Thought (CoT) framework for Duplex SDS, alternating\nbetween processing fixed-duration user input and generating responses in a\nblockwise manner. Using frame-level alignments, we create intermediate\ntargets-aligned user transcripts and system responses for each block.\nExperiments show that our approach produces more coherent and interpretable\nresponses than existing duplex methods while supporting lower-latency and\noverlapping interactions compared to turn-by-turn systems.",
      "authors": [
        "Siddhant Arora",
        "Jinchuan Tian",
        "Hayato Futami",
        "Jiatong Shi",
        "Yosuke Kashiwagi",
        "Emiru Tsunoo",
        "Shinji Watanabe"
      ],
      "published": "2025-10-02T14:33:05Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02066v1",
      "primary_area": "audio_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出SCoT流式思维链框架，用于全双工端到端语音对话系统。通过分块处理用户输入和生成响应，结合帧级对齐创建中间目标，解决了传统VAD无法区分停顿与对话结束的问题。相比现有双工方法，该系统能产生更连贯可解释的响应，同时支持更低延迟和重叠交互。",
      "order": 353
    },
    {
      "arxiv_id": "2510.02044v1",
      "title": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming\n  Tool Usage",
      "summary": "End-to-end speech-in speech-out dialogue systems are emerging as a powerful\nalternative to traditional ASR-LLM-TTS pipelines, generating more natural,\nexpressive responses with significantly lower latency. However, these systems\nremain prone to hallucinations due to limited factual grounding. While\ntext-based dialogue systems address this challenge by integrating tools such as\nweb search and knowledge graph APIs, we introduce the first approach to extend\ntool use directly into speech-in speech-out systems. A key challenge is that\ntool integration substantially increases response latency, disrupting\nconversational flow. To mitigate this, we propose Streaming Retrieval-Augmented\nGeneration (Streaming RAG), a novel framework that reduces user-perceived\nlatency by predicting tool queries in parallel with user speech, even before\nthe user finishes speaking. Specifically, we develop a post-training pipeline\nthat teaches the model when to issue tool calls during ongoing speech and how\nto generate spoken summaries that fuse audio queries with retrieved text\nresults, thereby improving both accuracy and responsiveness. To evaluate our\napproach, we construct AudioCRAG, a benchmark created by converting queries\nfrom the publicly available CRAG dataset into speech form. Experimental results\ndemonstrate that our streaming RAG approach increases QA accuracy by up to 200%\nrelative (from 11.1% to 34.2% absolute) and further enhances user experience by\nreducing tool use latency by 20%. Importantly, our streaming RAG approach is\nmodality-agnostic and can be applied equally to typed input, paving the way for\nmore agentic, real-time AI assistants.",
      "authors": [
        "Siddhant Arora",
        "Haidar Khan",
        "Kai Sun",
        "Xin Luna Dong",
        "Sajal Choudhary",
        "Seungwhan Moon",
        "Xinyuan Zhang",
        "Adithya Sagar",
        "Surya Teja Appini",
        "Kaushik Patnaik",
        "Sanat Sharma",
        "Shinji Watanabe",
        "Anuj Kumar",
        "Ahmed Aly",
        "Yue Liu",
        "Florian Metze",
        "Zhaojiang Lin"
      ],
      "published": "2025-10-02T14:18:20Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02044v1",
      "primary_area": "audio_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Streaming RAG框架，将工具使用扩展至语音对话系统，通过在用户说话时并行预测工具查询来降低延迟，实验显示问答准确率提升200%，延迟减少20%，适用于实时AI助手。",
      "order": 354
    },
    {
      "arxiv_id": "2510.02025v1",
      "title": "Style Over Story: A Process-Oriented Study of Authorial Creativity in\n  Large Language Models",
      "summary": "Evaluations of large language models (LLMs)' creativity have focused\nprimarily on the quality of their outputs rather than the processes that shape\nthem. This study takes a process-oriented approach, drawing on narratology to\nexamine LLMs as computational authors. We introduce constraint-based\ndecision-making as a lens for authorial creativity. Using controlled prompting\nto assign authorial personas, we analyze the creative preferences of the\nmodels. Our findings show that LLMs consistently emphasize Style over other\nelements, including Character, Event, and Setting. By also probing the\nreasoning the models provide for their choices, we show that distinctive\nprofiles emerge across models and argue that our approach provides a novel\nsystematic tool for analyzing AI's authorial creativity.",
      "authors": [
        "Donghoon Jung",
        "Jiwoo Choi",
        "Songeun Chae",
        "Seohyon Jung"
      ],
      "published": "2025-10-02T13:57:14Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02025v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究采用过程导向方法，通过叙事学视角分析大语言模型作为计算作者的创作特性。研究发现LLMs在创作决策中普遍更重视风格而非角色、事件等其他元素，并通过约束提示揭示不同模型的创作偏好特征，为系统分析AI作者创造力提供了新工具。",
      "order": 355
    },
    {
      "arxiv_id": "2510.01995v1",
      "title": "LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and\n  Target",
      "summary": "Online social media platforms are central to everyday communication and\ninformation seeking. While these platforms serve positive purposes, they also\nprovide fertile ground for the spread of hate speech, offensive language, and\nbullying content targeting individuals, organizations, and communities. Such\ncontent undermines safety, participation, and equity online. Reliable detection\nsystems are therefore needed, especially for low-resource languages where\nmoderation tools are limited. In Bangla, prior work has contributed resources\nand models, but most are single-task (e.g., binary hate/offense) with limited\ncoverage of multi-facet signals (type, severity, target). We address these gaps\nby introducing the first multi-task Bangla hate-speech dataset,\nBanglaMultiHate, one of the largest manually annotated corpus to date. Building\non this resource, we conduct a comprehensive, controlled comparison spanning\nclassical baselines, monolingual pretrained models, and LLMs under zero-shot\nprompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a\nlow-resource setting and reveal a consistent trend: although LoRA-tuned LLMs\nare competitive with BanglaBERT, culturally and linguistically grounded\npretraining remains critical for robust performance. Together, our dataset and\nfindings establish a stronger benchmark for developing culturally aligned\nmoderation tools in low-resource contexts. For reproducibility, we will release\nthe dataset and all related scripts.",
      "authors": [
        "Md Arid Hasan",
        "Firoj Alam",
        "Md Fahad Hossain",
        "Usman Naseem",
        "Syed Ishtiaque Ahmed"
      ],
      "published": "2025-10-02T13:17:11Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01995v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究针对孟加拉语提出首个多任务仇恨言论检测数据集BanglaMultiHate，系统比较了传统基线、单语预训练模型和LLM在零样本提示与LoRA微调下的表现。研究发现，尽管LoRA调优的LLM与BanglaBERT性能相当，但基于文化语言背景的预训练对低资源环境下的稳健检测仍至关重要，为开发文化适配的内容审核工具建立了新基准。",
      "order": 356
    },
    {
      "arxiv_id": "2510.01989v1",
      "title": "Exploring Database Normalization Effects on SQL Generation",
      "summary": "Schema design, particularly normalization, is a critical yet often overlooked\nfactor in natural language to SQL (NL2SQL) systems. Most prior research\nevaluates models on fixed schemas, overlooking the influence of design on\nperformance. We present the first systematic study of schema normalization's\nimpact, evaluating eight leading large language models on synthetic and\nreal-world datasets with varied normalization levels. We construct controlled\nsynthetic datasets with formal normalization (1NF-3NF) and real academic paper\ndatasets with practical schemes. Our results show that denormalized schemas\noffer high accuracy on simple retrieval queries, even with cost-effective\nmodels in zero-shot settings. In contrast, normalized schemas (2NF/3NF)\nintroduce challenges such as errors in base table selection and join type\nprediction; however, these issues are substantially mitigated by providing\nfew-shot examples. For aggregation queries, normalized schemas yielded better\nperformance, mainly due to their robustness against the data duplication and\nNULL value issues that cause errors in denormalized schemas. These findings\nsuggest that the optimal schema design for NL2SQL applications depends on the\ntypes of queries to be supported. Our study demonstrates the importance of\nconsidering schema design when developing NL2SQL interfaces and integrating\nadaptive schema selection for real-world scenarios.",
      "authors": [
        "Ryosuke Kohita"
      ],
      "published": "2025-10-02T13:11:30Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01989v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "code_generation",
      "tldr_zh": "本研究首次系统探讨了数据库规范化对自然语言转SQL生成的影响，评估了8个主流大语言模型在不同规范化程度数据集上的表现。研究发现：非规范化模式在简单检索查询中准确率高，而规范化模式（2NF/3NF）虽在基础表选择和连接类型预测上存在挑战，但通过少量示例可显著改善；对于聚合查询，规范化模式因避免数据冗余和NULL值问题表现更优。研究表明NL2SQL应用的最佳模式设计取决于查询类型，强调了实际场景中自适应模式选择的重要性。",
      "order": 357
    },
    {
      "arxiv_id": "2510.01976v1",
      "title": "Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion,\n  Argument, and Topic Annotations",
      "summary": "Our interpretation of value concepts is shaped by our sociocultural\nbackground and lived experiences, and is thus subjective. Recognizing\nindividual value interpretations is important for developing AI systems that\ncan align with diverse human perspectives and avoid bias toward majority\nviewpoints. To this end, we investigate whether a language model can predict\nindividual value interpretations by leveraging multi-dimensional subjective\nannotations as a proxy for their interpretive lens. That is, we evaluate\nwhether providing examples of how an individual annotates Sentiment, Emotion,\nArgument, and Topics (SEAT dimensions) helps a language model in predicting\ntheir value interpretations. Our experiment across different zero- and few-shot\nsettings demonstrates that providing all SEAT dimensions simultaneously yields\nsuperior performance compared to individual dimensions and a baseline where no\ninformation about the individual is provided. Furthermore, individual\nvariations across annotators highlight the importance of accounting for the\nincorporation of individual subjective annotators. To the best of our\nknowledge, this controlled setting, although small in size, is the first\nattempt to go beyond demographics and investigate the impact of annotation\nbehavior on value prediction, providing a solid foundation for future\nlarge-scale validation.",
      "authors": [
        "Adina Nicola Dobrinoiu",
        "Ana Cristiana Marcu",
        "Amir Homayounirad",
        "Luciano Cavalcante Siebert",
        "Enrico Liscio"
      ],
      "published": "2025-10-02T12:51:33Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01976v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出SEAT框架，通过情感、情绪、论证和主题等多维度主观标注作为个体解释视角的代理，探索语言模型预测个人价值观解释的能力。实验表明同时使用所有SEAT维度在零样本和少样本设置下表现最优，强调了考虑个体主观标注差异对AI系统与多样化人类视角对齐的重要性。",
      "order": 358
    },
    {
      "arxiv_id": "2510.01932v1",
      "title": "Veri-R1: Toward Precise and Faithful Claim Verification via Online\n  Reinforcement Learning",
      "summary": "Claim verification with large language models (LLMs) has recently attracted\nconsiderable attention, owing to their superior reasoning capabilities and\ntransparent verification pathways compared to traditional answer-only\njudgments. Online claim verification requires iterative evidence retrieval and\nreasoning, yet existing approaches mainly rely on prompt engineering or\npredesigned reasoning workflows without offering a unified training paradigm to\nimprove necessary skills. Therefore, we introduce Veri-R1, an online\nreinforcement learning (RL) framework that enables an LLM to interact with a\nsearch engine and to receive reward signals that explicitly shape its planning,\nretrieval, and reasoning behaviors. The dynamic interaction between models and\nretrieval systems more accurately reflects real-world verification scenarios\nand fosters comprehensive verification skills. Empirical results show that\nVeri-R1 improves joint accuracy by up to 30% and doubles evidence score, often\nsurpassing larger-scale counterparts. Ablation studies further reveal the\nimpact of reward components and the link between output logits and label\naccuracy. Our results highlight the effectiveness of online RL for precise and\nfaithful claim verification and provide a foundation for future research. We\nrelease our code to support community progress in LLM empowered claim\nverification.",
      "authors": [
        "Qi He",
        "Cheng Qian",
        "Xiusi Chen",
        "Bingxiang He",
        "Yi R.",
        "Fung",
        "Heng Ji"
      ],
      "published": "2025-10-02T11:49:48Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01932v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "Veri-R1是一种基于在线强化学习的框架，通过让大语言模型与搜索引擎交互并接收奖励信号，提升其在声明验证中的规划、检索和推理能力。实验表明该方法将联合准确率提升高达30%，证据分数翻倍，优于更大规模的模型，为精确可信的声明验证提供了有效解决方案。",
      "order": 359
    },
    {
      "arxiv_id": "2510.01929v1",
      "title": "Inverse Language Modeling towards Robust and Grounded LLMs",
      "summary": "The current landscape of defensive mechanisms for LLMs is fragmented and\nunderdeveloped, unlike prior work on classifiers. To further promote\nadversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a\nunified framework that simultaneously 1) improves the robustness of LLMs to\ninput perturbations, and, at the same time, 2) enables native grounding by\ninverting model outputs to identify potentially toxic or unsafe input triggers.\nILM transforms LLMs from static generators into analyzable and robust systems,\npotentially helping RED teaming. ILM can lay the foundation for next-generation\nLLMs that are not only robust and grounded but also fundamentally more\ncontrollable and trustworthy. The code is publicly available at\ngithub.com/davegabe/pag-llm.",
      "authors": [
        "Davide Gabrielli",
        "Simone Sestito",
        "Iacopo Masi"
      ],
      "published": "2025-10-02T11:47:18Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01929v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出逆向语言建模(ILM)统一框架，通过反转模型输出来识别潜在有害输入，同时提升LLM对抗扰动的鲁棒性，实现原生基础化，将静态生成器转变为可分析、鲁棒的系统，为构建更可控可信的下一代LLM奠定基础。",
      "order": 360
    },
    {
      "arxiv_id": "2510.01925v1",
      "title": "Enhancing Large Language Model Reasoning with Reward Models: An\n  Analytical Survey",
      "summary": "Reward models (RMs) play a critical role in enhancing the reasoning\nperformance of LLMs. For example, they can provide training signals to finetune\nLLMs during reinforcement learning (RL) and help select the best answer from\nmultiple candidates during inference. In this paper, we provide a systematic\nintroduction to RMs, along with a comprehensive survey of their applications in\nLLM reasoning. We first review fundamental concepts of RMs, including their\narchitectures, training methodologies, and evaluation techniques. Then, we\nexplore their key applications: (1) guiding generation and selecting optimal\noutputs during LLM inference, (2) facilitating data synthesis and iterative\nself-improvement for LLMs, and (3) providing training signals in RL-based\nfinetuning. Finally, we address critical open questions regarding the\nselection, generalization, evaluation, and enhancement of RMs, based on\nexisting research and our own empirical findings. Our analysis aims to provide\nactionable insights for the effective deployment and advancement of RMs for LLM\nreasoning.",
      "authors": [
        "Qiyuan Liu",
        "Hao Xu",
        "Xuhong Chen",
        "Wei Chen",
        "Yee Whye Teh",
        "Ning Miao"
      ],
      "published": "2025-10-02T11:42:17Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01925v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文系统综述了奖励模型在增强大语言模型推理能力中的应用，涵盖其架构设计、训练方法、评估技术，以及在推理引导、数据合成、强化学习微调等关键场景的使用，并探讨了奖励模型选择、泛化与优化等开放性问题。",
      "order": 361
    },
    {
      "arxiv_id": "2510.01902v1",
      "title": "Constrained Adaptive Rejection Sampling",
      "summary": "Language Models (LMs) are increasingly used in applications where generated\noutputs must satisfy strict semantic or syntactic constraints. Existing\napproaches to constrained generation fall along a spectrum: greedy constrained\ndecoding methods enforce validity during decoding but distort the LM's\ndistribution, while rejection sampling (RS) preserves fidelity but wastes\ncomputation by discarding invalid outputs. Both extremes are problematic in\ndomains such as program fuzzing, where both validity and diversity of samples\nare essential. We present Constrained Adaptive Rejection Sampling (CARS), an\napproach that strictly improves the sample-efficiency of RS without\ndistributional distortion. CARS begins with unconstrained LM sampling and\nadaptively rules out constraint-violating continuations by recording them in a\ntrie and subtracting their probability mass from future draws. This adaptive\npruning ensures that prefixes proven invalid are never revisited, acceptance\nrates improve monotonically, and the resulting samples exactly follow the\nconstrained distribution. In experiments on a variety of domains -- e.g.,\nprogram fuzzing and molecular generation -- CARS consistently achieves higher\nefficiency -- measured in the number of LM forward passes per valid sample --\nwhile also producing stronger sample diversity than both GCD and methods that\napproximate the LM's distribution.",
      "authors": [
        "Paweł Parys",
        "Sairam Vaidya",
        "Taylor Berg-Kirkpatrick",
        "Loris D'Antoni"
      ],
      "published": "2025-10-02T11:17:26Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01902v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "code_generation",
      "tldr_zh": "本文提出约束自适应拒绝采样(CARS)方法，在保持语言模型分布不变的前提下，通过自适应剪枝技术排除违反约束的文本延续，显著提升采样效率。该方法在程序模糊测试和分子生成等需要满足严格约束的领域表现出色，相比现有方法在效率和样本多样性方面均有优势。",
      "order": 362
    },
    {
      "arxiv_id": "2510.01879v1",
      "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration",
      "summary": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs.",
      "authors": [
        "Yisu Wang",
        "Ming Wang",
        "Haoyuan Song",
        "Wenjie Huang",
        "Chaozheng Wang",
        "Yi Xie",
        "Xuming Ran"
      ],
      "published": "2025-10-02T10:35:39Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01879v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "REPAIR是一种终身编辑框架，通过渐进式自适应干预和重整合机制解决大语言模型后训练中的高成本和副作用问题。该框架采用闭环反馈和动态内存管理来缓解大规模连续编辑的不稳定性，通过频繁知识融合和强局部性保护减少意外连锁效应。实验表明REPAIR在多个模型家族中提升编辑准确率10%-30%，显著减少知识遗忘。",
      "order": 363
    },
    {
      "arxiv_id": "2510.01845v1",
      "title": "Model Merging to Maintain Language-Only Performance in Developmentally\n  Plausible Multimodal Models",
      "summary": "State-of-the-art vision-and-language models consist of many parameters and\nlearn from enormous datasets, surpassing the amounts of linguistic data that\nchildren are exposed to as they acquire a language. This paper presents our\napproach to the multimodal track of the BabyLM challenge addressing this\ndiscrepancy. We develop language-only and multimodal models in low-resource\nsettings using developmentally plausible datasets, with our multimodal models\noutperforming previous BabyLM baselines. One finding in the multimodal language\nmodel literature is that these models tend to underperform in\n\\textit{language-only} tasks. Therefore, we focus on maintaining language-only\nabilities in multimodal models. To this end, we experiment with \\textit{model\nmerging}, where we fuse the parameters of multimodal models with those of\nlanguage-only models using weighted linear interpolation. Our results\ncorroborate the findings that multimodal models underperform in language-only\nbenchmarks that focus on grammar, and model merging with text-only models can\nhelp alleviate this problem to some extent, while maintaining multimodal\nperformance.",
      "authors": [
        "Ece Takmaz",
        "Lisa Bylinina",
        "Jakub Dotlacil"
      ],
      "published": "2025-10-02T09:38:25Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01845v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究针对多模态模型在纯语言任务中表现不佳的问题，提出模型融合方法，通过加权线性插值将多模态模型与纯语言模型参数融合，在保持多模态性能的同时提升语言能力，基于BabyLM挑战的低资源发展合理数据集验证了方法的有效性。",
      "order": 364
    },
    {
      "arxiv_id": "2510.01833v1",
      "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for\n  LLM Reasoning",
      "summary": "Large language models (LLMs) have demonstrated remarkable reasoning abilities\nin complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,\ndue to their autoregressive token-level generation, the reasoning process is\nlargely constrained to local decision-making and lacks global planning. This\nlimitation frequently results in redundant, incoherent, or inaccurate\nreasoning, which significantly degrades overall performance. Existing\napproaches, such as tree-based algorithms and reinforcement learning (RL),\nattempt to address this issue but suffer from high computational costs and\noften fail to produce optimal reasoning trajectories. To tackle this challenge,\nwe propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy\nOptimization PTA-GRPO, a two-stage framework designed to improve both\nhigh-level planning and fine-grained CoT reasoning. In the first stage, we\nleverage advanced LLMs to distill CoT into compact high-level guidance, which\nis then used for supervised fine-tuning (SFT). In the second stage, we\nintroduce a guidance-aware RL method that jointly optimizes the final output\nand the quality of high-level guidance, thereby enhancing reasoning\neffectiveness. We conduct extensive experiments on multiple mathematical\nreasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across\ndiverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and\nLLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently\nachieves stable and significant improvements across different models and tasks,\nvalidating its effectiveness and generalization.",
      "authors": [
        "Zhihao Dou",
        "Qinjian Zhao",
        "Zhongwei Wan",
        "Dinggen Zhang",
        "Weida Wang",
        "Towsif Raiyan",
        "Benteng Chen",
        "Qingtao Pan",
        "Yang Ouyang",
        "Zhiqiang Gao",
        "Shufei Zhang",
        "Sumon Biswas"
      ],
      "published": "2025-10-02T09:28:13Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01833v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出PTA-GRPO两阶段框架，通过高层规划指导增强大语言模型的推理能力。第一阶段利用先进LLM将思维链提炼为紧凑的高层指导进行监督微调；第二阶段引入指导感知的强化学习方法，联合优化最终输出与高层指导质量。在多个数学推理基准测试中验证了该方法能稳定提升不同模型的推理性能。",
      "order": 365
    },
    {
      "arxiv_id": "2510.01832v1",
      "title": "SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with\n  Reinforcement Learning",
      "summary": "Semi-structured content in HTML tables, lists, and infoboxes accounts for a\nsubstantial share of factual data on the web, yet the formatting complicates\nusage, and reliably extracting structured information from them remains\nchallenging. Existing methods either lack generalization or are\nresource-intensive due to per-page LLM inference. In this paper, we introduce\nSCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novel\nreinforcement learning framework that leverages layout similarity across\nwebpages within the same site as a reward signal. Instead of processing each\npage individually, SCRIBES generates reusable extraction scripts that can be\napplied to groups of structurally similar webpages. Our approach further\nimproves by iteratively training on synthetic annotations from in-the-wild\nCommonCrawl data. Experiments show that our approach outperforms strong\nbaselines by over 13% in script quality and boosts downstream question\nanswering accuracy by more than 4% for GPT-4o, enabling scalable and\nresource-efficient web information extraction.",
      "authors": [
        "Shicheng Liu",
        "Kai Sun",
        "Lisheng Fu",
        "Xilun Chen",
        "Xinyuan Zhang",
        "Zhaojiang Lin",
        "Rulin Shao",
        "Yue Liu",
        "Anuj Kumar",
        "Wen-tau Yih",
        "Xin Luna Dong"
      ],
      "published": "2025-10-02T09:27:15Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01832v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "SCRIBES是一种基于强化学习的网络规模半结构化数据提取框架，利用网页布局相似性作为奖励信号生成可重用提取脚本，相比传统方法提升脚本质量13%以上，并在问答任务中提升GPT-4o准确率4%，实现高效可扩展的网页信息提取。",
      "order": 366
    },
    {
      "arxiv_id": "2510.01831v1",
      "title": "Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical\n  Errors",
      "summary": "Large Language Models (LLMs) demonstrate strong mathematical problem-solving\nabilities but frequently fail on problems that deviate syntactically from their\ntraining distribution. We identify a systematic failure mode, syntactic blind\nspots, in which models misapply familiar reasoning strategies to problems that\nare semantically straightforward but phrased in unfamiliar ways. These errors\nare not due to gaps in mathematical competence, but rather reflect a brittle\ncoupling between surface form and internal representation. To test this, we\nrephrase incorrectly answered questions using syntactic templates drawn from\ncorrect examples. These rephrasings, which preserve semantics while reducing\nstructural complexity, often lead to correct answers. We quantify syntactic\ncomplexity using a metric based on Dependency Locality Theory (DLT), and show\nthat higher DLT scores are associated with increased failure rates across\nmultiple datasets. Our findings suggest that many reasoning errors stem from\nstructural misalignment rather than conceptual difficulty, and that\nsyntax-aware interventions can reveal and mitigate these inductive failures.",
      "authors": [
        "Dane Williamson",
        "Yangfeng Ji",
        "Matthew Dwyer"
      ],
      "published": "2025-10-02T09:26:26Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01831v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "研究发现大型语言模型在数学问题解决中存在'句法盲点'：当问题表述偏离训练数据分布时，即使语义简单，模型也会错误应用推理策略。通过基于依存局部理论的复杂度度量，证实句法复杂性与错误率正相关，表明这些错误源于结构错位而非概念理解不足。",
      "order": 367
    },
    {
      "arxiv_id": "2510.01817v1",
      "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention\n  Mechanism with Query Heads Reduction",
      "summary": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA)\nmechanism, has become the de facto standard for state-of-the-art models in\nartificial intelligence. However, the quadratic computational complexity of MHA\nwith respect to sequence length presents a significant barrier to scaling,\nparticularly for applications involving long contexts. Prevailing solutions,\nsuch as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have\neffectively addressed the memory bandwidth bottleneck that dominates\nautoregressive inference latency by sharing Key and Value projections. While\nhighly successful, these methods do not reduce the fundamental number of\nfloating-point operations (FLOPs) required for the attention score computation,\nwhich remains a critical bottleneck for training and full-sequence processing.\nThis paper introduces Sparse Query Attention (SQA), a novel attention\narchitecture that pursues an alternative and complementary optimization path.\nInstead of reducing Key/Value heads, SQA reduces the number of Query heads.\nThis architectural modification directly decreases the computational complexity\nof the attention mechanism by a factor proportional to the reduction in query\nheads, thereby lowering the overall FLOPs. This work presents the theoretical\nfoundation of SQA, its mathematical formulation, and a family of architectural\nvariants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate\nthat SQA can achieve significant throughput improvements of up to 3x in\ncomputation-bound scenarios such as model pre-training, fine-tuning, and\nencoder-based tasks, with only a minimal impact on model quality in preliminary\nsmallscale experiments. SQA was discovered serendipitously during the\ndevelopment of the upcoming Reactive Transformer architecture, suggesting its\npotential as a powerful tool for building more efficient and scalable models",
      "authors": [
        "Adam Filipek"
      ],
      "published": "2025-10-02T09:01:38Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01817v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出稀疏查询注意力(SQA)机制，通过减少查询头数量直接降低注意力计算复杂度，在长序列处理(32k-200k tokens)中实现高达3倍的计算吞吐量提升，对模型质量影响极小。该方法与MQA/GQA形成互补优化路径，特别适用于预训练、微调和编码器任务等计算密集型场景。",
      "order": 368
    },
    {
      "arxiv_id": "2510.01801v1",
      "title": "Detecting LLM-Generated Spam Reviews by Integrating Language Model\n  Embeddings and Graph Neural Network",
      "summary": "The rise of large language models (LLMs) has enabled the generation of highly\npersuasive spam reviews that closely mimic human writing. These reviews pose\nsignificant challenges for existing detection systems and threaten the\ncredibility of online platforms. In this work, we first create three realistic\nLLM-generated spam review datasets using three distinct LLMs, each guided by\nproduct metadata and genuine reference reviews. Evaluations by GPT-4.1 confirm\nthe high persuasion and deceptive potential of these reviews. To address this\nthreat, we propose FraudSquad, a hybrid detection model that integrates text\nembeddings from a pre-trained language model with a gated graph transformer for\nspam node classification. FraudSquad captures both semantic and behavioral\nsignals without relying on manual feature engineering or massive training\nresources. Experiments show that FraudSquad outperforms state-of-the-art\nbaselines by up to 44.22% in precision and 43.01% in recall on three\nLLM-generated datasets, while also achieving promising results on two\nhuman-written spam datasets. Furthermore, FraudSquad maintains a modest model\nsize and requires minimal labeled training data, making it a practical solution\nfor real-world applications. Our contributions include new synthetic datasets,\na practical detection framework, and empirical evidence highlighting the\nurgency of adapting spam detection to the LLM era. Our code and datasets are\navailable at: https://anonymous.4open.science/r/FraudSquad-5389/.",
      "authors": [
        "Xin Liu",
        "Rongwu Xu",
        "Xinyi Jia",
        "Jason Liao",
        "Jiao Sun",
        "Ling Huang",
        "Wei Xu"
      ],
      "published": "2025-10-02T08:42:35Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01801v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出FraudSquad混合检测模型，结合预训练语言模型文本嵌入和图神经网络，有效检测LLM生成的垃圾评论。在三个LLM生成数据集上比现有方法精确度提升44.22%，召回率提升43.01%，模型轻量且所需标注数据少，适用于实际部署。",
      "order": 369
    },
    {
      "arxiv_id": "2510.01792v1",
      "title": "Comparison of Unsupervised Metrics for Evaluating Judicial Decision\n  Extraction",
      "summary": "The rapid advancement of artificial intelligence in legal natural language\nprocessing demands scalable methods for evaluating text extraction from\njudicial decisions. This study evaluates 16 unsupervised metrics, including\nnovel formulations, to assess the quality of extracting seven semantic blocks\nfrom 1,000 anonymized Russian judicial decisions, validated against 7,168\nexpert reviews on a 1--5 Likert scale. These metrics, spanning document-based,\nsemantic, structural, pseudo-ground truth, and legal-specific categories,\noperate without pre-annotated ground truth. Bootstrapped correlations, Lin's\nconcordance correlation coefficient (CCC), and mean absolute error (MAE) reveal\nthat Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =\n0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =\n0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density\n(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative\ncorrelations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin\nCCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using\ngpt-4.1-mini via g4f, suggests limited specialization for legal textse. These\nfindings highlight that unsupervised metrics, including LLM-based approaches,\nenable scalable screening but, with moderate correlations and low CCC values,\ncannot fully replace human judgment in high-stakes legal contexts. This work\nadvances legal NLP by providing annotation-free evaluation tools, with\nimplications for judicial analytics and ethical AI deployment.",
      "authors": [
        "Ivan Leonidovich Litvak",
        "Anton Kostin",
        "Fedor Lashkin",
        "Tatiana Maksiyan",
        "Sergey Lagutin"
      ],
      "published": "2025-10-02T08:32:16Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01792v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "legal_ai",
      "tldr_zh": "本研究评估了16种无监督指标在司法文书语义块提取任务中的表现，基于1000份俄罗斯司法文书和7168份专家评分。研究发现术语频率一致性和覆盖率/块完整性指标与专家评分最为一致，而法律术语密度呈现负相关。LLM评估分数表现中等，表明无监督指标可用于规模化筛查，但在高风险的司法场景中尚不能完全替代人工判断。",
      "order": 370
    },
    {
      "arxiv_id": "2510.01782v1",
      "title": "Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware\n  Refusal in Factual Tasks",
      "summary": "Large Language Models (LLMs) should refuse to answer questions beyond their\nknowledge. This capability, which we term knowledge-aware refusal, is crucial\nfor factual reliability. However, existing metrics fail to faithfully measure\nthis ability. On the one hand, simple refusal-based metrics are biased by\nrefusal rates and yield inconsistent scores when models exhibit different\nrefusal tendencies. On the other hand, existing calibration metrics are\nproxy-based, capturing the performance of auxiliary calibration processes\nrather than the model's actual refusal behavior. In this work, we propose the\nRefusal Index (RI), a principled metric that measures how accurately LLMs\nrefuse questions they do not know. We define RI as Spearman's rank correlation\nbetween refusal probability and error probability. To make RI practically\nmeasurable, we design a lightweight two-pass evaluation method that efficiently\nestimates RI from observed refusal rates across two standard evaluation runs.\nExtensive experiments across 16 models and 5 datasets demonstrate that RI\naccurately quantifies a model's intrinsic knowledge-aware refusal capability in\nfactual tasks. Notably, RI remains stable across different refusal rates and\nprovides consistent model rankings independent of a model's overall accuracy\nand refusal rates. More importantly, RI provides insight into an important but\npreviously overlooked aspect of LLM factuality: while LLMs achieve high\naccuracy on factual tasks, their refusal behavior can be unreliable and\nfragile. This finding highlights the need to complement traditional accuracy\nmetrics with the Refusal Index for comprehensive factuality evaluation.",
      "authors": [
        "Wenbo Pan",
        "Jie Xu",
        "Qiguang Chen",
        "Junhao Dong",
        "Libo Qin",
        "Xinfeng Li",
        "Haining Yu",
        "Xiaohua Jia"
      ],
      "published": "2025-10-02T08:20:36Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01782v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Refusal Index(RI)指标，用于衡量大语言模型在事实性任务中拒绝回答未知问题的能力。RI通过拒绝概率与错误概率的秩相关性计算，实验表明该指标能稳定评估模型的知识感知拒绝能力，揭示LLMs即使准确率高，其拒绝行为仍不可靠。",
      "order": 371
    },
    {
      "arxiv_id": "2510.01736v1",
      "title": "Machine-interpretable Engineering Design Standards for Valve\n  Specification",
      "summary": "Engineering design processes use technical specifications and must comply\nwith standards. Product specifications, product type data sheets, and design\nstandards are still mainly document-centric despite the ambition to digitalize\nindustrial work. In this paper, we demonstrate how to transform information\nheld in engineering design standards into modular, reusable,\nmachine-interpretable ontologies and use the ontologies in quality assurance of\nthe plant design and equipment selection process. We use modelling patterns to\ncreate modular ontologies for knowledge captured in the text and in frequently\nreferenced tables in International Standards for piping, material and valve\ndesign. These modules are exchangeable, as stored in a W3C compliant format,\nand interoperable as they are aligned with the top-level ontology ISO DIS\n23726-3: Industrial Data Ontology (IDO).\n  We test these ontologies, created based on international material and piping\nstandards and industry norms, on a valve selection process. Valves are\ninstantiated in semantic asset models as individuals along with a semantic\nrepresentation of the environmental condition at their location on the asset.\nWe create \"functional location tags\" as OWL individuals that become instances\nof OWL class Valve Data Sheet (VDS) specified valves. Similarly we create\ninstances of manufacturer product type. Our approach enables automated\nvalidation that a specific VDS is compliant with relevant industry standards.\nUsing semantic reasoning and executable design rules, we also determine whether\nthe product type meets the valve specification. Creation of shared, reusable\nIDO-based modular ontologies for design standards enables semantic reasoning to\nbe applied to equipment selection processes and demonstrates the potential of\nthis approach for Standards Bodies wanting to transition to digitized Smart\nStandards.",
      "authors": [
        "Anders Gjerver",
        "Rune Frostad",
        "Vedrana Barisic",
        "Melinda Hodkiewicz",
        "Caitlin Woods",
        "Mihaly Fekete",
        "Arild Braathen Torjusen",
        "Johan Wilhelm Kluwer"
      ],
      "published": "2025-10-02T07:20:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01736v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出将工程设计标准转化为模块化、可复用、机器可解释的本体，应用于阀门选型过程的质量保证。通过建立符合W3C标准的本体模块，结合语义推理和可执行设计规则，实现阀门规格的自动化验证，展示了工业标准数字化应用的潜力。",
      "order": 372
    },
    {
      "arxiv_id": "2510.01719v1",
      "title": "What MLLMs Learn about When they Learn about Multimodal Reasoning:\n  Perception, Reasoning, or their Integration?",
      "summary": "Multimodal reasoning models have recently shown promise on challenging\ndomains such as olympiad-level geometry, yet their evaluation remains dominated\nby aggregate accuracy, a single score that obscures where and how models are\nimproving. We introduce MathLens, a benchmark designed to disentangle the\nsubskills of multimodal reasoning while preserving the complexity of\ntextbook-style geometry problems. The benchmark separates performance into\nthree components: Perception: extracting information from raw inputs,\nReasoning: operating on available information, and Integration: selecting\nrelevant perceptual evidence and applying it within reasoning. To support each\ntest, we provide annotations: visual diagrams, textual descriptions to evaluate\nreasoning in isolation, controlled questions that require both modalities, and\nprobes for fine-grained perceptual skills, all derived from symbolic\nspecifications of the problems to ensure consistency and robustness. Our\nanalysis reveals that different training approaches have uneven effects: First,\nreinforcement learning chiefly strengthens perception, especially when\nsupported by textual supervision, while textual SFT indirectly improves\nperception through reflective reasoning. Second, reasoning improves only in\ntandem with perception. Third, integration remains the weakest capacity, with\nresidual errors concentrated there once other skills advance. Finally,\nrobustness diverges: RL improves consistency under diagram variation, whereas\nmultimodal SFT reduces it through overfitting. We will release all data and\nexperimental logs.",
      "authors": [
        "Jiwan Chung",
        "Neel Joshi",
        "Pratyusha Sharma",
        "Youngjae Yu",
        "Vibhav Vineet"
      ],
      "published": "2025-10-02T06:58:29Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01719v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "education_ai",
      "tldr_zh": "本研究提出MathLens基准，用于解构多模态推理模型的三个核心能力：感知（信息提取）、推理（信息处理）和整合（感知与推理结合）。研究发现：强化学习主要提升感知能力，文本监督微调通过反思推理间接改善感知；推理能力需与感知同步提升；整合能力最为薄弱；不同训练方法在鲁棒性上表现各异。",
      "order": 373
    },
    {
      "arxiv_id": "2510.01688v1",
      "title": "Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation",
      "summary": "Recent advances in Large Language Models (LLMs) have brought significant\nimprovements to various service domains, including chatbots and medical\npre-consultation applications. In the healthcare domain, the most common\napproach for adapting LLMs to multi-turn dialogue generation is Supervised\nFine-Tuning (SFT). However, datasets for SFT in tasks like medical\npre-consultation typically exhibit a skewed turn-count distribution. Training\non such data induces a novel failure mechanism we term **Format Inertia**,\nwhere models tend to generate repetitive, format-correct, but diagnostically\nuninformative questions in long medical dialogues. To mitigate this observed\nfailure mechanism, we adopt a simple, data-centric method that rebalances the\nturn-count distribution of the training dataset. Experimental results show that\nour approach substantially alleviates Format Inertia in medical\npre-consultation.",
      "authors": [
        "Seungseop Lim",
        "Gibaeg Kim",
        "Wooseok Han",
        "Jean Seo",
        "Hyunkyung Lee",
        "Jaehyo Yoo",
        "Eunho Yang"
      ],
      "published": "2025-10-02T05:29:38Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01688v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出大语言模型在医疗预咨询中存在'格式惯性'失效机制，即模型在长对话中倾向于生成重复但诊断信息不足的问题。通过重新平衡训练数据的轮次分布，可有效缓解此问题。",
      "order": 374
    },
    {
      "arxiv_id": "2510.01687v1",
      "title": "Improving AGI Evaluation: A Data Science Perspective",
      "summary": "Evaluation of potential AGI systems and methods is difficult due to the\nbreadth of the engineering goal. We have no methods for perfect evaluation of\nthe end state, and instead measure performance on small tests designed to\nprovide directional indication that we are approaching AGI. In this work we\nargue that AGI evaluation methods have been dominated by a design philosophy\nthat uses our intuitions of what intelligence is to create synthetic tasks,\nthat have performed poorly in the history of AI. Instead we argue for an\nalternative design philosophy focused on evaluating robust task execution that\nseeks to demonstrate AGI through competence. This perspective is developed from\ncommon practices in data science that are used to show that a system can be\nreliably deployed. We provide practical examples of what this would mean for\nAGI evaluation.",
      "authors": [
        "John Hawkins"
      ],
      "published": "2025-10-02T05:27:29Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01687v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文从数据科学视角提出改进AGI评估方法，批判当前基于直觉设计合成任务的评估范式，主张采用注重稳健任务执行能力的评估哲学，借鉴数据科学中系统可靠部署的实践标准，为AGI评估提供具体实施方案。",
      "order": 375
    },
    {
      "arxiv_id": "2510.01685v1",
      "title": "How Do Language Models Compose Functions?",
      "summary": "While large language models (LLMs) appear to be increasingly capable of\nsolving compositional tasks, it is an open question whether they do so using\ncompositional mechanisms. In this work, we investigate how feedforward LLMs\nsolve two-hop factual recall tasks, which can be expressed compositionally as\n$g(f(x))$. We first confirm that modern LLMs continue to suffer from the\n\"compositionality gap\": i.e. their ability to compute both $z = f(x)$ and $y =\ng(z)$ does not entail their ability to compute the composition $y = g(f(x))$.\nThen, using logit lens on their residual stream activations, we identify two\nprocessing mechanisms, one which solves tasks $\\textit{compositionally}$,\ncomputing $f(x)$ along the way to computing $g(f(x))$, and one which solves\nthem $\\textit{directly}$, without any detectable signature of the intermediate\nvariable $f(x)$. Finally, we find that which mechanism is employed appears to\nbe related to the embedding space geometry, with the idiomatic mechanism being\ndominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in\nthe embedding spaces. We fully release our data and code at:\nhttps://github.com/apoorvkh/composing-functions .",
      "authors": [
        "Apoorv Khandelwal",
        "Ellie Pavlick"
      ],
      "published": "2025-10-02T05:21:34Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01685v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探讨语言模型如何执行组合任务，发现模型存在'组合性差距'，即能分别计算f(x)和g(z)不代表能计算g(f(x))。通过分析残差流激活，识别出两种处理机制：组合式（计算中间结果f(x)）和直接式（无中间变量痕迹）。机制选择与嵌入空间几何相关，当存在从x到g(f(x))的线性映射时，直接式机制占主导。",
      "order": 376
    },
    {
      "arxiv_id": "2510.01674v1",
      "title": "FOR-Prompting: From Objection to Revision via an Asymmetric Prompting\n  Protocol",
      "summary": "Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)\norganize internal deliberation but lack an explicit mechanism for external\nquestioning that elicits self-revision. We present FOR-Prompting (From\nObjection to Revision Prompting), an asymmetric protocol where a Defender\nproposes an answer, an Objectioner raises question-style objections with no\ndirect fixes, and a Host enforces consistency and closure. On GSM8K we observe\nabout a 22% point gain over single-prompt and accuracy on par with CoT, with\nmore than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1\njudge. FOR-Prompting also corrects mistakes without tools or human supervision\non tricky queries, and improves performance for small-scale model (approx. 19%\naccuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for\nsmall models and on personal device use. Beyond factual QA, qualitative\nanalyses on open-ended tasks show enhanced exploration and refinement, with\ndialogue traces that make assumptions and trade-offs explicit. The protocol is\nmodel agnostic and operates purely at the prompt level through role-structured\nturns, so it works with hosted and local models of different sizes without\nretraining, and it supports large-scale study of objection-guided reasoning.",
      "authors": [
        "He Zhang",
        "Anzhou Zhang",
        "Jian Dai"
      ],
      "published": "2025-10-02T04:57:58Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01674v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "FOR-Prompting是一种非对称推理协议，通过辩护者、质疑者和主持者三个角色实现自我修正。在GSM8K上相比单提示提升约22%，推理连贯性评分提高10%以上，能无监督修正错误，尤其提升小模型性能（Llama3.2:1b准确率提升19%）。该协议无需微调，适用于不同规模模型，支持开放式任务的探索优化。",
      "order": 377
    },
    {
      "arxiv_id": "2510.01670v1",
      "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
      "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.",
      "authors": [
        "Erfan Shayegani",
        "Keegan Hines",
        "Yue Dong",
        "Nael Abu-Ghazaleh",
        "Roman Lutz",
        "Spencer Whitehead",
        "Vidhisha Balachandran",
        "Besmira Nushi",
        "Vibhav Vineet"
      ],
      "published": "2025-10-02T04:52:15Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01670v1",
      "primary_area": "vla_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究发现计算机使用代理(CUAs)普遍存在盲目目标导向(BGD)现象，即不顾可行性、安全性和上下文盲目追求目标。作者开发了BLIND-ACT基准测试，在9个前沿模型中发现平均80.8%的BGD率，揭示了执行优先偏见、思维行动脱节等风险模式，强调了改进训练和推理干预的必要性。",
      "order": 378
    },
    {
      "arxiv_id": "2510.01659v1",
      "title": "MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue\n  Summarization",
      "summary": "Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging\napplications. To support the development of effective MDS models, robust\nautomatic evaluation methods are essential for reducing both cost and human\neffort. However, such methods require a strong meta-evaluation benchmark\ngrounded in human annotations. In this work, we introduce MDSEval, the first\nmeta-evaluation benchmark for MDS, consisting image-sharing dialogues,\ncorresponding summaries, and human judgments across eight well-defined quality\naspects. To ensure data quality and richfulness, we propose a novel filtering\nframework leveraging Mutually Exclusive Key Information (MEKI) across\nmodalities. Our work is the first to identify and formalize key evaluation\ndimensions specific to MDS. We benchmark state-of-the-art modal evaluation\nmethods, revealing their limitations in distinguishing summaries from advanced\nMLLMs and their susceptibility to various bias.",
      "authors": [
        "Yinhong Liu",
        "Jianfeng He",
        "Hang Su",
        "Ruixue Lian",
        "Yi Nian",
        "Jake Vincent",
        "Srikanth Vishnubhotla",
        "Robinson Piramuthu",
        "Saab Mansour"
      ],
      "published": "2025-10-02T04:38:27Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01659v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MDSEval——首个多模态对话摘要的元评估基准，包含图像共享对话、对应摘要及八项质量维度的人工标注。通过跨模态互斥关键信息筛选框架确保数据质量，首次系统定义MDS专属评估维度，并揭示现有评估方法在区分先进多模态大模型生成摘要时的局限性。",
      "order": 379
    },
    {
      "arxiv_id": "2510.01654v1",
      "title": "SoK: Measuring What Matters for Closed-Loop Security Agents",
      "summary": "Cybersecurity is a relentless arms race, with AI driven offensive systems\nevolving faster than traditional defenses can adapt. Research and tooling\nremain fragmented across isolated defensive functions, creating blind spots\nthat adversaries exploit. Autonomous agents capable of integrating, exploit\nconfirmation, remediation, and validation into a single closed loop offer\npromise, but the field lacks three essentials: a framework defining the agentic\ncapabilities of security systems across security life cycle, a principled\nmethod for evaluating closed loop agents, and a benchmark for measuring their\nperformance in practice. We introduce CLASP: the Closed-Loop Autonomous\nSecurity Performance framework which aligns the security lifecycle\n(reconnaissance, exploitation, root cause analysis, patch synthesis,\nvalidation) with core agentic capabilities (planning, tool use, memory,\nreasoning, reflection & perception) providing a common vocabulary and rubric\nfor assessing agentic capabilities in security tasks. By applying CLASP to 21\nrepresentative works, we map where systems demonstrate strengths, and where\ncapability gaps persist. We then define the Closed-Loop Capability (CLC) Score,\na composite metric quantifying both degree of loop closure and operational\neffectiveness, and outline the requirements for a closed loop benchmark.\nTogether, CLASP and the CLC Score, provide the vocabulary, diagnostics, and\nmeasurements needed to advance both function level performance and measure\nclosed loop security agents.",
      "authors": [
        "Mudita Khurana",
        "Raunak Jain"
      ],
      "published": "2025-10-02T04:20:35Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01654v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出CLASP框架，将安全生命周期与智能体核心能力对齐，建立评估闭环安全代理的统一标准。通过分析21个代表性工作识别能力差距，定义CLC评分量化闭环程度与操作效能，为推进安全代理性能提供词汇表、诊断工具和测量方法。",
      "order": 380
    },
    {
      "arxiv_id": "2510.01652v1",
      "title": "Learning to Look at the Other Side: A Semantic Probing Study of Word\n  Embeddings in LLMs with Enabled Bidirectional Attention",
      "summary": "Autoregressive Large Language Models (LLMs) demonstrate exceptional\nperformance in language understanding and generation. However, their\napplication in text embedding tasks has been relatively slow, along with the\nanalysis of their semantic representation in probing tasks, due to the\nconstraints of the unidirectional attention mechanism.\n  This paper aims to explore whether such constraints can be overcome by\nenabling bidirectional attention in LLMs. We tested different variants of the\nLlama architecture through additional training steps, progressively enabling\nbidirectional attention and unsupervised/supervised contrastive learning.",
      "authors": [
        "Zhaoxin Feng",
        "Jianfei Ma",
        "Emmanuele Chersoni",
        "Xiaojing Zhao",
        "Xiaoyi Bao"
      ],
      "published": "2025-10-02T04:18:13Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01652v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探讨了在自回归大语言模型中启用双向注意力机制以克服单向注意力的限制。通过训练Llama架构变体，逐步实现双向注意力和对比学习，分析了词嵌入语义表示在探测任务中的表现，旨在提升文本嵌入任务的性能。",
      "order": 381
    },
    {
      "arxiv_id": "2510.01645v1",
      "title": "Position: Privacy Is Not Just Memorization!",
      "summary": "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats.",
      "authors": [
        "Niloofar Mireshghallah",
        "Tianshi Li"
      ],
      "published": "2025-10-02T04:02:06Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01645v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文主张大语言模型隐私风险不应仅关注训练数据记忆，而需全面审视数据收集、推理时上下文泄露、自主代理能力及深度推理攻击等更紧迫威胁。通过对1322篇隐私论文的分析，揭示当前技术方案对核心隐私危害缺乏有效应对，呼吁研究社区转向跨学科的社会技术方法。",
      "order": 382
    },
    {
      "arxiv_id": "2510.01644v1",
      "title": "NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with\n  BERT",
      "summary": "Large Language Models (LLMs) suffer from a range of vulnerabilities that\nallow malicious users to solicit undesirable responses through manipulation of\nthe input text. These so-called jailbreak prompts are designed to trick the LLM\ninto circumventing the safety guardrails put in place to keep responses\nacceptable to the developer's policies. In this study, we analyse the ability\nof different machine learning models to distinguish jailbreak prompts from\ngenuine uses, including looking at our ability to identify jailbreaks that use\npreviously unseen strategies. Our results indicate that using current datasets\nthe best performance is achieved by fine tuning a Bidirectional Encoder\nRepresentations from Transformers (BERT) model end-to-end for identifying\njailbreaks. We visualise the keywords that distinguish jailbreak from genuine\nprompts and conclude that explicit reflexivity in prompt structure could be a\nsignal of jailbreak intention.",
      "authors": [
        "John Hawkins",
        "Aditya Pramar",
        "Rodney Beard",
        "Rohitash Chandra"
      ],
      "published": "2025-10-02T03:55:29Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01644v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探讨了使用机器学习方法检测大语言模型越狱攻击的能力，发现通过端到端微调BERT模型在现有数据集上表现最佳。研究分析了区分越狱提示与正常使用的关键词，并指出提示结构中的显式自反性可能是越狱意图的信号。",
      "order": 383
    },
    {
      "arxiv_id": "2510.01631v1",
      "title": "Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of\n  Scaling Laws, Benefits, and Pitfalls",
      "summary": "Training data plays a crucial role in Large Language Models (LLM) scaling,\nyet high quality data is of limited supply. Synthetic data techniques offer a\npotential path toward sidestepping these limitations. We conduct a large-scale\nempirical investigation (>1000 LLMs with >100k GPU hours) using a unified\nprotocol and scaling laws, comparing natural web data, diverse synthetic types\n(rephrased text, generated textbooks), and mixtures of natural and synthetic\ndata. Specifically, we found pre-training on rephrased synthetic data\n\\textit{alone} is not faster than pre-training on natural web texts; while\npre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts\ncan speed up 5-10x (to reach the same validation loss) at larger data budgets.\nPre-training on textbook-style synthetic data \\textit{alone} results in notably\nhigher loss on many downstream domains especially at small data budgets. \"Good\"\nratios of synthetic data in training data mixtures depend on the model size and\ndata budget, empirically converging to ~30% for rephrased synthetic data.\nLarger generator models do not necessarily yield better pre-training data than\n~8B-param models. These results contribute mixed evidence on \"model collapse\"\nduring large-scale single-round (n=1) model training on synthetic\ndata--training on rephrased synthetic data shows no degradation in performance\nin foreseeable scales whereas training on mixtures of textbook-style\npure-generated synthetic data shows patterns predicted by \"model collapse\". Our\nwork demystifies synthetic data in pre-training, validates its conditional\nbenefits, and offers practical guidance.",
      "authors": [
        "Feiyang Kang",
        "Newsha Ardalani",
        "Michael Kuchnik",
        "Youssef Emad",
        "Mostafa Elhoushi",
        "Shubhabrata Sengupta",
        "Shang-Wen Li",
        "Ramya Raghavendra",
        "Ruoxi Jia",
        "Carole-Jean Wu"
      ],
      "published": "2025-10-02T03:24:42Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01631v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过大规模实证分析（1000+ LLM，10万+ GPU小时）系统探讨了合成数据在LLM预训练中的作用。研究发现：单独使用改写式合成数据训练效果不优于自然网络文本；但1/3改写合成数据与2/3自然数据混合可加速训练5-10倍；教科书式合成数据单独训练会导致下游任务损失显著增加；最佳合成数据比例约30%且与模型规模相关；约80亿参数生成器已能产生优质训练数据；研究为'模型崩溃'理论提供了混合证据。",
      "order": 384
    },
    {
      "arxiv_id": "2510.01624v1",
      "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What\n  to Use Instead",
      "summary": "In post-training for reasoning Large Language Models (LLMs), the current\nstate of practice trains LLMs in two independent stages: Supervised Fine-Tuning\n(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as\n``RL'' below). In this work, we challenge whether high SFT scores translate to\nimproved performance after RL. We provide extensive counter-examples where this\nis not true. We find high SFT scores can be biased toward simpler or more\nhomogeneous data and are not reliably predictive of subsequent RL gains or\nscaled-up post-training effectiveness. In some cases, RL training on models\nwith improved SFT performance could lead to substantially worse outcome\ncompared to RL on the base model without SFT. We study alternative metrics and\nidentify generalization loss on held-out reasoning examples and Pass@large k\nperformance to provide strong proxies for the RL outcome. We trained hundreds\nof models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive\nevaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU\nhours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple\nstate-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL\nperformance, prediction based on generalization loss and Pass@large k achieves\nsubstantial higher precision, improving $R^2$ coefficient and Spearman's rank\ncorrelation coefficient by up to 0.5 (2x). This provides strong utility for\nbroad use cases. For example, in most experiments, we find SFT training on\nunique examples for a one epoch underperforms training on half examples for two\nepochs, either after SFT or SFT-then-RL; With the same SFT budget, training\nonly on short examples may lead to better SFT performance, though, it often\nleads to worse outcome after RL compared to training on examples with varying\nlengths. Evaluation tool will be open-sourced.",
      "authors": [
        "Feiyang Kang",
        "Michael Kuchnik",
        "Karthik Padthe",
        "Marin Vlastelica",
        "Ruoxi Jia",
        "Carole-Jean Wu",
        "Newsha Ardalani"
      ],
      "published": "2025-10-02T02:57:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01624v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究挑战了SFT高分必然提升RL后性能的传统认知，通过大规模实验证明高SFT分数可能因数据简单或同质而产生误导。提出泛化损失和Pass@large k作为更可靠的RL效果预测指标，在7个数学基准测试中验证其有效性，为LLM后训练提供新评估范式。",
      "order": 385
    },
    {
      "arxiv_id": "2510.01622v1",
      "title": "LLM4Rec: Large Language Models for Multimodal Generative Recommendation\n  with Causal Debiasing",
      "summary": "Contemporary generative recommendation systems face significant challenges in\nhandling multimodal data, eliminating algorithmic biases, and providing\ntransparent decision-making processes. This paper introduces an enhanced\ngenerative recommendation framework that addresses these limitations through\nfive key innovations: multimodal fusion architecture, retrieval-augmented\ngeneration mechanisms, causal inference-based debiasing, explainable\nrecommendation generation, and real-time adaptive learning capabilities. Our\nframework leverages advanced large language models as the backbone while\nincorporating specialized modules for cross-modal understanding, contextual\nknowledge integration, bias mitigation, explanation synthesis, and continuous\nmodel adaptation. Extensive experiments on three benchmark datasets\n(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistent\nimprovements in recommendation accuracy, fairness, and diversity compared to\nexisting approaches. The proposed framework achieves up to 2.3% improvement in\nNDCG@10 and 1.4% enhancement in diversity metrics while maintaining\ncomputational efficiency through optimized inference strategies.",
      "authors": [
        "Bo Ma",
        "Hang Li",
        "ZeHua Hu",
        "XiaoFan Gui",
        "LuYao Liu",
        "Simon Lau"
      ],
      "published": "2025-10-02T02:53:05Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01622v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出LLM4Rec框架，通过多模态融合架构、检索增强生成、因果去偏、可解释推荐和实时自适应学习五大创新，解决生成式推荐系统在处理多模态数据、消除算法偏见和提供透明决策方面的挑战。在三个基准数据集上的实验表明，该框架在推荐准确性、公平性和多样性方面均优于现有方法，NDCG@10指标提升达2.3%。",
      "order": 386
    },
    {
      "arxiv_id": "2510.01617v1",
      "title": "AMAS: Adaptively Determining Communication Topology for LLM-based\n  Multi-Agent System",
      "summary": "Although large language models (LLMs) have revolutionized natural language\nprocessing capabilities, their practical implementation as autonomous\nmulti-agent systems (MAS) for industrial problem-solving encounters persistent\nbarriers. Conventional MAS architectures are fundamentally restricted by\ninflexible, hand-crafted graph topologies that lack contextual responsiveness,\nresulting in diminished efficacy across varied academic and commercial\nworkloads. To surmount these constraints, we introduce AMAS, a\nparadigm-shifting framework that redefines LLM-based MAS through a novel\ndynamic graph designer. This component autonomously identifies task-specific\noptimal graph configurations via lightweight LLM adaptation, eliminating the\nreliance on monolithic, universally applied structural templates. Instead, AMAS\nexploits the intrinsic properties of individual inputs to intelligently direct\nquery trajectories through task-optimized agent pathways. Rigorous validation\nacross question answering, mathematical deduction, and code generation\nbenchmarks confirms that AMAS systematically exceeds state-of-the-art\nsingle-agent and multi-agent approaches across diverse LLM architectures. Our\ninvestigation establishes that context-sensitive structural adaptability\nconstitutes a foundational requirement for high-performance LLM MAS\ndeployments.",
      "authors": [
        "Hui Yi Leong",
        "Yuheng Li",
        "Yuqing Wu",
        "Wenwen Ouyang",
        "Wei Zhu",
        "Jiechao Gao"
      ],
      "published": "2025-10-02T02:50:22Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01617v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "AMAS提出了一种创新的多智能体系统框架，通过动态图设计器自适应确定LLM智能体间的通信拓扑结构。该框架能够根据具体任务需求自动优化智能体网络配置，在问答、数学推理和代码生成等任务中均超越了现有的单智能体和多智能体方法，证明了上下文敏感的结构适应性对高性能LLM多智能体系统部署的重要性。",
      "order": 387
    },
    {
      "arxiv_id": "2510.01616v1",
      "title": "Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single\n  Consumer GPU: Continual Pre-training, SFT, and DPO",
      "summary": "Small Language Models (SLMs) enable cost-effective, on-device and\nlatency-sensitive AI applications, yet their deployment in Traditional Chinese\n(TC) remains hindered by token-level instability - models unpredictably emit\nnon-TC characters or code-switch into other languages. We address this\npractical reliability gap by creating PureTC-1B, a three-stage stabilization\npipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned model\nreleased by Meta) using parameter-efficient LoRA adapters. Our method combines\nContinual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning\n(SFT) with instruction data, and Direct Preference Optimization (DPO) using\nTC-adherence preferences to improve monolingual robustness without full-model\nretraining. On a benchmark designed to simulate real-world usage, PureTC-1B\nachieves a 51.3% relative reduction (micro-average) in non-TC output tokens\nversus the base model. On a Named Entity Translation (NET) task, PureTC-1B\nfurther reduces incorrect-language tokens by 77.2% relative to Llama-3B and\n57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainable\neven at the 1B scale. The pipeline is reproducible, adapter-only, and\nhardware-friendly, offering practitioners a practical recipe to enhance\nlanguage stability for TC and potentially other non-English languages.",
      "authors": [
        "Yu-Cheng Chih",
        "Ming-Tao Duan",
        "Yong-Hao Hou"
      ],
      "published": "2025-10-02T02:50:12Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01616v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出PureTC-1B三阶段稳定化流程，通过持续预训练、监督微调和直接偏好优化，在单张消费级GPU上高效训练鲁棒繁体中文LLaMA-1B模型。相比基础模型，非繁体中文输出减少51.3%，在命名实体翻译任务中错误语言标记减少57.2-77.2%，显著提升模型语言稳定性。",
      "order": 388
    },
    {
      "arxiv_id": "2510.01612v1",
      "title": "RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical\n  Question Answering",
      "summary": "The exponential growth of biomedical literature creates significant\nchallenges for accessing precise medical information. Current biomedical\nquestion-answering systems primarily focus on short-form answers, failing to\nprovide the comprehensive explanations necessary for clinical decision-making.\nWe present RAG-BioQA, a novel framework combining retrieval-augmented\ngeneration with domain-specific fine-tuning to produce evidence-based,\nlong-form biomedical answers. Our approach integrates BioBERT embeddings with\nFAISS indexing and compares various re-ranking strategies (BM25, ColBERT,\nMonoT5) to optimize context selection before synthesizing evidence through a\nfine-tuned T5 model. Experimental results on the PubMedQA dataset show\nsignificant improvements over baselines, with our best model achieving\nsubstantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state\nof accessible, evidence-based biomedical knowledge retrieval.",
      "authors": [
        "Lovely Yeswanth Panchumarthi",
        "Sai Prasad Gudari",
        "Atharva Negi",
        "Praveen Raj Budime",
        "Harsit Upadhya"
      ],
      "published": "2025-10-02T02:49:09Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01612v1",
      "primary_area": "text_models",
      "secondary_focus": "long_context",
      "application_domain": "medical_ai",
      "tldr_zh": "RAG-BioQA框架结合检索增强生成与领域微调，针对生物医学长问答任务，通过BioBERT嵌入、FAISS索引及多种重排序策略优化上下文选择，在PubMedQA数据集上显著提升BLEU、ROUGE和METEOR指标，推动循证医学知识检索发展。",
      "order": 389
    },
    {
      "arxiv_id": "2510.01611v1",
      "title": "PychoBench: Evaluating the Psychology Intelligence of Large Language\n  Models",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: Can LLMs be effectively applied to psychological\ncounseling? To determine whether an LLM can effectively take on the role of a\npsychological counselor, the first step is to assess whether it meets the\nqualifications required for such a role, namely the ability to pass the U.S.\nNational Counselor Certification Exam (NCE). This is because, just as a human\ncounselor must pass a certification exam to practice, an LLM must demonstrate\nsufficient psychological knowledge to meet the standards required for such a\nrole. To address this, we introduce PsychoBench, a benchmark grounded in\nU.S.national counselor examinations, a licensure test for professional\ncounselors that requires about 70% accuracy to pass. PsychoBench comprises\napproximately 2,252 carefully curated single-choice questions, crafted to\nrequire deep understanding and broad enough to cover various sub-disciplines of\npsychology. This benchmark provides a comprehensive assessment of an LLM's\nability to function as a counselor. Our evaluation shows that advanced models\nsuch as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing\nthreshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)\nremain far below it. These results suggest that only frontier LLMs are\ncurrently capable of meeting counseling exam standards, highlighting both the\npromise and the challenges of developing psychology-oriented LLMs.",
      "authors": [
        "Min Zeng"
      ],
      "published": "2025-10-02T02:49:06Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01611v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出PsychoBench基准，基于美国国家心理咨询师认证考试构建，包含2252道单选题，用于评估大语言模型在心理学咨询领域的专业能力。测试结果显示，GPT-4o等前沿模型远超及格线，而较小开源模型表现不佳，表明目前仅有顶级大模型具备担任心理咨询师的知识储备。",
      "order": 390
    },
    {
      "arxiv_id": "2510.01606v1",
      "title": "Bridging Collaborative Filtering and Large Language Models with Dynamic\n  Alignment, Multimodal Fusion and Evidence-grounded Explanations",
      "summary": "Recent research has explored using Large Language Models for recommendation\ntasks by transforming user interaction histories and item metadata into text\nprompts, then having the LLM produce rankings or recommendations. A promising\napproach involves connecting collaborative filtering knowledge to LLM\nrepresentations through compact adapter networks, which avoids expensive\nfine-tuning while preserving the strengths of both components. Yet several\nchallenges persist in practice: collaborative filtering models often use static\nsnapshots that miss rapidly changing user preferences; many real-world items\ncontain rich visual and audio content beyond textual descriptions; and current\nsystems struggle to provide trustworthy explanations backed by concrete\nevidence. Our work introduces \\model{}, a framework that tackles these\nlimitations through three key innovations. We develop an online adaptation\nmechanism that continuously incorporates new user interactions through\nlightweight modules, avoiding the need to retrain large models. We create a\nunified representation that seamlessly combines collaborative signals with\nvisual and audio features, handling cases where some modalities may be\nunavailable. Finally, we design an explanation system that grounds\nrecommendations in specific collaborative patterns and item attributes,\nproducing natural language rationales users can verify. Our approach maintains\nthe efficiency of frozen base models while adding minimal computational\noverhead, making it practical for real-world deployment.",
      "authors": [
        "Bo Ma",
        "LuYao Liu",
        "Simon Lau",
        "Chandler Yuan",
        "and XueY Cui",
        "Rosie Zhang"
      ],
      "published": "2025-10-02T02:43:24Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01606v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种融合协同过滤与大语言模型的推荐框架，通过动态对齐机制实时更新用户偏好，整合文本、视觉和音频多模态特征，并生成基于具体证据的可验证解释，在保持基础模型高效性的同时提升推荐系统的适应性和可信度。",
      "order": 391
    },
    {
      "arxiv_id": "2510.01600v1",
      "title": "A Comparison of Independent and Joint Fine-tuning Strategies for\n  Retrieval-Augmented Generation",
      "summary": "A Comparison of Independent and Joint Fine-tuning Strategies for\nRetrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,\nAnoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP\n2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0\nKeywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),\nFine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and\ncompare strategies for fine-tuning Retrieval Augmented Generation (RAG)\npipelines, including independent fine-tuning, joint fine-tuning, and two-phase\nfine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular\nframework for question answering that is powered by two large language models\n(LLMs): an embedding model that retrieves context documents from a database\nthat are relevant to a given question, and a generator model that uses the\nretrieved context to generate an answer to the question. Both the embedding and\ngenerator models can be fine-tuned to increase performance of a RAG pipeline on\na new task, but multiple fine-tuning strategies exist with different costs and\nbenefits. In this paper, we evaluate and compare several RAG fine-tuning\nstrategies, including independent, joint, and two-phase fine-tuning. In our\nexperiments, we observe that all of these strategies achieve about equal\nimprovement in EM and F1 generation quality metrics, although they have\nsignificantly different computational costs. We conclude the optimal\nfine-tuning strategy to use depends on whether the training dataset includes\ncontext labels and whether a grid search over the learning rates for the\nembedding and generator models is required.",
      "authors": [
        "Neal Gregory Lawton",
        "Alfy Samuel",
        "Anoop Kumar",
        "Daben Liu"
      ],
      "published": "2025-10-02T02:30:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01600v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文比较了检索增强生成(RAG)管道的不同微调策略，包括独立微调、联合微调和两阶段微调。实验表明这些策略在生成质量指标上提升相当，但计算成本差异显著，最优策略选择取决于训练数据是否包含上下文标签以及是否需要学习率网格搜索。",
      "order": 392
    },
    {
      "arxiv_id": "2510.01591v1",
      "title": "CLUE: Non-parametric Verification from Experience via Hidden-State\n  Clustering",
      "summary": "Assessing the quality of Large Language Model (LLM) outputs presents a\ncritical challenge. Previous methods either rely on text-level information\n(e.g., reward models, majority voting), which can overfit to superficial cues,\nor on calibrated confidence from token probabilities, which would fail on\nless-calibrated models. Yet both of these signals are, in fact, partial\nprojections of a richer source of information: the model's internal hidden\nstates. Early layers, closer to token embeddings, preserve semantic and lexical\nfeatures that underpin text-based judgments, while later layers increasingly\nalign with output logits, embedding confidence-related information. This paper\nexplores hidden states directly as a unified foundation for verification. We\nshow that the correctness of a solution is encoded as a geometrically separable\nsignature within the trajectory of hidden activations. To validate this, we\npresent Clue (Clustering and Experience-based Verification), a deliberately\nminimalist, non-parametric verifier. With no trainable parameters, CLUE only\nsummarizes each reasoning trace by an hidden state delta and classifies\ncorrectness via nearest-centroid distance to ``success'' and ``failure''\nclusters formed from past experience. The simplicity of this method highlights\nthe strength of the underlying signal. Empirically, CLUE consistently\noutperforms LLM-as-a-judge baselines and matches or exceeds modern\nconfidence-based methods in reranking candidates, improving both top-1 and\nmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24\nwith a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%\n(top-maj@16).",
      "authors": [
        "Zhenwen Liang",
        "Ruosen Li",
        "Yujun Zhou",
        "Linfeng Song",
        "Dian Yu",
        "Xinya Du",
        "Haitao Mi",
        "Dong Yu"
      ],
      "published": "2025-10-02T02:14:33Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01591v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "CLUE提出一种基于隐藏状态聚类的无参数验证方法，通过分析LLM内部隐藏状态轨迹来区分答案正确性。该方法无需训练参数，仅通过计算与历史成功/失败聚类中心的距离进行分类，在多个基准测试中超越LLM自评估基线，显著提升模型准确率。",
      "order": 393
    },
    {
      "arxiv_id": "2510.01585v1",
      "title": "ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and\n  Long-Context Reasoning",
      "summary": "While Transformer architectures have demonstrated impressive scalability\nacross domains, they continue to face challenges in long-context reasoning,\ncomputational efficiency, and structural generalization - largely due to rigid\nlayer stacking, dense attention, and reliance on positional encodings. We\npresent ReSSFormer, a Recursive Sparse Structured Transformer that integrates\nthree complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for\niterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)\nfor efficient and focused context selection, and Self-Organizing Encoder\nStructure (SOES) for position-free structure induction. ReSSFormer replaces\nconventional depth stacking with recurrent inference, substitutes full\nattention with token- and expert-level sparsity, and models latent token\ntopology directly from content. Across language modeling, multi-hop QA, and\nstructure-sensitive tasks, ReSSFormer consistently outperforms strong baselines\nunder comparable FLOPs and parameter budgets, highlighting its scalability,\nefficiency, and structural flexibility.",
      "authors": [
        "Haochen You",
        "Baojing Liu"
      ],
      "published": "2025-10-02T02:05:30Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01585v1",
      "primary_area": "text_models",
      "secondary_focus": "['long_context', 'reasoning', 'model_architecture']",
      "application_domain": "general_purpose",
      "tldr_zh": "ReSSFormer是一种递归稀疏结构化Transformer，通过循环推理单元、自适应稀疏注意力模块和自组织编码器结构，解决了传统Transformer在长上下文推理、计算效率和结构泛化方面的挑战，在同等计算资源下优于现有基线模型。",
      "order": 394
    },
    {
      "arxiv_id": "2510.01581v1",
      "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,\n  Attentive Compression",
      "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time\ncompute, but this scaling must be allocated in line with task difficulty. On\none hand, short reasoning (underthinking) leads to errors on harder problems\nthat require extended reasoning steps; but, excessively long reasoning\n(overthinking) can be token-inefficient, generating unnecessary steps even\nafter reaching a correct intermediate solution. We refer to this as\nunder-adaptivity, where the model fails to modulate its response length\nappropriately given problems of varying difficulty. To address under-adaptivity\nand strike a balance between under- and overthinking, we propose TRAAC (Think\nRight with Adaptive, Attentive Compression), an online post-training RL method\nthat leverages the model's self-attention over a long reasoning trajectory to\nidentify important steps and prune redundant ones. TRAAC also estimates\ndifficulty and incorporates it into training rewards, thereby learning to\nallocate reasoning budget commensurate with example difficulty. Our approach\nimproves accuracy, reduces reasoning steps, and enables adaptive thinking\ncompared to base models and other RL baselines. Across a variety of tasks\n(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute\naccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%\ncompared to the base model, and a 7.9% accuracy gain paired with a 29.4% length\ndrop compared to the best RL baseline. TRAAC also shows strong generalization:\nalthough our models are trained on math datasets, they show accuracy and\nefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,\nand OptimalThinkingBench. Our analysis further verifies that TRAAC provides\nfine-grained adjustments to thinking budget based on difficulty and that a\ncombination of task-difficulty calibration and attention-based compression\nyields gains across diverse tasks.",
      "authors": [
        "Joykirat Singh",
        "Justin Chih-Yao Chen",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Akshay Nambi",
        "Mohit Bansal"
      ],
      "published": "2025-10-02T02:00:20Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01581v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出TRAAC方法，通过自适应注意力压缩解决思维模型中的欠适应问题，平衡推理不足与过度推理。该方法利用自注意力识别关键推理步骤并修剪冗余，根据问题难度分配计算资源，在多种任务上实现精度提升8.4%同时减少36.8%推理长度，并展现出良好的跨领域泛化能力。",
      "order": 395
    },
    {
      "arxiv_id": "2510.01574v1",
      "title": "Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query\n  Autocomplete",
      "summary": "We introduce a data-centric approach for mitigating presentation bias in\nreal-time neural query autocomplete systems through the use of synthetic\nprefixes. These prefixes are generated from complete user queries collected\nduring regular search sessions where autocomplete was not active. This allows\nus to enrich the training data for learning to rank models with more diverse\nand less biased examples. This method addresses the inherent bias in engagement\nsignals collected from live query autocomplete interactions, where model\nsuggestions influence user behavior. Our neural ranker is optimized for\nreal-time deployment under strict latency constraints and incorporates a rich\nset of features, including query popularity, seasonality, fuzzy match scores,\nand contextual signals such as department affinity, device type, and vertical\nalignment with previous user queries. To support efficient training, we\nintroduce a task-specific simplification of the listwise loss, reducing\ncomputational complexity from $O(n^2)$ to $O(n)$ by leveraging the query\nautocomplete structure of having only one ground-truth selection per prefix.\nDeployed in a large-scale e-commerce setting, our system demonstrates\nstatistically significant improvements in user engagement, as measured by mean\nreciprocal rank and related metrics. Our findings show that synthetic prefixes\nnot only improve generalization but also provide a scalable path toward bias\nmitigation in other low-latency ranking tasks, including related searches and\nquery recommendations.",
      "authors": [
        "Adithya Rajan",
        "Xiaoyu Liu",
        "Prateek Verma",
        "Vibhu Arora"
      ],
      "published": "2025-10-02T01:44:44Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01574v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种数据驱动方法，通过合成前缀缓解实时神经查询自动补全系统中的呈现偏差。该方法利用未启用自动补全时收集的完整用户查询生成前缀，为排序模型训练提供更多样化、偏差更小的数据。通过优化列表损失函数将计算复杂度从O(n²)降至O(n)，并在大规模电商场景中显著提升了用户参与度指标。",
      "order": 396
    },
    {
      "arxiv_id": "2510.01569v1",
      "title": "InvThink: Towards AI Safety via Inverse Reasoning",
      "summary": "We present InvThink, a simple yet powerful approach that gives large language\nmodels (LLMs) the capability of inverse thinking: reasoning through failure\nmodes before generating responses. Unlike existing safety alignment methods\nthat optimize directly for safe response, InvThink instructs models to 1)\nenumerate potential harms, 2) analyze their consequences, and 3) generate safe\noutputs that proactively avoid these risks. Our method reveals three key\nfindings: (i) safety improvements show stronger scaling with model size\ncompared to existing safety methods. (ii) InvThink mitigates safety tax; by\ntraining models to systematically consider failure modes, it preserves general\nreasoning capabilities on standard benchmarks. (iii) beyond general safety\ntasks, InvThink excels in high-stakes domains including external-facing\n(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,\nachieving up to 15.7% reduction in harmful responses compared to baseline\nmethods like SafetyPrompt. We further implement InvThink via supervised\nfine-tuning, and reinforcement learning across three LLM families. These\nresults suggest that inverse reasoning provides a scalable and generalizable\npath toward safer, more capable language models.",
      "authors": [
        "Yubin Kim",
        "Taehan Kim",
        "Eugene Park",
        "Chunjong Park",
        "Cynthia Breazeal",
        "Daniel McDuff",
        "Hae Won Park"
      ],
      "published": "2025-10-02T01:26:53Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01569v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出InvThink方法，通过逆向思维让大语言模型在生成回答前先推理潜在风险模式。该方法包含三个步骤：枚举潜在危害、分析后果、生成主动规避风险的输出。研究发现：安全性能随模型规模扩展更强；缓解安全税效应，保持通用推理能力；在高风险领域（医疗、金融、法律等）相比基线方法减少15.7%有害回答。通过监督微调和强化学习在三个LLM家族中实现，为构建更安全、更强大的语言模型提供可扩展路径。",
      "order": 397
    },
    {
      "arxiv_id": "2510.01531v1",
      "title": "Information Seeking for Robust Decision Making under Partial\n  Observability",
      "summary": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io",
      "authors": [
        "Djengo Cyun-Jyun Fang",
        "Tsung-Wei Ke"
      ],
      "published": "2025-10-02T00:06:32Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01531v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出InfoSeeker框架，将任务规划与信息寻求相结合，使LLM能在部分可观测环境中主动收集信息以校准内部动态模型，在不确定环境下实现鲁棒决策，相比现有方法性能提升74%，并在机器人操作和网页导航等任务中展现出色泛化能力。",
      "order": 398
    },
    {
      "arxiv_id": "2510.01526v1",
      "title": "One More Question is Enough, Expert Question Decomposition (EQD) Model\n  for Domain Quantitative Reasoning",
      "summary": "Domain-specific quantitative reasoning remains a major challenge for large\nlanguage models (LLMs), especially in fields requiring expert knowledge and\ncomplex question answering (QA). In this work, we propose Expert Question\nDecomposition (EQD), an approach designed to balance the use of domain\nknowledge with computational efficiency. EQD is built on a two-step fine-tuning\nframework and guided by a reward function that measures the effectiveness of\ngenerated sub-questions in improving QA outcomes. It requires only a few\nthousand training examples and a single A100 GPU for fine-tuning, with\ninference time comparable to zero-shot prompting. Beyond its efficiency, EQD\noutperforms state-of-the-art domain-tuned models and advanced prompting\nstrategies. We evaluate EQD in the financial domain, characterized by\nspecialized knowledge and complex quantitative reasoning, across four benchmark\ndatasets. Our method consistently improves QA performance by 0.6% to 10.5%\nacross different LLMs. Our analysis reveals an important insight: in\ndomain-specific QA, a single supporting question often provides greater benefit\nthan detailed guidance steps.",
      "authors": [
        "Mengyu Wang",
        "Sotirios Sabanis",
        "Miguel de Carvalho",
        "Shay B. Cohen",
        "Tiejun Ma"
      ],
      "published": "2025-10-01T23:45:45Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01526v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "financial_ai",
      "tldr_zh": "提出专家问题分解(EQD)模型，通过两步微调框架和奖励函数优化领域定量推理，在金融领域四个基准数据集上提升问答性能0.6%-10.5%，发现单一支持性问题比详细指导步骤更有效。",
      "order": 399
    },
    {
      "arxiv_id": "2510.01513v1",
      "title": "From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods\n  for Multimodal Content Analysis and Understanding",
      "summary": "Analysis of multi-modal content can be tricky, computationally expensive, and\nrequire a significant amount of engineering efforts. Lots of work with\npre-trained models on static data is out there, yet fusing these opensource\nmodels and methods with complex data such as videos is relatively challenging.\nIn this paper, we present a framework that enables efficiently prototyping\npipelines for multi-modal content analysis. We craft a candidate recipe for a\npipeline, marrying a set of pre-trained models, to convert videos into a\ntemporal semi-structured data format. We translate this structure further to a\nframe-level indexed knowledge graph representation that is query-able and\nsupports continual learning, enabling the dynamic incorporation of new\ndomain-specific knowledge through an interactive medium.",
      "authors": [
        "Basem Rizk",
        "Joel Walsh",
        "Mark Core",
        "Benjamin Nye"
      ],
      "published": "2025-10-01T23:20:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01513v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一个多模态内容分析框架，通过整合预训练模型将视频转换为时序半结构化数据，并进一步构建可查询的帧级索引知识图谱，支持持续学习和动态融入领域知识。",
      "order": 400
    },
    {
      "arxiv_id": "2510.01470v1",
      "title": "Extracting O*NET Features from the NLx Corpus to Build Public Use\n  Aggregate Labor Market Data",
      "summary": "Data from online job postings are difficult to access and are not built in a\nstandard or transparent manner. Data included in the standard taxonomy and\noccupational information database (O*NET) are updated infrequently and based on\nsmall survey samples. We adopt O*NET as a framework for building natural\nlanguage processing tools that extract structured information from job\npostings. We publish the Job Ad Analysis Toolkit (JAAT), a collection of\nopen-source tools built for this purpose, and demonstrate its reliability and\naccuracy in out-of-sample and LLM-as-a-Judge testing. We extract more than 10\nbillion data points from more than 155 million online job ads provided by the\nNational Labor Exchange (NLx) Research Hub, including O*NET tasks, occupation\ncodes, tools, and technologies, as well as wages, skills, industry, and more\nfeatures. We describe the construction of a dataset of occupation, state, and\nindustry level features aggregated by monthly active jobs from 2015 - 2025. We\nillustrate the potential for research and future uses in education and\nworkforce development.",
      "authors": [
        "Stephen Meisenbacher",
        "Svetlozar Nestorov",
        "Peter Norlander"
      ],
      "published": "2025-10-01T21:27:11Z",
      "primary_category": "cs.CY",
      "arxiv_url": "https://arxiv.org/abs/2510.01470v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "education_ai",
      "tldr_zh": "本研究开发了基于O*NET框架的自然语言处理工具JAAT，用于从在线招聘广告中提取结构化劳动力市场数据。该工具从1.55亿条招聘广告中提取了超过100亿个数据点，构建了2015-2025年月度活跃工作的职业、州和行业级特征数据集，为教育和劳动力发展研究提供公开可用的聚合数据。",
      "order": 401
    },
    {
      "arxiv_id": "2510.01469v1",
      "title": "A-VERT: Agnostic Verification with Embedding Ranking Targets",
      "summary": "The automatic evaluation of Language Model (LM) responses is a critical piece\nin the development of benchmarks and metrics, both for model training and\nquality assessment of production model endpoints. The current approaches to\nresponse classification relies on methods that are too expensive (i.e.\nLLM-as-a-Judge) or that are far from real-world conditions (string-matching,\nlogprob). In this paper, a structure-free evaluation method is presented. The\nmethod makes use of semantic embedding distances to match target candidates\nwith arbitrary LM-generated text, resulting in a robust classification of the\nresponse at a relatively low compute cost (embedding models of less than $10B$\nparameters). The results show a regression score of ~0.97 and an accuracy of\n~96% against human annotators, tested over 3 data sets and 3 different LM\narchitectures.",
      "authors": [
        "Nicolás Aguirre",
        "Ramiro Caso",
        "Ramiro Rodríguez Colmeiro",
        "Mauro Santelli",
        "Joaquín Toranzo Calderón"
      ],
      "published": "2025-10-01T21:26:03Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01469v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "A-VERT提出一种基于语义嵌入距离的无结构语言模型响应评估方法，通过计算嵌入向量相似度匹配目标候选与生成文本，在低计算成本（<100亿参数）下实现约96%的准确率，解决了传统方法成本过高或脱离实际的问题。",
      "order": 402
    },
    {
      "arxiv_id": "2510.01459v1",
      "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM\n  Reasoning",
      "summary": "Since the release of Deepseek-R1, reinforcement learning with verifiable\nrewards (RLVR) has become a central approach for training large language models\n(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss\nfunctions to make RLVR more efficient and effective. In this paper, motivated\nby studies of overthinking in LLMs, we propose Length-aware Sampling for Policy\nOptimization (LSPO), a novel meta-RLVR algorithm that dynamically selects\ntraining data at each step based on the average response length. We evaluate\nLSPO across multiple base models and datasets, demonstrating that it\nconsistently improves learning effectiveness. In addition, we conduct a\ndetailed ablation study to examine alternative ways of incorporating length\nsignals into dynamic sampling, offering further insights and highlighting\npromising directions for future research.",
      "authors": [
        "Weizhe Chen",
        "Sven Koenig",
        "Bistra Dilkina"
      ],
      "published": "2025-10-01T20:57:22Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01459v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出LSPO算法，一种基于响应长度的动态采样策略优化方法，用于提升大语言模型在推理任务中的训练效率。通过根据平均响应长度动态选择训练数据，该算法在多个基础模型和数据集上均能有效提升学习效果，并提供了关于长度信号融入采样策略的深入分析。",
      "order": 403
    },
    {
      "arxiv_id": "2510.01444v1",
      "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
      "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce $\\textbf{VOGUE (Visual Uncertainty Guided\nExploration)}$, a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.",
      "authors": [
        "Rui Liu",
        "Dian Yu",
        "Tong Zheng",
        "Runpeng Dai",
        "Zongxia Li",
        "Wenhao Yu",
        "Zhenwen Liang",
        "Linfeng Song",
        "Haitao Mi",
        "Pratap Tokekar",
        "Dong Yu"
      ],
      "published": "2025-10-01T20:32:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01444v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "VOGUE是一种新颖的多模态强化学习方法，通过将视觉输入视为随机上下文并量化策略对视觉扰动的敏感性，将探索从文本空间转移到视觉空间。该方法使用对称KL散度创建不确定性感知探索信号，结合token熵奖励和退火采样策略，在三个视觉数学基准和三个通用推理基准上平均提升2.6%和3.7%的准确率，有效缓解RL微调中的探索衰减问题。",
      "order": 404
    },
    {
      "arxiv_id": "2510.01394v1",
      "title": "Optimal Stopping vs Best-of-$N$ for Inference Time Optimization",
      "summary": "Large language model (LLM) generation often requires balancing output quality\nagainst inference cost, especially when using multiple generations. We\nintroduce a new framework for inference-time optimization based on the\nclassical Pandora's Box problem. Viewing each generation as opening a costly\n\"box\" with random reward, we develop algorithms that decide when to stop\ngenerating without knowing the underlying reward distribution. Our first\ncontribution is a UCB-style Pandora's Box algorithm, which achieves performance\nthat is provably close to Weitzman's algorithm, the optimal strategy when the\ndistribution is known. We further adapt this method to practical LLM settings\nby addressing reward scaling across prompts via a Bradley-Terry inspired\ntransformation. This leads to an adaptive inference-time optimization method\nthat normalizes rewards and learns stopping thresholds on the fly. Experiments\non the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,\nshow that our adaptive strategy can obtain the same performance as non-adaptive\nBest-of-N sampling while requiring 15-35 percent fewer generations on average.\nOur results establish a principled bridge between optimal stopping theory and\ninference-time scaling, providing both theoretical performance bounds and\npractical efficiency gains for LLM deployment.",
      "authors": [
        "Yusuf Kalayci",
        "Vinod Raman",
        "Shaddin Dughmi"
      ],
      "published": "2025-10-01T19:25:59Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01394v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出基于潘多拉盒子问题的大语言模型推理优化框架，通过UCB式算法自适应决定生成停止时机，在AlpacaFarm和HH-RLHF数据集上实验表明，相比传统Best-of-N采样方法可减少15-35%的生成次数，同时保持相同性能表现。",
      "order": 405
    },
    {
      "arxiv_id": "2510.01391v1",
      "title": "TAG-EQA: Text-And-Graph for Event Question Answering via Structured\n  Prompting Strategies",
      "summary": "Large language models (LLMs) excel at general language tasks but often\nstruggle with event-based questions-especially those requiring causal or\ntemporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question\nAnswering), a prompting framework that injects causal event graphs into LLM\ninputs by converting structured relations into natural-language statements.\nTAG-EQA spans nine prompting configurations, combining three strategies\n(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,\ngraph-only, text+graph), enabling a systematic analysis of when and how\nstructured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA\nimproves accuracy by 5% on average over text-only baselines, with gains up to\n12% in zero-shot settings and 18% when graph-augmented CoT prompting is\neffective. While performance varies by model and configuration, our findings\nshow that causal graphs can enhance event reasoning in LLMs without\nfine-tuning, offering a flexible way to encode structure in prompt-based QA.",
      "authors": [
        "Maithili Kadam",
        "Francis Ferraro"
      ],
      "published": "2025-10-01T19:23:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01391v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "TAG-EQA是一种通过将因果事件图转换为自然语言陈述注入LLM输入的提示框架，结合三种策略与三种输入模态，在TORQUESTRA基准测试中比纯文本基线平均准确率提升5%，最高可达18%，证明无需微调即可增强LLM的事件推理能力。",
      "order": 406
    },
    {
      "arxiv_id": "2510.01375v1",
      "title": "Fine-tuning with RAG for Improving LLM Learning of New Skills",
      "summary": "Large language model (LLM) agents deployed for multi-step tasks frequently\nfail in predictable ways: attempting actions with unmet preconditions, issuing\nredundant commands, or mishandling environment constraints. While\nretrieval-augmented generation (RAG) can improve performance by providing\nruntime guidance, it requires maintaining external knowledge databases and adds\ncomputational overhead at every deployment. We propose a simple pipeline that\nconverts inference-time retrieval into learned competence through distillation.\nOur approach: (1) extracts compact, reusable hints from agent failures, (2)\nuses these hints to generate improved teacher trajectories via one-shot\nretrieval at episode start, and (3) trains student models on these trajectories\nwith hint strings removed, forcing internalization rather than memorization.\nAcross two interactive benchmarks, ALFWorld (household tasks) and WebShop\n(online shopping), distilled students consistently outperform baseline agents,\nachieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving\nWebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens\nthan retrieval-augmented teachers depending on the environment. The approach\ngeneralizes across model scales (7B/14B parameters) and agent architectures\n(ReAct/StateAct), demonstrating that retrieval benefits can be effectively\ninternalized through targeted fine-tuning without permanent runtime\ndependencies.",
      "authors": [
        "Humaid Ibrahim",
        "Nikolai Rozanov",
        "Marek Rei"
      ],
      "published": "2025-10-01T19:03:48Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01375v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种通过知识蒸馏将检索增强生成(RAG)转化为学习能力的方法，从智能体失败中提取紧凑提示，生成改进的教师轨迹，并训练学生模型内部化这些知识。在ALFWorld和WebShop基准测试中，该方法显著提升成功率(ALFWorld达91%，WebShop达72%)，同时减少10-60%的token使用量，证明检索优势可通过微调有效内化而无需运行时依赖。",
      "order": 407
    },
    {
      "arxiv_id": "2510.01367v1",
      "title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by\n  Measuring Reasoning Effort",
      "summary": "Reward hacking, where a reasoning model exploits loopholes in a reward\nfunction to achieve high rewards without solving the intended task, poses a\nsignificant threat. This behavior may be explicit, i.e. verbalized in the\nmodel's chain-of-thought (CoT), or implicit, where the CoT appears benign thus\nbypasses CoT monitors. To detect implicit reward hacking, we propose TRACE\n(Truncated Reasoning AUC Evaluation). Our key observation is that hacking\noccurs when exploiting the loophole is easier than solving the actual task.\nThis means that the model is using less `effort' than required to achieve high\nreward. TRACE quantifies effort by measuring how early a model's reasoning\nbecomes sufficient to pass a verifier. We progressively truncate a model's CoT\nat various lengths, force the model to answer, and measure the verifier-passing\nrate at each cutoff. A hacking model, which takes a shortcut, will achieve a\nhigh passing rate with only a small fraction of its CoT, yielding a large area\nunder the accuracy-vs-length curve. TRACE achieves over 65% gains over our\nstrongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B\nmonitor in coding. We further show that TRACE can discover unknown loopholes\nduring training. Overall, TRACE offers a scalable unsupervised approach for\noversight where current monitoring methods prove ineffective.",
      "authors": [
        "Xinpeng Wang",
        "Nitish Joshi",
        "Barbara Plank",
        "Rico Angell",
        "He He"
      ],
      "published": "2025-10-01T18:49:45Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01367v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出TRACE方法检测推理模型中的隐式奖励破解行为，通过截断思维链测量推理努力程度来识别模型是否走捷径而非真正解决问题。在数学推理和代码生成任务中分别比现有监测方法提升65%和30%以上。",
      "order": 408
    },
    {
      "arxiv_id": "2510.01354v1",
      "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
      "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.",
      "authors": [
        "Yinuo Liu",
        "Ruohan Xu",
        "Xilong Wang",
        "Yuqi Jia",
        "Neil Zhenqiang Gong"
      ],
      "published": "2025-10-01T18:34:06Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01354v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "WAInjectBench是首个针对网络代理的提示注入攻击检测基准研究。该工作通过细粒度威胁模型分类构建了包含恶意/良性文本和图像的数据集，系统评估了多种检测方法。研究发现现有检测器能有效识别显式文本指令或可见图像扰动攻击，但对隐式指令和不可察觉扰动的攻击检测效果较差。",
      "order": 409
    },
    {
      "arxiv_id": "2510.01353v1",
      "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in\n  Multi-Platform Dynamic Agent Environments",
      "summary": "Recent works on context and memory benchmarking have primarily focused on\nconversational instances but the need for evaluating memory in dynamic\nenterprise environments is crucial for its effective application. We introduce\nMEMTRACK, a benchmark designed to evaluate long-term memory and state tracking\nin multi-platform agent environments. MEMTRACK models realistic organizational\nworkflows by integrating asynchronous events across multiple communication and\nproductivity platforms such as Slack, Linear and Git. Each benchmark instance\nprovides a chronologically platform-interleaved timeline, with noisy,\nconflicting, cross-referring information as well as potential\ncodebase/file-system comprehension and exploration. Consequently, our benchmark\ntests memory capabilities such as acquistion, selection and conflict\nresolution. We curate the MEMTRACK dataset through both manual expert driven\ndesign and scalable agent based synthesis, generating ecologically valid\nscenarios grounded in real world software development processes. We introduce\npertinent metrics for Correctness, Efficiency, and Redundancy that capture the\neffectiveness of memory mechanisms beyond simple QA performance. Experiments\nacross SoTA LLMs and memory backends reveal challenges in utilizing memory\nacross long horizons, handling cross-platform dependencies, and resolving\ncontradictions. Notably, the best performing GPT-5 model only achieves a 60\\%\nCorrectness score on MEMTRACK. This work provides an extensible framework for\nadvancing evaluation research for memory-augmented agents, beyond existing\nfocus on conversational setups, and sets the stage for multi-agent,\nmulti-platform memory benchmarking in complex organizational settings",
      "authors": [
        "Darshan Deshpande",
        "Varun Gangal",
        "Hersh Mehta",
        "Anand Kannappan",
        "Rebecca Qian",
        "Peng Wang"
      ],
      "published": "2025-10-01T18:34:03Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01353v1",
      "primary_area": "text_models",
      "secondary_focus": "long_context",
      "application_domain": "code_generation",
      "tldr_zh": "MEMTRACK是一个评估多平台动态代理环境中长期记忆与状态跟踪的基准测试，通过整合Slack、Linear和Git等平台的异步事件模拟真实组织工作流，测试记忆获取、选择和冲突解决等能力。研究显示当前最先进的GPT-5模型在MEMTRACK上仅获得60%的正确率，突显了长时程记忆、跨平台依赖处理和矛盾解决方面的挑战。",
      "order": 410
    },
    {
      "arxiv_id": "2510.01346v1",
      "title": "Aristotle: IMO-level Automated Theorem Proving",
      "summary": "We introduce Aristotle, an AI system that combines formal verification with\ninformal reasoning, achieving gold-medal-equivalent performance on the 2025\nInternational Mathematical Olympiad problems. Aristotle integrates three main\ncomponents: a Lean proof search system, an informal reasoning system that\ngenerates and formalizes lemmas, and a dedicated geometry solver. Our system\ndemonstrates state-of-the-art performance with favorable scaling properties for\nautomated theorem proving.",
      "authors": [
        "Tudor Achim",
        "Alex Best",
        "Kevin Der",
        "Mathïs Fédérico",
        "Sergei Gukov",
        "Daniel Halpern-Leister",
        "Kirsten Henningsgard",
        "Yury Kudryashov",
        "Alexander Meiburg",
        "Martin Michelsen",
        "Riley Patterson",
        "Eric Rodriguez",
        "Laura Scharff",
        "Vikram Shanker",
        "Vladmir Sicca",
        "Hari Sowrirajan",
        "Aidan Swope",
        "Matyas Tamas",
        "Vlad Tenev",
        "Jonathan Thomm",
        "Harold Williams",
        "Lawrence Wu"
      ],
      "published": "2025-10-01T18:21:13Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01346v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "education_ai",
      "tldr_zh": "Aristotle是一个结合形式化验证与非正式推理的AI系统，在2025年国际数学奥林匹克竞赛中达到金牌级别表现。系统集成Lean证明搜索、引理生成与形式化模块及专用几何求解器，在自动定理证明领域展现顶尖性能与良好扩展性。",
      "order": 411
    },
    {
      "arxiv_id": "2510.01336v1",
      "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
      "summary": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.",
      "authors": [
        "Avinash Kumar",
        "Sujay Sanghavi",
        "Poulami Das"
      ],
      "published": "2025-10-01T18:04:14Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01336v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "HiSpec提出了一种分层推测解码框架，利用早退模型进行低开销中间验证，通过重用KV缓存和隐藏状态提升资源效率，在保持准确性的同时将大语言模型推理吞吐量平均提升1.28倍。",
      "order": 412
    },
    {
      "arxiv_id": "2510.01180v1",
      "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\ningredient for unlocking complex reasoning capabilities in large language\nmodels. Recent work ProRL has shown promise in scaling RL by increasing the\nnumber of training steps. However, performance plateaus after thousands of\nsteps, with clear diminishing returns from allocating more computation to\nadditional training. In this work, we investigate a complementary paradigm for\nscaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to\nexhaustively Broaden exploration, which yields continuous performance gains\nbeyond the saturation point observed in ProRL when scaling the number of\ntraining steps. Our approach is motivated by a mass balance equation analysis\nallowing us to characterize the rate of change in probability mass for correct\nand incorrect tokens during the reinforcement process. We show that under a\none-step RL assumption, sampled rollout tokens always contribute to\ncorrect-mass expansion, while unsampled tokens outside rollouts may lead to\ngains or losses depending on their distribution and the net reward balance.\nImportantly, as the number of rollouts per example N increases, the effect of\nunsampled terms diminishes, ensuring overall correct-mass expansion. To\nvalidate our theoretical analysis, we conduct simulations under more relaxed\nconditions and find that a sufficiently large rollout size N-corresponding to\nample exploration-guarantees an increase in the probability mass of all correct\ntokens. Empirically, BroRL revives models saturated after 3K ProRL training\nsteps and demonstrates robust, continuous improvement, achieving\nstate-of-the-art results for the 1.5B model across diverse benchmarks.",
      "authors": [
        "Jian Hu",
        "Mingjie Liu",
        "Ximing Lu",
        "Fang Wu",
        "Zaid Harchaoui",
        "Shizhe Diao",
        "Yejin Choi",
        "Pavlo Molchanov",
        "Jun Yang",
        "Jan Kautz",
        "Yi Dong"
      ],
      "published": "2025-10-01T17:59:02Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01180v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "BroRL提出通过增加每个训练样本的rollout数量至数百次来扩展强化学习，解决了ProRL在数千步训练后性能饱和的问题。理论分析表明，扩大探索范围能确保正确token概率质量的持续增长，实验证明该方法能使饱和模型复苏并在多个基准测试中取得最优结果。",
      "order": 413
    },
    {
      "arxiv_id": "2510.01304v1",
      "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
      "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .",
      "authors": [
        "Yu Zeng",
        "Wenxuan Huang",
        "Shiting Huang",
        "Xikun Bao",
        "Yukun Qi",
        "Yiming Zhao",
        "Qiuchen Wang",
        "Lin Chen",
        "Zehui Chen",
        "Huaian Chen",
        "Wanli Ouyang",
        "Feng Zhao"
      ],
      "published": "2025-10-01T17:58:05Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01304v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出AGILE方法，通过将拼图任务构建为交互式学习过程，增强视觉语言模型的感知与推理能力。该方法让模型生成可执行代码与环境交互，通过渐进式探索和视觉反馈提升性能。实验显示在2×2拼图任务中准确率从9.5%提升至82.8%，并在9个通用视觉任务上平均提升3.1%，为解决多模态强化学习数据稀缺问题提供了可扩展方案。",
      "order": 414
    },
    {
      "arxiv_id": "2510.01179v1",
      "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
      "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.",
      "authors": [
        "Zhangchen Xu",
        "Adriana Meza Soria",
        "Shawn Tan",
        "Anurag Roy",
        "Ashish Sunil Agrawal",
        "Radha Poovendran",
        "Rameswar Panda"
      ],
      "published": "2025-10-01T17:58:03Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01179v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "TOUCAN项目构建了目前最大的开源工具代理数据集，包含150万条从近500个真实MCP环境中合成的轨迹数据。该数据集通过多样化查询生成、质量过滤和多教师模型轨迹生成，解决了现有工具代理数据在多样性、真实性和复杂性方面的不足。基于TOUCAN微调的模型在BFCL V3和MCP-Universe基准测试中表现优异，超越了更大的闭源模型。",
      "order": 415
    },
    {
      "arxiv_id": "2510.01174v1",
      "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
      "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.",
      "authors": [
        "Yanzhe Chen",
        "Kevin Qinghong Lin",
        "Mike Zheng Shou"
      ],
      "published": "2025-10-01T17:56:48Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01174v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "education_ai",
      "tldr_zh": "Code2Video提出了一种以代码为中心的教育视频生成框架，通过三个协作智能体（规划器、编码器、评审器）将教学内容转换为可执行的Python代码并生成专业教育视频。该方法在MMMC基准测试中表现优异，相比直接代码生成提升40%，视频质量接近人工制作水平。",
      "order": 416
    },
    {
      "arxiv_id": "2510.01172v1",
      "title": "Energy-Regularized Sequential Model Editing on Hyperspheres",
      "summary": "Large language models (LLMs) require constant updates to remain aligned with\nevolving real-world knowledge. Model editing offers a lightweight alternative\nto retraining, but sequential editing often destabilizes representations and\ninduces catastrophic forgetting. In this work, we seek to better understand and\nmitigate performance degradation caused by sequential editing. We hypothesize\nthat hyperspherical uniformity, a property that maintains uniform distribution\nof neuron weights on a hypersphere, helps the model remain stable, retain prior\nknowledge, while still accommodate new updates. We use Hyperspherical Energy\n(HE) to quantify neuron uniformity during editing, and examine its correlation\nwith editing performance. Empirical studies across widely used editing methods\nreveals a strong correlation between HE dynamics and editing performance, with\nediting failures consistently coinciding with high HE fluctuations. We further\ntheoretically prove that HE dynamics impose a lower bound on the degradation of\npretrained knowledge, highlighting why HE stability is crucial for knowledge\nretention. Motivated by these insights, we propose SPHERE (Sparse Projection\nfor Hyperspherical Energy-Regularized Editing), an HE-driven regularization\nstrategy that stabilizes neuron weight distributions, ultimately preserving\nprior knowledge while enabling reliable sequential updates. Specifically,\nSPHERE identifies a sparse space complementary to the principal hyperspherical\ndirections of the pretrained weight matrices and projects new knowledge onto\nit, attenuating perturbations on the principal directions. Extensive\nexperiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the\nbest baseline in editing capability by an average of 16.41%, while most\nfaithfully preserving general model performance, thereby offering a principled\npath toward reliable large-scale knowledge editing.",
      "authors": [
        "Qingyuan Liu",
        "Jia-Chen Gu",
        "Yunzhi Yao",
        "Hong Wang",
        "Nanyun Peng"
      ],
      "published": "2025-10-01T17:55:43Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01172v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出SPHERE方法，通过超球面能量正则化稳定神经元权重分布，解决大语言模型顺序编辑中的灾难性遗忘问题。该方法在保持原有知识的同时实现可靠更新，在LLaMA3和Qwen2.5上编辑能力平均提升16.41%。",
      "order": 417
    },
    {
      "arxiv_id": "2510.01171v1",
      "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity",
      "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.",
      "authors": [
        "Jiayi Zhang",
        "Simon Yu",
        "Derek Chong",
        "Anthony Sicilia",
        "Michael R. Tomz",
        "Christopher D. Manning",
        "Weiyan Shi"
      ],
      "published": "2025-10-01T17:55:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01171v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出'语言化采样'方法解决LLM对齐训练中的模式崩溃问题。研究发现偏好数据的典型性偏见是模式崩溃的根本原因，通过让模型生成多个回答及其概率分布，显著提升了创意写作、对话模拟等任务的多样性(1.6-2.1倍)，且不影响准确性和安全性。",
      "order": 418
    },
    {
      "arxiv_id": "2510.01167v1",
      "title": "Simultaneous Multi-objective Alignment Across Verifiable and\n  Non-verifiable Rewards",
      "summary": "Aligning large language models to human preferences is inherently\nmultidimensional, yet most pipelines collapse heterogeneous signals into a\nsingle optimizeable objective. We seek to answer what it would take to\nsimultaneously align a model across various domains spanning those with:\nverifiable rewards (mathematical accuracy), non-verifiable subjective\npreferences (human values), and complex interactive scenarios (multi-turn AI\ntutoring dialogues). Such multi-objective reinforcement learning setups are\noften plagued by the individual objectives being at odds with each other,\nresulting in inefficient training and little user control during inference. We\npropose a unified framework that: (i) standardizes {process reward model} (PRM)\ntraining across both verifiable and non-verifiable settings to better supervise\nmodels' chain-of-thought reasoning; (ii) performs {multi-objective alignment}\nby training the LLM with our $\\textbf{M}$ulti-$\\textbf{A}$ction-$\\textbf{H}$ead\n$\\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the\nvector correspond to the various objectives instead of a single scalar; and\n(iii) demonstrates how such a system provides fine-grained inference-time user\ncontrol. Experiments across math reasoning, value alignment, and multi-turn\ndialogue show that our framework improves performance across multiple\nobjectives simultaneously, while minimizing cross-objective trade-offs and\nenabling flexible inference time user control. The code can be found at\nhttps://github.com/pearls-lab/multiobj-align.",
      "authors": [
        "Yiran Shen",
        "Yu Xia",
        "Jonathan Chang",
        "Prithviraj Ammanabrolu"
      ],
      "published": "2025-10-01T17:54:15Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01167v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出统一框架解决大语言模型多目标对齐问题，涵盖可验证奖励（数学准确性）与不可验证主观偏好（人类价值观）。通过标准化过程奖励模型训练、多动作头DPO方法和向量化奖励，在数学推理、价值对齐和多轮对话中实现多目标同步优化，减少目标间权衡，并提供细粒度推理时用户控制。",
      "order": 419
    },
    {
      "arxiv_id": "2510.01165v1",
      "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient\n  Few-Shot Reasoning",
      "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks,\nbut their effectiveness often depends on the quality of the provided context.\nRetrieval-Augmented Generation (RAG) enriches prompts with external\ninformation, but its reliance on static databases constrains adaptability and\ncan result in irrelevant demonstrations. In this work, we propose a Generative\nRetrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach\nwhere an LLM model is trained to generate input-specific concise\ndemonstrations. By tailoring demonstrations to each input, our method offers\nbetter contextual support than traditional RAG approaches. We demonstrate the\nsuperiority of GRAD under budget constraints, where we limit both the number of\ntokens used per demonstration and the number of tokens used for the final\noutput. Trained solely on a math dataset, GRAD consistently outperforms strong\nbaselines on Qwen2.5-14B across mathematical reasoning and advanced STEM\nquestions, highlighting GRAD's robust generalization to out-of-distribution\n(OOD) domains such as physics, chemistry, and computer science. Furthermore, we\nshow that demonstrations generated by trained smaller models can effectively\nguide larger target models, reducing training costs while maintaining\ncompetitive accuracy. Overall, this work introduces a scalable demonstration\ngenerator model presenting the first step toward a dynamic few-shot learning\nparadigm in resource-constrained settings. We release the code used for the\nproject.",
      "authors": [
        "Oussama Gabouj",
        "Kamel Charaf",
        "Ivan Zakazov",
        "Nicolas Baldwin",
        "Robert West"
      ],
      "published": "2025-10-01T17:52:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01165v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出GRAD方法，通过训练LLM生成针对特定输入的简洁示例，替代传统检索增强生成中的静态数据库。该方法在数学推理和STEM问题上表现优异，能有效泛化至物理、化学等新领域，并在计算资源受限条件下保持竞争力。",
      "order": 420
    },
    {
      "arxiv_id": "2510.01164v1",
      "title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social\n  Welfare",
      "summary": "Large language models (LLMs) are increasingly entrusted with high-stakes\ndecisions that affect human welfare. However, the principles and values that\nguide these models when distributing scarce societal resources remain largely\nunexamined. To address this, we introduce the Social Welfare Function (SWF)\nBenchmark, a dynamic simulation environment where an LLM acts as a sovereign\nallocator, distributing tasks to a heterogeneous community of recipients. The\nbenchmark is designed to create a persistent trade-off between maximizing\ncollective efficiency (measured by Return on Investment) and ensuring\ndistributive fairness (measured by the Gini coefficient). We evaluate 20\nstate-of-the-art LLMs and present the first leaderboard for social welfare\nallocation. Our findings reveal three key insights: (i) A model's general\nconversational ability, as measured by popular leaderboards, is a poor\npredictor of its allocation skill. (ii) Most LLMs exhibit a strong default\nutilitarian orientation, prioritizing group productivity at the expense of\nsevere inequality. (iii) Allocation strategies are highly vulnerable, easily\nperturbed by output-length constraints and social-influence framing. These\nresults highlight the risks of deploying current LLMs as societal\ndecision-makers and underscore the need for specialized benchmarks and targeted\nalignment for AI governance.",
      "authors": [
        "Zhengliang Shi",
        "Ruotian Ma",
        "Jen-tse Huang",
        "Xinbei Ma",
        "Xingyu Chen",
        "Mengru Wang",
        "Qu Yang",
        "Yue Wang",
        "Fanghua Ye",
        "Ziyang Chen",
        "Shanyi Wang",
        "Cixing Li",
        "Wenxuan Wang",
        "Zhaopeng Tu",
        "Xiaolong Li",
        "Zhaochun Ren",
        "Linus"
      ],
      "published": "2025-10-01T17:52:31Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01164v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究引入社会福利函数基准测试，评估20个先进大语言模型在资源分配中的表现。研究发现：模型对话能力与分配技能无关；多数模型呈现功利主义倾向，牺牲公平追求效率；分配策略易受输出长度和社会框架影响。结果揭示了当前LLM作为社会决策者的风险，强调需要专门基准和针对性对齐。",
      "order": 421
    },
    {
      "arxiv_id": "2510.01157v1",
      "title": "Backdoor Attacks Against Speech Language Models",
      "summary": "Large Language Models (LLMs) and their multimodal extensions are becoming\nincreasingly popular. One common approach to enable multimodality is to cascade\ndomain-specific encoders with an LLM, making the resulting model inherit\nvulnerabilities from all of its components. In this work, we present the first\nsystematic study of audio backdoor attacks against speech language models. We\ndemonstrate its effectiveness across four speech encoders and three datasets,\ncovering four tasks: automatic speech recognition (ASR), speech emotion\nrecognition, and gender and age prediction. The attack consistently achieves\nhigh success rates, ranging from 90.76% to 99.41%. To better understand how\nbackdoors propagate, we conduct a component-wise analysis to identify the most\nvulnerable stages of the pipeline. Finally, we propose a fine-tuning-based\ndefense that mitigates the threat of poisoned pretrained encoders.",
      "authors": [
        "Alexandrine Fortier",
        "Thomas Thebaud",
        "Jesús Villalba",
        "Najim Dehak",
        "Patrick Cardinal"
      ],
      "published": "2025-10-01T17:45:04Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01157v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究首次系统性地探讨了针对语音语言模型的音频后门攻击，在四种语音编码器和三个数据集上验证了攻击有效性，攻击成功率高达90.76%-99.41%。通过组件分析识别了流程中最脆弱环节，并提出基于微调的防御方法以缓解预训练编码器中毒威胁。",
      "order": 422
    },
    {
      "arxiv_id": "2510.01152v1",
      "title": "Pay-Per-Search Models are Abstention Models",
      "summary": "LLMs cannot reliably recognize their parametric knowledge boundaries and\noften hallucinate answers to outside-of-boundary questions. In contrast, humans\nrecognize their limitations and can either seek external help for such\nquestions or abstain. In this paper, we introduce MASH (Modeling Abstention via\nSelective Help-seeking), a training framework that readily extracts abstentions\nfrom LLMs. Our key idea is that any external help-seeking by an LLM, i.e.\nsearch tool use, can serve as a proxy for abstention if the external help\n(search) is appropriately penalized while simultaneously rewarding answer\naccuracy. MASH operationalizes this idea using reinforcement learning with a\npay-per-search reward.\n  We run experiments on three knowledge-intensive QA datasets. Our results show\nthat MASH substantially improves upon the selective help-seeking performance of\nprior efficient search approaches; on multi-hop datasets, MASH improves answer\naccuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf\nabstention -- it can distinguish between unanswerable/answerable questions and\nselectively generate responses for answerable questions -- showcasing behavior\nanalogous to specialized abstention approaches. We emphasize that contrary to\nprior abstention methods, MASH does not require pre-determining knowledge\nboundaries to construct training data. Instead, MASH's abstentions are a\nby-product of training for the auxiliary selective help-seeking task. Overall,\nwe show that MASH training effectively aligns search tool use with parametric\nknowledge, which can be successfully leveraged for making abstention decisions.",
      "authors": [
        "Mustafa Omer Gul",
        "Claire Cardie",
        "Tanya Goyal"
      ],
      "published": "2025-10-01T17:41:54Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01152v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MASH训练框架，通过按搜索付费的强化学习奖励机制，将大语言模型的外部求助行为转化为知识边界识别能力。该方法无需预先定义知识边界，在三个知识问答数据集上显著提升答案准确率7.6%，并能有效区分可答/不可答问题，实现类似专业弃权方法的性能。",
      "order": 423
    },
    {
      "arxiv_id": "2510.01146v1",
      "title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models",
      "summary": "Evaluation using Large Language Model (LLM) judges has been widely adopted in\nEnglish and shown to be effective for automatic evaluation. However, their\nperformance does not generalize well to non-English settings, and it remains\nunclear what constitutes effective multilingual training for such judges. In\nthis paper, we introduce mR3, a massively multilingual, rubric-agnostic reward\nreasoning model trained on 72 languages, achieving the broadest language\ncoverage in reward modeling to date. We present a comprehensive study of data\nand curriculum selection for training to identify effective strategies and data\nsources for building high-quality reward models, including the integration of\ntarget-language reasoning datasets. Our approach attains state-of-the-art\nperformance on multilingual reward model benchmarks, surpassing much larger\nmodels (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness\nis further confirmed through extensive ablation studies. Our models, data, and\ncode are available as open source at https://github.com/rubricreward/mr3.",
      "authors": [
        "David Anugraha",
        "Shou-Yi Hung",
        "Zilu Tang",
        "Annie En-Shiun Lee",
        "Derry Tanti Wijaya",
        "Genta Indra Winata"
      ],
      "published": "2025-10-01T17:36:59Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01146v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "mR3是一个支持72种语言的多语言奖励推理模型，通过研究数据和课程选择策略，在保持模型规模较小（比GPT-OSS-120B小9倍）的同时，在多语言奖励模型基准测试中达到最先进性能，并开源模型、数据和代码。",
      "order": 424
    },
    {
      "arxiv_id": "2510.01145v1",
      "title": "Automatic Speech Recognition (ASR) for African Low-Resource Languages: A\n  Systematic Literature Review",
      "summary": "ASR has achieved remarkable global progress, yet African low-resource\nlanguages remain rigorously underrepresented, producing barriers to digital\ninclusion across the continent with more than +2000 languages. This systematic\nliterature review (SLR) explores research on ASR for African languages with a\nfocus on datasets, models and training methods, evaluation techniques,\nchallenges, and recommends future directions. We employ the PRISMA 2020\nprocedures and search DBLP, ACM Digital Library, Google Scholar, Semantic\nScholar, and arXiv for studies published between January 2020 and July 2025. We\ninclude studies related to ASR datasets, models or metrics for African\nlanguages, while excluding non-African, duplicates, and low-quality studies\n(score <3/5). We screen 71 out of 2,062 records and we record a total of 74\ndatasets across 111 languages, encompassing approximately 11,206 hours of\nspeech. Fewer than 15% of research provided reproducible materials, and dataset\nlicensing is not clear. Self-supervised and transfer learning techniques are\npromising, but are hindered by limited pre-training data, inadequate coverage\nof dialects, and the availability of resources. Most of the researchers use\nWord Error Rate (WER), with very minimal use of linguistically informed scores\nsuch as Character Error Rate (CER) or Diacritic Error Rate (DER), and thus with\nlimited application in tonal and morphologically rich languages. The existing\nevidence on ASR systems is inconsistent, hindered by issues like dataset\navailability, poor annotations, licensing uncertainties, and limited\nbenchmarking. Nevertheless, the rise of community-driven initiatives and\nmethodological advancements indicates a pathway for improvement. Sustainable\ndevelopment for this area will also include stakeholder partnership, creation\nof ethically well-balanced datasets, use of lightweight modelling techniques,\nand active benchmarking.",
      "authors": [
        "Sukairaj Hafiz Imam",
        "Tadesse Destaw Belay",
        "Kedir Yassin Husse",
        "Ibrahim Said Ahmad",
        "Idris Abdulmumin",
        "Hadiza Ali Umar",
        "Muhammad Yahuza Bello",
        "Joyce Nakatumba-Nabende",
        "Seid Muhie Yimam",
        "Shamsuddeen Hassan Muhammad"
      ],
      "published": "2025-10-01T17:36:06Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01145v1",
      "primary_area": "audio_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本文系统综述了2020-2025年间非洲低资源语言自动语音识别(ASR)研究现状。分析了74个数据集(覆盖111种语言、约1.1万小时语音)，发现仅15%研究提供可复现材料。自监督与迁移学习虽有效但受限于数据稀缺、方言覆盖不足。评估主要依赖WER，缺乏对声调语言的针对性指标。未来发展需加强社区合作、伦理数据集构建和轻量化建模。",
      "order": 425
    },
    {
      "arxiv_id": "2510.01135v1",
      "title": "Prompt Curriculum Learning for Efficient LLM Post-Training",
      "summary": "We introduce Prompt Curriculum Learning (PCL), a lightweight reinforcement\nlearning (RL) algorithm that selects intermediate-difficulty prompts using a\nlearned value model to post-train language models. Since post-training LLMs via\nRL remains sensitive to batching and prompt selection strategies, we first\nconduct a series of systematic experiments where we (1) determine the optimal\ntraining batch size that balances generation efficiency and gradient quality\nand (2) establish the importance of focusing on prompts of intermediate\ndifficulty for the policy. We build upon these results to design PCL, which\nidentifies prompts of intermediate difficulty for the current policy in an\non-policy manner by using a value model that is concurrently updated based on\nthe current policy. By focusing on informative prompts that yield high\neffective ratios, PCL achieves either the highest performance or requires\nsignificantly less time to reach comparable performance to its counterparts.\nCompared to rollout-based filtering methods, PCL avoids costly rollouts and\nachieves $12.1\\times$ and $16.9\\times$ faster speed on identifying\nintermediate-difficulty prompts when training on MATH and DeepScaleR,\nrespectively. We further demonstrate that our value model accurately predicts\nprompt difficulty and allows PCL to focus on progressively more challenging\nprompts during RL. Our results present a new methodology that delivers improved\ntradeoff between upper-bound performance and efficiency for reasoning-focused\nRL.",
      "authors": [
        "Zhaolin Gao",
        "Joongwon Kim",
        "Wen Sun",
        "Thorsten Joachims",
        "Sid Wang",
        "Richard Yuanzhe Pang",
        "Liang Tan"
      ],
      "published": "2025-10-01T17:24:28Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01135v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出提示课程学习(PCL)，一种轻量级强化学习算法，通过价值模型选择中等难度提示来优化大语言模型后训练。该方法避免了昂贵的rollout过程，在MATH和DeepScaleR数据集上分别实现12.1倍和16.9倍的提示筛选加速，在推理性能与训练效率间取得更好平衡。",
      "order": 426
    },
    {
      "arxiv_id": "2510.01132v1",
      "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
      "summary": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
      "authors": [
        "Ruiyi Wang",
        "Prithviraj Ammanabrolu"
      ],
      "published": "2025-10-01T17:23:04Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01132v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对多轮强化学习训练大语言模型代理，系统分析了环境、奖励和策略三大设计要素。通过TextWorld、ALFWorld和SWE-Gym等测试环境，研究发现：环境复杂度影响泛化能力，奖励密度需与RL算法匹配，策略需平衡SFT与RL训练比例。最终提出跨三大要素协同设计的训练方案。",
      "order": 427
    },
    {
      "arxiv_id": "2510.01076v1",
      "title": "Research on the Integration of Embodied Intelligence and Reinforcement\n  Learning in Textual Domains",
      "summary": "This article addresses embodied intelligence and reinforcement learning\nintegration in the field of text processing, aiming to enhance text handling\nwith more intelligence on the basis of embodied intelligence's perception and\naction superiority and reinforcement learning's decision optimization\ncapability. Through detailed theoretical explanation and experimental\nexploration, a novel integration model is introduced. This model has been\ndemonstrated to be very effective in a wide range oftext processing tasks,\nvalidating its applicative potential",
      "authors": [
        "Haonan Wang",
        "Junfeng Sun",
        "Mingjia Zhao",
        "Wei Liu"
      ],
      "published": "2025-10-01T16:21:04Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01076v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究具身智能与强化学习在文本处理领域的融合，结合具身智能的感知行动优势和强化学习的决策优化能力，提出了一种新型融合模型。通过理论分析和实验验证，该模型在多种文本处理任务中表现出色，展现了广泛的应用潜力。",
      "order": 428
    },
    {
      "arxiv_id": "2510.01052v1",
      "title": "Hybrid Dialogue State Tracking for Persian Chatbots: A Language\n  Model-Based Approach",
      "summary": "Dialogue State Tracking (DST) is an essential element of conversational AI\nwith the objective of deeply understanding the conversation context and leading\nit toward answering user requests. Due to high demands for open-domain and\nmulti-turn chatbots, the traditional rule-based DST is not efficient enough,\nsince it cannot provide the required adaptability and coherence for human-like\nexperiences in complex conversations. This study proposes a hybrid DST model\nthat utilizes rule-based methods along with language models, including BERT for\nslot filling and intent detection, XGBoost for intent validation, GPT for DST,\nand online agents for real-time answer generation. This model is uniquely\ndesigned to be evaluated on a comprehensive Persian multi-turn dialogue dataset\nand demonstrated significantly improved accuracy and coherence over existing\nmethods in Persian-based chatbots. The results demonstrate how effectively a\nhybrid approach may improve DST capabilities, paving the way for conversational\nAI systems that are more customized, adaptable, and human-like.",
      "authors": [
        "Samin Mahdipour Aghabagher",
        "Saeedeh Momtazi"
      ],
      "published": "2025-10-01T15:57:19Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01052v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出了一种混合对话状态跟踪模型，结合规则方法与语言模型（包括BERT用于槽填充和意图检测、XGBoost用于意图验证、GPT用于DST），在波斯语多轮对话数据集上显著提升了准确性和连贯性，为波斯语聊天机器人提供了更人性化的对话体验。",
      "order": 429
    },
    {
      "arxiv_id": "2510.01051v1",
      "title": "GEM: A Gym for Agentic LLMs",
      "summary": "The training paradigm for large language models (LLMs) is moving from static\ndatasets to experience-based learning, where agents acquire skills via\ninteracting with complex environments. To facilitate this transition we\nintroduce GEM (General Experience Maker), an open-source environment simulator\ndesigned for the age of LLMs. Analogous to OpenAI-Gym for traditional\nreinforcement learning (RL), GEM provides a standardized framework for the\nenvironment-agent interface, including asynchronous vectorized execution for\nhigh throughput, and flexible wrappers for easy extensibility. GEM also\nfeatures a diverse suite of environments, robust integrated tools, and\nsingle-file example scripts demonstrating using GEM with five popular RL\ntraining frameworks. Along with this, we also provide a set of baselines across\n24 environments using REINFORCE with Return Batch Normalization (ReBN), which\n-- unlike GRPO -- is compatible with the full RL setting of dense per-turn\nrewards and offers better credit assignment. We further conduct apple-to-apple\nbenchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings\nusing GEM to shed light on the algorithmic designs. Lastly, GEM also functions\nas a convenient evaluation toolkit besides a training environment. We hope this\nframework can help accelerate future agentic LLM research.",
      "authors": [
        "Zichen Liu",
        "Anya Sims",
        "Keyu Duan",
        "Changyu Chen",
        "Simon Yu",
        "Xiangxin Zhou",
        "Haotian Xu",
        "Shaopan Xiong",
        "Bo Liu",
        "Chenmien Tan",
        "Chuen Yang Beh",
        "Weixun Wang",
        "Hao Zhu",
        "Weiyan Shi",
        "Diyi Yang",
        "Michael Shieh",
        "Yee Whye Teh",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "published": "2025-10-01T15:55:57Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01051v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "GEM是一个专为大型语言模型设计的开源环境模拟器，类似于传统强化学习中的OpenAI-Gym。它提供标准化的环境-代理接口、异步向量化执行、多样化环境套件和集成工具，支持与五种主流RL训练框架集成。论文还通过基准测试比较了PPO、GRPO和REINFORCE算法在不同设置下的性能，旨在加速未来智能LLM代理的研究发展。",
      "order": 430
    },
    {
      "arxiv_id": "2510.01048v1",
      "title": "Interpreting Language Models Through Concept Descriptions: A Survey",
      "summary": "Understanding the decision-making processes of neural networks is a central\ngoal of mechanistic interpretability. In the context of Large Language Models\n(LLMs), this involves uncovering the underlying mechanisms and identifying the\nroles of individual model components such as neurons and attention heads, as\nwell as model abstractions such as the learned sparse features extracted by\nSparse Autoencoders (SAEs). A rapidly growing line of work tackles this\nchallenge by using powerful generator models to produce open-vocabulary,\nnatural language concept descriptions for these components. In this paper, we\nprovide the first survey of the emerging field of concept descriptions for\nmodel components and abstractions. We chart the key methods for generating\nthese descriptions, the evolving landscape of automated and human metrics for\nevaluating them, and the datasets that underpin this research. Our synthesis\nreveals a growing demand for more rigorous, causal evaluation. By outlining the\nstate of the art and identifying key challenges, this survey provides a roadmap\nfor future research toward making models more transparent.",
      "authors": [
        "Nils Feldhus",
        "Laura Kopf"
      ],
      "published": "2025-10-01T15:51:44Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01048v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文首次系统综述了通过自然语言概念描述解释语言模型的新兴领域，涵盖概念生成方法、评估指标与数据集，指出当前研究需加强因果验证，为提升模型透明度提供路线图。",
      "order": 431
    },
    {
      "arxiv_id": "2510.01047v1",
      "title": "Authentic Discrete Diffusion Model",
      "summary": "We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally\nredefines prior pseudo-discrete approaches by preserving core diffusion\ncharacteristics directly in the one-hot space through a suite of coordinated\nmechanisms. Unlike conventional \"pseudo\" discrete diffusion (PDD) methods, ADD\nreformulates the diffusion input by directly using float-encoded one-hot class\ndata, without relying on diffusing in the continuous latent spaces or masking\npolicies. At its core, a timestep-conditioned cross-entropy loss is introduced\nbetween the diffusion model's outputs and the original one-hot labels. This\nsynergistic design establishes a bridge between discriminative and generative\nlearning. Our experiments demonstrate that ADD not only achieves superior\nperformance on classification tasks compared to the baseline, but also exhibits\nexcellent text generation capabilities on Image captioning. Extensive ablations\nvalidate the measurable gains of each component.",
      "authors": [
        "Xiao Li",
        "Jiaqi Zhang",
        "Shuxiang Zhang",
        "Tianshui Chen",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "published": "2025-10-01T15:51:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01047v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "提出真实离散扩散(ADD)框架，通过在一热空间中保留核心扩散特性，重新定义离散扩散方法。该框架直接使用浮点编码的一热类别数据，引入时间步条件交叉熵损失，在分类任务和图像描述文本生成中均表现出优越性能。",
      "order": 432
    },
    {
      "arxiv_id": "2510.01028v1",
      "title": "Syntax-Guided Diffusion Language Models with User-Integrated\n  Personalization",
      "summary": "Large language models have made revolutionary progress in generating\nhuman-like text, yet their outputs often tend to be generic, exhibiting\ninsufficient structural diversity, which limits personalized expression. Recent\nadvances in diffusion models have opened new opportunities for improving\nlanguage generation beyond the limitations of autoregressive paradigms. In this\nwork, we propose a syntax-guided diffusion language model that integrates\nstructural supervision and personalized conditioning to enhance text quality,\ndiversity, and controllability. We introduce a cascaded framework that\ngenerates syntactic guidance before conditional text generation, and further\ngeneralize it to a novel noncascaded architecture for better alignment between\nstructure and content. By incorporating syntactic information in the generating\nprocess, the proposed model better captures the lexical and structural\ncharacteristics of stylistic sentence construction. To enable fine-grained\npersonalization, we develop a shared representation mechanism that facilitates\ninformation integration across users, supporting both faithful stylistic\ngeneration and generalizable zero-shot inference. Extensive experiments on\nmultiple tasks demonstrate the superiority of our approach in fluency,\ndiversity, and stylistic fidelity. Further qualitative analyses highlight its\ninterpretability and flexibility in learning personalized patterns.",
      "authors": [
        "Ruqian Zhang",
        "Yijiao Zhang",
        "Juan Shen",
        "Zhongyi Zhu",
        "Annie Qu"
      ],
      "published": "2025-10-01T15:33:12Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01028v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种语法引导的扩散语言模型，通过结构监督和个性化条件增强文本生成的质量、多样性和可控性。该模型采用级联和非级联架构，在生成过程中融入句法信息以更好地捕捉风格化句子的词汇和结构特征，并通过共享表示机制实现细粒度个性化，在多个任务上展现出优越的流畅性、多样性和风格保真度。",
      "order": 433
    },
    {
      "arxiv_id": "2510.01025v1",
      "title": "Shape Happens: Automatic Feature Manifold Discovery in LLMs via\n  Supervised Multi-Dimensional Scaling",
      "summary": "The linear representation hypothesis states that language models (LMs) encode\nconcepts as directions in their latent space, forming organized,\nmultidimensional manifolds. Prior efforts focus on discovering specific\ngeometries for specific features, and thus lack generalization. We introduce\nSupervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to\nautomatically discover feature manifolds. We apply SMDS to temporal reasoning\nas a case study, finding that different features form various geometric\nstructures such as circles, lines, and clusters. SMDS reveals many insights on\nthese structures: they consistently reflect the properties of the concepts they\nrepresent; are stable across model families and sizes; actively support\nreasoning in models; and dynamically reshape in response to context changes.\nTogether, our findings shed light on the functional role of feature manifolds,\nsupporting a model of entity-based reasoning in which LMs encode and transform\nstructured representations.",
      "authors": [
        "Federico Tiblias",
        "Irina Bigoulaeva",
        "Jingcheng Niu",
        "Simone Balloccu",
        "Iryna Gurevych"
      ],
      "published": "2025-10-01T15:30:47Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01025v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出监督多维缩放(SMDS)方法，自动发现语言模型中特征流形的几何结构。研究表明不同特征形成圆形、线性等多样几何形态，这些结构稳定反映概念属性、支持模型推理，并随语境动态变化，揭示了特征流形在实体推理中的功能作用。",
      "order": 434
    },
    {
      "arxiv_id": "2510.01003v1",
      "title": "Improving Code Localization with Repository Memory",
      "summary": "Code localization is a fundamental challenge in repository-level software\nengineering tasks such as bug fixing. While existing methods equip language\nagents with comprehensive tools/interfaces to fetch information from the\nrepository, they overlook the critical aspect of memory, where each instance is\ntypically handled from scratch assuming no prior repository knowledge. In\ncontrast, human developers naturally build long-term repository memory, such as\nthe functionality of key modules and associations between various bug types and\ntheir likely fix locations. In this work, we augment language agents with such\nmemory by leveraging a repository's commit history - a rich yet underutilized\nresource that chronicles the codebase's evolution. We introduce tools that\nallow the agent to retrieve from a non-parametric memory encompassing recent\nhistorical commits and linked issues, as well as functionality summaries of\nactively evolving parts of the codebase identified via commit patterns. We\ndemonstrate that augmenting such a memory can significantly improve LocAgent, a\nstate-of-the-art localization framework, on both SWE-bench-verified and the\nmore recent SWE-bench-live benchmarks. Our research contributes towards\ndeveloping agents that can accumulate and leverage past experience for\nlong-horizon tasks, more closely emulating the expertise of human developers.",
      "authors": [
        "Boshi Wang",
        "Weijian Xu",
        "Yunsheng Li",
        "Mei Gao",
        "Yujia Xie",
        "Huan Sun",
        "Dongdong Chen"
      ],
      "published": "2025-10-01T15:10:15Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.01003v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "code_generation",
      "tldr_zh": "本研究通过利用代码仓库的提交历史，为语言智能体构建长期存储库记忆，显著提升了代码定位能力。该方法在SWE-bench基准测试中验证了有效性，使智能体能够积累和利用历史经验，更接近人类开发者的专业水平。",
      "order": 435
    },
    {
      "arxiv_id": "2510.00982v1",
      "title": "Spiralformer: Low Latency Encoder for Streaming Speech Recognition with\n  Circular Layer Skipping and Early Exiting",
      "summary": "For streaming speech recognition, a Transformer-based encoder has been widely\nused with block processing. Although many studies addressed improving emission\nlatency of transducers, little work has been explored for improving encoding\nlatency of the block processing. We seek to reduce latency by frequently\nemitting a chunk with a small shift rather than scarce large-chunk emissions,\nresulting in higher computational costs. To efficiently compute with the small\nchunk shift, we propose a new encoder, Spiralformer, tailored for block\nprocessing by combining layer dropping and early exiting. We skip layer\ncomputation in a cyclic manner and shift the computed layer in each block\nspirally, which completes computation for all the layers over the block\nprocessing. Experimentally, we observed that our method achieved 21.6%\nreduction in the averaged token emission delay in Librispeech, and 7.0% in CSJ,\ncompared with the baseline with similar computational cost and word error\nrates.",
      "authors": [
        "Emiru Tsunoo",
        "Hayato Futami",
        "Yosuke Kashiwagi",
        "Siddhant Arora",
        "Shinji Watanabe"
      ],
      "published": "2025-10-01T14:56:45Z",
      "primary_category": "eess.AS",
      "arxiv_url": "https://arxiv.org/abs/2510.00982v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "Spiralformer是一种用于流式语音识别的低延迟编码器，通过循环层跳过和提前退出机制，在小块移位下高效处理音频，在Librispeech和CSJ数据集上分别实现了21.6%和7.0%的平均令牌发射延迟降低，同时保持相近的计算成本和词错误率。",
      "order": 436
    },
    {
      "arxiv_id": "2510.00977v1",
      "title": "It Takes Two: Your GRPO Is Secretly DPO",
      "summary": "Group Relative Policy Optimization (GRPO) is a prominent reinforcement\nlearning algorithm for post-training Large Language Models (LLMs). It is\ncommonly believed that GRPO necessitates a large group size to ensure stable\ntraining via precise statistical estimation, which incurs substantial\ncomputational overhead. In this work, we challenge this assumption by reframing\nGRPO as a form of contrastive learning, which reveals a fundamental connection\nto Direct Preference Optimization (DPO). Motivated by DPO's empirical success,\nwe investigate the minimal two-rollout case (2-GRPO), a configuration\npreviously deemed infeasible. We provide a rigorous theoretical analysis to\nvalidate 2-GRPO and demonstrate empirically that it achieves performance on par\nwith 16-GRPO, despite using only 1/8 of the rollouts and reducing training time\nby over 70%.",
      "authors": [
        "Yihong Wu",
        "Liheng Ma",
        "Lei Ding",
        "Muzhi Li",
        "Xinyu Wang",
        "Kejia Chen",
        "Zhan Su",
        "Zhanguang Zhang",
        "Chenyang Huang",
        "Yingxue Zhang",
        "Mark Coates",
        "Jian-Yun Nie"
      ],
      "published": "2025-10-01T14:52:11Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00977v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究挑战了GRPO算法需要大组规模的传统认知，通过将其重构为对比学习形式，揭示了与DPO的根本联系。理论分析和实验证明，仅需两个rollout的2-GRPO即可达到与16-GRPO相当的性能，同时减少70%以上训练时间和7/8的rollout需求。",
      "order": 437
    },
    {
      "arxiv_id": "2510.00962v1",
      "title": "Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning\n  Benchmarks",
      "summary": "Large language models (LLMs) are ubiquitous in modern day natural language\nprocessing. However, previous work has shown degraded LLM performance for\nunder-represented English dialects. We analyze the effects of typifying\n\"standard\" American English language questions as non-\"standard\" dialectal\nvariants on multiple choice question answering tasks and find up to a 20%\nreduction in accuracy. Additionally, we investigate the grammatical basis of\nunder-performance in non-\"standard\" English questions. We find that individual\ngrammatical rules have varied effects on performance, but some are more\nconsequential than others: three specific grammar rules (existential \"it\", zero\ncopula, and y'all) can explain the majority of performance degradation observed\nin multiple dialects. We call for future work to investigate bias mitigation\nmethods focused on individual, high-impact grammatical structures.",
      "authors": [
        "Eileen Pan",
        "Anna Seo Gyeong Choi",
        "Maartje ter Hoeve",
        "Skyler Seto",
        "Allison Koenecke"
      ],
      "published": "2025-10-01T14:35:16Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00962v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究分析大语言模型在非标准英语方言上的表现偏差，发现在选择题任务中准确率下降高达20%，并识别出三个关键语法规则（存在性'it'、零系词和'y'all'）是导致多方言性能下降的主要原因，呼吁未来研究针对高影响语法结构开发偏见缓解方法。",
      "order": 438
    },
    {
      "arxiv_id": "2510.00931v1",
      "title": "Making, not Taking, the Best of N",
      "summary": "Obtaining high-quality generations in modern LLMs has largely been framed as\na selection problem: identifying a single winning generation from a diverse\npool of N samples, the Best-of-N (BoN). Yet, this approach is inherently\nzero-sum, discarding diverse and potentially useful information from the pool.\nInstead, we explore a collaborative setup, where all candidates can potentially\ncontribute to the final winning generation. To this end, we propose Fusion-of-N\n(FusioN): a method that uses a general LLM judge to synthesize the most\ninformative elements of each sample into a single final answer. We compare\nFusioN to BoN in two settings, (i) test-time scaling, where we sample and\naggregate from a single model at test-time (ii) synthetic data generation,\nwhere we fuse samples from a pool of diverse teachers to improve a student\nmodel. We extensively benchmark both setups across 11 languages, 3 diverse\ntasks and varying model scales. Across the bench, FusioN consistently\noutperforms BoN showing versatility and robustness both in test-time scaling\nand in downstream gains from synthetic data generation. We also perform\nextensive analysis on FusioN, where it shows surprising strengths and\nrobustness under challenging settings. These results show that we should shift\nhow we think about evaluating and utilizing LLM generations from a monolithic\nmeasure of quality, to embracing their polylithic nature. This shift allows us\nto integrate diverse strengths, unlock latent potential, and achieve\nimprovements that were previously inaccessible through selection alone.",
      "authors": [
        "Ammar Khairi",
        "Daniel D'souza",
        "Marzieh Fadaee",
        "Julia Kreutzer"
      ],
      "published": "2025-10-01T14:14:31Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00931v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Fusion-of-N方法，通过LLM法官融合多个生成样本的优势元素，替代传统的Best-of-N选择策略。在测试时扩展和合成数据生成两种场景下，该方法在11种语言、3类任务中均优于BoN，展现了利用生成多样性的新范式。",
      "order": 439
    },
    {
      "arxiv_id": "2510.00919v2",
      "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in\n  Olympic-Level Physics Problem Solving",
      "summary": "Retrieval-augmented generation (RAG) with foundation models has achieved\nstrong performance across diverse tasks, but their capacity for expert-level\nreasoning-such as solving Olympiad-level physics problems-remains largely\nunexplored. Inspired by the way students prepare for competitions by reviewing\npast problems, we investigate the potential of RAG to enhance physics reasoning\nin foundation models. We introduce PhoPile, a high-quality multimodal dataset\nspecifically designed for Olympiad-level physics, enabling systematic study of\nretrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,\ncapturing the inherently multimodal nature of physics problem solving. Using\nPhoPile, we benchmark RAG-augmented foundation models, covering both large\nlanguage models (LLMs) and large multimodal models (LMMs) with multiple\nretrievers. Our results demonstrate that integrating retrieval with physics\ncorpora can improve model performance, while also highlighting challenges that\nmotivate further research in retrieval-augmented physics reasoning.",
      "authors": [
        "Shunfeng Zheng",
        "Yudi Zhang",
        "Meng Fang",
        "Zihan Zhang",
        "Zhitan Wu",
        "Mykola Pechenizkiy",
        "Ling Chen"
      ],
      "published": "2025-10-01T13:57:53Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00919v2",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "education_ai",
      "tldr_zh": "本研究引入PhoPile数据集，系统评估检索增强生成在奥林匹克物理问题解决中的表现，发现结合物理知识检索能提升基础模型性能，同时揭示了多模态推理面临的挑战。",
      "order": 440
    },
    {
      "arxiv_id": "2510.00908v1",
      "title": "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval\n  with Multilingual LLMs",
      "summary": "Cross-lingual information retrieval (CLIR) addresses the challenge of\nretrieving relevant documents written in languages different from that of the\noriginal query. Research in this area has typically framed the task as\nmonolingual retrieval augmented by translation, treating retrieval methods and\ncross-lingual capabilities in isolation. Both monolingual and cross-lingual\nretrieval usually follow a pipeline of query expansion, ranking, re-ranking\nand, increasingly, question answering. Recent advances, however, have shifted\nfrom translation-based methods toward embedding-based approaches and leverage\nmultilingual large language models (LLMs), for which aligning representations\nacross languages remains a central challenge. The emergence of cross-lingual\nembeddings and multilingual LLMs has introduced a new paradigm, offering\nimproved retrieval performance and enabling answer generation. This survey\nprovides a comprehensive overview of developments from early translation-based\nmethods to state-of-the-art embedding-driven and generative techniques. It\npresents a structured account of core CLIR components, evaluation practices,\nand available resources. Persistent challenges such as data imbalance and\nlinguistic variation are identified, while promising directions are suggested\nfor advancing equitable and effective cross-lingual information retrieval. By\nsituating CLIR within the broader landscape of information retrieval and\nmultilingual language processing, this work not only reviews current\ncapabilities but also outlines future directions for building retrieval systems\nthat are robust, inclusive, and adaptable.",
      "authors": [
        "Roksana Goworek",
        "Olivia Macmillan-Scott",
        "Eda B. Özyiğit"
      ],
      "published": "2025-10-01T13:50:05Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.00908v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文综述跨语言信息检索(CLIR)的发展，从早期翻译方法到基于嵌入和多语言大语言模型的先进技术。重点分析跨语言表示对齐的核心挑战，涵盖检索组件、评估方法和资源，指出数据不平衡和语言变异等持续问题，并展望构建鲁棒、包容、自适应检索系统的未来方向。",
      "order": 441
    },
    {
      "arxiv_id": "2510.00890v1",
      "title": "Span-level Detection of AI-generated Scientific Text via Contrastive\n  Learning and Structural Calibration",
      "summary": "The rapid adoption of large language models (LLMs) in scientific writing\nraises serious concerns regarding authorship integrity and the reliability of\nscholarly publications. Existing detection approaches mainly rely on\ndocument-level classification or surface-level statistical cues; however, they\nneglect fine-grained span localization, exhibit weak calibration, and often\nfail to generalize across disciplines and generators. To address these\nlimitations, we present Sci-SpanDet, a structure-aware framework for detecting\nAI-generated scholarly texts. The proposed method combines section-conditioned\nstylistic modeling with multi-level contrastive learning to capture nuanced\nhuman-AI differences while mitigating topic dependence, thereby enhancing\ncross-domain robustness. In addition, it integrates BIO-CRF sequence labeling\nwith pointer-based boundary decoding and confidence calibration to enable\nprecise span-level detection and reliable probability estimates. Extensive\nexperiments on a newly constructed cross-disciplinary dataset of 100,000\nannotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,\nLLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with\nF1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows\nstrong resilience under adversarial rewriting and maintains balanced accuracy\nacross IMRaD sections and diverse disciplines, substantially surpassing\nexisting baselines. To ensure reproducibility and to foster further research on\nAI-generated text detection in scholarly documents, the curated dataset and\nsource code will be publicly released upon publication.",
      "authors": [
        "Zhen Yin",
        "Shenghua Wang"
      ],
      "published": "2025-10-01T13:35:14Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00890v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "Sci-SpanDet是一种基于对比学习和结构校准的AI生成科学文本检测框架，通过章节条件风格建模和多层次对比学习捕获人机写作差异，结合BIO-CRF序列标注实现细粒度片段定位，在跨学科数据集上达到SOTA性能，F1(AI)达80.17，对对抗性重写具有强鲁棒性。",
      "order": 442
    },
    {
      "arxiv_id": "2510.00880v1",
      "title": "HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate\n  Hallucinations in Retrieval-Augmented Generation",
      "summary": "Large Language Models (LLMs) excel in many NLP tasks but remain prone to\nhallucinations, limiting trust in real-world applications. We present\nHalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating\nhallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies\ndocument-claim pairs as grounded or hallucinated and produces evidence-grounded\njustifications for transparency. Our approach combines (i) a domain-agnostic\nsynthetic dataset derived from FineWeb and refined through multi-stage curation\nand data reformation, (ii) synthetic grounded and hallucinated claims, and\n(iii) preference-based fine-tuning with Odds Ratio Preference Optimization to\ndistill large-model reasoning into a smaller backbone. On the RAGTruth subset\nof the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy\n(BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian\n3.3 (8B; 82.2%) while using roughly half their parameters. Over the full\nbenchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as\nGPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon\nacceptance.",
      "authors": [
        "Loris Bergeron",
        "Ioana Buhnila",
        "Jérôme François",
        "Radu State"
      ],
      "published": "2025-10-01T13:28:20Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00880v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "HalluGuard是一个40亿参数的小型推理模型，专门用于缓解检索增强生成中的幻觉问题。该模型通过证据基础分类文档-声明对，并生成透明化解释，在保持参数效率的同时达到与更大模型相当的准确率。",
      "order": 443
    },
    {
      "arxiv_id": "2510.00866v2",
      "title": "The Data-Quality Illusion: Rethinking Classifier-Based Quality Filtering\n  for LLM Pretraining",
      "summary": "Large-scale models are pretrained on massive web-crawled datasets containing\ndocuments of mixed quality, making data filtering essential. A popular method\nis Classifier-based Quality Filtering (CQF), which trains a binary classifier\nto distinguish between pretraining data and a small, high-quality set. It\nassigns each pretraining document a quality score defined as the classifier's\nscore and retains only the top-scoring ones. We provide an in-depth analysis of\nCQF. We show that while CQF improves downstream task performance, it does not\nnecessarily enhance language modeling on the high-quality dataset. We explain\nthis paradox by the fact that CQF implicitly filters the high-quality dataset\nas well. We further compare the behavior of models trained with CQF to those\ntrained on synthetic data of increasing quality, obtained via random token\npermutations, and find starkly different trends. Our results challenge the view\nthat CQF captures a meaningful notion of data quality.",
      "authors": [
        "Thiziri Nait Saada",
        "Louis Bethune",
        "Michal Klein",
        "David Grangier",
        "Marco Cuturi",
        "Pierre Ablin"
      ],
      "published": "2025-10-01T13:15:15Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00866v2",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文深入分析了基于分类器的质量过滤方法在LLM预训练中的应用，发现该方法虽能提升下游任务性能，但未必改善高质量数据集上的语言建模能力。研究通过对比合成数据训练模型的行为，质疑了CQF方法对数据质量的有效衡量，揭示了其质量幻觉现象。",
      "order": 444
    },
    {
      "arxiv_id": "2510.00861v1",
      "title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented\n  LLMs",
      "summary": "While search-augmented large language models (LLMs) exhibit impressive\ncapabilities, their reliability in complex multi-hop reasoning remains limited.\nThis limitation arises from three fundamental challenges: decomposition errors,\nwhere tasks are incorrectly broken down; retrieval missing, where key evidence\nfails to be retrieved; and reasoning errors, where flawed logic propagates\nthrough the reasoning chain. A single failure in any of these stages can derail\nthe final answer. We propose Erasable Reinforcement Learning (ERL), a novel\nframework that transforms fragile reasoning into a robust process. ERL\nexplicitly identifies faulty steps, erases them, and regenerates reasoning in\nplace, preventing defective logic from propagating through the reasoning chain.\nThis targeted correction mechanism turns brittle reasoning into a more\nresilient process. Models trained with ERL, termed ESearch, achieve substantial\nimprovements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model\nachieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and\n+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest\nthat erasable reinforcement learning provides a powerful paradigm shift for\nrobust multi-step reasoning in LLMs.",
      "authors": [
        "Ziliang Wang",
        "Kang An",
        "Xuhui Zheng",
        "Faqiang Qian",
        "Weikun Zhang",
        "Cijun Ouyang",
        "Jialu Cai",
        "Yuhang Wang",
        "Yichao Wu"
      ],
      "published": "2025-10-01T13:10:36Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00861v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出可擦除强化学习(ERL)框架，解决检索增强大语言模型在多跳推理中的三大挑战：分解错误、检索缺失和推理错误。ERL通过识别错误步骤、擦除并重新生成推理，防止错误逻辑在推理链中传播。实验表明，ESearch模型在多个基准测试中显著超越现有最优结果，为LLM的鲁棒多步推理提供了新范式。",
      "order": 445
    },
    {
      "arxiv_id": "2510.00857v1",
      "title": "ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous\n  LLMs",
      "summary": "As large language models (LLMs) evolve from conversational assistants into\nautonomous agents, evaluating the safety of their actions becomes critical.\nPrior safety benchmarks have primarily focused on preventing generation of\nharmful content, such as toxic text. However, they overlook the challenge of\nagents taking harmful actions when the most effective path to an operational\ngoal conflicts with human safety. To address this gap, we introduce\nManagerBench, a benchmark that evaluates LLM decision-making in realistic,\nhuman-validated managerial scenarios. Each scenario forces a choice between a\npragmatic but harmful action that achieves an operational goal, and a safe\naction that leads to worse operational performance. A parallel control set,\nwhere potential harm is directed only at inanimate objects, measures a model's\npragmatism and identifies its tendency to be overly safe. Our findings indicate\nthat the frontier LLMs perform poorly when navigating this safety-pragmatism\ntrade-off. Many consistently choose harmful options to advance their\noperational goals, while others avoid harm only to become overly safe and\nineffective. Critically, we find this misalignment does not stem from an\ninability to perceive harm, as models' harm assessments align with human\njudgments, but from flawed prioritization. ManagerBench is a challenging\nbenchmark for a core component of agentic behavior: making safe choices when\noperational goals and alignment values incentivize conflicting actions.\nBenchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.",
      "authors": [
        "Adi Simhi",
        "Jonathan Herzig",
        "Martin Tutek",
        "Itay Itzhak",
        "Idan Szpektor",
        "Yonatan Belinkov"
      ],
      "published": "2025-10-01T13:08:33Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00857v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ManagerBench基准，用于评估自主LLM在安全性与实用性之间的权衡能力。该基准通过真实管理场景测试模型决策，发现前沿LLM在此权衡中表现不佳：要么为达成目标选择有害行为，要么过度保守导致效率低下。研究表明问题不在于危害识别能力，而在于优先级排序缺陷。",
      "order": 446
    },
    {
      "arxiv_id": "2510.00855v1",
      "title": "Can World Models Benefit VLMs for World Dynamics?",
      "summary": "Trained on internet-scale video data, generative world models are\nincreasingly recognized as powerful world simulators that can generate\nconsistent and plausible dynamics over structure, motion, and physics. This\nraises a natural question: with the advent of strong video foundational models,\nmight they supplant conventional vision encoder paradigms for general-purpose\nmultimodal understanding? While recent studies have begun to explore the\npotential of world models on common vision tasks, these explorations typically\nlack a systematic investigation of generic, multimodal tasks. In this work, we\nstrive to investigate the capabilities when world model priors are transferred\ninto Vision-Language Models: we re-purpose a video diffusion model as a\ngenerative encoder to perform a single denoising step and treat the resulting\nlatents as a set of visual embedding. We empirically investigate this class of\nmodels, which we refer to as World-Language Models (WorldLMs), and we find that\ngenerative encoders can capture latents useful for downstream understanding\nthat show distinctions from conventional encoders. Naming our best-performing\nvariant Dynamic Vision Aligner (DyVA), we further discover that this method\nsignificantly enhances spatial reasoning abilities and enables single-image\nmodels to perform multi-frame reasoning. Through the curation of a suite of\nvisual reasoning tasks, we find DyVA to surpass both open-source and\nproprietary baselines, achieving state-of-the-art or comparable performance. We\nattribute these gains to WorldLM's inherited motion-consistency internalization\nfrom video pre-training. Finally, we systematically explore extensive model\ndesigns to highlight promising directions for future work. We hope our study\ncan pave the way for a new family of VLMs that leverage priors from world\nmodels and are on a promising path towards generalist vision learners.",
      "authors": [
        "Kevin Zhang",
        "Kuangzhi Ge",
        "Xiaowei Chi",
        "Renrui Zhang",
        "Shaojun Shi",
        "Zhen Dong",
        "Sirui Han",
        "Shanghang Zhang"
      ],
      "published": "2025-10-01T13:07:05Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00855v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探讨将世界模型先验知识融入视觉语言模型(VLMs)的潜力，提出名为WorldLM的新型架构，其中最佳变体DyVA通过视频扩散模型的生成编码器显著增强空间推理能力，使单图像模型具备多帧推理功能，在视觉推理任务中达到领先水平。",
      "order": 447
    },
    {
      "arxiv_id": "2510.00845v2",
      "title": "Mechanistic Interpretability as Statistical Estimation: A Variance\n  Analysis of EAP-IG",
      "summary": "The development of trustworthy artificial intelligence requires moving beyond\nblack-box performance metrics toward an understanding of models' internal\ncomputations. Mechanistic Interpretability (MI) aims to meet this need by\nidentifying the algorithmic mechanisms underlying model behaviors. Yet, the\nscientific rigor of MI critically depends on the reliability of its findings.\nIn this work, we argue that interpretability methods, such as circuit\ndiscovery, should be viewed as statistical estimators, subject to questions of\nvariance and robustness. To illustrate this statistical framing, we present a\nsystematic stability analysis of a state-of-the-art circuit discovery method:\nEAP-IG. We evaluate its variance and robustness through a comprehensive suite\nof controlled perturbations, including input resampling, prompt paraphrasing,\nhyperparameter variation, and injected noise within the causal analysis itself.\nAcross a diverse set of models and tasks, our results demonstrate that EAP-IG\nexhibits high structural variance and sensitivity to hyperparameters,\nquestioning the stability of its findings. Based on these results, we offer a\nset of best-practice recommendations for the field, advocating for the routine\nreporting of stability metrics to promote a more rigorous and statistically\ngrounded science of interpretability.",
      "authors": [
        "Maxime Méloux",
        "François Portet",
        "Maxime Peyrard"
      ],
      "published": "2025-10-01T12:55:34Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00845v2",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究将机制可解释性视为统计估计问题，对EAP-IG电路发现方法进行方差分析。通过输入重采样、提示改写、超参数变化等扰动测试，发现该方法存在高结构方差和超参数敏感性，质疑其发现稳定性。建议在可解释性研究中常规报告稳定性指标以提升科学严谨性。",
      "order": 448
    },
    {
      "arxiv_id": "2510.00829v1",
      "title": "Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based\n  Machine Translation",
      "summary": "\\textbf{RE}trieval-\\textbf{A}ugmented \\textbf{L}LM-based \\textbf{M}achine\n\\textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like\nidiomatic translation, but its reliability under noisy retrieval contexts\nremains poorly understood despite this being a common challenge in real-world\ndeployment. To address this gap, we propose a noise synthesis framework and new\nmetrics to evaluate the robustness of REAL-MT systematically. Using this\nframework, we instantiate REAL-MT with Qwen-series models, including standard\nLLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate\ntheir performance on idiomatic translation across high-, medium-, and\nlow-resource language pairs under synthesized noise. Our results show that\nlow-resource language pairs, which rely more heavily on retrieved context,\ndegrade more severely under noise than high-resource ones and often produce\nnonsensical translations. Although LRMs possess enhanced reasoning\ncapabilities, they show no improvement in error correction and are even more\nsusceptible to noise, tending to rationalize incorrect contexts. We find that\nthis stems from an attention shift away from the source idiom to noisy content,\nwhile confidence increases despite declining accuracy, indicating poor\ncalibration. To mitigate these issues, we investigate training-free and\nfine-tuning strategies, which improve robustness at the cost of performance in\nclean contexts, revealing a fundamental trade-off. Our findings highlight the\nlimitations of current approaches, underscoring the need for self-verifying\nintegration mechanisms.",
      "authors": [
        "Yanming Sun",
        "Runzhe Zhan",
        "Chi Seng Cheang",
        "Han Wu",
        "Xuebo Liu",
        "Yuyao Niu",
        "Fengying Ye",
        "Kaixin Lan",
        "Lidia S. Chao",
        "Derek F. Wong"
      ],
      "published": "2025-10-01T12:43:55Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00829v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究系统评估了检索增强LLM机器翻译(REAL-MT)在噪声环境下的鲁棒性。通过构建噪声合成框架，发现低资源语言对在噪声干扰下性能急剧下降，大推理模型虽具增强推理能力但更易受噪声影响且会合理化错误上下文。研究揭示了注意力从源语习语向噪声内容偏移的问题，并提出无需训练与微调两种改进策略，在清洁语境性能与噪声鲁棒性间存在根本性权衡。",
      "order": 449
    },
    {
      "arxiv_id": "2510.00810v1",
      "title": "Family Matters: Language Transfer and Merging for Adapting Small LLMs to\n  Faroese",
      "summary": "We investigate how to adapt small, efficient LLMs to Faroese, a low-resource\nNorth Germanic language. Starting from English models, we continue pre-training\non related Scandinavian languages, either individually or combined via merging,\nbefore fine-tuning on Faroese. We compare full fine-tuning with\nparameter-efficient tuning using LoRA, evaluating their impact on both\nlinguistic accuracy and text comprehension. Due to the lack of existing Faroese\nevaluation data, we construct two new minimal-pair benchmarks from adapted and\nnewly collected datasets and complement them with human evaluations by Faroese\nlinguists. Our results demonstrate that transfer from related languages is\ncrucial, though the optimal source language depends on the task: Icelandic\nenhances linguistic accuracy, whereas Danish boosts comprehension. Similarly,\nthe choice between full fine-tuning and LoRA is task-dependent: LoRA improves\nlinguistic acceptability and slightly increases human evaluation scores on the\nbase model, while full fine-tuning yields stronger comprehension performance\nand better preserves model capabilities during downstream fine-tuning.",
      "authors": [
        "Jenny Kunz",
        "Iben Nyholm Debess",
        "Annika Simonsen"
      ],
      "published": "2025-10-01T12:17:09Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00810v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探索如何将小型高效LLM适配到低资源法罗语。通过从英语模型出发，在相关斯堪的纳维亚语言上继续预训练，再对法罗语进行微调。实验表明：冰岛语提升语言准确性，丹麦语增强理解能力；LoRA优化语言可接受性，全微调则强化理解性能并保持模型能力。",
      "order": 450
    },
    {
      "arxiv_id": "2510.00808v1",
      "title": "What You See is What You Ask: Evaluating Audio Descriptions",
      "summary": "Audio descriptions (ADs) narrate important visual details in movies, enabling\nBlind and Low Vision (BLV) users to understand narratives and appreciate visual\ndetails. Existing works in automatic AD generation mostly focus on few-second\ntrimmed clips, and evaluate them by comparing against a single ground-truth\nreference AD. However, writing ADs is inherently subjective. Through alignment\nand analysis of two independent AD tracks for the same movies, we quantify the\nsubjectivity in when and whether to describe, and what and how to highlight.\nThus, we show that working with trimmed clips is inadequate. We propose ADQA, a\nQA benchmark that evaluates ADs at the level of few-minute long, coherent video\nsegments, testing whether they would help BLV users understand the story and\nappreciate visual details. ADQA features visual appreciation (VA) questions\nabout visual facts and narrative understanding (NU) questions based on the\nplot. Through ADQA, we show that current AD generation methods lag far behind\nhuman-authored ADs. We conclude with several recommendations for future work\nand introduce a public leaderboard for benchmarking.",
      "authors": [
        "Divy Kala",
        "Eshika Khandelwal",
        "Makarand Tapaswi"
      ],
      "published": "2025-10-01T12:14:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00808v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ADQA评估基准，针对音频描述(AD)的主观性挑战，通过分析两部独立AD轨道的对齐，量化描述时机、内容选择的主观差异。该基准基于数分钟连贯视频片段，包含视觉欣赏和叙事理解问题，显示当前自动AD生成方法远落后于人工创作，并为未来研究提供建议和公开排行榜。",
      "order": 451
    },
    {
      "arxiv_id": "2510.00743v1",
      "title": "From Scores to Preferences: Redefining MOS Benchmarking for Speech\n  Quality Reward Modeling",
      "summary": "Assessing the perceptual quality of synthetic speech is crucial for guiding\nthe development and refinement of speech generation models. However, it has\ntraditionally relied on human subjective ratings such as the Mean Opinion Score\n(MOS), which depend on manual annotations and often suffer from inconsistent\nrating standards and poor reproducibility. To address these limitations, we\nintroduce MOS-RMBench, a unified benchmark that reformulates diverse MOS\ndatasets into a preference-comparison setting, enabling rigorous evaluation\nacross different datasets. Building on MOS-RMBench, we systematically construct\nand evaluate three paradigms for reward modeling: scalar reward models,\nsemi-scalar reward models, and generative reward models (GRMs). Our experiments\nreveal three key findings: (1) scalar models achieve the strongest overall\nperformance, consistently exceeding 74% accuracy; (2) most models perform\nconsiderably worse on synthetic speech than on human speech; and (3) all models\nstruggle on pairs with very small MOS differences. To improve performance on\nthese challenging pairs, we propose a MOS-aware GRM that incorporates an\nMOS-difference-based reward function, enabling the model to adaptively scale\nrewards according to the difficulty of each sample pair. Experimental results\nshow that the MOS-aware GRM significantly improves fine-grained quality\ndiscrimination and narrows the gap with scalar models on the most challenging\ncases. We hope this work will establish both a benchmark and a methodological\nframework to foster more rigorous and scalable research in automatic speech\nquality assessment.",
      "authors": [
        "Yifei Cao",
        "Changhao Jiang",
        "Jiabao Zhuang",
        "Jiajun Sun",
        "Ming Zhang",
        "Zhiheng Xi",
        "Hui Li",
        "Shihan Dou",
        "Yuran Wang",
        "Yunke Zhang",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "published": "2025-10-01T10:27:51Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00743v1",
      "primary_area": "audio_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MOS-RMBench基准，将传统语音质量评分MOS转换为偏好比较设置，系统评估了标量、半标量和生成式奖励模型三种范式。研究发现标量模型性能最佳（准确率超74%），但所有模型在合成语音和小MOS差异样本上表现较差。提出的MOS感知生成奖励模型通过自适应奖励缩放，显著提升了细粒度质量判别能力。",
      "order": 452
    },
    {
      "arxiv_id": "2510.00694v1",
      "title": "ALARB: An Arabic Legal Argument Reasoning Benchmark",
      "summary": "We introduce ALARB, a dataset and suite of tasks designed to evaluate the\nreasoning capabilities of large language models (LLMs) within the Arabic legal\ndomain. While existing Arabic benchmarks cover some knowledge-intensive tasks\nsuch as retrieval and understanding, substantial datasets focusing specifically\non multistep reasoning for Arabic LLMs, especially in open-ended contexts, are\nlacking. The dataset comprises over 13K commercial court cases from Saudi\nArabia, with each case including the facts presented, the reasoning of the\ncourt, the verdict, as well as the cited clauses extracted from the regulatory\ndocuments. We define a set of challenging tasks leveraging this dataset and\nreflecting the complexity of real-world legal reasoning, including verdict\nprediction, completion of reasoning chains in multistep legal arguments, and\nidentification of relevant regulations based on case facts. We benchmark a\nrepresentative selection of current open and closed Arabic LLMs on these tasks\nand demonstrate the dataset's utility for instruction tuning. Notably, we show\nthat instruction-tuning a modest 12B parameter model using ALARB significantly\nenhances its performance in verdict prediction and Arabic verdict generation,\nreaching a level comparable to that of GPT-4o.",
      "authors": [
        "Harethah Abu Shairah",
        "Somayah AlHarbi",
        "Abdulaziz AlHussein",
        "Sameer Alsabea",
        "Omar Shaqaqi",
        "Hebah AlShamlan",
        "Omar Knio",
        "George Turkiyyah"
      ],
      "published": "2025-10-01T09:15:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00694v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "legal_ai",
      "tldr_zh": "ALARB是一个阿拉伯语法律论证推理基准数据集，包含13K+沙特商业法庭案例，用于评估大语言模型在阿拉伯法律领域的多步推理能力，包括判决预测、推理链补全和相关法规识别等任务。研究表明，使用该数据集进行指令微调可显著提升模型性能。",
      "order": 453
    },
    {
      "arxiv_id": "2510.00691v1",
      "title": "Inclusive Easy-to-Read Generation for Individuals with Cognitive\n  Impairments",
      "summary": "Ensuring accessibility for individuals with cognitive impairments is\nessential for autonomy, self-determination, and full citizenship. However,\nmanual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to\nscale, limiting access to crucial information in healthcare, education, and\ncivic life. AI-driven ETR generation offers a scalable solution but faces key\nchallenges, including dataset scarcity, domain adaptation, and balancing\nlightweight learning of Large Language Models (LLMs). In this paper, we\nintroduce ETR-fr, the first dataset for ETR text generation fully compliant\nwith European ETR guidelines. We implement parameter-efficient fine-tuning on\nPLMs and LLMs to establish generative baselines. To ensure high-quality and\naccessible outputs, we introduce an evaluation framework based on automatic\nmetrics supplemented by human assessments. The latter is conducted using a\n36-question evaluation form that is aligned with the guidelines. Overall\nresults show that PLMs perform comparably to LLMs and adapt effectively to\nout-of-domain texts.",
      "authors": [
        "François Ledoyen",
        "Gaël Dias",
        "Alexis Lechervy",
        "Jeremie Pantin",
        "Fabrice Maurel",
        "Youssef Chahir",
        "Elisa Gouzonnat",
        "Mélanie Berthelot",
        "Stanislas Moravac",
        "Armony Altinier",
        "Amy Khairalla"
      ],
      "published": "2025-10-01T09:13:18Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00691v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出首个符合欧洲简易阅读指南的ETR-fr数据集，通过参数高效微调方法在预训练模型和大语言模型上建立生成基线，并开发了结合自动指标与人工评估的质量框架。研究表明预训练模型在简易阅读文本生成任务中表现与大模型相当，并能有效适应跨领域文本。",
      "order": 454
    },
    {
      "arxiv_id": "2510.00685v1",
      "title": "Stochastic Self-Organization in Multi-Agent Systems",
      "summary": "Multi-agent systems (MAS) based on Large Language Models (LLMs) have the\npotential to solve tasks that are beyond the reach of any single LLM. However,\nthis potential can only be realized when the collaboration mechanism between\nagents is optimized. Specifically, optimizing the communication structure\nbetween agents is critical for fruitful collaboration. Most existing approaches\nrely on fixed topologies, pretrained graph generators, optimization over edges,\nor employ external LLM judges, thereby adding to the complexity. In this work,\nwe introduce a response-conditioned framework that adapts communication\non-the-fly. Agents independently generate responses to the user query and\nassess peer contributions using an approximation of the Shapley value. A\ndirected acyclic graph (DAG) is then constructed to regulate the propagation of\nthe responses among agents, which ensures stable and efficient message\ntransmission from high-contributing agents to others. This graph is dynamically\nupdated based on the agent responses from the previous collaboration round.\nSince the proposed framework enables the self-organization of agents without\nadditional supervision or training, we refer to it as SelfOrg. The SelfOrg\nframework goes beyond task- and query-level optimization and takes into account\nthe stochastic nature of agent responses. Experiments with both strong and weak\nLLM backends demonstrate robust performance, with significant gains in the weak\nregime where prior methods collapse. We also theoretically show that multiple\nagents increase the chance of correctness and that the correct responses\nnaturally dominate the information flow.",
      "authors": [
        "Nurbek Tastan",
        "Samuel Horvath",
        "Karthik Nandakumar"
      ],
      "published": "2025-10-01T09:08:04Z",
      "primary_category": "cs.MA",
      "arxiv_url": "https://arxiv.org/abs/2510.00685v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出SelfOrg框架，通过基于Shapley值近似评估智能体贡献并构建有向无环图，实现多智能体系统的随机自组织通信优化。该方法无需额外监督或训练，在弱模型场景下表现稳健，理论上证明多智能体可提高正确性且正确响应主导信息流。",
      "order": 455
    },
    {
      "arxiv_id": "2510.00671v1",
      "title": "Milco: Learned Sparse Retrieval Across Languages via a Multilingual\n  Connector",
      "summary": "Learned Sparse Retrieval (LSR) combines the efficiency of bi-encoders with\nthe transparency of lexical matching, but existing approaches struggle to scale\nbeyond English. We introduce MILCO, an LSR architecture that maps queries and\ndocuments from different languages into a shared English lexical space via a\nmultilingual connector. MILCO is trained with a specialized two-stage regime\nthat combines Sparse Alignment Pretraining with contrastive training to provide\nrepresentation transparency and effectiveness while mitigating semantic\ncollapse. Motivated by the observation that uncommon entities are often lost\nwhen projected into English, we propose a new LexEcho head, which enhances\nrobustness by augmenting the English lexical representation with a\nsource-language view obtained through a special [ECHO] token. MILCO achieves\nstate-of-the-art multilingual and cross-lingual LSR performance, outperforming\nleading dense, sparse, and multi-vector baselines such as BGE-M3 and\nQwen3-Embed on standard multilingual benchmarks, while supporting dynamic\nefficiency through post-hoc pruning. Notably, when using mass-based pruning to\nreduce document representations to only 30 active dimensions on average, MILCO\n560M outperforms the similarly-sized Qwen3-Embed 0.6B with 1024 dimensions.",
      "authors": [
        "Thong Nguyen",
        "Yibin Lei",
        "Jia-Huei Ju",
        "Eugene Yang",
        "Andrew Yates"
      ],
      "published": "2025-10-01T08:58:25Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.00671v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "MILCO是一种跨语言学习稀疏检索架构，通过多语言连接器将不同语言的查询和文档映射到共享的英语词汇空间。采用两阶段训练策略和LexEcho增强机制，在保持表示透明度的同时提升多语言检索性能，支持动态剪枝优化效率，在多项基准测试中超越现有密集/稀疏检索模型。",
      "order": 456
    },
    {
      "arxiv_id": "2510.00662v1",
      "title": "Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to\n  Easy-to-Read Text Generation",
      "summary": "Simplifying complex texts is essential for ensuring equitable access to\ninformation, especially for individuals with cognitive impairments. The\nEasy-to-Read (ETR) initiative offers a framework for making content accessible\nto the neurodivergent population, but the manual creation of such texts remains\ntime-consuming and resource-intensive. In this work, we investigate the\npotential of large language models (LLMs) to automate the generation of ETR\ncontent. To address the scarcity of aligned corpora and the specificity of ETR\nconstraints, we propose a multi-task learning (MTL) approach that trains models\njointly on text summarization, text simplification, and ETR generation. We\nexplore two different strategies: multi-task retrieval-augmented generation\n(RAG) for in-context learning, and MTL-LoRA for parameter-efficient\nfine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a\nnew high-quality dataset, demonstrate the benefits of multi-task setups over\nsingle-task baselines across all configurations. Moreover, results show that\nthe RAG-based strategy enables generalization in out-of-domain settings, while\nMTL-LoRA outperforms all learning strategies within in-domain configurations.",
      "authors": [
        "François Ledoyen",
        "Gaël Dias",
        "Jeremie Pantin",
        "Alexis Lechervy",
        "Fabrice Maurel",
        "Youssef Chahir"
      ],
      "published": "2025-10-01T08:44:05Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00662v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "education_ai",
      "tldr_zh": "本研究提出多任务学习方法，利用大语言模型自动生成易读文本，通过文本摘要、简化和易读生成联合训练，解决了认知障碍人群的信息获取难题。实验证明多任务策略优于单任务基线，RAG方法支持跨领域泛化，MTL-LoRA在领域内表现最佳。",
      "order": 457
    },
    {
      "arxiv_id": "2510.00647v1",
      "title": "MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for\n  Alt-text Generation",
      "summary": "The alt-text generation task produces concise, context-relevant descriptions\nof images, enabling blind and low-vision users to access online images. Despite\nthe capabilities of large vision-language models, alt-text generation\nperformance remains limited due to noisy user annotations, inconsistent\nstandards, and MLLMs' insensitivity to contextual information. Previous efforts\nto fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT\nrelies on accurate target annotations, which are often flawed in user-generated\nalt-text. To address this, we propose Multi-faceted Cross-modal Direct\nPreference Optimization (MCM-DPO), which improves alt-text generation by\nlearning to identify better options in preference pairs without requiring\nprecise annotations. MCM-DPO optimizes preferences across single, paired, and\nmulti-preference dimensions, covering textual, visual, and cross-modal factors.\nIn light of the scarcity of high-quality annotated and preference-labeled\ndatasets for alt-text, we constructed two large-scale, high-quality datasets\nnamed TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include\n202k annotated alt-text samples and 18k preference pairs that cover diverse\npreference dimensions, aiming to support further research in this domain.\nExperimental results show that our proposed MCM-DPO method consistently\noutperforms both DPO and SFT, establishing a new state of the art in alt-text\ngeneration. We release the code and data here:\nhttps://github.com/LVUGAI/MCM-DPO",
      "authors": [
        "Jinlan Fu",
        "Shenzhen Huangfu",
        "Hao Fei",
        "Yichong Huang",
        "Xiaoyu Shen",
        "Xipeng Qiu",
        "See-Kiong Ng"
      ],
      "published": "2025-10-01T08:25:18Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00647v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MCM-DPO方法，通过多维度跨模态直接偏好优化改进替代文本生成，无需精确标注即可从偏好对中学习更好选项。该方法在文本、视觉和跨模态因素上优化偏好，并构建了两个大规模高质量数据集TAlt和PAlt。实验表明MCM-DPO在替代文本生成任务上优于DPO和SFT方法，达到新的最优性能。",
      "order": 458
    },
    {
      "arxiv_id": "2510.00636v1",
      "title": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution",
      "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.",
      "authors": [
        "Alessio Devoto",
        "Maximilian Jeblick",
        "Simon Jégou"
      ],
      "published": "2025-10-01T08:12:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00636v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "提出'期望注意力'方法，通过预测未来查询分布来估计KV对重要性，实现无需训练的KV缓存压缩。该方法在预填充和解码阶段均有效，性能优于现有技术，并发布了包含20多种技术的KVPress压缩库。",
      "order": 459
    },
    {
      "arxiv_id": "2510.00629v2",
      "title": "Tenyidie Syllabification corpus creation and deep learning applications",
      "summary": "The Tenyidie language is a low-resource language of the Tibeto-Burman family\nspoken by the Tenyimia Community of Nagaland in the north-eastern part of India\nand is considered a major language in Nagaland. It is tonal,\nSubject-Object-Verb, and highly agglutinative in nature. Being a low-resource\nlanguage, very limited research on Natural Language Processing (NLP) has been\nconducted. To the best of our knowledge, no work on syllabification has been\nreported for this language. Among the many NLP tasks, syllabification or\nsyllabication is an important task in which the given word syllables are\nidentified. The contribution of this work is the creation of 10,120 syllabified\nTenyidie words and the application of the Deep Learning techniques on the\ncreated corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and\nEncoder-decoder deep learning architectures on our created dataset. In our\ndataset split of 80:10:10 (train:validation:test) set, we achieved the highest\naccuracy of 99.21% with BLSTM model on the test set. This work will find its\napplication in numerous other NLP applications, such as morphological analysis,\npart-of-speech tagging, machine translation, etc, for the Tenyidie Language.\n  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF;\nEncoder-decoder",
      "authors": [
        "Teisovi Angami",
        "Kevisino Khate"
      ],
      "published": "2025-10-01T08:00:59Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00629v2",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究为低资源藏缅语系Tenyidie语言创建了首个包含10,120个音节化词汇的语料库，并应用LSTM、BLSTM、BLSTM+CRF及编码器-解码器等深度学习架构进行音节划分任务。在80:10:10的数据集划分下，BLSTM模型在测试集上达到99.21%的最高准确率，为后续形态分析、词性标注等NLP任务奠定基础。",
      "order": 460
    },
    {
      "arxiv_id": "2510.00628v1",
      "title": "Hearing the Order: Investigating Selection Bias in Large Audio-Language\n  Models",
      "summary": "Large audio-language models (LALMs) are often used in tasks that involve\nreasoning over ordered options. An open question is whether their predictions\nare influenced by the order of answer choices, which would indicate a form of\nselection bias and undermine their reliability. In this paper, we identify and\nanalyze this problem in LALMs. We demonstrate that no model is immune to this\nbias through extensive experiments on six LALMs across three widely used\nbenchmarks and their spoken counterparts. Shuffling the order of answer options\ncan cause performance fluctuations of up to 24% and even change model rankings,\nraising concerns about the reliability of current evaluation practices. We also\nstudy permutation-based strategies and show that they can mitigate bias in most\ncases. Our work represents the first systematic investigation of this issue in\nLALMs, and we hope it raises awareness and motivates further research in this\ndirection.",
      "authors": [
        "Yu-Xiang Lin",
        "Chen-An Li",
        "Sheng-Lun Wei",
        "Po-Chun Chen",
        "Hsin-Hsi Chen",
        "Hung-yi Lee"
      ],
      "published": "2025-10-01T08:00:58Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00628v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究首次系统性地揭示大型音频语言模型在有序选项推理任务中存在选择偏差问题。实验表明，答案选项的排列顺序会导致模型性能波动高达24%并改变模型排名，现有评估方法可靠性存疑。研究还验证了基于排列的策略在多数情况下能有效缓解此类偏差。",
      "order": 461
    },
    {
      "arxiv_id": "2510.00626v1",
      "title": "When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning\n  in Large Audio-Language Models",
      "summary": "Large audio-language models (LALMs) unify speech and text processing, but\ntheir robustness in noisy real-world settings remains underexplored. We\ninvestigate how irrelevant audio, such as silence, synthetic noise, and\nenvironmental sounds, affects text reasoning tasks where audio is unnecessary.\nAcross three text-based benchmarks, we find that even non-informative audio\nreduces accuracy and increases prediction volatility; the severity of\ninterference scales with longer durations, higher amplitudes, and elevated\ndecoding temperatures. Silence, often assumed neutral, destabilizes outputs as\nstrongly as synthetic noise. While larger models show greater resilience,\nvulnerabilities persist across all evaluated systems. We further test\nmitigation strategies and find that prompting shows limited effectiveness,\nwhereas self-consistency improves stability at the cost of increased\ncomputation. Our results reveal cross-modal interference as a key robustness\nchallenge and highlight the need for efficient fusion strategies that preserve\nreasoning performance in the presence of irrelevant inputs.",
      "authors": [
        "Chen-An Li",
        "Tzu-Han Lin",
        "Hung-yi Lee"
      ],
      "published": "2025-10-01T07:59:45Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00626v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探讨大型音频-语言模型中无关音频（包括静音、合成噪声和环境声音）对文本推理任务的影响。研究发现即使非信息性音频也会降低准确性并增加预测波动性，静音的干扰程度与合成噪声相当。虽然较大模型表现出更强韧性，但所有系统均存在脆弱性。缓解策略中提示效果有限，而自一致性方法可提高稳定性但增加计算成本。",
      "order": 462
    },
    {
      "arxiv_id": "2510.00620v1",
      "title": "HARPA: A Testability-Driven, Literature-Grounded Framework for Research\n  Ideation",
      "summary": "While there has been a surge of interest in automated scientific discovery\n(ASD), especially with the emergence of LLMs, it remains challenging for tools\nto generate hypotheses that are both testable and grounded in the scientific\nliterature. Additionally, existing ideation tools are not adaptive to prior\nexperimental outcomes. We developed HARPA to address these challenges by\nincorporating the ideation workflow inspired by human researchers. HARPA first\nidentifies emerging research trends through literature mining, then explores\nhypothesis design spaces, and finally converges on precise, testable hypotheses\nby pinpointing research gaps and justifying design choices. Our evaluations\nshow that HARPA-generated hypothesis-driven research proposals perform\ncomparably to a strong baseline AI-researcher across most qualitative\ndimensions (e.g., specificity, novelty, overall quality), but achieve\nsignificant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness\n(+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the\nASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11\nout of 40) and fewer failures (16 vs. 21 out of 40), showing that expert\nfeasibility judgments track with actual execution success. Furthermore, to\nsimulate how researchers continuously refine their understanding of what\nhypotheses are both testable and potentially interesting from experience, HARPA\nlearns a reward model that scores new hypotheses based on prior experimental\noutcomes, achieving approx. a 28\\% absolute gain over HARPA's untrained\nbaseline scorer. Together, these methods represent a step forward in the field\nof AI-driven scientific discovery.",
      "authors": [
        "Rosni Vasu",
        "Peter Jansen",
        "Pao Siangliulue",
        "Cristina Sarasua",
        "Abraham Bernstein",
        "Peter Clark",
        "Bhavana Dalvi Mishra"
      ],
      "published": "2025-10-01T07:52:19Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00620v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "HARPA是一个基于文献挖掘的研究构思框架，通过识别研究趋势、探索假设设计空间并收敛到可测试假设，显著提升了AI生成研究提案的可行性和文献基础性。评估显示其在可行性和基础性方面优于基线，并能通过奖励模型从实验经验中学习改进假设生成。",
      "order": 463
    },
    {
      "arxiv_id": "2510.00615v1",
      "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
      "summary": "Large language models (LLMs) are increasingly deployed as agents in dynamic,\nreal-world environments, where success requires both reasoning and effective\ntool use. A central challenge for agentic tasks is the growing context length,\nas agents must accumulate long histories of actions and observations. This\nexpansion raises costs and reduces efficiency in long-horizon tasks, yet prior\nwork on context compression has mostly focused on single-step tasks or narrow\napplications. We introduce Agent Context Optimization (ACON), a unified\nframework that optimally compresses both environment observations and\ninteraction histories into concise yet informative condensations. ACON\nleverages compression guideline optimization in natural language space: given\npaired trajectories where full context succeeds but compressed context fails,\ncapable LLMs analyze the causes of failure, and the compression guideline is\nupdated accordingly. Furthermore, we propose distilling the optimized LLM\ncompressor into smaller models to reduce the overhead of the additional module.\nExperiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON\nreduces memory usage by 26-54% (peak tokens) while largely preserving task\nperformance, preserves over 95% of accuracy when distilled into smaller\ncompressors, and enhances smaller LMs as long-horizon agents with up to 46%\nperformance improvement.",
      "authors": [
        "Minki Kang",
        "Wei-Ning Chen",
        "Dongge Han",
        "Huseyin A. Inan",
        "Lukas Wutschitz",
        "Yanzhi Chen",
        "Robert Sim",
        "Saravan Rajmohan"
      ],
      "published": "2025-10-01T07:43:49Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00615v1",
      "primary_area": "text_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "ACON提出了一种针对长视野LLM智能体的上下文优化框架，通过自然语言空间中的压缩指南优化，将环境观察和交互历史压缩为简洁信息。该方法可减少26-54%内存使用，保持95%以上任务精度，并能蒸馏到小模型中提升性能达46%。",
      "order": 464
    },
    {
      "arxiv_id": "2510.00586v1",
      "title": "Eyes-on-Me: Scalable RAG Poisoning through Transferable\n  Attention-Steering Attractors",
      "summary": "Existing data poisoning attacks on retrieval-augmented generation (RAG)\nsystems scale poorly because they require costly optimization of poisoned\ndocuments for each target phrase. We introduce Eyes-on-Me, a modular attack\nthat decomposes an adversarial document into reusable Attention Attractors and\nFocus Regions. Attractors are optimized to direct attention to the Focus\nRegion. Attackers can then insert semantic baits for the retriever or malicious\ninstructions for the generator, adapting to new targets at near zero cost. This\nis achieved by steering a small subset of attention heads that we empirically\nidentify as strongly correlated with attack success. Across 18 end-to-end RAG\nsettings (3 datasets $\\times$ 2 retrievers $\\times$ 3 generators), Eyes-on-Me\nraises average attack success rates from 21.9 to 57.8 (+35.9 points,\n2.6$\\times$ over prior work). A single optimized attractor transfers to unseen\nblack box retrievers and generators without retraining. Our findings establish\na scalable paradigm for RAG data poisoning and show that modular, reusable\ncomponents pose a practical threat to modern AI systems. They also reveal a\nstrong link between attention concentration and model outputs, informing\ninterpretability research.",
      "authors": [
        "Yen-Shan Chen",
        "Sian-Yao Huang",
        "Cheng-Lin Yang",
        "Yun-Nung Chen"
      ],
      "published": "2025-10-01T07:07:22Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00586v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Eyes-on-Me攻击方法，通过可转移的注意力引导吸引器实现可扩展的RAG系统数据投毒。该方法将对抗文档分解为可重用的注意力吸引器和焦点区域，无需针对每个目标短语进行昂贵优化，在18种RAG设置中将平均攻击成功率从21.9%提升至57.8%，揭示了注意力集中与模型输出的强关联性。",
      "order": 465
    },
    {
      "arxiv_id": "2510.00582v1",
      "title": "SAGE-LD: Towards Scalable and Generalizable End-to-End Language\n  Diarization via Simulated Data Augmentation",
      "summary": "In this paper, we present a neural spoken language diarization model that\nsupports an unconstrained span of languages within a single framework. Our\napproach integrates a learnable query-based architecture grounded in\nmultilingual awareness, with large-scale pretraining on simulated\ncode-switching data. By jointly leveraging these two components, our method\novercomes the limitations of conventional approaches in data scarcity and\narchitecture optimization, and generalizes effectively to real-world\nmultilingual settings across diverse environments. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance on several\nlanguage diarization benchmarks, with a relative performance improvement of 23%\nto 52% over previous methods. We believe that this work not only advances\nresearch in language diarization but also establishes a foundational framework\nfor code-switching speech technologies.",
      "authors": [
        "Sangmin Lee",
        "Woongjib Choi",
        "Jihyun Kim",
        "Hong-Goo Kang"
      ],
      "published": "2025-10-01T07:01:33Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00582v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出SAGE-LD模型，通过可学习查询架构和模拟语码转换数据的大规模预训练，实现单框架内支持任意语言的端到端语音语言分离。该方法克服了传统方法在数据稀缺和架构优化上的局限，在多个基准测试中相对性能提升23%-52%，为语码转换语音技术建立了基础框架。",
      "order": 466
    },
    {
      "arxiv_id": "2510.00579v1",
      "title": "CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs",
      "summary": "Chain-of-Thought (CoT) prompting has emerged as a powerful approach to\nenhancing the reasoning capabilities of Large Language Models (LLMs). However,\nexisting implementations, such as in-context learning and fine-tuning, remain\ncostly and inefficient. To improve CoT reasoning at a lower cost, and inspired\nby the task vector paradigm, we introduce CoT Vectors, compact representations\nthat encode task-general, multi-step reasoning knowledge. Through experiments\nwith Extracted CoT Vectors, we observe pronounced layer-wise instability,\nmanifesting as a U-shaped performance curve that reflects a systematic\nthree-stage reasoning process in LLMs. To address this limitation, we propose\nLearnable CoT Vectors, optimized under a teacher-student framework to provide\nmore stable and robust guidance. Extensive evaluations across diverse\nbenchmarks and models demonstrate that CoT Vectors not only outperform existing\nbaselines but also achieve performance comparable to parameter-efficient\nfine-tuning methods, while requiring fewer trainable parameters. Moreover, by\ntreating CoT Vectors as a probe, we uncover how their effectiveness varies due\nto latent space structure, information density, acquisition mechanisms, and\npre-training differences, offering new insights into the functional\norganization of multi-step reasoning in LLMs. The source code will be released.",
      "authors": [
        "Li Li",
        "Ziyi Wang",
        "Yongliang Wu",
        "Jianfei Cai",
        "Xu Yang"
      ],
      "published": "2025-10-01T06:58:23Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00579v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出CoT向量，一种编码多步推理知识的紧凑表示方法。通过提取式和可学习式两种CoT向量，解决了现有推理方法成本高、层间不稳定的问题。实验表明该方法在减少可训练参数的同时，性能媲美参数高效微调，并揭示了LLM多步推理的功能组织机制。",
      "order": 467
    },
    {
      "arxiv_id": "2510.00568v1",
      "title": "ReSeek: A Self-Correcting Framework for Search Agents with Instructive\n  Rewards",
      "summary": "Search agents powered by Large Language Models (LLMs) have demonstrated\nsignificant potential in tackling knowledge-intensive tasks. Reinforcement\nlearning (RL) has emerged as a powerful paradigm for training these agents to\nperform complex, multi-step reasoning. However, prior RL-based methods often\nrely on sparse or rule-based rewards, which can lead agents to commit to\nsuboptimal or erroneous reasoning paths without the ability to recover. To\naddress these limitations, we propose ReSeek, a novel self-correcting framework\nfor training search agents. Our framework introduces a self-correction\nmechanism that empowers the agent to dynamically identify and recover from\nerroneous search paths during an episode. By invoking a special JUDGE action,\nthe agent can judge the information and re-plan its search strategy. To guide\nthis process, we design a dense, instructive process reward function, which\ndecomposes into a correctness reward for retrieving factual information and a\nutility reward for finding information genuinely useful for the query.\nFurthermore, to mitigate the risk of data contamination in existing datasets,\nwe introduce FictionalHot, a new and challenging benchmark with recently\ncurated questions requiring complex reasoning. Being intuitively reasonable and\npractically simple, extensive experiments show that agents trained with ReSeek\nsignificantly outperform SOTA baselines in task success rate and path\nfaithfulness.",
      "authors": [
        "Shiyu Li",
        "Yang Tang",
        "Yifan Wang",
        "Peiming Li",
        "Xi Chen"
      ],
      "published": "2025-10-01T06:44:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00568v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "ReSeek是一种新型自校正搜索代理训练框架，通过引入JUDGE动作和密集指导性奖励机制，使代理能够在搜索过程中动态识别并纠正错误推理路径。该框架包含正确性奖励和实用性奖励，并在新构建的FictionalHot基准测试中显著优于现有方法。",
      "order": 468
    },
    {
      "arxiv_id": "2510.00567v1",
      "title": "Are Large Language Models Chronically Online Surfers? A Dataset for\n  Chinese Internet Meme Explanation",
      "summary": "Large language models (LLMs) are trained on vast amounts of text from the\nInternet, but do they truly understand the viral content that rapidly spreads\nonline -- commonly known as memes? In this paper, we introduce CHIME, a dataset\nfor CHinese Internet Meme Explanation. The dataset comprises popular\nphrase-based memes from the Chinese Internet, annotated with detailed\ninformation on their meaning, origin, example sentences, types, etc. To\nevaluate whether LLMs understand these memes, we designed two tasks. In the\nfirst task, we assessed the models' ability to explain a given meme, identify\nits origin, and generate appropriate example sentences. The results show that\nwhile LLMs can explain the meanings of some memes, their performance declines\nsignificantly for culturally and linguistically nuanced meme types.\nAdditionally, they consistently struggle to provide accurate origins for the\nmemes. In the second task, we created a set of multiple-choice questions (MCQs)\nrequiring LLMs to select the most appropriate meme to fill in a blank within a\ncontextual sentence. While the evaluated models were able to provide correct\nanswers, their performance remains noticeably below human levels. We have made\nCHIME public and hope it will facilitate future research on computational meme\nunderstanding.",
      "authors": [
        "Yubo Xie",
        "Chenkai Wang",
        "Zongyang Ma",
        "Fahui Miao"
      ],
      "published": "2025-10-01T06:41:46Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00567v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出CHIME数据集，用于评估大语言模型对中文网络流行语的理解能力。研究发现，虽然模型能解释部分梗的含义，但在文化背景和起源识别方面表现不佳，且整体水平仍低于人类。该数据集旨在促进计算模因理解的研究。",
      "order": 469
    },
    {
      "arxiv_id": "2510.00546v1",
      "title": "ThinkBrake: Mitigating Overthinking in Tool Reasoning",
      "summary": "Small reasoning models (SRMs) often overthink during tool use: they reach a\ncorrect tool-argument configuration, then continue reasoning and overwrite it\nwith an incorrect final call. We diagnose overthinking via oracle rollouts that\ninject </think> at sentence boundaries. On the Berkeley Function Calling\nLeaderboard (BFCL), this oracle termination lifts average accuracy from 85.8\\%\nto 94.2\\% while reducing tokens by 80-94\\%, revealing substantial recoverable\nheadroom and potential redundant reasoning. While prior work on concise\nreasoning has largely targeted mathematics, tool reasoning remains\nunderexplored. We adapt various early-termination baselines to tool use and\nintroduce ThinkBrake, a training-free decoding heuristic. ThinkBrake monitors\nthe log-probability margin between </think> and the current top token at\nsentence boundaries and triggers termination when this margin becomes small.\nAcross BFCL's single turn, non-live and live splits, ThinkBrake preserves or\nimproves accuracy while reducing tokens up to 25\\%, outperforming various\nbaselines.",
      "authors": [
        "Minjae Oh",
        "Sangjun Song",
        "Seungkyu Lee",
        "Sungmin Jo",
        "Yohan Jo"
      ],
      "published": "2025-10-01T06:04:57Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00546v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "ThinkBrake是一种无需训练的推理解码方法，针对小型推理模型在工具使用中的过度思考问题。通过监控句子边界处</think>与当前最佳token的对数概率差，在差值较小时触发终止，在BFCL基准上保持或提升准确率的同时减少25%的token使用，优于多种基线方法。",
      "order": 470
    },
    {
      "arxiv_id": "2510.00536v1",
      "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
      "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.",
      "authors": [
        "Kung-Hsiang Huang",
        "Haoyi Qiu",
        "Yutong Dai",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "published": "2025-10-01T05:37:54Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00536v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "GUI-KV是一种针对图形界面代理的KV缓存压缩方法，通过空间显著性引导和时间冗余评分技术，在保持精度的同时显著降低计算开销。该方法无需重新训练，在标准基准测试中可减少38.9%的解码计算量并提升4.1%的任务准确率。",
      "order": 471
    },
    {
      "arxiv_id": "2510.00526v1",
      "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised\n  Fine-Tuning across the Model Capability Continuum",
      "summary": "Supervised fine-tuning (SFT) is the standard approach for post-training large\nlanguage models (LLMs), yet it often shows limited generalization. We trace\nthis limitation to its default training objective: negative log likelihood\n(NLL). While NLL is classically optimal when training from scratch,\npost-training operates in a different paradigm and could violate its optimality\nassumptions, where models already encode task-relevant priors and supervision\ncan be long and noisy. To this end, we study a general family of\nprobability-based objectives and characterize their effectiveness under\ndifferent conditions. Through comprehensive experiments and extensive ablation\nstudies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a\ncritical dimension that governs objective behavior: the model-capability\ncontinuum. Near the model-strong end, prior-leaning objectives that downweight\nlow-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants)\nconsistently outperform NLL; toward the model-weak end, NLL dominates; in\nbetween, no single objective prevails. Our theoretical analysis further\nelucidates how objectives trade places across the continuum, providing a\nprincipled foundation for adapting objectives to model capability. Our code is\navailable at https://github.com/GaotangLi/Beyond-Log-Likelihood.",
      "authors": [
        "Gaotang Li",
        "Ruizhong Qiu",
        "Xiusi Chen",
        "Heng Ji",
        "Hanghang Tong"
      ],
      "published": "2025-10-01T05:17:47Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00526v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文挑战监督微调中负对数似然(NLL)的默认地位，提出基于概率的目标函数家族。研究发现目标函数效果取决于模型能力连续体：强模型端倾向先验目标(如-p、-p¹⁰)，弱模型端NLL占优，中间无单一最优。通过7个模型、14个基准、3个领域的实验验证，为根据模型能力选择目标函数提供理论依据。",
      "order": 472
    },
    {
      "arxiv_id": "2510.00514v1",
      "title": "EuroSpeech: A Multilingual Speech Corpus",
      "summary": "Recent progress in speech processing has highlighted that high-quality\nperformance across languages requires substantial training data for each\nindividual language. While existing multilingual datasets cover many languages,\nthey often contain insufficient data for most languages. Thus, trained models\nperform poorly on the majority of the supported languages. Our work addresses\nthis challenge by introducing a scalable pipeline for constructing speech\ndatasets from parliamentary recordings. The proposed pipeline includes robust\ncomponents for media retrieval and a two-stage alignment algorithm designed to\nhandle non-verbatim transcripts and long-form audio. Applying this pipeline to\nrecordings from 22 European parliaments, we extract over 61k hours of aligned\nspeech segments, achieving substantial per-language coverage with 19 languages\nexceeding 1k hours and 22 languages exceeding 500 hours of high-quality speech\ndata. We obtain an average 41.8\\% reduction in word error rates over baselines\nwhen finetuning an existing ASR model on our dataset, demonstrating the\nusefulness of our approach.",
      "authors": [
        "Samuel Pfisterer",
        "Florian Grötschla",
        "Luca A. Lanzendörfer",
        "Florian Yan",
        "Roger Wattenhofer"
      ],
      "published": "2025-10-01T04:51:45Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00514v1",
      "primary_area": "audio_models",
      "secondary_focus": "alignment",
      "application_domain": "legal_ai",
      "tldr_zh": "EuroSpeech是一个从22个欧洲议会录音中构建的多语言语音语料库，包含超过61k小时的高质量对齐语音数据，涵盖19种语言超过1000小时。通过创新的两阶段对齐算法处理非逐字转录和长音频，在ASR模型微调中相比基线将词错误率平均降低41.8%。",
      "order": 473
    },
    {
      "arxiv_id": "2510.00510v1",
      "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
      "summary": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness and adaptability. We\npropose a generalist agent architecture that integrates three core components:\na collective multi-agent framework combining planning and execution agents with\ncritic model voting, a hierarchical memory system spanning working, semantic,\nand procedural layers, and a refined tool suite for search, code execution, and\nmultimodal parsing. Evaluated on a comprehensive benchmark, our framework\nconsistently outperforms open-source baselines and approaches the performance\nof proprietary systems. These results demonstrate the importance of\nsystem-level integration and highlight a path toward scalable, resilient, and\nadaptive AI assistants capable of operating across diverse domains and tasks.",
      "authors": [
        "Jiarun Liu",
        "Shiyue Xu",
        "Shangkun Liu",
        "Yang Li",
        "Wen Liu",
        "Min Liu",
        "Xiaoqing Zhou",
        "Hanmin Wang",
        "Shilin Jia",
        "zhen Wang",
        "Shaohua Tian",
        "Hanhao Li",
        "Junbo Zhang",
        "Yongli Yu",
        "Peng Cao",
        "Haofen Wang"
      ],
      "published": "2025-10-01T04:41:58Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00510v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出JoyAgent-JDGenie通用智能体架构，集成多智能体协作框架、分层记忆系统和精细化工具套件，在综合基准测试中超越开源基线并接近专有系统性能，展示了系统级集成对构建可扩展、鲁棒自适应AI助手的重要性。",
      "order": 474
    },
    {
      "arxiv_id": "2510.00508v1",
      "title": "Copy-Paste to Mitigate Large Language Model Hallucinations",
      "summary": "While Retrieval-Augmented Generation (RAG) enables large language models\n(LLMs) to generate contextually grounded responses, contextual faithfulness\nremains challenging as LLMs may not consistently trust provided context,\nleading to hallucinations that undermine reliability. We observe an inverse\ncorrelation between response copying degree and context-unfaithful\nhallucinations on RAGTruth, suggesting that higher copying degrees reduce\nhallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,\nobtained through two-stage high-copying response preference training. We design\nthree prompting methods to enhance copying degree, demonstrating that\nhigh-copying responses achieve superior contextual faithfulness and\nhallucination control. These approaches enable a fully automated pipeline that\ntransforms generated responses into high-copying preference data for training\nCopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best\nperformance in both counterfactual and original contexts, remarkably with 12.2%\nto 24.5% accuracy improvements on FaithEval over the best baseline, while\nrequiring only 365 training samples -- 1/50th of baseline data. To elucidate\nCopyPasteLLM's effectiveness, we propose the Context-Parameter Copying\nCapturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates\nreliance on internal parametric knowledge rather than external knowledge during\ngeneration. All codes are available at\nhttps://github.com/longyongchao/CopyPasteLLM",
      "authors": [
        "Yongchao Long",
        "Xian Wu",
        "Yingying Zhang",
        "Xianbin Wen",
        "Yuxi Zhou",
        "Shenda Hong"
      ],
      "published": "2025-10-01T04:40:04Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00508v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出CopyPasteLLM方法，通过两阶段高复制响应偏好训练来缓解大语言模型的幻觉问题。研究发现响应复制程度与上下文不忠实幻觉呈负相关，设计了三种提示方法增强复制程度，仅需365个训练样本即可在多个基准测试中显著提升准确性12.2%-24.5%，同时揭示了模型重新校准对内部参数知识的依赖机制。",
      "order": 475
    },
    {
      "arxiv_id": "2510.00507v1",
      "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via\n  Knowledge Graphs",
      "summary": "As multimodal LLM-driven agents continue to advance in autonomy and\ngeneralization, evaluation based on static datasets can no longer adequately\nassess their true capabilities in dynamic environments and diverse tasks.\nExisting LLM-based synthetic data methods are largely designed for LLM training\nand evaluation, and thus cannot be directly applied to agent tasks that require\ntool use and interactive capabilities. While recent studies have explored\nautomatic agent task generation with LLMs, most efforts remain limited to text\nor image analysis, without systematically modeling multi-step interactions in\nweb environments. To address these challenges, we propose Graph2Eval, a\nknowledge graph-based framework that automatically generates both multimodal\ndocument comprehension tasks and web interaction tasks, enabling comprehensive\nevaluation of agents' reasoning, collaboration, and interactive capabilities.\nIn our approach, knowledge graphs constructed from multi-source external data\nserve as the task space, where we translate semantic relations into structured\nmultimodal tasks using subgraph sampling, task templates, and meta-paths. A\nmulti-stage filtering pipeline based on node reachability, LLM scoring, and\nsimilarity analysis is applied to guarantee the quality and executability of\nthe generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of\nmultiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures\nreasoning, collaboration, and interaction capabilities. We instantiate the\nframework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning\ndocument comprehension and web interaction scenarios. Experiments show that\nGraph2Eval efficiently generates tasks that differentiate agent and model\nperformance, revealing gaps in reasoning, collaboration, and web interaction\nacross different settings and offering a new perspective for agent evaluation.",
      "authors": [
        "Yurun Chen",
        "Xavier Hu",
        "Yuhan Liu",
        "Ziqi Wang",
        "Zeyi Liao",
        "Lin Chen",
        "Feng Wei",
        "Yuxi Qian",
        "Bo Zheng",
        "Keting Yin",
        "Shengyu Zhang"
      ],
      "published": "2025-10-01T04:37:54Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00507v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "Graph2Eval是一个基于知识图谱的框架，能够自动生成多模态文档理解任务和网页交互任务，用于全面评估智能体的推理、协作和交互能力。该框架通过子图采样、任务模板和元路径将语义关系转化为结构化任务，并采用多阶段过滤确保任务质量和可执行性。实验表明其能有效生成区分不同智能体性能的任务，为智能体评估提供新视角。",
      "order": 476
    },
    {
      "arxiv_id": "2510.00499v2",
      "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
      "summary": "Spoken dialogue systems often rely on cascaded pipelines that transcribe,\nprocess, and resynthesize speech. While effective, this design discards\nparalinguistic cues and limits expressivity. Recent end-to-end methods reduce\nlatency and better preserve these cues, yet still rely on text intermediates,\ncreating a fundamental bottleneck. We present MOSS-Speech, a true\nspeech-to-speech large language model that directly understands and generates\nspeech without relying on text guidance. Our approach combines a modality-based\nlayer-splitting architecture with a frozen pre-training strategy, preserving\nthe reasoning and knowledge of pretrained text LLMs while adding native speech\ncapabilities. Experiments show that our model achieves state-of-the-art results\nin spoken question answering and delivers comparable speech-to-speech\nperformance relative to existing text-guided systems, while still maintaining\ncompetitive text performance. By narrowing the gap between text-guided and\ndirect speech generation, our work establishes a new paradigm for expressive\nand efficient end-to-end speech interaction.",
      "authors": [
        "Xingjian Zhao",
        "Zhe Xu",
        "Qinyuan Cheng",
        "Zhaoye Fei",
        "Luozhijie Jin",
        "Yang Wang",
        "Hanfu Chen",
        "Yaozhou Jiang",
        "Qinghui Gao",
        "Ke Chen",
        "Ruixiao Li",
        "Mingshu Chen",
        "Ruiming Wang",
        "Wenbo Zhang",
        "Yiyang Zhang",
        "Donghua Yu",
        "Yang Gao",
        "Xiaogui Yang",
        "Yitian Gong",
        "Yuanfan Xu",
        "Yaqian Zhou",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "published": "2025-10-01T04:32:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00499v2",
      "primary_area": "audio_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "MOSS-Speech提出了一种无需文本指导的真正语音到语音大语言模型，通过模态分层架构和冻结预训练策略，在保留文本LLM推理能力的同时实现原生语音处理，在口语问答任务中达到先进水平。",
      "order": 477
    },
    {
      "arxiv_id": "2510.00496v2",
      "title": "Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via\n  Sensitivity Perturbations",
      "summary": "Although numerous strategies have recently been proposed to enhance the\nautonomous interaction capabilities of multimodal agents in graphical user\ninterface (GUI), their reliability remains limited when faced with complex or\nout-of-domain tasks. This raises a fundamental question: Are existing\nmultimodal agents reasoning spuriously? In this paper, we propose\n\\textbf{Agent-ScanKit}, a systematic probing framework to unravel the memory\nand reasoning capabilities of multimodal agents under controlled perturbations.\nSpecifically, we introduce three orthogonal probing paradigms: visual-guided,\ntext-guided, and structure-guided, each designed to quantify the contributions\nof memorization and reasoning without requiring access to model internals. In\nfive publicly available GUI benchmarks involving 18 multimodal agents, the\nresults demonstrate that mechanical memorization often outweighs systematic\nreasoning. Most of the models function predominantly as retrievers of\ntraining-aligned knowledge, exhibiting limited generalization. Our findings\nunderscore the necessity of robust reasoning modeling for multimodal agents in\nreal-world scenarios, offering valuable insights toward the development of\nreliable multimodal agents.",
      "authors": [
        "Pengzhou Cheng",
        "Lingzhong Dong",
        "Zeng Wu",
        "Zongru Wu",
        "Zhuosheng Zhang",
        "Gongshen Liu"
      ],
      "published": "2025-10-01T04:29:39Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00496v2",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Agent-ScanKit框架，通过视觉、文本和结构三种扰动范式系统评估多模态代理的记忆与推理能力。在5个GUI基准测试中分析18个模型发现，现有代理主要依赖机械记忆而非系统推理，训练知识检索能力优于泛化能力，强调了开发具备稳健推理能力的多模态代理的必要性。",
      "order": 478
    },
    {
      "arxiv_id": "2510.00482v1",
      "title": "Agent Fine-tuning through Distillation for Domain-specific LLMs in\n  Microdomains",
      "summary": "Agentic large language models (LLMs) have become prominent for autonomously\ninteracting with external environments and performing multi-step reasoning\ntasks. Most approaches leverage these capabilities via in-context learning with\nfew-shot prompts, but this often results in lengthy inputs and higher\ncomputational costs. Agent fine-tuning offers an alternative by enabling LLMs\nto internalize procedural reasoning and domain-specific knowledge through\ntraining on relevant data and demonstration trajectories. While prior studies\nhave focused on general domains, their effectiveness in specialized technical\nmicrodomains remains unclear. This paper explores agent fine-tuning for domain\nadaptation within Hitachi's JP1 middleware, a microdomain for specialized IT\noperations. We fine-tuned LLMs using JP1-specific datasets derived from domain\nmanuals and distilled reasoning trajectories generated by LLMs themselves,\nenhancing decision making accuracy and search efficiency. During inference, we\nused an agentic prompt with retrieval-augmented generation and introduced a\ncontext-answer extractor to improve information relevance. On JP1 certification\nexam questions, our method achieved a 14% performance improvement over the base\nmodel, demonstrating the potential of agent fine-tuning for domain-specific\nreasoning in complex microdomains.",
      "authors": [
        "Yawen Xue",
        "Masaya Tsunokake",
        "Yuta Koreeda",
        "Ekant Muljibhai Amin",
        "Takashi Sumiyoshi",
        "Yasuhiro Sogawa"
      ],
      "published": "2025-10-01T04:04:53Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00482v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种通过蒸馏技术对智能体大语言模型进行微调的方法，专门针对日立JP1中间件这一技术微领域。通过使用领域手册数据和模型自生成的推理轨迹进行微调，结合检索增强生成和上下文答案提取器，在JP1认证考试中相比基础模型性能提升14%，展示了在复杂微领域中领域特定推理的潜力。",
      "order": 479
    },
    {
      "arxiv_id": "2510.00449v1",
      "title": "Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context\n  User Reviews",
      "summary": "Personalizing the outputs of large language models (LLMs) to align with\nindividual user preferences is an active research area. However, previous\nstudies have mainly focused on classification or ranking tasks and have not\nconsidered Likert-scale rating prediction, a regression task that requires both\nlanguage and mathematical reasoning to be solved effectively. This task has\nsignificant industrial applications, but the utilization of LLMs remains\nunderexplored, particularly regarding the capabilities of off-the-shelf LLMs.\nThis study investigates the performance of off-the-shelf LLMs on rating\nprediction, providing different in-context information. Through comprehensive\nexperiments with eight models across three datasets, we demonstrate that\nuser-written reviews significantly improve the rating prediction performance of\nLLMs. This result is comparable to traditional methods like matrix\nfactorization, highlighting the potential of LLMs as a promising solution for\nthe cold-start problem. We also find that the reviews for concrete items are\nmore effective than general preference descriptions that are not based on any\nspecific item. Furthermore, we discover that prompting LLMs to first generate a\nhypothetical review enhances the rating prediction performance. Our code is\navailable at https://github.com/ynklab/rating-prediction-with-reviews.",
      "authors": [
        "Koki Ryu",
        "Hitomi Yanaka"
      ],
      "published": "2025-10-01T03:04:20Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00449v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探索现成大语言模型在评分预测任务中的表现，通过引入用户评论作为上下文信息，显著提升预测准确性。实验表明具体商品评论比通用偏好描述更有效，且让模型首先生成假设性评论可进一步优化预测效果，为解决冷启动问题提供新思路。",
      "order": 480
    },
    {
      "arxiv_id": "2510.00446v1",
      "title": "LongCodeZip: Compress Long Context for Code Language Models",
      "summary": "Code generation under long contexts is becoming increasingly critical as\nLarge Language Models (LLMs) are required to reason over extensive information\nin the codebase. While recent advances enable code LLMs to process long inputs,\nhigh API costs and generation latency remain substantial bottlenecks. Existing\ncontext pruning techniques, such as LLMLingua, achieve promising results for\ngeneral text but overlook code-specific structures and dependencies, leading to\nsuboptimal performance in programming tasks. In this paper, we propose\nLongCodeZip, a novel plug-and-play code compression framework designed\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\ncoarse-grained compression, which identifies and ranks function-level chunks\nusing conditional perplexity with respect to the instruction, retaining only\nthe most relevant functions; and (2) fine-grained compression, which segments\nretained functions into blocks based on perplexity and selects an optimal\nsubset under an adaptive token budget to maximize relevance. Evaluations across\nmultiple tasks, including code completion, summarization, and question\nanswering, show that LongCodeZip consistently outperforms baseline methods,\nachieving up to a 5.6x compression ratio without degrading task performance. By\neffectively reducing context size while preserving essential information,\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\nscenarios, advancing the efficiency and capability of code intelligence\napplications.",
      "authors": [
        "Yuling Shi",
        "Yichun Qian",
        "Hongyu Zhang",
        "Beijun Shen",
        "Xiaodong Gu"
      ],
      "published": "2025-10-01T02:54:57Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00446v1",
      "primary_area": "text_models",
      "secondary_focus": "['long_context', 'model_compression']",
      "application_domain": "code_generation",
      "tldr_zh": "LongCodeZip是针对代码大模型设计的双阶段压缩框架，通过粗粒度函数筛选和细粒度代码块选择，在保持任务性能的同时实现高达5.6倍压缩比，有效解决长代码上下文处理中的API成本和延迟问题。",
      "order": 481
    },
    {
      "arxiv_id": "2510.00444v1",
      "title": "TokMem: Tokenized Procedural Memory for Large Language Models",
      "summary": "Large language models rely heavily on prompts to specify tasks, recall\nknowledge and guide reasoning. However, this reliance is inefficient as prompts\nmust be re-read at each step, scale poorly across tasks, and lack mechanisms\nfor modular reuse. We introduce TokMem, a tokenized procedural memory that\nstores recurring procedures as compact, trainable embeddings. Each memory token\nencodes both an address to a procedure and a control signal that steers\ngeneration, enabling targeted behavior with constant-size overhead. To support\ncontinual adaptation, TokMem keeps the backbone model frozen, allowing new\nprocedures to be added without interfering with existing ones. We evaluate\nTokMem on 1,000 tasks for atomic recall, and on function-calling tasks for\ncompositional recall, where it consistently outperforms retrieval-augmented\ngeneration while avoiding repeated context overhead, and fine-tuning with far\nfewer parameters. These results establish TokMem as a scalable and modular\nalternative to prompt engineering and fine-tuning, offering an explicit\nprocedural memory for LLMs.",
      "authors": [
        "Zijun Wu",
        "Yongchang Hao",
        "Lili Mou"
      ],
      "published": "2025-10-01T02:51:58Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00444v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "TokMem提出了一种令牌化程序记忆机制，将重复性程序存储为紧凑可训练的嵌入向量，每个记忆令牌包含程序地址和控制信号，在保持骨干模型冻结的同时支持持续适应。在1000个任务评估中，该方案在原子回忆和组合回忆任务上均优于检索增强生成方法，且避免了重复上下文开销，为LLMs提供了可扩展的模块化程序记忆替代方案。",
      "order": 482
    },
    {
      "arxiv_id": "2510.00436v1",
      "title": "Automated Evaluation can Distinguish the Good and Bad AI Responses to\n  Patient Questions about Hospitalization",
      "summary": "Automated approaches to answer patient-posed health questions are rising, but\nselecting among systems requires reliable evaluation. The current gold standard\nfor evaluating the free-text artificial intelligence (AI) responses--human\nexpert review--is labor-intensive and slow, limiting scalability. Automated\nmetrics are promising yet variably aligned with human judgments and often\ncontext-dependent. To address the feasibility of automating the evaluation of\nAI responses to hospitalization-related questions posed by patients, we\nconducted a large systematic study of evaluation approaches. Across 100 patient\ncases, we collected responses from 28 AI systems (2800 total) and assessed them\nalong three dimensions: whether a system response (1) answers the question, (2)\nappropriately uses clinical note evidence, and (3) uses general medical\nknowledge. Using clinician-authored reference answers to anchor metrics,\nautomated rankings closely matched expert ratings. Our findings suggest that\ncarefully designed automated evaluation can scale comparative assessment of AI\nsystems and support patient-clinician communication.",
      "authors": [
        "Sarvesh Soni",
        "Dina Demner-Fushman"
      ],
      "published": "2025-10-01T02:39:37Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00436v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究系统评估了28个AI系统对100个住院相关患者问题的回答质量，通过三个维度（回答问题、使用临床证据、运用医学知识）验证了自动化评估方法可有效替代人工专家评审，为医疗AI对话系统的规模化评估提供了可行方案。",
      "order": 483
    },
    {
      "arxiv_id": "2510.00404v2",
      "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features",
      "summary": "Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs.",
      "authors": [
        "Xudong Zhu",
        "Mohammad Mahdi Khalili",
        "Zhihui Zhu"
      ],
      "published": "2025-10-01T01:29:31Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00404v2",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出AbsTopK稀疏自编码器，通过解构稀疏编码的邻近梯度方法，解决了现有SAE因非负约束无法表示双向概念的问题。新方法基于ℓ₀稀疏约束，保留正负激活值，在四个大语言模型和七项任务中验证了其重建保真度和可解释性的提升，能单特征编码对立概念。",
      "order": 484
    },
    {
      "arxiv_id": "2510.00374v1",
      "title": "GDLNN: Marriage of Programming Language and Neural Networks for Accurate\n  and Easy-to-Explain Graph Classification",
      "summary": "We present GDLNN, a new graph machine learning architecture, for graph\nclassification tasks. GDLNN combines a domain-specific programming language,\ncalled GDL, with neural networks. The main strength of GDLNN lies in its GDL\nlayer, which generates expressive and interpretable graph representations.\nSince the graph representation is interpretable, existing model explanation\ntechniques can be directly applied to explain GDLNN's predictions. Our\nevaluation shows that the GDL-based representation achieves high accuracy on\nmost graph classification benchmark datasets, outperforming dominant graph\nlearning methods such as GNNs. Applying an existing model explanation technique\nalso yields high-quality explanations of GDLNN's predictions. Furthermore, the\ncost of GDLNN is low when the explanation cost is included.",
      "authors": [
        "Minseok Jeon",
        "Seunghyun Park"
      ],
      "published": "2025-10-01T00:44:58Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00374v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "GDLNN是一种新型图机器学习架构，通过结合领域特定编程语言GDL与神经网络，在图形分类任务中实现高精度且易于解释的预测。该模型在多数基准数据集上超越主流图学习方法，并能直接应用现有解释技术提供高质量预测解释。",
      "order": 485
    },
    {
      "arxiv_id": "2510.02315v1",
      "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity",
      "summary": "Text-to-image (T2I) models excel on single-entity prompts but struggle with\nmulti-subject descriptions, often showing attribute leakage, identity\nentanglement, and subject omissions. We introduce the first theoretical\nframework with a principled, optimizable objective for steering sampling\ndynamics toward multi-subject fidelity. Viewing flow matching (FM) through\nstochastic optimal control (SOC), we formulate subject disentanglement as\ncontrol over a trained FM sampler. This yields two architecture-agnostic\nalgorithms: (i) a training-free test-time controller that perturbs the base\nvelocity with a single-pass update, and (ii) Adjoint Matching, a lightweight\nfine-tuning rule that regresses a control network to a backward adjoint signal\nwhile preserving base-model capabilities. The same formulation unifies prior\nattention heuristics, extends to diffusion models via a flow-diffusion\ncorrespondence, and provides the first fine-tuning route explicitly designed\nfor multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and\nStable Diffusion XL, both algorithms consistently improve multi-subject\nalignment while maintaining base-model style. Test-time control runs\nefficiently on commodity GPUs, and fine-tuned controllers trained on limited\nprompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal\nControl for Unentangled Subjects), which achieves state-of-the-art\nmulti-subject fidelity across models.",
      "authors": [
        "Eric Tillmann Bill",
        "Enis Simsar",
        "Thomas Hofmann"
      ],
      "published": "2025-10-02T17:59:58Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02315v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出首个理论框架，通过随机最优控制优化流匹配采样过程，解决多主体文本到图像生成中的属性泄漏、身份纠缠和主体遗漏问题。开发了两种架构无关算法：无需训练的测试时控制器和轻量级微调方法Adjoint Matching，在多个主流模型上显著提升多主体对齐效果，同时保持基础模型风格。",
      "order": 486
    },
    {
      "arxiv_id": "2510.02314v1",
      "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions",
      "summary": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/",
      "authors": [
        "Bo-Hsu Ke",
        "You-Zhe Xie",
        "Yu-Lun Liu",
        "Wei-Chen Chiu"
      ],
      "published": "2025-10-02T17:59:57Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02314v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种针对3D高斯泼溅(3DGS)的密度引导投毒攻击方法，通过核密度估计识别低密度区域注入高斯点，嵌入视角依赖的幻觉物体，在投毒视角清晰可见而几乎不影响正常视角，并采用自适应噪声策略破坏多视角一致性，显著提升攻击效果。",
      "order": 487
    },
    {
      "arxiv_id": "2510.02313v1",
      "title": "Clink! Chop! Thud! -- Learning Object Sounds from Real-World\n  Interactions",
      "summary": "Can a model distinguish between the sound of a spoon hitting a hardwood floor\nversus a carpeted one? Everyday object interactions produce sounds unique to\nthe objects involved. We introduce the sounding object detection task to\nevaluate a model's ability to link these sounds to the objects directly\ninvolved. Inspired by human perception, our multimodal object-aware framework\nlearns from in-the-wild egocentric videos. To encourage an object-centric\napproach, we first develop an automatic pipeline to compute segmentation masks\nof the objects involved to guide the model's focus during training towards the\nmost informative regions of the interaction. A slot attention visual encoder is\nused to further enforce an object prior. We demonstrate state of the art\nperformance on our new task along with existing multimodal action understanding\ntasks.",
      "authors": [
        "Mengyu Yang",
        "Yiming Chen",
        "Haozheng Pei",
        "Siddhant Agarwal",
        "Arun Balajee Vasudevan",
        "James Hays"
      ],
      "published": "2025-10-02T17:59:52Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02313v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出一种多模态对象感知框架，通过真实世界交互视频学习物体声音识别。采用自动分割管道和槽注意力视觉编码器，在声音对象检测任务中实现最先进性能。",
      "order": 488
    },
    {
      "arxiv_id": "2510.02311v1",
      "title": "Inferring Dynamic Physical Properties from Video Foundation Models",
      "summary": "We study the task of predicting dynamic physical properties from videos. More\nspecifically, we consider physical properties that require temporal information\nto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,\nand dynamic friction of an object sliding on a surface. To this end, we make\nthe following contributions: (i) We collect a new video dataset for each\nphysical property, consisting of synthetic training and testing splits, as well\nas a real split for real world evaluation. (ii) We explore three ways to infer\nthe physical property from videos: (a) an oracle method where we supply the\nvisual cues that intrinsically reflect the property using classical computer\nvision techniques; (b) a simple read out mechanism using a visual prompt and\ntrainable prompt vector for cross-attention on pre-trained video generative and\nself-supervised models; and (c) prompt strategies for Multi-modal Large\nLanguage Models (MLLMs). (iii) We show that video foundation models trained in\na generative or self-supervised manner achieve a similar performance, though\nbehind that of the oracle, and MLLMs are currently inferior to the other\nmodels, though their performance can be improved through suitable prompting.",
      "authors": [
        "Guanqi Zhan",
        "Xianzheng Ma",
        "Weidi Xie",
        "Andrew Zisserman"
      ],
      "published": "2025-10-02T17:59:50Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02311v1",
      "primary_area": "video_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探索从视频中推断动态物理属性的方法，包括弹性、粘度和动摩擦。通过构建新数据集并比较三种方法：基于传统视觉线索的预言机方法、预训练视频模型的提示机制以及多模态大语言模型。结果显示生成式和自监督视频基础模型表现相近，多模态大模型性能相对较弱但可通过提示优化提升。",
      "order": 489
    },
    {
      "arxiv_id": "2510.02307v1",
      "title": "NoiseShift: Resolution-Aware Noise Recalibration for Better\n  Low-Resolution Image Generation",
      "summary": "Text-to-image diffusion models trained on a fixed set of resolutions often\nfail to generalize, even when asked to generate images at lower resolutions\nthan those seen during training. High-resolution text-to-image generators are\ncurrently unable to easily offer an out-of-the-box budget-efficient alternative\nto their users who might not need high-resolution images. We identify a key\ntechnical insight in diffusion models that when addressed can help tackle this\nlimitation: Noise schedulers have unequal perceptual effects across\nresolutions. The same level of noise removes disproportionately more signal\nfrom lower-resolution images than from high-resolution images, leading to a\ntrain-test mismatch. We propose NoiseShift, a training-free method that\nrecalibrates the noise level of the denoiser conditioned on resolution size.\nNoiseShift requires no changes to model architecture or sampling schedule and\nis compatible with existing models. When applied to Stable Diffusion 3, Stable\nDiffusion 3.5, and Flux-Dev, quality at low resolutions is significantly\nimproved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and\nFlux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by\n10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results\ndemonstrate the effectiveness of NoiseShift in mitigating resolution-dependent\nartifacts and enhancing the quality of low-resolution image generation.",
      "authors": [
        "Ruozhen He",
        "Moayed Haji-Ali",
        "Ziyan Yang",
        "Vicente Ordonez"
      ],
      "published": "2025-10-02T17:59:43Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02307v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "NoiseShift是一种无需训练的方法，通过根据分辨率大小重新校准去噪器的噪声水平，解决了扩散模型在不同分辨率下生成质量不一致的问题。该方法兼容现有模型，在Stable Diffusion 3、3.5和Flux-Dev上显著提升了低分辨率图像生成质量，平均FID指标提升达2.44%-15.89%。",
      "order": 490
    },
    {
      "arxiv_id": "2510.02300v1",
      "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
      "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.",
      "authors": [
        "Runqian Wang",
        "Yilun Du"
      ],
      "published": "2025-10-02T17:59:06Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02300v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出平衡匹配(EqM)生成建模框架，摒弃传统扩散/流模型的时间条件动态，通过隐式能量模型的平衡梯度学习实现优化驱动采样。在ImageNet 256×256上达到1.90 FID，支持去噪、异常检测和图像合成等任务，为流模型与能量模型搭建了更紧密的桥梁。",
      "order": 491
    },
    {
      "arxiv_id": "2510.02296v1",
      "title": "Continual Personalization for Diffusion Models",
      "summary": "Updating diffusion models in an incremental setting would be practical in\nreal-world applications yet computationally challenging. We present a novel\nlearning strategy of Concept Neuron Selection (CNS), a simple yet effective\napproach to perform personalization in a continual learning scheme. CNS\nuniquely identifies neurons in diffusion models that are closely related to the\ntarget concepts. In order to mitigate catastrophic forgetting problems while\npreserving zero-shot text-to-image generation ability, CNS finetunes concept\nneurons in an incremental manner and jointly preserves knowledge learned of\nprevious concepts. Evaluation of real-world datasets demonstrates that CNS\nachieves state-of-the-art performance with minimal parameter adjustments,\noutperforming previous methods in both single and multi-concept personalization\nworks. CNS also achieves fusion-free operation, reducing memory storage and\nprocessing time for continual personalization.",
      "authors": [
        "Yu-Chien Liao",
        "Jr-Jen Chen",
        "Chi-Pin Huang",
        "Ci-Siang Lin",
        "Meng-Lin Wu",
        "Yu-Chiang Frank Wang"
      ],
      "published": "2025-10-02T17:58:56Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02296v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出概念神经元选择(CNS)方法，通过在扩散模型中识别与目标概念相关的神经元进行增量微调，实现持续个性化学习。该方法能有效缓解灾难性遗忘问题，保持零样本文本到图像生成能力，同时减少内存占用和处理时间，在单概念和多概念个性化任务中均达到最先进性能。",
      "order": 492
    },
    {
      "arxiv_id": "2510.02295v1",
      "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
      "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.",
      "authors": [
        "Enxin Song",
        "Wenhao Chai",
        "Shusheng Yang",
        "Ethan Armand",
        "Xiaojun Shan",
        "Haiyang Xu",
        "Jianwen Xie",
        "Zhuowen Tu"
      ],
      "published": "2025-10-02T17:58:54Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02295v1",
      "primary_area": "vla_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "VideoNSA通过原生稀疏注意力机制增强视频语言模型，在Qwen2.5-VL基础上采用硬件感知的混合注意力策略：文本保持稠密注意力，视频使用稀疏注意力。该方法在21.6万视频指令数据集上端到端训练，显著提升长视频理解、时序推理和空间基准测试性能，支持128K令牌扩展并发现全局-局部注意力分配的最优模式。",
      "order": 493
    },
    {
      "arxiv_id": "2510.02292v1",
      "title": "From Behavioral Performance to Internal Competence: Interpreting\n  Vision-Language Models with VLM-Lens",
      "summary": "We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,\nanalysis, and interpretation of vision-language models (VLMs) by supporting the\nextraction of intermediate outputs from any layer during the forward pass of\nopen-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that\nabstracts away model-specific complexities and supports user-friendly operation\nacross diverse VLMs. It currently supports 16 state-of-the-art base VLMs and\ntheir over 30 variants, and is extensible to accommodate new models without\nchanging the core logic.\n  The toolkit integrates easily with various interpretability and analysis\nmethods. We demonstrate its usage with two simple analytical experiments,\nrevealing systematic differences in the hidden representations of VLMs across\nlayers and target concepts. VLM-Lens is released as an open-sourced project to\naccelerate community efforts in understanding and improving VLMs.",
      "authors": [
        "Hala Sheta",
        "Eric Huang",
        "Shuyu Wu",
        "Ilia Alenabi",
        "Jiajun Hong",
        "Ryker Lin",
        "Ruoxi Ning",
        "Daniel Wei",
        "Jialin Yang",
        "Jiawei Zhou",
        "Ziqiao Ma",
        "Freda Shi"
      ],
      "published": "2025-10-02T17:58:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02292v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "VLM-Lens是一个用于系统化评估、分析和解释视觉语言模型的开源工具包，支持从开源VLM任意层提取中间输出，提供统一配置接口，已集成16个主流模型及其30多个变体，可加速社区对VLM的理解与改进。",
      "order": 494
    },
    {
      "arxiv_id": "2510.02291v1",
      "title": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling",
      "summary": "We study the problem of posterior sampling using pretrained discrete\ndiffusion foundation models, aiming to recover images from noisy measurements\nwithout retraining task-specific models. While diffusion models have achieved\nremarkable success in generative modeling, most advances rely on continuous\nGaussian diffusion. In contrast, discrete diffusion offers a unified framework\nfor jointly modeling categorical data such as text and images. Beyond\nunification, discrete diffusion provides faster inference, finer control, and\nprincipled training-free Bayesian inference, making it particularly well-suited\nfor posterior sampling. However, existing approaches to discrete diffusion\nposterior sampling face severe challenges: derivative-free guidance yields\nsparse signals, continuous relaxations limit applicability, and split Gibbs\nsamplers suffer from the curse of dimensionality. To overcome these\nlimitations, we introduce Anchored Posterior Sampling (APS) for masked\ndiffusion foundation models, built on two key innovations -- quantized\nexpectation for gradient-like guidance in discrete embedding space, and\nanchored remasking for adaptive decoding. Our approach achieves\nstate-of-the-art performance among discrete diffusion samplers across linear\nand nonlinear inverse problems on the standard benchmarks. We further\ndemonstrate the benefits of our approach in training-free stylization and\ntext-guided editing.",
      "authors": [
        "Litu Rout",
        "Andreas Lugmayr",
        "Yasamin Jafarian",
        "Srivatsan Varadharajan",
        "Constantine Caramanis",
        "Sanjay Shakkottai",
        "Ira Kemelmacher-Shlizerman"
      ],
      "published": "2025-10-02T17:58:37Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02291v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出锚定后验采样(APS)方法，针对离散扩散基础模型的后验采样问题。通过量化期望和锚定重掩码两项创新技术，在离散嵌入空间实现梯度式引导和自适应解码，在线性和非线性逆问题中达到最先进性能，支持无需训练的风格化和文本引导编辑。",
      "order": 495
    },
    {
      "arxiv_id": "2510.02287v1",
      "title": "MultiModal Action Conditioned Video Generation",
      "summary": "Current video models fail as world model as they lack fine-graiend control.\nGeneral-purpose household robots require real-time fine motor control to handle\ndelicate tasks and urgent situations. In this work, we introduce fine-grained\nmultimodal actions to capture such precise control. We consider senses of\nproprioception, kinesthesia, force haptics, and muscle activation. Such\nmultimodal senses naturally enables fine-grained interactions that are\ndifficult to simulate with text-conditioned generative models. To effectively\nsimulate fine-grained multisensory actions, we develop a feature learning\nparadigm that aligns these modalities while preserving the unique information\neach modality provides. We further propose a regularization scheme to enhance\ncausality of the action trajectory features in representing intricate\ninteraction dynamics. Experiments show that incorporating multimodal senses\nimproves simulation accuracy and reduces temporal drift. Extensive ablation\nstudies and downstream applications demonstrate the effectiveness and\npracticality of our work.",
      "authors": [
        "Yichen Li",
        "Antonio Torralba"
      ],
      "published": "2025-10-02T17:57:06Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02287v1",
      "primary_area": "video_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种多模态动作条件视频生成方法，通过整合本体感觉、运动觉、力触觉和肌肉激活等精细控制信号，解决了现有视频模型缺乏细粒度控制的问题。开发的特征学习范式能对齐多模态信息同时保留各自特性，并通过正则化增强动作轨迹特征的因果性，实验表明该方法提高了模拟精度并减少了时间漂移。",
      "order": 496
    },
    {
      "arxiv_id": "2510.02284v1",
      "title": "Learning to Generate Object Interactions with Physics-Guided Video\n  Diffusion",
      "summary": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.",
      "authors": [
        "David Romero",
        "Ariana Bermudez",
        "Hao Li",
        "Fabio Pizzati",
        "Ivan Laptev"
      ],
      "published": "2025-10-02T17:56:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02284v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出KineMask方法，通过物理引导的视频扩散模型实现逼真的物体交互生成。该方法采用两阶段训练策略，结合单张图像和指定物体速度生成包含运动推断和未来交互的视频，在合成场景和真实场景中均显著提升了物体交互的物理合理性。",
      "order": 497
    },
    {
      "arxiv_id": "2510.02283v1",
      "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
      "summary": "Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/",
      "authors": [
        "Justin Cui",
        "Jie Wu",
        "Ming Li",
        "Tao Yang",
        "Xiaojie Li",
        "Rui Wang",
        "Andrew Bai",
        "Yuanhao Ban",
        "Cho-Jui Hsieh"
      ],
      "published": "2025-10-02T17:55:42Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02283v1",
      "primary_area": "video_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Self-Forcing++方法，通过利用教师模型的丰富知识为自生成长视频提供片段指导，有效缓解长视频生成中的质量退化问题。该方法无需长视频教师监督或重新训练，即可将视频长度扩展至教师能力的20倍以上，最高支持生成4分15秒视频，在保真度和一致性上显著优于基线方法。",
      "order": 498
    },
    {
      "arxiv_id": "2510.02282v1",
      "title": "VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning\n  MLLMs and RL",
      "summary": "With the rapid advancement of AI-generated videos, there is an urgent need\nfor effective detection tools to mitigate societal risks such as misinformation\nand reputational harm. In addition to accurate classification, it is essential\nthat detection models provide interpretable explanations to ensure transparency\nfor regulators and end users. To address these challenges, we introduce\nVidGuard-R1, the first video authenticity detector that fine-tunes a\nmulti-modal large language model (MLLM) using group relative policy\noptimization (GRPO). Our model delivers both highly accurate judgments and\ninsightful reasoning. We curate a challenging dataset of 140k real and\nAI-generated videos produced by state-of-the-art generation models, carefully\ndesigning the generation process to maximize discrimination difficulty. We then\nfine-tune Qwen-VL using GRPO with two specialized reward models that target\ntemporal artifacts and generation complexity. Extensive experiments demonstrate\nthat VidGuard-R1 achieves state-of-the-art zero-shot performance on existing\nbenchmarks, with additional training pushing accuracy above 95%. Case studies\nfurther show that VidGuard-R1 produces precise and interpretable rationales\nbehind its predictions. The code is publicly available at\nhttps://VidGuard-R1.github.io.",
      "authors": [
        "Kyoungjun Park",
        "Yifan Yang",
        "Juheon Yi",
        "Shicheng Zheng",
        "Yifei Shen",
        "Dongqi Han",
        "Caihua Shan",
        "Muhammad Muaz",
        "Lili Qiu"
      ],
      "published": "2025-10-02T17:55:37Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02282v1",
      "primary_area": "video_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "VidGuard-R1是首个通过多模态大语言模型与强化学习相结合的视频真实性检测系统，采用GRPO算法微调Qwen-VL模型，在14万真实与AI生成视频数据集上实现超过95%的检测准确率，并能提供可解释的检测依据，有效应对虚假信息等社会风险。",
      "order": 499
    },
    {
      "arxiv_id": "2510.02270v1",
      "title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for\n  Fine-Grained Image Classification",
      "summary": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP.",
      "authors": [
        "Sathira Silva",
        "Eman Ali",
        "Chetan Arora",
        "Muhammad Haris Khan"
      ],
      "published": "2025-10-02T17:47:39Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02270v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "microCLIP提出了一种无监督的CLIP自适应框架，通过粗-细粒度令牌融合实现细粒度图像分类。核心创新包括：基于显著性注意力的令牌融合模块构建细粒度[FG]令牌并与全局[CLS]令牌对齐；双头LLM分类器提供稳定伪标签；动态知识聚合迭代优化。在13个细粒度基准上平均提升2.90%准确率，仅需轻量适配。",
      "order": 500
    },
    {
      "arxiv_id": "2510.02268v1",
      "title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with\n  Camera Conditioning",
      "summary": "We study view-invariant imitation learning by explicitly conditioning\npolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we\nshow that conditioning on extrinsics significantly improves generalization\nacross viewpoints for standard behavior cloning policies, including ACT,\nDiffusion Policy, and SmolVLA. To evaluate policy robustness under realistic\nviewpoint shifts, we introduce six manipulation tasks in RoboSuite and\nManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling\nbackground cues from camera pose. Our analysis reveals that policies without\nextrinsics often infer camera pose using visual cues from static backgrounds in\nfixed scenes; this shortcut collapses when workspace geometry or camera\nplacement shifts. Conditioning on extrinsics restores performance and yields\nrobust RGB-only control without depth. We release the tasks, demonstrations,\nand code at https://ripl.github.io/know_your_camera/ .",
      "authors": [
        "Tianchong Jiang",
        "Jingtian Ji",
        "Xiangshan Tan",
        "Jiading Fang",
        "Anand Bhattad",
        "Vitor Guizilini",
        "Matthew R. Walter"
      ],
      "published": "2025-10-02T17:47:06Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.02268v1",
      "primary_area": "vla_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出通过显式条件化相机外参实现视角不变的模仿学习，使用Plucker嵌入表示像素光线。实验表明该方法显著提升了ACT、Diffusion Policy和SmolVLA等策略在不同视角下的泛化能力，在RoboSuite和ManiSkill的六个操纵任务中验证了其鲁棒性，解决了固定场景中策略依赖背景线索推断相机位姿的缺陷。",
      "order": 501
    },
    {
      "arxiv_id": "2510.02266v1",
      "title": "NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual\n  Reconstruction of Complex Scenes",
      "summary": "Reconstructing visual information from brain activity via computer vision\ntechnology provides an intuitive understanding of visual neural mechanisms.\nDespite progress in decoding fMRI data with generative models, achieving\naccurate cross-subject reconstruction of visual stimuli remains challenging and\ncomputationally demanding. This difficulty arises from inter-subject\nvariability in neural representations and the brain's abstract encoding of core\nsemantic features in complex visual inputs. To address these challenges, we\npropose NeuroSwift, which integrates complementary adapters via diffusion:\nAutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter\nis trained on Stable Diffusion generated images paired with COCO captions to\nemulate higher visual cortex encoding. For cross-subject generalization, we\npretrain on one subject and then fine-tune only 17 percent of parameters (fully\nconnected layers) for new subjects, while freezing other components. This\nenables state-of-the-art performance with only one hour of training per subject\non lightweight GPUs (three RTX 4090), and it outperforms existing methods.",
      "authors": [
        "Shiyi Zhang",
        "Dong Liang",
        "Yihang Zhou"
      ],
      "published": "2025-10-02T17:45:43Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02266v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_compression",
      "application_domain": "medical_ai",
      "tldr_zh": "NeuroSwift是一种轻量级跨被试fMRI视觉重建框架，通过融合AutoKL和CLIP适配器的扩散模型，仅需微调17%参数即可实现复杂场景的跨被试视觉重建，在轻量级GPU上单被试训练仅需1小时，性能优于现有方法。",
      "order": 502
    },
    {
      "arxiv_id": "2510.02264v1",
      "title": "Paving the Way Towards Kinematic Assessment Using Monocular Video: A\n  Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose\n  Estimators Against Inertial Sensors in Daily Living Activities",
      "summary": "Advances in machine learning and wearable sensors offer new opportunities for\ncapturing and analyzing human movement outside specialized laboratories.\nAccurate assessment of human movement under real-world conditions is essential\nfor telemedicine, sports science, and rehabilitation. This preclinical\nbenchmark compares monocular video-based 3D human pose estimation models with\ninertial measurement units (IMUs), leveraging the VIDIMU dataset containing a\ntotal of 13 clinically relevant daily activities which were captured using both\ncommodity video cameras and five IMUs. During this initial study only healthy\nsubjects were recorded, so results cannot be generalized to pathological\ncohorts. Joint angles derived from state-of-the-art deep learning frameworks\n(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA\nBodyTrack) were evaluated against joint angles computed from IMU data using\nOpenSim inverse kinematics following the Human3.6M dataset format with 17\nkeypoints. Among them, MotionAGFormer demonstrated superior performance,\nachieving the lowest overall RMSE ($9.27\\deg \\pm 4.80\\deg$) and MAE ($7.86\\deg\n\\pm 4.18\\deg$), as well as the highest Pearson correlation ($0.86 \\pm 0.15$)\nand the highest coefficient of determination $R^{2}$ ($0.67 \\pm 0.28$). The\nresults reveal that both technologies are viable for out-of-the-lab kinematic\nassessment. However, they also highlight key trade-offs between video- and\nsensor-based approaches including costs, accessibility, and precision. This\nstudy clarifies where off-the-shelf video models already provide clinically\npromising kinematics in healthy adults and where they lag behind IMU-based\nestimates while establishing valuable guidelines for researchers and clinicians\nseeking to develop robust, cost-effective, and user-friendly solutions for\ntelehealth and remote patient monitoring.",
      "authors": [
        "Mario Medrano-Paredes",
        "Carmen Fernández-González",
        "Francisco-Javier Díaz-Pernas",
        "Hichem Saoudi",
        "Javier González-Alonso",
        "Mario Martínez-Zarzuela"
      ],
      "published": "2025-10-02T17:44:31Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02264v1",
      "primary_area": "video_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究比较了基于单目视频的3D人体姿态估计模型与惯性传感器在日常生活活动中的性能，使用VIDIMU数据集评估了四种深度学习框架。MotionAGFormer表现最佳，整体RMSE为9.27°±4.80°，皮尔逊相关系数达0.86±0.15。研究揭示了视频与传感器方案在成本、可及性和精度间的权衡，为远程医疗和患者监测提供了实用指南。",
      "order": 503
    },
    {
      "arxiv_id": "2510.02262v1",
      "title": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video\n  Understanding",
      "summary": "Video Large Language Models (VLMs) have achieved remarkable results on a\nvariety of vision language tasks, yet their practical use is limited by the\n\"needle in a haystack\" problem: the massive number of visual tokens produced\nfrom raw video frames exhausts the model's context window. Existing solutions\nalleviate this issue by selecting a sparse set of frames, thereby reducing\ntoken count, but such frame-wise selection discards essential temporal\ndynamics, leading to suboptimal reasoning about motion and event continuity. In\nthis work we systematically explore the impact of temporal information and\ndemonstrate that extending selection from isolated key frames to key clips,\nwhich are short, temporally coherent segments, improves video understanding. To\nmaintain a fixed computational budget while accommodating the larger token\nfootprint of clips, we propose an adaptive resolution strategy that dynamically\nbalances spatial resolution and clip length, ensuring a constant token count\nper video. Experiments on three long-form video benchmarks demonstrate that our\ntraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and\n10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These\nresults highlight the importance of preserving temporal coherence in frame\nselection and provide a practical pathway for scaling Video LLMs to real world\nvideo understanding applications. Project webpage is available at\nhttps://guangyusun.com/f2c .",
      "authors": [
        "Guangyu Sun",
        "Archit Singhal",
        "Burak Uzkent",
        "Mubarak Shah",
        "Chen Chen",
        "Garin Kessler"
      ],
      "published": "2025-10-02T17:43:01Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02262v1",
      "primary_area": "video_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出F2C方法，通过从关键帧选择扩展到关键片段选择，并采用自适应分辨率策略平衡空间分辨率与片段长度，在固定计算预算下提升长视频理解性能。该方法在三个长视频基准测试中显著优于均匀采样，最高提升达10.3%。",
      "order": 504
    },
    {
      "arxiv_id": "2510.02253v1",
      "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
      "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
      "authors": [
        "Zihan Zhou",
        "Shilin Lu",
        "Shuli Leng",
        "Shaocong Zhang",
        "Zhuming Lian",
        "Xinlei Yu",
        "Adams Wai-Kin Kong"
      ],
      "published": "2025-10-02T17:39:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02253v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "DragFlow是首个利用FLUX强大先验进行拖拽式图像编辑的框架，通过区域监督和仿射变换解决DiT特征结构化不足的问题，集成个性化适配器保持主体一致性，使用MLLM解决任务歧义，在DragBench-DR和ReD Bench基准测试中达到最先进水平。",
      "order": 505
    },
    {
      "arxiv_id": "2510.02250v1",
      "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
      "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
      "authors": [
        "Gonzalo Gonzalez-Pumariega",
        "Vincent Tu",
        "Chih-Lun Lee",
        "Jiachen Yang",
        "Ang Li",
        "Xin Eric Wang"
      ],
      "published": "2025-10-02T17:37:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02250v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Behavior Best-of-N (bBoN)方法，通过生成多个执行轨迹并使用行为叙述进行选择，显著提升计算机使用代理的可靠性和成功率。在OSWorld基准测试中达到69.9%的新SOTA，接近人类72%的水平，并在跨操作系统任务中展现强大泛化能力，证明了规模化代理在结构化轨迹理解和选择下的卓越效果。",
      "order": 506
    },
    {
      "arxiv_id": "2510.02240v1",
      "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
      "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.",
      "authors": [
        "Sicheng Feng",
        "Kaiwen Tuo",
        "Song Wang",
        "Lingdong Kong",
        "Jianke Zhu",
        "Huan Wang"
      ],
      "published": "2025-10-02T17:29:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02240v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出RewardMap框架，通过多阶段强化学习解决细粒度视觉推理中的稀疏奖励问题。该方法构建了ReasonMap-Plus数据集提供密集奖励信号，并采用难度感知奖励设计和从感知到推理的多阶段训练策略，在多个基准测试中平均提升3.47%，显著增强了多模态大语言模型的视觉理解和推理能力。",
      "order": 507
    },
    {
      "arxiv_id": "2510.02230v1",
      "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.",
      "authors": [
        "Phuc Minh Nguyen",
        "Chinh D. La",
        "Duy M. H. Nguyen",
        "Nitesh V. Chawla",
        "Binh T. Nguyen",
        "Khoa D. Doan"
      ],
      "published": "2025-10-02T17:17:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02230v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文揭示强化学习与可验证奖励(RLVR)在提升大语言模型推理能力时存在的悖论：反而会缩小推理边界。研究发现RLVR存在负干扰现象和赢家通吃效应，导致模型收敛于狭窄的解题策略。作者提出针对低概率问题的数据筛选算法，显著提升了Pass@k性能。",
      "order": 508
    },
    {
      "arxiv_id": "2510.02226v1",
      "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models",
      "summary": "Recent advances in generative video models have enabled the creation of\nhigh-quality videos based on natural language prompts. However, these models\nfrequently lack fine-grained temporal control, meaning they do not allow users\nto specify when particular visual elements should appear within a generated\nsequence. In this work, we introduce TempoControl, a method that allows for\ntemporal alignment of visual concepts during inference, without requiring\nretraining or additional supervision. TempoControl utilizes cross-attention\nmaps, a key component of text-to-video diffusion models, to guide the timing of\nconcepts through a novel optimization approach. Our method steers attention\nusing three complementary principles: aligning its temporal shape with a\ncontrol signal (via correlation), amplifying it where visibility is needed (via\nenergy), and maintaining spatial focus (via entropy). TempoControl allows\nprecise control over timing while ensuring high video quality and diversity. We\ndemonstrate its effectiveness across various video generation applications,\nincluding temporal reordering for single and multiple objects, as well as\naction and audio-aligned generation.",
      "authors": [
        "Shira Schiber",
        "Ofir Lindenbaum",
        "Idan Schwartz"
      ],
      "published": "2025-10-02T17:13:35Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02226v1",
      "primary_area": "video_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "TempoControl是一种无需重新训练即可实现文本到视频生成模型时序控制的方法，通过优化交叉注意力图来精确控制视觉概念在视频序列中的出现时机，支持时序重排、动作和音频对齐等多种应用场景。",
      "order": 509
    },
    {
      "arxiv_id": "2510.02213v1",
      "title": "MMDEW: Multipurpose Multiclass Density Estimation in the Wild",
      "summary": "Density map estimation can be used to estimate object counts in dense and\noccluded scenes where discrete counting-by-detection methods fail. We propose a\nmulticategory counting framework that leverages a Twins pyramid\nvision-transformer backbone and a specialised multi-class counting head built\non a state-of-the-art multiscale decoding approach. A two-task design adds a\nsegmentation-based Category Focus Module, suppressing inter-category cross-talk\nat training time. Training and evaluation on the VisDrone and iSAID benchmarks\ndemonstrates superior performance versus prior multicategory crowd-counting\napproaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11\nunderscores the necessity of crowd counting methods in dense scenes. The\nmethod's regional loss opens up multi-class crowd counting to new domains,\ndemonstrated through the application to a biodiversity monitoring dataset,\nhighlighting its capacity to inform conservation efforts and enable scalable\necological insights.",
      "authors": [
        "Villanelle O'Reilly",
        "Jonathan Cox",
        "Georgios Leontidis",
        "Marc Hanheide",
        "Petra Bosilj",
        "James Brown"
      ],
      "published": "2025-10-02T16:57:29Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02213v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "MMDEW提出一种多类别密度估计框架，采用Twins金字塔视觉transformer骨干网络和专用多类计数头，通过双任务设计和类别聚焦模块减少类别间干扰。在VisDrone和iSAID基准测试中表现优异，相比现有方法MAE降低33%-64%，并成功应用于生物多样性监测，为生态保护提供可扩展的解决方案。",
      "order": 510
    },
    {
      "arxiv_id": "2510.02208v1",
      "title": "Measurement-Guided Consistency Model Sampling for Inverse Problems",
      "summary": "Diffusion models have become powerful generative priors for solving inverse\nimaging problems, but their reliance on slow multi-step sampling limits\npractical deployment. Consistency models address this bottleneck by enabling\nhigh-quality generation in a single or only a few steps, yet their direct\nadaptation to inverse problems is underexplored. In this paper, we present a\nmodified consistency sampling approach tailored for inverse problem\nreconstruction: the sampler's stochasticity is guided by a\nmeasurement-consistency mechanism tied to the measurement operator, which\nenforces fidelity to the acquired measurements while retaining the efficiency\nof consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom\ndatasets demonstrate consistent improvements in perceptual and pixel-level\nmetrics, including Fr\\'echet Inception Distance, Kernel Inception Distance,\npeak signal-to-noise ratio, and structural similarity index measure, compared\nto baseline consistency sampling, yielding competitive or superior\nreconstructions with only a handful of steps.",
      "authors": [
        "Amirreza Tanevardi",
        "Pooria Abbas Rad Moghadam",
        "Sajjad Amini"
      ],
      "published": "2025-10-02T16:53:07Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.02208v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种改进的一致性采样方法，用于解决逆成像问题。通过测量一致性机制引导采样随机性，在保持一致性模型高效生成的同时确保测量保真度。在Fashion-MNIST和LSUN Bedroom数据集上的实验表明，仅需少量步骤即可获得优于基线方法的感知质量和像素级指标。",
      "order": 511
    },
    {
      "arxiv_id": "2510.02197v1",
      "title": "Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition:\n  A Machine Learning Approach for Small-Scale Farming Applications",
      "summary": "Accurate livestock identification is a cornerstone of modern farming: it\nsupports health monitoring, breeding programs, and productivity tracking.\nHowever, common pig identification methods, such as ear tags and microchips,\nare often unreliable, costly, target pure breeds, and thus impractical for\nsmall-scale farmers. To address this gap, we propose a noninvasive biometric\nidentification approach that leverages uniqueness of the auricular vein\npatterns. To this end, we have collected 800 ear images from 20 mixed-breed\npigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a\nstandard smartphone and simple back lighting. A multistage computer vision\npipeline was developed to enhance vein visibility, extract structural and\nspatial features, and generate biometric signatures. These features were then\nclassified using machine learning models. Support Vector Machines (SVM)\nachieved the highest accuracy: correctly identifying pigs with 98.12% precision\nacross mixed-breed populations. The entire process from image processing to\nclassification was completed in an average of 8.3 seconds, demonstrating\nfeasibility for real-time farm deployment. We believe that by replacing fragile\nphysical identifiers with permanent biological markers, this system provides\nfarmers with a cost-effective and stress-free method of animal identification.\nMore broadly, the findings confirm the practicality of auricular vein\nbiometrics for digitizing livestock management, reinforcing its potential to\nextend the benefits of precision farming to resource-constrained agricultural\ncommunities.",
      "authors": [
        "Emmanuel Nsengiyumvaa",
        "Leonard Niyitegekaa",
        "Eric Umuhoza"
      ],
      "published": "2025-10-02T16:45:43Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02197v1",
      "primary_area": "video_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出一种基于耳廓静脉模式识别的杂交猪身份识别方法，针对小规模养殖场景。通过智能手机采集800张猪耳图像，采用多阶段计算机视觉流程增强静脉可见性并提取特征，结合支持向量机模型实现98.12%的识别准确率，平均处理时间8.3秒。该方法为非侵入式生物识别提供了成本效益高的解决方案，有望推动精准养殖在资源受限农业社区的应用。",
      "order": 512
    },
    {
      "arxiv_id": "2510.02186v1",
      "title": "GeoPurify: A Data-Efficient Geometric Distillation Framework for\n  Open-Vocabulary 3D Segmentation",
      "summary": "Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to\n3D semantic segmentation expose a persistent trade-off. Directly projecting 2D\nfeatures into 3D yields noisy and fragmented predictions, whereas enforcing\ngeometric coherence necessitates costly training pipelines and large-scale\nannotated 3D data. We argue that this limitation stems from the dominant\nsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with\n3D geometric structure. The geometric cues are not eliminated during the\n2D-to-3D transfer but remain latent within the noisy and view-aggregated\nfeatures. To exploit this property, we propose GeoPurify that applies a small\nStudent Affinity Network to purify 2D VLM-generated 3D point features using\ngeometric priors distilled from a 3D self-supervised teacher model. During\ninference, we devise a Geometry-Guided Pooling module to further denoise the\npoint cloud and ensure the semantic and structural consistency. Benefiting from\nlatent geometric information and the learned affinity network, GeoPurify\neffectively mitigates the trade-off and achieves superior data efficiency.\nExtensive experiments on major 3D benchmarks demonstrate that GeoPurify\nachieves or surpasses state-of-the-art performance while utilizing only about\n1.5% of the training data. Our codes and checkpoints are available at\n[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).",
      "authors": [
        "Weijia Dou",
        "Xu Zhang",
        "Yi Bin",
        "Jian Liu",
        "Bo Peng",
        "Guoqing Wang",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "published": "2025-10-02T16:37:56Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02186v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "GeoPurify提出了一种数据高效的几何蒸馏框架，用于解决2D视觉语言模型特征向3D语义分割迁移时的噪声和几何不一致问题。该方法通过学生亲和网络从自监督教师模型中提取几何先验来净化3D点特征，并在推理时使用几何引导池化模块确保语义和结构一致性，仅需约1.5%的训练数据即可达到或超越最先进性能。",
      "order": 513
    },
    {
      "arxiv_id": "2510.02182v1",
      "title": "Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex\n  with Mutual Information-Guided Diffusion",
      "summary": "Understanding how neural populations in higher visual areas encode\nobject-centered visual information remains a central challenge in computational\nneuroscience. Prior works have investigated representational alignment between\nartificial neural networks and the visual cortex. Nevertheless, these findings\nare indirect and offer limited insights to the structure of neural populations\nthemselves. Similarly, decoding-based methods have quantified semantic features\nfrom neural populations but have not uncovered their underlying organizations.\nThis leaves open a scientific question: \"how feature-specific visual\ninformation is distributed across neural populations in higher visual areas,\nand whether it is organized into structured, semantically meaningful\nsubspaces.\" To tackle this problem, we present MIG-Vis, a method that leverages\nthe generative power of diffusion models to visualize and validate the\nvisual-semantic attributes encoded in neural latent subspaces. Our method first\nuses a variational autoencoder to infer a group-wise disentangled neural latent\nsubspace from neural populations. Subsequently, we propose a mutual information\n(MI)-guided diffusion synthesis procedure to visualize the specific\nvisual-semantic features encoded by each latent group. We validate MIG-Vis on\nmulti-session neural spiking datasets from the inferior temporal (IT) cortex of\ntwo macaques. The synthesized results demonstrate that our method identifies\nneural latent groups with clear semantic selectivity to diverse visual\nfeatures, including object pose, inter-category transformations, and\nintra-class content. These findings provide direct, interpretable evidence of\nstructured semantic representation in the higher visual cortex and advance our\nunderstanding of its encoding principles.",
      "authors": [
        "Yule Wang",
        "Joseph Yu",
        "Chengrui Li",
        "Weihan Li",
        "Anqi Wu"
      ],
      "published": "2025-10-02T16:33:40Z",
      "primary_category": "q-bio.NC",
      "arxiv_url": "https://arxiv.org/abs/2510.02182v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出MIG-Vis方法，利用扩散模型生成能力可视化高阶视觉皮层神经群体的语义选择性。通过变分自编码器推断解耦的神经潜在子空间，结合互信息引导的扩散合成技术，在猕猴颞下皮层数据中识别出对物体姿态、类别转换等视觉特征具有明确选择性的神经群组，为视觉皮层编码原理提供了直接证据。",
      "order": 514
    },
    {
      "arxiv_id": "2510.02178v1",
      "title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical\n  Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis",
      "summary": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available.",
      "authors": [
        "Jialin Gao",
        "Donghao Zhou",
        "Mingjian Liang",
        "Lihao Liu",
        "Chi-Wing Fu",
        "Xiaowei Hu",
        "Pheng-Ann Heng"
      ],
      "published": "2025-10-02T16:30:37Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.02178v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "DisCo-Layout是一种新颖的多智能体框架，用于3D室内布局合成，通过解耦和协调语义与物理优化来解决传统方法泛化能力不足的问题。该框架包含语义优化工具修正抽象物体关系，物理优化工具解决具体空间问题，并通过规划器、设计器和评估器的多智能体协作生成逼真、连贯且可泛化的室内布局。",
      "order": 515
    },
    {
      "arxiv_id": "2510.02155v1",
      "title": "Unlocking Vision-Language Models for Video Anomaly Detection via\n  Fine-Grained Prompting",
      "summary": "Prompting has emerged as a practical way to adapt frozen vision-language\nmodels (VLMs) for video anomaly detection (VAD). Yet, existing prompts are\noften overly abstract, overlooking the fine-grained human-object interactions\nor action semantics that define complex anomalies in surveillance videos. We\npropose ASK-Hint, a structured prompting framework that leverages\naction-centric knowledge to elicit more accurate and interpretable reasoning\nfrom frozen VLMs. Our approach organizes prompts into semantically coherent\ngroups (e.g. violence, property crimes, public safety) and formulates\nfine-grained guiding questions that align model predictions with discriminative\nvisual cues. Extensive experiments on UCF-Crime and XD-Violence show that\nASK-Hint consistently improves AUC over prior baselines, achieving\nstate-of-the-art performance compared to both fine-tuned and training-free\nmethods. Beyond accuracy, our framework provides interpretable reasoning traces\ntowards anomaly and demonstrates strong generalization across datasets and VLM\nbackbones. These results highlight the critical role of prompt granularity and\nestablish ASK-Hint as a new training-free and generalizable solution for\nexplainable video anomaly detection.",
      "authors": [
        "Shu Zou",
        "Xinyu Tian",
        "Lukas Wesemann",
        "Fabian Waschkowski",
        "Zhaoyuan Yang",
        "Jing Zhang"
      ],
      "published": "2025-10-02T16:06:31Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02155v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ASK-Hint框架，通过细粒度提示解锁视觉语言模型在视频异常检测中的应用。该方法构建结构化提示，利用动作相关知识引导模型关注人-物交互细节，在UCF-Crime和XD-Violence数据集上实现SOTA性能，无需训练即可提供可解释的异常推理轨迹。",
      "order": 516
    },
    {
      "arxiv_id": "2510.02114v1",
      "title": "FRIEREN: Federated Learning with Vision-Language Regularization for\n  Segmentation",
      "summary": "Federeated Learning (FL) offers a privacy-preserving solution for Semantic\nSegmentation (SS) tasks to adapt to new domains, but faces significant\nchallenges from these domain shifts, particularly when client data is\nunlabeled. However, most existing FL methods unrealistically assume access to\nlabeled data on remote clients or fail to leverage the power of modern Vision\nFoundation Models (VFMs). Here, we propose a novel and challenging task,\nFFREEDG, in which a model is pretrained on a server's labeled source dataset\nand subsequently trained across clients using only their unlabeled data,\nwithout ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a\nframework that leverages the knowledge of a VFM by integrating vision and\nlanguage modalities. Our approach employs a Vision-Language decoder guided by\nCLIP-based text embeddings to improve semantic disambiguation and uses a\nweak-to-strong consistency learning strategy for robust local training on\npseudo-labels. Our experiments on synthetic-to-real and\nclear-to-adverse-weather benchmarks demonstrate that our framework effectively\ntackles this new task, achieving competitive performance against established\ndomain generalization and adaptation methods and setting a strong baseline for\nfuture research.",
      "authors": [
        "Ding-Ruei Shen"
      ],
      "published": "2025-10-02T15:21:49Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02114v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "FRIEREN提出了一种联邦学习框架，利用视觉-语言模型解决语义分割中的领域适应问题。该方法通过CLIP文本嵌入引导的视觉-语言解码器改善语义消歧，采用弱到强一致性学习策略在客户端未标注数据上进行训练，在合成到真实和恶劣天气场景的基准测试中表现出色。",
      "order": 517
    },
    {
      "arxiv_id": "2510.02109v1",
      "title": "SpurBreast: A Curated Dataset for Investigating Spurious Correlations in\n  Real-world Breast MRI Classification",
      "summary": "Deep neural networks (DNNs) have demonstrated remarkable success in medical\nimaging, yet their real-world deployment remains challenging due to spurious\ncorrelations, where models can learn non-clinical features instead of\nmeaningful medical patterns. Existing medical imaging datasets are not designed\nto systematically study this issue, largely due to restrictive licensing and\nlimited supplementary patient data. To address this gap, we introduce\nSpurBreast, a curated breast MRI dataset that intentionally incorporates\nspurious correlations to evaluate their impact on model performance. Analyzing\nover 100 features involving patient, device, and imaging protocol, we identify\ntwo dominant spurious signals: magnetic field strength (a global feature\ninfluencing the entire image) and image orientation (a local feature affecting\nspatial alignment). Through controlled dataset splits, we demonstrate that DNNs\ncan exploit these non-clinical signals, achieving high validation accuracy\nwhile failing to generalize to unbiased test data. Alongside these two datasets\ncontaining spurious correlations, we also provide benchmark datasets without\nspurious correlations, allowing researchers to systematically investigate\nclinically relevant and irrelevant features, uncertainty estimation,\nadversarial robustness, and generalization strategies. Models and datasets are\navailable at https://github.com/utkuozbulak/spurbreast.",
      "authors": [
        "Jong Bum Won",
        "Wesley De Neve",
        "Joris Vankerschaver",
        "Utku Ozbulak"
      ],
      "published": "2025-10-02T15:16:20Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.02109v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "SpurBreast是一个专门设计的乳腺MRI数据集，用于研究医学影像分类中的伪相关性。该数据集包含患者、设备和成像协议等100多个特征，识别出磁场强度和图像方向两个主要伪信号，揭示了深度神经网络可能依赖非临床特征而非医学模式的问题，为系统研究模型泛化性和鲁棒性提供基准。",
      "order": 518
    },
    {
      "arxiv_id": "2510.02100v1",
      "title": "When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based\n  Tracking in Surgical Videos",
      "summary": "Video object segmentation (VOS) models such as SAM2 offer promising zero-shot\ntracking capabilities for surgical videos using minimal user input. Among the\navailable input types, point-based tracking offers an efficient and low-cost\nalternative, yet its reliability and failure cases in complex surgical\nenvironments are not well understood. In this work, we systematically analyze\nthe failure modes of point-based tracking in laparoscopic cholecystectomy\nvideos. Focusing on three surgical targets, the gallbladder, grasper, and\nL-hook electrocautery, we compare the performance of point-based tracking with\nsegmentation mask initialization. Our results show that point-based tracking is\ncompetitive for surgical tools but consistently underperforms for anatomical\ntargets, where tissue similarity and ambiguous boundaries lead to failure.\nThrough qualitative analysis, we reveal key factors influencing tracking\noutcomes and provide several actionable recommendations for selecting and\nplacing tracking points to improve performance in surgical video analysis.",
      "authors": [
        "Woowon Jang",
        "Jiwon Im",
        "Juseung Choi",
        "Niki Rashidian",
        "Wesley De Neve",
        "Utku Ozbulak"
      ],
      "published": "2025-10-02T15:06:49Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02100v1",
      "primary_area": "video_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究系统分析了SAM2模型在腹腔镜胆囊切除手术视频中点跟踪的失效模式。结果显示，点跟踪在手术器械上表现良好，但在胆囊等解剖目标上因组织相似性和边界模糊而表现不佳。研究提出了改进跟踪点选择和放置的具体建议。",
      "order": 519
    },
    {
      "arxiv_id": "2510.02097v1",
      "title": "Mapping Historic Urban Footprints in France: Balancing Quality,\n  Scalability and AI Techniques",
      "summary": "Quantitative analysis of historical urban sprawl in France before the 1970s\nis hindered by the lack of nationwide digital urban footprint data. This study\nbridges this gap by developing a scalable deep learning pipeline to extract\nurban areas from the Scan Histo historical map series (1925-1950), which\nproduces the first open-access, national-scale urban footprint dataset for this\npivotal period. Our key innovation is a dual-pass U-Net approach designed to\nhandle the high radiometric and stylistic complexity of historical maps. The\nfirst pass, trained on an initial dataset, generates a preliminary map that\nidentifies areas of confusion, such as text and roads, to guide targeted data\naugmentation. The second pass uses a refined dataset and the binarized output\nof the first model to minimize radiometric noise, which significantly reduces\nfalse positives. Deployed on a high-performance computing cluster, our method\nprocesses 941 high-resolution tiles covering the entirety of metropolitan\nFrance. The final mosaic achieves an overall accuracy of 73%, effectively\ncapturing diverse urban patterns while overcoming common artifacts like labels\nand contour lines. We openly release the code, training datasets, and the\nresulting nationwide urban raster to support future research in long-term\nurbanization dynamics.",
      "authors": [
        "Walid Rabehi",
        "Marion Le Texier",
        "Rémi Lemoy"
      ],
      "published": "2025-10-02T15:04:53Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02097v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究开发了一种双通道U-Net深度学习流水线，从法国历史地图中提取1925-1950年间的城市足迹数据。该方法通过首轮模型识别混淆区域并指导数据增强，次轮模型利用二值化输出减少辐射噪声，成功处理了覆盖法国全境的941个高分辨率图块，最终生成首个全国尺度的历史城市足迹数据集，准确率达73%。",
      "order": 520
    },
    {
      "arxiv_id": "2510.02086v1",
      "title": "VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and\n  Segmentation",
      "summary": "Accurate detection and segmentation of brain tumors from magnetic resonance\nimaging (MRI) are essential for diagnosis, treatment planning, and clinical\nmonitoring. While convolutional architectures such as U-Net have long been the\nbackbone of medical image segmentation, their limited capacity to capture\nlong-range dependencies constrains performance on complex tumor structures.\nRecent advances in diffusion models have demonstrated strong potential for\ngenerating high-fidelity medical images and refining segmentation boundaries.\n  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor\nDetection and Segmentation framework, a transformer-driven diffusion framework\nfor brain tumor detection and segmentation. By embedding a vision transformer\nat the core of the diffusion process, the model leverages global contextual\nreasoning together with iterative denoising to enhance both volumetric accuracy\nand boundary precision. The transformer backbone enables more effective\nmodeling of spatial relationships across entire MRI volumes, while diffusion\nrefinement mitigates voxel-level errors and recovers fine-grained tumor\ndetails.\n  This hybrid design provides a pathway toward improved robustness and\nscalability in neuro-oncology, moving beyond conventional U-Net baselines.\nExperimental validation on MRI brain tumor datasets demonstrates consistent\ngains in Dice similarity and Hausdorff distance, underscoring the potential of\ntransformer-guided diffusion models to advance the state of the art in tumor\nsegmentation.",
      "authors": [
        "Arman Behnam"
      ],
      "published": "2025-10-02T14:52:08Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02086v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "VGDM提出了一种基于视觉Transformer引导扩散模型的脑肿瘤检测与分割框架，通过结合全局上下文推理和迭代去噪过程，在MRI图像上实现了比传统U-Net更精确的肿瘤体积和边界分割，在神经肿瘤学领域展现出优越性能。",
      "order": 521
    },
    {
      "arxiv_id": "2510.02069v1",
      "title": "Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy\n  Objects",
      "summary": "Accurate reconstruction and relighting of glossy objects remain a\nlongstanding challenge, as object shape, material properties, and illumination\nare inherently difficult to disentangle. Existing neural rendering approaches\noften rely on simplified BRDF models or parameterizations that couple diffuse\nand specular components, which restricts faithful material recovery and limits\nrelighting fidelity. We propose a relightable framework that integrates a\nmicrofacet BRDF with the specular-glossiness parameterization into 2D Gaussian\nSplatting with deferred shading. This formulation enables more physically\nconsistent material decomposition, while diffusion-based priors for surface\nnormals and diffuse color guide early-stage optimization and mitigate\nambiguity. A coarse-to-fine optimization of the environment map accelerates\nconvergence and preserves high-dynamic-range specular reflections. Extensive\nexperiments on complex, glossy scenes demonstrate that our method achieves\nhigh-quality geometry and material reconstruction, delivering substantially\nmore realistic and consistent relighting under novel illumination compared to\nexisting Gaussian splatting methods.",
      "authors": [
        "Georgios Kouros",
        "Minye Wu",
        "Tinne Tuytelaars"
      ],
      "published": "2025-10-02T14:34:46Z",
      "primary_category": "cs.GR",
      "arxiv_url": "https://arxiv.org/abs/2510.02069v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种可重光照的框架，将微表面BRDF与高光-光泽度参数化集成到2D高斯泼溅中，结合法线和漫反射先验指导优化，实现高质量光泽物体几何与材质重建，在新光照下产生更真实的重光照效果。",
      "order": 522
    },
    {
      "arxiv_id": "2510.02043v1",
      "title": "Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers",
      "summary": "Pose estimation refers to tracking a human's full body posture, including\ntheir head, torso, arms, and legs. The problem is challenging in practical\nsettings where the number of body sensors are limited. Past work has shown\npromising results using conditional diffusion models, where the pose prediction\nis conditioned on both <location, rotation> measurements from the sensors.\nUnfortunately, nearly all these approaches generalize poorly across users,\nprimarly because location measurements are highly influenced by the body size\nof the user. In this paper, we formulate pose estimation as an inverse problem\nand design an algorithm capable of zero-shot generalization. Our idea utilizes\na pre-trained diffusion model and conditions it on rotational measurements\nalone; the priors from this model are then guided by a likelihood term, derived\nfrom the measured locations. Thus, given any user, our proposed InPose method\ngeneratively estimates the highly likely sequence of poses that best explains\nthe sparse on-body measurements.",
      "authors": [
        "Sahil Bhandary Karnoor",
        "Romit Roy Choudhury"
      ],
      "published": "2025-10-02T14:16:43Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02043v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出InPose方法，将人体姿态估计构建为逆问题，利用预训练扩散模型仅基于旋转测量进行条件生成，通过位置测量的似然项引导，实现零样本跨用户泛化的人体姿态追踪。",
      "order": 523
    },
    {
      "arxiv_id": "2510.02037v1",
      "title": "A Multicentric Dataset for Training and Benchmarking Breast Cancer\n  Segmentation in H&E Slides",
      "summary": "Automated semantic segmentation of whole-slide images (WSIs) stained with\nhematoxylin and eosin (H&E) is essential for large-scale artificial\nintelligence-based biomarker analysis in breast cancer. However, existing\npublic datasets for breast cancer segmentation lack the morphological diversity\nneeded to support model generalizability and robust biomarker validation across\nheterogeneous patient cohorts. We introduce BrEast cancEr hisTopathoLogy\nsEgmentation (BEETLE), a dataset for multiclass semantic segmentation of\nH&E-stained breast cancer WSIs. It consists of 587 biopsies and resections from\nthree collaborating clinical centers and two public datasets, digitized using\nseven scanners, and covers all molecular subtypes and histological grades.\nUsing diverse annotation strategies, we collected annotations across four\nclasses - invasive epithelium, non-invasive epithelium, necrosis, and other -\nwith particular focus on morphologies underrepresented in existing datasets,\nsuch as ductal carcinoma in situ and dispersed lobular tumor cells. The\ndataset's diversity and relevance to the rapidly growing field of automated\nbiomarker quantification in breast cancer ensure its high potential for reuse.\nFinally, we provide a well-curated, multicentric external evaluation set to\nenable standardized benchmarking of breast cancer segmentation models.",
      "authors": [
        "Carlijn Lems",
        "Leslie Tessier",
        "John-Melle Bokhorst",
        "Mart van Rijthoven",
        "Witali Aswolinskiy",
        "Matteo Pozzi",
        "Natalie Klubickova",
        "Suzanne Dintzis",
        "Michela Campora",
        "Maschenka Balkenhol",
        "Peter Bult",
        "Joey Spronck",
        "Thomas Detone",
        "Mattia Barbareschi",
        "Enrico Munari",
        "Giuseppe Bogina",
        "Jelle Wesseling",
        "Esther H. Lips",
        "Francesco Ciompi",
        "Frédérique Meeuwsen",
        "Jeroen van der Laak"
      ],
      "published": "2025-10-02T14:09:21Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.02037v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "BEETLE数据集：首个多中心乳腺癌H&E切片语义分割数据集，包含587个活检样本，涵盖所有分子亚型和组织学分级，特别关注导管原位癌等罕见形态，为AI生物标志物分析提供标准化基准。",
      "order": 524
    },
    {
      "arxiv_id": "2510.02034v1",
      "title": "GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object\n  Morphing",
      "summary": "We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape\nand texture morphing from multi-view images. Previous approaches usually rely\non point clouds or require pre-defined homeomorphic mappings for untextured\ndata. Our method overcomes these limitations by leveraging mesh-guided 3D\nGaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.\nThe core of our framework is a unified deformation strategy that anchors\n3DGaussians to reconstructed mesh patches, ensuring geometrically consistent\ntransformations while preserving texture fidelity through topology-aware\nconstraints. In parallel, our framework establishes unsupervised semantic\ncorrespondence by using the mesh topology as a geometric prior and maintains\nstructural integrity via physically plausible point trajectories. This\nintegrated approach preserves both local detail and global semantic coherence\nthroughout the morphing process with out requiring labeled data. On our\nproposed TexMorph benchmark, GaussianMorphing substantially outperforms prior\n2D/3D methods, reducing color consistency error ($\\Delta E$) by 22.2% and EI by\n26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/",
      "authors": [
        "Mengtian Li",
        "Yunshu Bai",
        "Yimin Chu",
        "Yijun Shen",
        "Zhongmei Li",
        "Weifeng Ge",
        "Zhifeng Xie",
        "Chaofeng Chen"
      ],
      "published": "2025-10-02T14:07:55Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02034v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "GaussianMorphing提出了一种基于网格引导3D高斯溅射的语义感知3D形态变换框架，通过将高斯点锚定到重建网格面片实现几何一致变形，利用网格拓扑作为几何先验建立无监督语义对应，在TexMorph基准测试中颜色一致性误差降低22.2%，在保持纹理保真度的同时实现高保真几何与外观建模。",
      "order": 525
    },
    {
      "arxiv_id": "2510.02030v1",
      "title": "kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring",
      "summary": "A comprehensive understanding of animal behavior ecology depends on scalable\napproaches to quantify and interpret complex, multidimensional behavioral\npatterns. Traditional field observations are often limited in scope,\ntime-consuming, and labor-intensive, hindering the assessment of behavioral\nresponses across landscapes. To address this, we present kabr-tools (Kenyan\nAnimal Behavior Recognition Tools), an open-source package for automated\nmulti-species behavioral monitoring. This framework integrates drone-based\nvideo with machine learning systems to extract behavioral, social, and spatial\nmetrics from wildlife footage. Our pipeline leverages object detection,\ntracking, and behavioral classification systems to generate key metrics,\nincluding time budgets, behavioral transitions, social interactions, habitat\nassociations, and group composition dynamics. Compared to ground-based methods,\ndrone-based observations significantly improved behavioral granularity,\nreducing visibility loss by 15% and capturing more transitions with higher\naccuracy and continuity. We validate kabr-tools through three case studies,\nanalyzing 969 behavioral sequences, surpassing the capacity of traditional\nmethods for data capture and annotation. We found that, like Plains zebras,\nvigilance in Grevy's zebras decreases with herd size, but, unlike Plains\nzebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit\nstrong behavioral inertia, with rare transitions to alert behaviors and\nobserved spatial segregation between Grevy's zebras, Plains zebras, and\ngiraffes in mixed-species herds. By enabling automated behavioral monitoring at\nscale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing\nconservation, biodiversity research, and ecological monitoring.",
      "authors": [
        "Jenna Kline",
        "Maksim Kholiavchenko",
        "Samuel Stevens",
        "Nina van Tiel",
        "Alison Zhong",
        "Namrata Banerji",
        "Alec Sheets",
        "Sowbaranika Balasubramaniam",
        "Isla Duporge",
        "Matthew Thompson",
        "Elizabeth Campolongo",
        "Jackson Miliko",
        "Neil Rosser",
        "Tanya Berger-Wolf",
        "Charles V. Stewart",
        "Daniel I. Rubenstein"
      ],
      "published": "2025-10-02T14:03:55Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02030v1",
      "primary_area": "video_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "kabr-tools是一个开源的多物种行为自动监测框架，整合无人机视频与机器学习系统，通过目标检测、跟踪和行为分类提取野生动物行为指标。相比传统方法，该工具提升行为数据采集粒度15%，在三个案例研究中验证了其在大规模生态监测中的有效性。",
      "order": 526
    },
    {
      "arxiv_id": "2510.02028v1",
      "title": "LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud\n  Reconstruction",
      "summary": "This work proposed a 3D autoencoder architecture, named LiLa-Net, which\nencodes efficient features from real traffic environments, employing only the\nLiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,\nequipped with Velodyne LiDAR. The system leverage skip connections concept to\nimprove the performance without using extensive resources as the\nstate-of-the-art architectures. Key changes include reducing the number of\nencoder layers and simplifying the skip connections, while still producing an\nefficient and representative latent space which allows to accurately\nreconstruct the original point cloud. Furthermore, an effective balance has\nbeen achieved between the information carried by the skip connections and the\nlatent encoding, leading to improved reconstruction quality without\ncompromising performance. Finally, the model demonstrates strong generalization\ncapabilities, successfully reconstructing objects unrelated to the original\ntraffic environment.",
      "authors": [
        "Mario Resino",
        "Borja Pérez",
        "Jaime Godoy",
        "Abdulla Al-Kaff",
        "Fernando García"
      ],
      "published": "2025-10-02T14:00:20Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02028v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "LiLa-Net是一种轻量级3D点云自编码器，通过减少编码器层数和简化跳跃连接，在保持重建精度的同时降低计算资源需求，展示了在非交通环境物体上的良好泛化能力。",
      "order": 527
    },
    {
      "arxiv_id": "2510.02001v1",
      "title": "Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using\n  GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output\n  (SLSO) Framework",
      "summary": "In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to\nautomatically generate jaw cyst findings on dental panoramic radiographs. To\nimprove accuracy, we constructed a Self-correction Loop with Structured Output\n(SLSO) framework and verified its effectiveness. A 10-step process was\nimplemented for 22 cases of jaw cysts, including image input and analysis,\nstructured data generation, tooth number extraction and consistency checking,\niterative regeneration when inconsistencies were detected, and finding\ngeneration with subsequent restructuring and consistency verification. A\ncomparative experiment was conducted using the conventional Chain-of-Thought\n(CoT) method across seven evaluation items: transparency, internal structure,\nborders, root resorption, tooth movement, relationships with other structures,\nand tooth number. The results showed that the proposed SLSO framework improved\noutput accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates\nfor tooth number, tooth movement, and root resorption, respectively. In the\nsuccessful cases, a consistently structured output was achieved after up to\nfive regenerations. Although statistical significance was not reached because\nof the small size of the dataset, the overall SLSO framework enforced negative\nfinding descriptions, suppressed hallucinations, and improved tooth number\nidentification accuracy. However, the accurate identification of extensive\nlesions spanning multiple teeth is limited. Nevertheless, further refinement is\nrequired to enhance overall performance and move toward a practical finding\ngeneration system.",
      "authors": [
        "Nanaka Hosokawa",
        "Ryo Takahashi",
        "Tomoya Kitano",
        "Yukihiro Iida",
        "Chisako Muramatsu",
        "Tatsuro Hayashi",
        "Yuta Seino",
        "Xiangrong Zhou",
        "Takeshi Hara",
        "Akitoshi Katsumata",
        "Hiroshi Fujita"
      ],
      "published": "2025-10-02T13:22:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02001v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究利用GPT-4o多模态能力，开发了自校正结构化输出(SLSO)框架来自动生成牙科全景片中颌骨囊肿的诊断发现。通过10步处理流程对22例病例进行实验，相比传统思维链方法，在牙齿编号、牙齿移动和牙根吸收等项目的准确率分别提升66.9%、33.3%和28.6%。该框架能有效抑制幻觉现象并提高结构化输出一致性，但针对跨多牙大面积病变的识别仍有局限。",
      "order": 528
    },
    {
      "arxiv_id": "2510.01997v1",
      "title": "Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing\n  Routing in Lightweight Image Super-Resolution",
      "summary": "Image Super-Resolution (SR) aims to reconstruct high-resolution images from\nlow-resolution counterparts, but the computational complexity of deep\nlearning-based methods often hinders practical deployment. CAMixer is the\npioneering work to integrate the advantages of existing lightweight SR methods\nand proposes a content-aware mixer to route token mixers of varied complexities\naccording to the difficulty of content recovery. However, several limitations\nremain, such as poor adaptability, coarse-grained masking and spatial\ninflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking\nmechanism that identifies pure pixels and exempts them from expensive\ncomputations. PP utilizes fixed color center points to classify pixels into\ndistinct categories, enabling fine-grained, spatially flexible masking while\nmaintaining adaptive flexibility. Integrated into the state-of-the-art\nATD-light model, PP-ATD-light achieves superior SR performance with minimal\noverhead, outperforming CAMixer-ATD-light in reconstruction quality and\nparameter efficiency when saving a similar amount of computation.",
      "authors": [
        "Junyu Wu",
        "Jie Tang",
        "Jie Liu",
        "Gangshan Wu"
      ],
      "published": "2025-10-02T13:18:43Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01997v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "Pure-Pass提出一种像素级掩码机制，通过固定颜色中心点分类像素，在轻量级图像超分辨率中实现细粒度、自适应的动态令牌混合路由，相比CAMixer具有更好的重建质量和参数效率。",
      "order": 529
    },
    {
      "arxiv_id": "2510.01991v1",
      "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing",
      "summary": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges\nwith view, temporal, and non-editing region consistency, as well as with\nhandling complex text instructions. To address these issues, we propose\n4DGS-Craft, a consistent and interactive 4DGS editing framework. We first\nintroduce a 4D-aware InstructPix2Pix model to ensure both view and temporal\nconsistency. This model incorporates 4D VGGT geometry features extracted from\nthe initial scene, enabling it to capture underlying 4D geometric structures\nduring editing. We further enhance this model with a multi-view grid module\nthat enforces consistency by iteratively refining multi-view input images while\njointly optimizing the underlying 4D scene. Furthermore, we preserve the\nconsistency of non-edited regions through a novel Gaussian selection mechanism,\nwhich identifies and optimizes only the Gaussians within the edited regions.\nBeyond consistency, facilitating user interaction is also crucial for effective\n4DGS editing. Therefore, we design an LLM-based module for user intent\nunderstanding. This module employs a user instruction template to define atomic\nediting operations and leverages an LLM for reasoning. As a result, our\nframework can interpret user intent and decompose complex instructions into a\nlogical sequence of atomic operations, enabling it to handle intricate user\ncommands and further enhance editing performance. Compared to related works,\nour approach enables more consistent and controllable 4D scene editing. Our\ncode will be made available upon acceptance.",
      "authors": [
        "Lei Liu",
        "Can Wang",
        "Zhenghao Chen",
        "Dong Xu"
      ],
      "published": "2025-10-02T13:13:19Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01991v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "4DGS-Craft是一个一致且交互式的4D高斯泼溅编辑框架，通过4D感知InstructPix2Pix模型确保视角和时间一致性，采用高斯选择机制保护未编辑区域，并利用基于LLM的模块解析用户意图，将复杂指令分解为原子操作序列，实现更可控的4D场景编辑。",
      "order": 530
    },
    {
      "arxiv_id": "2510.01990v1",
      "title": "TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy\n  Agri-product Grading",
      "summary": "The 'trust deficit' in online fruit and vegetable e-commerce stems from the\ninability of digital transactions to provide direct sensory perception of\nproduct quality. This paper constructs a 'Trust Pyramid' model through\n'dual-source verification' of consumer trust. Experiments confirm that quality\nis the cornerstone of trust. The study reveals an 'impossible triangle' in\nagricultural product grading, comprising biological characteristics,\ntimeliness, and economic viability, highlighting the limitations of traditional\nabsolute grading standards. To quantitatively assess this trade-off, we propose\nthe 'Triangular Trust Index' (TTI). We redefine the role of algorithms from\n'decision-makers' to 'providers of transparent decision-making bases',\ndesigning the explainable AI framework--TriAlignXA. This framework supports\ntrustworthy online transactions within agricultural constraints through\nmulti-objective optimization. Its core relies on three engines: the\nBio-Adaptive Engine for granular quality description; the Timeliness\nOptimization Engine for processing efficiency; and the Economic Optimization\nEngine for cost control. Additionally, the \"Pre-Mapping Mechanism\" encodes\nprocess data into QR codes, transparently conveying quality information.\nExperiments on grading tasks demonstrate significantly higher accuracy than\nbaseline models. Empirical evidence and theoretical analysis verify the\nframework's balancing capability in addressing the \"impossible triangle\". This\nresearch provides comprehensive support--from theory to practice--for building\na trustworthy online produce ecosystem, establishing a critical pathway from\nalgorithmic decision-making to consumer trust.",
      "authors": [
        "Jianfei Xie",
        "Ziyang Li"
      ],
      "published": "2025-10-02T13:13:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01990v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出TriAlignXA可解释框架，解决农产品电商中的信任缺失问题。通过构建'信任金字塔'模型和'三角信任指数'，在生物特性、时效性和经济可行性三个维度实现多目标优化。该框架包含三个核心引擎：生物自适应引擎、时效优化引擎和经济优化引擎，并采用预映射机制将流程数据编码为二维码。实验证明其在分级任务中精度显著优于基线模型，有效平衡了农产品分级的'不可能三角'。",
      "order": 531
    },
    {
      "arxiv_id": "2510.01982v1",
      "title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models",
      "summary": "The integration of online reinforcement learning (RL) into diffusion and flow\nmodels has recently emerged as a promising approach for aligning generative\nmodels with human preferences. Stochastic sampling via Stochastic Differential\nEquations (SDE) is employed during the denoising process to generate diverse\ndenoising directions for RL exploration. While existing methods effectively\nexplore potential high-value samples, they suffer from sub-optimal preference\nalignment due to sparse and narrow reward signals. To address these challenges,\nwe propose a novel Granular-GRPO ($\\text{G}^2$RPO ) framework that achieves\nprecise and comprehensive reward assessments of sampling directions in\nreinforcement learning of flow models. Specifically, a Singular Stochastic\nSampling strategy is introduced to support step-wise stochastic exploration\nwhile enforcing a high correlation between the reward and the injected noise,\nthereby facilitating a faithful reward for each SDE perturbation. Concurrently,\nto eliminate the bias inherent in fixed-granularity denoising, we introduce a\nMulti-Granularity Advantage Integration module that aggregates advantages\ncomputed at multiple diffusion scales, producing a more comprehensive and\nrobust evaluation of the sampling directions. Experiments conducted on various\nreward models, including both in-domain and out-of-domain evaluations,\ndemonstrate that our $\\text{G}^2$RPO significantly outperforms existing\nflow-based GRPO baselines,highlighting its effectiveness and robustness.",
      "authors": [
        "Yujie Zhou",
        "Pengyang Ling",
        "Jiazi Bu",
        "Yibin Wang",
        "Yuhang Zang",
        "Jiaqi Wang",
        "Li Niu",
        "Guangtao Zhai"
      ],
      "published": "2025-10-02T12:57:12Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01982v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出G²RPO框架，通过奇异随机采样策略和多粒度优势集成模块，解决流模型强化学习中奖励信号稀疏和偏差问题，显著提升了偏好对齐效果和模型鲁棒性。",
      "order": 532
    },
    {
      "arxiv_id": "2510.01978v1",
      "title": "ROI-GS: Interest-based Local Quality 3D Gaussian Splatting",
      "summary": "We tackle the challenge of efficiently reconstructing 3D scenes with high\ndetail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods\nallocate resources uniformly across the scene, limiting fine detail to Regions\nOf Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an\nobject-aware framework that enhances local details through object-guided camera\nselection, targeted Object training, and seamless integration of high-fidelity\nobject of interest reconstructions into the global scene. Our method\nprioritizes higher resolution details on chosen objects while maintaining\nreal-time performance. Experiments show that ROI-GS significantly improves\nlocal quality (up to 2.96 dB PSNR), while reducing overall model size by\n$\\approx 17\\%$ of baseline and achieving faster training for a scene with a\nsingle object of interest, outperforming existing methods.",
      "authors": [
        "Quoc-Anh Bui",
        "Gilles Rougeron",
        "Géraldine Morin",
        "Simone Gasparini"
      ],
      "published": "2025-10-02T12:54:15Z",
      "primary_category": "cs.GR",
      "arxiv_url": "https://arxiv.org/abs/2510.01978v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "ROI-GS提出一种基于兴趣区域的3D高斯泼溅方法，通过目标引导的相机选择、针对性对象训练和高保真重建集成，在保持实时性能的同时显著提升感兴趣对象的局部细节质量（PSNR最高提升2.96dB），并将模型大小减少约17%。",
      "order": 533
    },
    {
      "arxiv_id": "2510.01967v1",
      "title": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using\n  ZK-SNARKs",
      "summary": "As image generation models grow increasingly powerful and accessible,\nconcerns around authenticity, ownership, and misuse of synthetic media have\nbecome critical. The ability to generate lifelike images indistinguishable from\nreal ones introduces risks such as misinformation, deepfakes, and intellectual\nproperty violations. Traditional watermarking methods either degrade image\nquality, are easily removed, or require access to confidential model internals\n- making them unsuitable for secure and scalable deployment. We are the first\nto introduce ZK-WAGON, a novel system for watermarking image generation models\nusing the Zero-Knowledge Succinct Non Interactive Argument of Knowledge\n(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing\nmodel weights, generation prompts, or any sensitive internal information. We\npropose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively\nconvert key layers of an image generation model into a circuit, reducing proof\ngeneration time significantly. Generated ZK-SNARK proofs are imperceptibly\nembedded into a generated image via Least Significant Bit (LSB) steganography.\nWe demonstrate this system on both GAN and Diffusion models, providing a\nsecure, model-agnostic pipeline for trustworthy AI image generation.",
      "authors": [
        "Aadarsh Anantha Ramakrishnan",
        "Shubham Agarwal",
        "Selvanayagam S",
        "Kunwar Singh"
      ],
      "published": "2025-10-02T12:39:57Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01967v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "ZK-WAGON是一种基于ZK-SNARKs的图像生成模型水印系统，通过选择性层电路转换技术实现高效验证，利用LSB隐写术在图像中嵌入不可感知的水印，保护模型知识产权而不泄露敏感信息。",
      "order": 534
    },
    {
      "arxiv_id": "2510.01954v1",
      "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in\n  MLLMs",
      "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.",
      "authors": [
        "Yongyi Su",
        "Haojie Zhang",
        "Shijie Li",
        "Nanqing Liu",
        "Jingyi Liao",
        "Junyi Pan",
        "Yuan Liu",
        "Xiaofen Xing",
        "Chong Sun",
        "Chen Li",
        "Nancy F. Chen",
        "Shuicheng Yan",
        "Xulei Yang",
        "Xun Xu"
      ],
      "published": "2025-10-02T12:23:57Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01954v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Patch-as-Decodable-Token (PaDT)方法，通过将图像块作为可解码标记与文本标记交错处理，使多模态大语言模型能直接生成检测、分割等视觉输出。该方法采用视觉参考标记和轻量解码器，在多项视觉任务中达到最先进性能。",
      "order": 535
    },
    {
      "arxiv_id": "2510.01948v1",
      "title": "ClustViT: Clustering-based Token Merging for Semantic Segmentation",
      "summary": "Vision Transformers can achieve high accuracy and strong generalization\nacross various contexts, but their practical applicability on real-world\nrobotic systems is limited due to their quadratic attention complexity. Recent\nworks have focused on dynamically merging tokens according to the image\ncomplexity. Token merging works well for classification but is less suited to\ndense prediction. We propose ClustViT, where we expand upon the Vision\nTransformer (ViT) backbone and address semantic segmentation. Within our\narchitecture, a trainable Cluster module merges similar tokens along the\nnetwork guided by pseudo-clusters from segmentation masks. Subsequently, a\nRegenerator module restores fine details for downstream heads. Our approach\nachieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different\ndatasets, with comparable segmentation accuracy. Our code and models will be\nmade publicly available.",
      "authors": [
        "Fabio Montello",
        "Ronja Güldenring",
        "Lazaros Nalpantidis"
      ],
      "published": "2025-10-02T12:15:40Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01948v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "ClustViT提出基于聚类的令牌合并方法，针对语义分割任务优化Vision Transformer。通过可训练的聚类模块合并相似令牌，再经再生模块恢复细节，在保持分割精度的同时显著降低计算复杂度，实现最高2.18倍GFLOPs减少和1.64倍推理加速。",
      "order": 536
    },
    {
      "arxiv_id": "2510.01934v1",
      "title": "Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors",
      "summary": "Few-shot anomaly detection streamlines and simplifies industrial safety\ninspection. However, limited samples make accurate differentiation between\nnormal and abnormal features challenging, and even more so under\ncategory-agnostic conditions. Large-scale pre-training of foundation visual\nencoders has advanced many fields, as the enormous quantity of data helps to\nlearn the general distribution of normal images. We observe that the anomaly\namount in an image directly correlates with the difference in the learnt\nembeddings and utilize this to design a few-shot anomaly detector termed\nFoundAD. This is done by learning a nonlinear projection operator onto the\nnatural image manifold. The simple operator acts as an effective tool for\nanomaly detection to characterize and identify out-of-distribution regions in\nan image. Extensive experiments show that our approach supports multi-class\ndetection and achieves competitive performance while using substantially fewer\nparameters than prior methods. Backed up by evaluations with multiple\nfoundation encoders, including fresh DINOv3, we believe this idea broadens the\nperspective on foundation features and advances the field of few-shot anomaly\ndetection.",
      "authors": [
        "Guangyao Zhai",
        "Yue Zhou",
        "Xinyan Deng",
        "Lars Heckler",
        "Nassir Navab",
        "Benjamin Busam"
      ],
      "published": "2025-10-02T11:53:20Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01934v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出FoundAD方法，利用基础视觉编码器的大规模预训练能力，通过非线性投影算子学习自然图像流形，实现少样本异常检测。该方法在类别无关条件下支持多类检测，性能优异且参数更少，为工业安全检测提供了简化方案。",
      "order": 537
    },
    {
      "arxiv_id": "2510.01919v1",
      "title": "GFSR-Net: Guided Focus via Segment-Wise Relevance Network for\n  Interpretable Deep Learning in Medical Imaging",
      "summary": "Deep learning has achieved remarkable success in medical image analysis,\nhowever its adoption in clinical practice is limited by a lack of\ninterpretability. These models often make correct predictions without\nexplaining their reasoning. They may also rely on image regions unrelated to\nthe disease or visual cues, such as annotations, that are not present in\nreal-world conditions. This can reduce trust and increase the risk of\nmisleading diagnoses. We introduce the Guided Focus via Segment-Wise Relevance\nNetwork (GFSR-Net), an approach designed to improve interpretability and\nreliability in medical imaging. GFSR-Net uses a small number of human\nannotations to approximate where a person would focus within an image\nintuitively, without requiring precise boundaries or exhaustive markings,\nmaking the process fast and practical. During training, the model learns to\nalign its focus with these areas, progressively emphasizing features that carry\ndiagnostic meaning. This guidance works across different types of natural and\nmedical images, including chest X-rays, retinal scans, and dermatological\nimages. Our experiments demonstrate that GFSR achieves comparable or superior\naccuracy while producing saliency maps that better reflect human expectations.\nThis reduces the reliance on irrelevant patterns and increases confidence in\nautomated diagnostic tools.",
      "authors": [
        "Jhonatan Contreras",
        "Thomas Bocklitz"
      ],
      "published": "2025-10-02T11:35:47Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.01919v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "GFSR-Net提出一种基于分段相关性的引导聚焦网络，通过少量人工标注指导模型关注医学图像中的诊断相关区域，在保持准确性的同时提升深度学习模型的可解释性，减少对无关特征的依赖，增强临床诊断可信度。",
      "order": 538
    },
    {
      "arxiv_id": "2510.01914v1",
      "title": "Automated Defect Detection for Mass-Produced Electronic Components Based\n  on YOLO Object Detection Models",
      "summary": "Since the defect detection of conventional industry components is\ntime-consuming and labor-intensive, it leads to a significant burden on quality\ninspection personnel and makes it difficult to manage product quality. In this\npaper, we propose an automated defect detection system for the dual in-line\npackage (DIP) that is widely used in industry, using digital camera optics and\na deep learning (DL)-based model. The two most common defect categories of DIP\nare examined: (1) surface defects, and (2) pin-leg defects. However, the lack\nof defective component images leads to a challenge for detection tasks. To\nsolve this problem, the ConSinGAN is used to generate a suitable-sized dataset\nfor training and testing. Four varieties of the YOLO model are investigated\n(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.\nThe proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in\naccuracy of 95.50\\%, detection time of 285 ms, and is far superior to\nthreshold-based approaches. In addition, the supervisory control and data\nacquisition (SCADA) system is developed, and the associated sensor architecture\nis described. The proposed automated defect detection can be easily established\nwith numerous types of defects or insufficient defect data.",
      "authors": [
        "Wei-Lung Mao",
        "Chun-Chi Wang",
        "Po-Heng Chou",
        "Yen-Ting Liu"
      ],
      "published": "2025-10-02T11:33:16Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01914v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出基于YOLO目标检测模型的自动化缺陷检测系统，针对工业中广泛使用的双列直插封装元件。通过ConSinGAN生成训练数据集解决缺陷样本不足问题，比较了YOLOv3/v4/v7/v9四种模型，其中YOLOv7结合ConSinGAN在准确率(95.50%)和检测时间(285ms)上表现最优，并开发了SCADA系统架构。",
      "order": 539
    },
    {
      "arxiv_id": "2510.01912v1",
      "title": "Flow-Matching Guided Deep Unfolding for Hyperspectral Image\n  Reconstruction",
      "summary": "Hyperspectral imaging (HSI) provides rich spatial-spectral information but\nremains costly to acquire due to hardware limitations and the difficulty of\nreconstructing three-dimensional data from compressed measurements. Although\ncompressive sensing systems such as CASSI improve efficiency, accurate\nreconstruction is still challenged by severe degradation and loss of fine\nspectral details. We propose the Flow-Matching-guided Unfolding network (FMU),\nwhich, to our knowledge, is the first to integrate flow matching into HSI\nreconstruction by embedding its generative prior within a deep unfolding\nframework. To further strengthen the learned dynamics, we introduce a mean\nvelocity loss that enforces global consistency of the flow, leading to a more\nrobust and accurate reconstruction. This hybrid design leverages the\ninterpretability of optimization-based methods and the generative capacity of\nflow matching. Extensive experiments on both simulated and real datasets show\nthat FMU significantly outperforms existing approaches in reconstruction\nquality. Code and models will be available at https://github.com/YiAi03/FMU.",
      "authors": [
        "Yi Ai",
        "Yuanhao Cai",
        "Yulun Zhang",
        "Xiaokang Yang"
      ],
      "published": "2025-10-02T11:32:00Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01912v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出FMU网络，首次将流匹配技术融入高光谱图像重建，通过深度展开框架嵌入生成先验，并引入平均速度损失增强流一致性，在模拟和真实数据集上显著优于现有方法。",
      "order": 540
    },
    {
      "arxiv_id": "2510.01845v1",
      "title": "Model Merging to Maintain Language-Only Performance in Developmentally\n  Plausible Multimodal Models",
      "summary": "State-of-the-art vision-and-language models consist of many parameters and\nlearn from enormous datasets, surpassing the amounts of linguistic data that\nchildren are exposed to as they acquire a language. This paper presents our\napproach to the multimodal track of the BabyLM challenge addressing this\ndiscrepancy. We develop language-only and multimodal models in low-resource\nsettings using developmentally plausible datasets, with our multimodal models\noutperforming previous BabyLM baselines. One finding in the multimodal language\nmodel literature is that these models tend to underperform in\n\\textit{language-only} tasks. Therefore, we focus on maintaining language-only\nabilities in multimodal models. To this end, we experiment with \\textit{model\nmerging}, where we fuse the parameters of multimodal models with those of\nlanguage-only models using weighted linear interpolation. Our results\ncorroborate the findings that multimodal models underperform in language-only\nbenchmarks that focus on grammar, and model merging with text-only models can\nhelp alleviate this problem to some extent, while maintaining multimodal\nperformance.",
      "authors": [
        "Ece Takmaz",
        "Lisa Bylinina",
        "Jakub Dotlacil"
      ],
      "published": "2025-10-02T09:38:25Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01845v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究针对多模态语言模型在纯语言任务中表现不佳的问题，提出模型融合方法。通过在低资源环境下构建发展合理的语言和多模态模型，采用加权线性插值融合多模态与纯语言模型的参数，在保持多模态性能的同时有效缓解纯语言任务性能下降问题。",
      "order": 541
    },
    {
      "arxiv_id": "2510.01841v1",
      "title": "Leveraging Prior Knowledge of Diffusion Model for Person Search",
      "summary": "Person search aims to jointly perform person detection and re-identification\nby localizing and identifying a query person within a gallery of uncropped\nscene images. Existing methods predominantly utilize ImageNet pre-trained\nbackbones, which may be suboptimal for capturing the complex spatial context\nand fine-grained identity cues necessary for person search. Moreover, they rely\non a shared backbone feature for both person detection and re-identification,\nleading to suboptimal features due to conflicting optimization objectives. In\nthis paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a\nnovel framework that leverages a pre-trained diffusion model while eliminating\nthe optimization conflict between two sub-tasks. We analyze key properties of\ndiffusion priors and propose three specialized modules: (i) Diffusion-Guided\nRegion Proposal Network (DGRPN) for enhanced person localization, (ii)\nMulti-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and\n(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage\ntext-aligned diffusion features. DiffPS sets a new state-of-the-art on\nCUHK-SYSU and PRW.",
      "authors": [
        "Giyeol Kim",
        "Sooyoung Yang",
        "Jihyong Oh",
        "Myungjoo Kang",
        "Chanho Eom"
      ],
      "published": "2025-10-02T09:36:26Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01841v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出DiffPS框架，利用预训练扩散模型的先验知识解决行人搜索任务。通过三个专用模块（DGRPN、MSFRN、SFAN）分别优化行人定位、减轻形状偏差和利用文本对齐特征，在CUHK-SYSU和PRW数据集上达到最先进性能，解决了检测与重识别任务间的优化冲突问题。",
      "order": 542
    },
    {
      "arxiv_id": "2510.01829v1",
      "title": "Calibrating the Full Predictive Class Distribution of 3D Object\n  Detectors for Autonomous Driving",
      "summary": "In autonomous systems, precise object detection and uncertainty estimation\nare critical for self-aware and safe operation. This work addresses confidence\ncalibration for the classification task of 3D object detectors. We argue that\nit is necessary to regard the calibration of the full predictive confidence\ndistribution over all classes and deduce a metric which captures the\ncalibration of dominant and secondary class predictions. We propose two\nauxiliary regularizing loss terms which introduce either calibration of the\ndominant prediction or the full prediction vector as a training goal. We\nevaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet\nand DSVT-Pillar and find that combining our loss term, which regularizes for\ncalibration of the full class prediction, and isotonic regression lead to the\nbest calibration of CenterPoint and PillarNet with respect to both dominant and\nsecondary class predictions. We further find that DSVT-Pillar can not be\njointly calibrated for dominant and secondary predictions using the same\nmethod.",
      "authors": [
        "Cornelius Schröder",
        "Marius-Raphael Schlüter",
        "Markus Lienkamp"
      ],
      "published": "2025-10-02T09:22:03Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01829v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对自动驾驶中的3D物体检测器，提出校准完整预测类别分布的方法，通过引入两个正则化损失项来优化主导类别和次要类别的预测校准，实验表明结合完整类别预测校准和等渗回归可在多个模型中实现最佳校准效果。",
      "order": 543
    },
    {
      "arxiv_id": "2510.01784v1",
      "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation",
      "summary": "Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models.",
      "authors": [
        "Xiaofei Wu",
        "Guozhen Zhang",
        "Zhiyong Xu",
        "Yuan Zhou",
        "Qinglin Lu",
        "Xuming He"
      ],
      "published": "2025-10-02T08:22:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01784v1",
      "primary_area": "video_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MemoryPack和Direct Forcing两种方法解决长视频生成的双重挑战：MemoryPack通过可学习的上下文检索机制联合建模短长期依赖，实现分钟级时序一致性；Direct Forcing通过单步近似策略改善训练-推理对齐，减少误差累积。两者结合显著提升了长视频生成的上下文一致性和可靠性。",
      "order": 544
    },
    {
      "arxiv_id": "2510.01767v1",
      "title": "LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for\n  Large-Scale Scene Reconstruction",
      "summary": "3D Gaussian Splatting (3DGS) has established itself as an efficient\nrepresentation for real-time, high-fidelity 3D scene reconstruction. However,\nscaling 3DGS to large and unbounded scenes such as city blocks remains\ndifficult. Existing divide-and-conquer methods alleviate memory pressure by\npartitioning the scene into blocks, but introduce new bottlenecks: (i)\npartitions suffer from severe load imbalance since uniform or heuristic splits\ndo not reflect actual computational demands, and (ii) coarse-to-fine pipelines\nfail to exploit the coarse stage efficiently, often reloading the entire model\nand incurring high overhead. In this work, we introduce LoBE-GS, a novel\nLoad-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers\nthe large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning\nmethod that reduces preprocessing from hours to minutes, an optimization-based\nstrategy that balances visible Gaussians -- a strong proxy for computational\nload -- across blocks, and two lightweight techniques, visibility cropping and\nselective densification, to further reduce training cost. Evaluations on\nlarge-scale urban and outdoor datasets show that LoBE-GS consistently achieves\nup to $2\\times$ faster end-to-end training time than state-of-the-art\nbaselines, while maintaining reconstruction quality and enabling scalability to\nscenes infeasible with vanilla 3DGS.",
      "authors": [
        "Sheng-Hsiang Hung",
        "Ting-Yu Yen",
        "Wei-Fang Sun",
        "Simon See",
        "Shih-Hsuan Hung",
        "Hung-Kuo Chu"
      ],
      "published": "2025-10-02T08:02:06Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01767v1",
      "primary_area": "video_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "LOBE-GS提出了一种负载均衡的高效3D高斯泼溅框架，通过深度感知分区、优化策略和轻量级技术，解决了大规模场景重建中的负载不均和训练效率问题，在保持重建质量的同时将端到端训练速度提升至现有方法的2倍。",
      "order": 545
    },
    {
      "arxiv_id": "2510.01758v1",
      "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in\n  Vision Tasks",
      "summary": "Latent representations are critical for the performance and robustness of\nmachine learning models, as they encode the essential features of data in a\ncompact and informative manner. However, in vision tasks, these representations\nare often affected by noisy or irrelevant features, which can degrade the\nmodel's performance and generalization capabilities. This paper presents a\nnovel approach for enhancing latent representations using unsupervised Dynamic\nFeature Selection (DFS). For each instance, the proposed method identifies and\nremoves misleading or redundant information in images, ensuring that only the\nmost relevant features contribute to the latent space. By leveraging an\nunsupervised framework, our approach avoids reliance on labeled data, making it\nbroadly applicable across various domains and datasets. Experiments conducted\non image datasets demonstrate that models equipped with unsupervised DFS\nachieve significant improvements in generalization performance across various\ntasks, including clustering and image generation, while incurring a minimal\nincrease in the computational cost.",
      "authors": [
        "Bruno Corcuera",
        "Carlos Eiras-Franco",
        "Brais Cancela"
      ],
      "published": "2025-10-02T07:46:59Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01758v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种无监督动态特征选择方法，通过识别并移除图像中的误导性或冗余信息来增强视觉任务的潜在表示。该方法不依赖标注数据，在聚类和图像生成等任务中显著提升模型泛化性能，仅带来轻微计算开销增加。",
      "order": 546
    },
    {
      "arxiv_id": "2510.01749v1",
      "title": "Towards Photonic Band Diagram Generation with Transformer-Latent\n  Diffusion Models",
      "summary": "Photonic crystals enable fine control over light propagation at the\nnanoscale, and thus play a central role in the development of photonic and\nquantum technologies. Photonic band diagrams (BDs) are a key tool to\ninvestigate light propagation into such inhomogeneous structured materials.\nHowever, computing BDs requires solving Maxwell's equations across many\nconfigurations, making it numerically expensive, especially when embedded in\noptimization loops for inverse design techniques, for example. To address this\nchallenge, we introduce the first approach for BD generation based on diffusion\nmodels, with the capacity to later generalize and scale to arbitrary three\ndimensional structures. Our method couples a transformer encoder, which\nextracts contextual embeddings from the input structure, with a latent\ndiffusion model to generate the corresponding BD. In addition, we provide\ninsights into why transformers and diffusion models are well suited to capture\nthe complex interference and scattering phenomena inherent to photonics, paving\nthe way for new surrogate modeling strategies in this domain.",
      "authors": [
        "Valentin Delchevalerie",
        "Nicolas Roy",
        "Arnaud Bougaham",
        "Alexandre Mayer",
        "Benoît Frénay",
        "Michaël Lobet"
      ],
      "published": "2025-10-02T07:35:28Z",
      "primary_category": "physics.optics",
      "arxiv_url": "https://arxiv.org/abs/2510.01749v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出首个基于扩散模型的光子能带图生成方法，结合Transformer编码器和潜在扩散模型，可高效计算光子晶体中的光传播特性，为光子学领域替代建模提供新策略。",
      "order": 547
    },
    {
      "arxiv_id": "2510.01715v1",
      "title": "PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal\n  Positional Encoding and Reinforcement Learning",
      "summary": "Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based\nalgorithm, enabling AI-driven artistic image synthesis. However, existing CNN\nand transformer-based models struggle to scale efficiently to complex styles\nand high-resolution inputs. We introduce PyramidStyler, a transformer framework\nwith Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding\nthat captures both local details and global context while reducing\ncomputational load. We further incorporate reinforcement learning to\ndynamically optimize stylization, accelerating convergence. Trained on\nMicrosoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to\n2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s\ninference--and yields further improvements (content 2.03; style 0.75) with\nminimal speed penalty (1.40 s) when using RL. These results demonstrate\nreal-time, high-quality artistic rendering, with broad applications in media\nand design.",
      "authors": [
        "Raahul Krishna Durairaju",
        "K. Saruladha"
      ],
      "published": "2025-10-02T06:54:52Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01715v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "PyramidStyler是一种基于Transformer的神经风格迁移框架，采用金字塔位置编码和强化学习技术。该模型通过分层多尺度编码捕获局部细节和全局上下文，同时降低计算负载，在COCO和WikiArt数据集上训练后，内容损失降低62.6%，风格损失降低57.4%，推理速度达1.39秒，实现了实时高质量艺术渲染。",
      "order": 548
    },
    {
      "arxiv_id": "2510.01704v1",
      "title": "Holistic Order Prediction in Natural Scenes",
      "summary": "Even in controlled settings, understanding instance-wise geometries is a\nchallenging task for a wide range of visual models. Although specialized\nsystems exist, modern arts rely on expensive input formats (category labels,\nbinary segmentation masks) and inference costs (a quadratic amount of forward\npasses). We mitigate these limitations by proposing InstaFormer, a network\ncapable of holistic order prediction. That is, solely given an input RGB image,\nInstaFormer returns the full occlusion and depth orderings for all the\ninstances in the scene in a single forward pass. At its core, InstaFormer\nrelies on interactions between object queries and latent mask descriptors that\nsemantically represent the same objects while carrying complementary\ninformation. We comprehensively benchmark and ablate our approach to highlight\nits effectiveness. Our code and models are open-source and available at this\nURL: https://github.com/SNU-VGILab/InstaOrder.",
      "authors": [
        "Pierre Musacchio",
        "Hyunmin Lee",
        "Jaesik Park"
      ],
      "published": "2025-10-02T06:24:12Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01704v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "提出InstaFormer网络，通过单次前向传播从RGB图像中预测场景中所有实例的完整遮挡和深度顺序，无需昂贵输入格式，结合对象查询与潜在掩码描述符的交互实现高效几何理解。",
      "order": 549
    },
    {
      "arxiv_id": "2510.01700v1",
      "title": "VaPR -- Vision-language Preference alignment for Reasoning",
      "summary": "Preference finetuning methods like Direct Preference Optimization (DPO) with\nAI-generated feedback have shown promise in aligning Large Vision-Language\nModels (LVLMs) with human preferences. However, existing techniques overlook\nthe prevalence of noise in synthetic preference annotations in the form of\nstylistic and length biases. To this end, we introduce a hard-negative response\ngeneration framework based on LLM-guided response editing, that produces\nrejected responses with targeted errors, maintaining stylistic and length\nsimilarity to the accepted ones. Using this framework, we develop the VaPR\ndataset, comprising 30K high-quality samples, to finetune three LVLM families:\nLLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver\nsignificant performance improvements across ten benchmarks, achieving average\ngains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable\nimprovements on reasoning tasks. A scaling analysis shows that performance\nconsistently improves with data size, with LLaVA models benefiting even at\nsmaller scales. Moreover, VaPR reduces the tendency to answer \"Yes\" in binary\nquestions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we\nshow that the framework generalizes to open-source LLMs as editors, with models\ntrained on VaPR-OS achieving ~99% of the performance of models trained on\n\\name, which is synthesized using GPT-4o. Our data, models, and code can be\nfound on the project page https://vap-r.github.io",
      "authors": [
        "Rohan Wadhawan",
        "Fabrice Y Harel-Canada",
        "Zi-Yi Dou",
        "Suhaila Shakiah",
        "Robinson Piramuthu",
        "Nanyun Peng"
      ],
      "published": "2025-10-02T06:10:43Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01700v1",
      "primary_area": "vla_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "VaPR提出基于LLM引导的硬负样本生成框架，通过编辑响应创建风格和长度相似但含目标错误的拒绝样本，构建30K高质量数据集用于视觉语言模型偏好对齐。在三大LVLM家族上显著提升推理性能，平均增益达6.5%(LLaVA)，并有效缓解二元问题中的肯定回答倾向。",
      "order": 550
    },
    {
      "arxiv_id": "2510.01691v1",
      "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment\n  Abilities in MLLMs",
      "summary": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate\nfor clinical AI, yet existing approaches remain constrained by scalar,\nscore-based metrics and fail to reflect the descriptive, human-like reasoning\nprocess central to expert evaluation. To address this gap, we introduce\nMedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning\nparadigm for language-based evaluation of medical image quality with\nMulti-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary\ntasks: (1) MedQ-Perception, which probes low-level perceptual capability via\nhuman-curated questions on fundamental visual attributes; and (2)\nMedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,\naligning model evaluation with human-like reasoning on image quality. The\nbenchmark spans five imaging modalities and over forty quality attributes,\ntotaling 2,600 perceptual queries and 708 reasoning assessments, covering\ndiverse image sources including authentic clinical acquisitions, images with\nsimulated degradations via physics-based reconstructions, and AI-generated\nimages. To evaluate reasoning ability, we propose a multi-dimensional judging\nprotocol that assesses model outputs along four complementary axes. We further\nconduct rigorous human-AI alignment validation by comparing LLM-based judgement\nwith radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates\nthat models exhibit preliminary but unstable perceptual and reasoning skills,\nwith insufficient accuracy for reliable clinical use. These findings highlight\nthe need for targeted optimization of MLLMs in medical IQA. We hope that\nMedQ-Bench will catalyze further exploration and unlock the untapped potential\nof MLLMs for medical image quality evaluation.",
      "authors": [
        "Jiyao Liu",
        "Jinjie Wei",
        "Wanying Qu",
        "Chenglong Ma",
        "Junzhi Ning",
        "Yunheng Li",
        "Ying Chen",
        "Xinzhe Luo",
        "Pengcheng Chen",
        "Xin Gao",
        "Ming Hu",
        "Huihui Xu",
        "Xin Wang",
        "Shujian Gao",
        "Dingkang Yang",
        "Zhongying Deng",
        "Jin Ye",
        "Lihao Liu",
        "Junjun He",
        "Ningsheng Xu"
      ],
      "published": "2025-10-02T05:42:00Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01691v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "medical_ai",
      "tldr_zh": "MedQ-Bench是首个针对多模态大语言模型的医学图像质量评估基准，包含感知和推理两大任务，涵盖5种成像模态和40多个质量属性。评估显示现有模型具备初步但不稳定的医学图像质量评估能力，尚无法满足临床可靠性要求，需进一步优化。",
      "order": 551
    },
    {
      "arxiv_id": "2510.01686v1",
      "title": "FreeViS: Training-free Video Stylization with Inconsistent References",
      "summary": "Video stylization plays a key role in content creation, but it remains a\nchallenging problem. Na\\\"ively applying image stylization frame-by-frame hurts\ntemporal consistency and reduces style richness. Alternatively, training a\ndedicated video stylization model typically requires paired video data and is\ncomputationally expensive. In this paper, we propose FreeViS, a training-free\nvideo stylization framework that generates stylized videos with rich style\ndetails and strong temporal coherence. Our method integrates multiple stylized\nreferences to a pretrained image-to-video (I2V) model, effectively mitigating\nthe propagation errors observed in prior works, without introducing flickers\nand stutters. In addition, it leverages high-frequency compensation to\nconstrain the content layout and motion, together with flow-based motion cues\nto preserve style textures in low-saliency regions. Through extensive\nevaluations, FreeViS delivers higher stylization fidelity and superior temporal\nconsistency, outperforming recent baselines and achieving strong human\npreference. Our training-free pipeline offers a practical and economic solution\nfor high-quality, temporally coherent video stylization. The code and videos\ncan be accessed via https://xujiacong.github.io/FreeViS/",
      "authors": [
        "Jiacong Xu",
        "Yiqun Mei",
        "Ke Zhang",
        "Vishal M. Patel"
      ],
      "published": "2025-10-02T05:27:06Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01686v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "FreeViS是一种无需训练的视频风格化框架，通过整合多个风格化参考到预训练图像转视频模型中，在保持时间一致性的同时提供丰富的风格细节。该方法采用高频补偿和基于光流的运动线索，有效解决了传统方法中的闪烁和卡顿问题，无需配对视频数据即可实现高质量视频风格化。",
      "order": 552
    },
    {
      "arxiv_id": "2510.01683v1",
      "title": "Uncovering Overconfident Failures in CXR Models via\n  Augmentation-Sensitivity Risk Scoring",
      "summary": "Deep learning models achieve strong performance in chest radiograph (CXR)\ninterpretation, yet fairness and reliability concerns persist. Models often\nshow uneven accuracy across patient subgroups, leading to hidden failures not\nreflected in aggregate metrics. Existing error detection approaches -- based on\nconfidence calibration or out-of-distribution (OOD) detection -- struggle with\nsubtle within-distribution errors, while image- and representation-level\nconsistency-based methods remain underexplored in medical imaging. We propose\nan augmentation-sensitivity risk scoring (ASRS) framework to identify\nerror-prone CXR cases. ASRS applies clinically plausible rotations ($\\pm\n15^\\circ$/$\\pm 30^\\circ$) and measures embedding shifts with the RAD-DINO\nencoder. Sensitivity scores stratify samples into stability quartiles, where\nhighly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)\ndespite high AUROC and confidence. ASRS provides a label-free means for\nselective prediction and clinician review, improving fairness and safety in\nmedical AI.",
      "authors": [
        "Han-Jay Shu",
        "Wei-Ning Chiu",
        "Shun-Ting Chang",
        "Meng-Ping Huang",
        "Takeshi Tohyama",
        "Ahram Han",
        "Po-Chih Kuo"
      ],
      "published": "2025-10-02T05:15:40Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01683v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出增强敏感性风险评分(ASRS)框架，通过临床合理旋转和RAD-DINO编码器测量嵌入变化，识别胸片模型中易出错病例。该方法能在保持高AUROC和置信度情况下，检测出召回率降低0.2-0.3的敏感样本，为医疗AI提供无标签选择性预测工具，提升公平性和安全性。",
      "order": 553
    },
    {
      "arxiv_id": "2510.01681v1",
      "title": "Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning",
      "summary": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet they\nfrequently struggle with tasks requiring precise understanding and handling of\nfine-grained visual elements. This is mainly due to information loss during\nimage encoding or insufficient attention to critical regions. Recent work has\nshown promise by incorporating pixel-level visual information into the\nreasoning process, enabling VLMs to access high-resolution visual details\nduring their thought process. However, this pixel-level information is often\noverused, leading to inefficiency and distraction from irrelevant visual\ndetails. To address these challenges, we propose the first framework for\nadaptive pixel reasoning that dynamically determines necessary pixel-level\noperations based on the input query. Specifically, we first apply\noperation-aware supervised fine-tuning to establish baseline competence in\ntextual reasoning and visual operations, then design a novel rollout-guided\nreinforcement learning framework relying on feedback of the model's own\nresponses, which enables the VLM to determine when pixel operations should be\ninvoked based on query difficulty. Experiments on extensive multimodal\nreasoning benchmarks show that our model achieves superior performance while\nsignificantly reducing unnecessary visual operations. Impressively, our model\nachieves 73.4\\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of\nonly 20.1\\%, improving accuracy and simultaneously reducing tool usage by\n66.5\\% compared to the previous methods.",
      "authors": [
        "Xuchen Li",
        "Xuzhao Li",
        "Jiahui Gao",
        "Renjie Pi",
        "Shiyu Hu",
        "Wentao Zhang"
      ],
      "published": "2025-10-02T05:14:52Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01681v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出首个自适应像素推理框架，通过操作感知微调和基于模型自身反馈的强化学习，动态决定何时调用像素级操作。在HR-Bench 4K上达到73.4%准确率，同时将工具使用率降至20.1%，相比之前方法在提升精度的同时减少66.5%的不必要视觉操作。",
      "order": 554
    },
    {
      "arxiv_id": "2510.01678v1",
      "title": "An Efficient Deep Template Matching and In-Plane Pose Estimation Method\n  via Template-Aware Dynamic Convolution",
      "summary": "In industrial inspection and component alignment tasks, template matching\nrequires efficient estimation of a target's position and geometric state\n(rotation and scaling) under complex backgrounds to support precise downstream\noperations. Traditional methods rely on exhaustive enumeration of angles and\nscales, leading to low efficiency under compound transformations. Meanwhile,\nmost deep learning-based approaches only estimate similarity scores without\nexplicitly modeling geometric pose, making them inadequate for real-world\ndeployment. To overcome these limitations, we propose a lightweight end-to-end\nframework that reformulates template matching as joint localization and\ngeometric regression, outputting the center coordinates, rotation angle, and\nindependent horizontal and vertical scales. A Template-Aware Dynamic\nConvolution Module (TDCM) dynamically injects template features at inference to\nguide generalizable matching. The compact network integrates depthwise\nseparable convolutions and pixel shuffle for efficient matching. To enable\ngeometric-annotation-free training, we introduce a rotation-shear-based\naugmentation strategy with structure-aware pseudo labels. A lightweight\nrefinement module further improves angle and scale precision via local\noptimization. Experiments show our 3.07M model achieves high precision and 14ms\ninference under compound transformations. It also demonstrates strong\nrobustness in small-template and multi-object scenarios, making it highly\nsuitable for deployment in real-time industrial applications. The code is\navailable at:https://github.com/ZhouJ6610/PoseMatch-TDCM.",
      "authors": [
        "Ke Jia",
        "Ji Zhou",
        "Hanxin Li",
        "Zhigan Zhou",
        "Haojie Chu",
        "Xiaojie Li"
      ],
      "published": "2025-10-02T05:05:43Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01678v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种高效的深度模板匹配与平面内姿态估计方法，通过模板感知动态卷积模块实现端到端的联合定位与几何回归。该方法在复杂背景下能快速估计目标位置、旋转角度和缩放比例，仅需3.07M参数和14ms推理时间，适用于工业检测等实时应用场景。",
      "order": 555
    },
    {
      "arxiv_id": "2510.01677v1",
      "title": "Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal\n  Sentiment Analysis",
      "summary": "Multimodal sentiment analysis (MSA) leverages information fusion from diverse\nmodalities (e.g., text, audio, visual) to enhance sentiment prediction.\nHowever, simple fusion techniques often fail to account for variations in\nmodality quality, such as those that are noisy, missing, or semantically\nconflicting. This oversight leads to suboptimal performance, especially in\ndiscerning subtle emotional nuances. To mitigate this limitation, we introduce\na simple yet efficient \\textbf{A}daptive \\textbf{G}ated \\textbf{F}usion\n\\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion\nmechanism based on information entropy and modality importance. This mechanism\nmitigates the influence of noisy modalities and prioritizes informative cues\nfollowing unimodal encoding and cross-modal interaction. Experiments on\nCMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong\nbaselines in accuracy, effectively discerning subtle emotions with robust\nperformance. Visualization analysis of feature representations demonstrates\nthat AGFN enhances generalization by learning from a broader feature\ndistribution, achieved by reducing the correlation between feature location and\nprediction error, thereby decreasing reliance on specific locations and\ncreating more robust multimodal feature representations.",
      "authors": [
        "Han Wu",
        "Yanming Sun",
        "Yunhe Yang",
        "Derek F. Wong"
      ],
      "published": "2025-10-02T05:05:41Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01677v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出自适应门控融合网络(AGFN)，通过基于信息熵和模态重要性的双门融合机制，动态调整多模态特征权重，有效处理噪声、缺失或语义冲突的模态数据，在CMU-MOSI和CMU-MOSEI数据集上显著提升情感分析准确率，增强模型对细微情感的识别能力和泛化性能。",
      "order": 556
    },
    {
      "arxiv_id": "2510.01669v1",
      "title": "UniVerse: Unleashing the Scene Prior of Video Diffusion Models for\n  Robust Radiance Field Reconstruction",
      "summary": "This paper tackles the challenge of robust reconstruction, i.e., the task of\nreconstructing a 3D scene from a set of inconsistent multi-view images. Some\nrecent works have attempted to simultaneously remove image inconsistencies and\nperform reconstruction by integrating image degradation modeling into neural 3D\nscene representations.However, these methods rely heavily on dense observations\nfor robustly optimizing model parameters.To address this issue, we propose to\ndecouple robust reconstruction into two subtasks: restoration and\nreconstruction, which naturally simplifies the optimization process.To this\nend, we introduce UniVerse, a unified framework for robust reconstruction based\non a video diffusion model. Specifically, UniVerse first converts inconsistent\nimages into initial videos, then uses a specially designed video diffusion\nmodel to restore them into consistent images, and finally reconstructs the 3D\nscenes from these restored images.Compared with case-by-case per-view\ndegradation modeling, the diffusion model learns a general scene prior from\nlarge-scale data, making it applicable to diverse image\ninconsistencies.Extensive experiments on both synthetic and real-world datasets\ndemonstrate the strong generalization capability and superior performance of\nour method in robust reconstruction. Moreover, UniVerse can control the style\nof the reconstructed 3D scene. Project page:\nhttps://jin-cao-tma.github.io/UniVerse.github.io/",
      "authors": [
        "Jin Cao",
        "Hongrui Wu",
        "Ziyong Feng",
        "Hujun Bao",
        "Xiaowei Zhou",
        "Sida Peng"
      ],
      "published": "2025-10-02T04:50:18Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01669v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "UniVerse提出统一框架解决多视角图像不一致的3D场景重建难题，通过视频扩散模型将不一致图像转换为连续视频并修复为一致图像，最终重建3D场景。该方法利用大规模数据学习场景先验，无需逐案建模图像退化，在合成和真实数据集上展现强大泛化能力，并能控制重建场景风格。",
      "order": 557
    },
    {
      "arxiv_id": "2510.01666v1",
      "title": "Median2Median: Zero-shot Suppression of Structured Noise in Images",
      "summary": "Image denoising is a fundamental problem in computer vision and medical\nimaging. However, real-world images are often degraded by structured noise with\nstrong anisotropic correlations that existing methods struggle to remove. Most\ndata-driven approaches rely on large datasets with high-quality labels and\nstill suffer from limited generalizability, whereas existing zero-shot methods\navoid this limitation but remain effective only for independent and identically\ndistributed (i.i.d.) noise. To address this gap, we propose Median2Median\n(M2M), a zero-shot denoising framework designed for structured noise. M2M\nintroduces a novel sampling strategy that generates pseudo-independent\nsub-image pairs from a single noisy input. This strategy leverages directional\ninterpolation and generalized median filtering to adaptively exclude values\ndistorted by structured artifacts. To further enlarge the effective sampling\nspace and eliminate systematic bias, a randomized assignment strategy is\nemployed, ensuring that the sampled sub-image pairs are suitable for\nNoise2Noise training. In our realistic simulation studies, M2M performs on par\nwith state-of-the-art zero-shot methods under i.i.d. noise, while consistently\noutperforming them under correlated noise. These findings establish M2M as an\nefficient, data-free solution for structured noise suppression and mark the\nfirst step toward effective zero-shot denoising beyond the strict i.i.d.\nassumption.",
      "authors": [
        "Jianxu Wang",
        "Ge Wang"
      ],
      "published": "2025-10-02T04:47:00Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.01666v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "提出Median2Median零样本去噪框架，专门针对具有强各向异性相关的结构化噪声。通过方向插值和广义中值滤波生成伪独立子图像对，无需训练数据即可有效抑制医学图像等领域的结构化噪声，在相关噪声条件下优于现有零样本方法。",
      "order": 558
    },
    {
      "arxiv_id": "2510.01665v1",
      "title": "Non-Rigid Structure-from-Motion via Differential Geometry with\n  Recoverable Conformal Scale",
      "summary": "Non-rigid structure-from-motion (NRSfM), a promising technique for addressing\nthe mapping challenges in monocular visual deformable simultaneous localization\nand mapping (SLAM), has attracted growing attention. We introduce a novel\nmethod, called Con-NRSfM, for NRSfM under conformal deformations, encompassing\nisometric deformations as a subset. Our approach performs point-wise\nreconstruction using 2D selected image warps optimized through a graph-based\nframework. Unlike existing methods that rely on strict assumptions, such as\nlocally planar surfaces or locally linear deformations, and fail to recover the\nconformal scale, our method eliminates these constraints and accurately\ncomputes the local conformal scale. Additionally, our framework decouples\nconstraints on depth and conformal scale, which are inseparable in other\napproaches, enabling more precise depth estimation. To address the sensitivity\nof the formulated problem, we employ a parallel separable iterative\noptimization strategy. Furthermore, a self-supervised learning framework,\nutilizing an encoder-decoder network, is incorporated to generate dense 3D\npoint clouds with texture. Simulation and experimental results using both\nsynthetic and real datasets demonstrate that our method surpasses existing\napproaches in terms of reconstruction accuracy and robustness. The code for the\nproposed method will be made publicly available on the project website:\nhttps://sites.google.com/view/con-nrsfm.",
      "authors": [
        "Yongbo Chen",
        "Yanhao Zhang",
        "Shaifali Parashar",
        "Liang Zhao",
        "Shoudong Huang"
      ],
      "published": "2025-10-02T04:46:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01665v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Con-NRSfM方法，通过微分几何和可恢复共形尺度解决非刚性运动恢复结构问题。该方法采用图优化框架进行点对点重建，无需局部平面或线性变形假设，能精确计算共形尺度并分离深度约束。结合自监督学习生成带纹理的稠密3D点云，在合成和真实数据集上均展现优越的重建精度和鲁棒性。",
      "order": 559
    },
    {
      "arxiv_id": "2510.01662v1",
      "title": "Discrete Facial Encoding: : A Framework for Data-driven Facial Display\n  Discovery",
      "summary": "Facial expression analysis is central to understanding human behavior, yet\nexisting coding systems such as the Facial Action Coding System (FACS) are\nconstrained by limited coverage and costly manual annotation. In this work, we\nintroduce Discrete Facial Encoding (DFE), an unsupervised, data-driven\nalternative of compact and interpretable dictionary of facial expressions from\n3D mesh sequences learned through a Residual Vector Quantized Variational\nAutoencoder (RVQ-VAE). Our approach first extracts identity-invariant\nexpression features from images using a 3D Morphable Model (3DMM), effectively\ndisentangling factors such as head pose and facial geometry. We then encode\nthese features using an RVQ-VAE, producing a sequence of discrete tokens from a\nshared codebook, where each token captures a specific, reusable facial\ndeformation pattern that contributes to the overall expression. Through\nextensive experiments, we demonstrate that Discrete Facial Encoding captures\nmore precise facial behaviors than FACS and other facial encoding alternatives.\nWe evaluate the utility of our representation across three high-level\npsychological tasks: stress detection, personality prediction, and depression\ndetection. Using a simple Bag-of-Words model built on top of the learned\ntokens, our system consistently outperforms both FACS-based pipelines and\nstrong image and video representation learning models such as Masked\nAutoencoders. Further analysis reveals that our representation covers a wider\nvariety of facial displays, highlighting its potential as a scalable and\neffective alternative to FACS for psychological and affective computing\napplications.",
      "authors": [
        "Minh Tran",
        "Maksim Siniukov",
        "Zhangyu Jin",
        "Mohammad Soleymani"
      ],
      "published": "2025-10-02T04:44:45Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01662v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出离散面部编码(DFE)框架，通过残差向量量化变分自编码器从3D网格序列中学习紧凑可解释的面部表情字典。该方法能有效分离身份、姿态等干扰因素，在压力检测、人格预测和抑郁检测等心理任务中优于传统FACS系统和主流表示学习模型，为心理计算提供可扩展的替代方案。",
      "order": 560
    },
    {
      "arxiv_id": "2510.01660v1",
      "title": "VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual\n  Reprogramming",
      "summary": "Existing UDA pipelines fine-tune already well-trained backbone parameters for\nevery new source-and-target pair, resulting in the number of training\nparameters and storage memory growing linearly with each new pair, and also\npreventing the reuse of these well-trained backbone parameters.\n  Inspired by recent implications that existing backbones have textural biases,\nwe propose making use of domain-specific textural bias for domain adaptation\nvia visual reprogramming, namely VirDA.Instead of fine-tuning the full\nbackbone, VirDA prepends a domain-specific visual reprogramming layer to the\nbackbone. This layer produces visual prompts that act as an added textural bias\nto the input image, adapting its ``style'' to a target domain. To optimize\nthese visual reprogramming layers, we use multiple objective functions that\noptimize the intra- and inter-domain distribution differences when\ndomain-adapting visual prompts are applied. This process does not require\nmodifying the backbone parameters, allowing the same backbone to be reused\nacross different domains.\n  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M\ntrainable parameters. VirDA surpasses PDA, the state-of-the-art\nparameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its\nparameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans\nand FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%\nof their trainable parameters. Relative to the strongest current methods\n(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only\n2.2% and 1.1% accuracy, respectively.",
      "authors": [
        "Duy Nguyen",
        "Dat Nguyen"
      ],
      "published": "2025-10-02T04:40:42Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01660v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "VirDA提出一种无监督域自适应方法，通过视觉重编程层生成视觉提示来调整输入图像的纹理风格，无需微调主干网络参数。该方法在Office-31数据集上达到92.8%准确率，仅需1.5M可训练参数，显著提升了参数效率并支持主干网络跨域复用。",
      "order": 561
    },
    {
      "arxiv_id": "2510.01651v1",
      "title": "LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze\n  Inscription Recognition",
      "summary": "Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial\nstage of early Chinese writing and provide indispensable evidence for\narchaeological and historical studies. However, automatic BI recognition\nremains difficult due to severe visual degradation, multi-domain variability\nacross photographs, rubbings, and tracings, and an extremely long-tailed\ncharacter distribution. To address these challenges, we curate a large-scale BI\ndataset comprising 22454 full-page images and 198598 annotated characters\nspanning 6658 unique categories, enabling robust cross-domain evaluation.\nBuilding on this resource, we develop a two-stage detection-recognition\npipeline that first localizes inscriptions and then transcribes individual\ncharacters. To handle heterogeneous domains and rare classes, we equip the\npipeline with LadderMoE, which augments a pretrained CLIP encoder with\nladder-style MoE adapters, enabling dynamic expert specialization and stronger\nrobustness. Comprehensive experiments on single-character and full-page\nrecognition tasks demonstrate that our method substantially outperforms\nstate-of-the-art scene text recognition baselines, achieving superior accuracy\nacross head, mid, and tail categories as well as all acquisition modalities.\nThese results establish a strong foundation for bronze inscription recognition\nand downstream archaeological analysis.",
      "authors": [
        "Rixin Zhou",
        "Peiqiang Qiu",
        "Qian Zhang",
        "Chuntao Li",
        "Xi Yang"
      ],
      "published": "2025-10-02T04:14:14Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01651v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出LadderMoE方法，针对青铜器铭文识别中的视觉退化、多域差异和长尾分布问题，构建大规模数据集并开发两阶段检测-识别流程，通过梯状混合专家适配器增强CLIP编码器，在单字符和全页识别任务上显著优于现有方法。",
      "order": 562
    },
    {
      "arxiv_id": "2510.01641v1",
      "title": "FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion\n  Deblurring",
      "summary": "Recent advancements in image motion deblurring, driven by CNNs and\ntransformers, have made significant progress. Large-scale pre-trained diffusion\nmodels, which are rich in true-world modeling, have shown great promise for\nhigh-quality image restoration tasks such as deblurring, demonstrating stronger\ngenerative capabilities than CNN and transformer-based methods. However,\nchallenges such as unbearable inference time and compromised fidelity still\nlimit the full potential of the diffusion models. To address this, we introduce\nFideDiff, a novel single-step diffusion model designed for high-fidelity\ndeblurring. We reformulate motion deblurring as a diffusion-like process where\neach timestep represents a progressively blurred image, and we train a\nconsistency model that aligns all timesteps to the same clean image. By\nreconstructing training data with matched blur trajectories, the model learns\ntemporal consistency, enabling accurate one-step deblurring. We further enhance\nmodel performance by integrating Kernel ControlNet for blur kernel estimation\nand introducing adaptive timestep prediction. Our model achieves superior\nperformance on full-reference metrics, surpassing previous diffusion-based\nmethods and matching the performance of other state-of-the-art models. FideDiff\noffers a new direction for applying pre-trained diffusion models to\nhigh-fidelity image restoration tasks, establishing a robust baseline for\nfurther advancing diffusion models in real-world industrial applications. Our\ndataset and code will be available at https://github.com/xyLiu339/FideDiff.",
      "authors": [
        "Xiaoyang Liu",
        "Zhengyan Zhou",
        "Zihang Xu",
        "Jiezhang Cao",
        "Zheng Chen",
        "Yulun Zhang"
      ],
      "published": "2025-10-02T03:44:45Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01641v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "FideDiff是一种高效的单步扩散模型，用于高保真图像运动去模糊。通过将运动模糊重新定义为扩散过程，训练一致性模型将所有时间步对齐到清晰图像，结合模糊核估计和自适应时间步预测，在保持高质量的同时显著减少推理时间，为扩散模型在图像恢复任务中的实际应用提供了新方向。",
      "order": 563
    },
    {
      "arxiv_id": "2510.01640v1",
      "title": "Joint Deblurring and 3D Reconstruction for Macrophotography",
      "summary": "Macro lens has the advantages of high resolution and large magnification, and\n3D modeling of small and detailed objects can provide richer information.\nHowever, defocus blur in macrophotography is a long-standing problem that\nheavily hinders the clear imaging of the captured objects and high-quality 3D\nreconstruction of them. Traditional image deblurring methods require a large\nnumber of images and annotations, and there is currently no multi-view 3D\nreconstruction method for macrophotography. In this work, we propose a joint\ndeblurring and 3D reconstruction method for macrophotography. Starting from\nmulti-view blurry images captured, we jointly optimize the clear 3D model of\nthe object and the defocus blur kernel of each pixel. The entire framework\nadopts a differentiable rendering method to self-supervise the optimization of\nthe 3D model and the defocus blur kernel. Extensive experiments show that from\na small number of multi-view images, our proposed method can not only achieve\nhigh-quality image deblurring but also recover high-fidelity 3D appearance.",
      "authors": [
        "Yifan Zhao",
        "Liangchen Li",
        "Yuqi Zhou",
        "Kai Wang",
        "Yan Liang",
        "Juyong Zhang"
      ],
      "published": "2025-10-02T03:43:05Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01640v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "提出一种联合去模糊与3D重建方法，通过可微分渲染技术同时优化清晰3D模型和散焦模糊核，仅需少量多视角模糊图像即可实现高质量去模糊和高保真3D外观恢复。",
      "order": 564
    },
    {
      "arxiv_id": "2510.01623v1",
      "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
      "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.",
      "authors": [
        "Angen Ye",
        "Zeyu Zhang",
        "Boyuan Wang",
        "Xiaofeng Wang",
        "Dapeng Zhang",
        "Zheng Zhu"
      ],
      "published": "2025-10-02T02:54:03Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01623v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "VLA-R1提出一种增强视觉-语言-动作模型推理能力的方法，通过可验证奖励强化学习和群体相对策略优化，结合新构建的VLA-CoT-13K数据集，显著提升了模型在跨领域和真实机器人环境中的推理鲁棒性与执行准确性。",
      "order": 565
    },
    {
      "arxiv_id": "2510.01619v1",
      "title": "MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust\n  Physics-Based Dynamics",
      "summary": "While there has been significant progress in the field of 3D avatar creation\nfrom visual observations, modeling physically plausible dynamics of humans with\nloose garments remains a challenging problem. Although a few existing works\naddress this problem by leveraging physical simulation, they suffer from\nlimited accuracy or robustness to novel animation inputs. In this work, we\npresent MPMAvatar, a framework for creating 3D human avatars from multi-view\nvideos that supports highly realistic, robust animation, as well as\nphotorealistic rendering from free viewpoints. For accurate and robust dynamics\nmodeling, our key idea is to use a Material Point Method-based simulator, which\nwe carefully tailor to model garments with complex deformations and contact\nwith the underlying body by incorporating an anisotropic constitutive model and\na novel collision handling algorithm. We combine this dynamics modeling scheme\nwith our canonical avatar that can be rendered using 3D Gaussian Splatting with\nquasi-shadowing, enabling high-fidelity rendering for physically realistic\nanimations. In our experiments, we demonstrate that MPMAvatar significantly\noutperforms the existing state-of-the-art physics-based avatar in terms of (1)\ndynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and\nefficiency. Additionally, we present a novel application in which our avatar\ngeneralizes to unseen interactions in a zero-shot manner-which was not\nachievable with previous learning-based methods due to their limited simulation\ngeneralizability. Our project page is at:\nhttps://KAISTChangmin.github.io/MPMAvatar/",
      "authors": [
        "Changmin Lee",
        "Jihyun Lee",
        "Tae-Kyun Kim"
      ],
      "published": "2025-10-02T02:51:45Z",
      "primary_category": "cs.GR",
      "arxiv_url": "https://arxiv.org/abs/2510.01619v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "MPMAvatar提出了一种基于物质点法的3D高斯化身框架，通过各向异性本构模型和新型碰撞处理算法，实现了松散衣物物理动力学的精确建模与高保真渲染，在动态精度、渲染质量和鲁棒性方面超越现有方法。",
      "order": 566
    },
    {
      "arxiv_id": "2510.01618v1",
      "title": "Automated Genomic Interpretation via Concept Bottleneck Models for\n  Medical Robotics",
      "summary": "We propose an automated genomic interpretation module that transforms raw DNA\nsequences into actionable, interpretable decisions suitable for integration\ninto medical automation and robotic systems. Our framework combines Chaos Game\nRepresentation (CGR) with a Concept Bottleneck Model (CBM), enforcing\npredictions to flow through biologically meaningful concepts such as GC\ncontent, CpG density, and k mer motifs. To enhance reliability, we incorporate\nconcept fidelity supervision, prior consistency alignment, KL distribution\nmatching, and uncertainty calibration. Beyond accurate classification of HIV\nsubtypes across both in-house and LANL datasets, our module delivers\ninterpretable evidence that can be directly validated against biological\npriors. A cost aware recommendation layer further translates predictive outputs\ninto decision policies that balance accuracy, calibration, and clinical\nutility, reducing unnecessary retests and improving efficiency. Extensive\nexperiments demonstrate that the proposed system achieves state of the art\nclassification performance, superior concept prediction fidelity, and more\nfavorable cost benefit trade-offs compared to existing baselines. By bridging\nthe gap between interpretable genomic modeling and automated decision-making,\nthis work establishes a reliable foundation for robotic and clinical automation\nin genomic medicine.",
      "authors": [
        "Zijun Li",
        "Jinchang Zhang",
        "Ming Zhang",
        "Guoyu Lu"
      ],
      "published": "2025-10-02T02:51:34Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01618v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "提出一种结合混沌游戏表示与概念瓶颈模型的自动化基因组解释模块，通过生物意义概念（如GC含量、CpG密度等）进行HIV亚型分类，具备可解释性验证和成本感知决策功能，为医疗机器人和临床自动化提供可靠基础。",
      "order": 567
    },
    {
      "arxiv_id": "2510.01608v1",
      "title": "NPN: Non-Linear Projections of the Null-Space for Imaging Inverse\n  Problems",
      "summary": "Imaging inverse problems aims to recover high-dimensional signals from\nundersampled, noisy measurements, a fundamentally ill-posed task with infinite\nsolutions in the null-space of the sensing operator. To resolve this ambiguity,\nprior information is typically incorporated through handcrafted regularizers or\nlearned models that constrain the solution space. However, these priors\ntypically ignore the task-specific structure of that null-space. In this work,\nwe propose \\textit{Non-Linear Projections of the Null-Space} (NPN), a novel\nclass of regularization that, instead of enforcing structural constraints in\nthe image domain, promotes solutions that lie in a low-dimensional projection\nof the sensing matrix's null-space with a neural network. Our approach has two\nkey advantages: (1) Interpretability: by focusing on the structure of the\nnull-space, we design sensing-matrix-specific priors that capture information\northogonal to the signal components that are fundamentally blind to the sensing\nprocess. (2) Flexibility: NPN is adaptable to various inverse problems,\ncompatible with existing reconstruction frameworks, and complementary to\nconventional image-domain priors. We provide theoretical guarantees on\nconvergence and reconstruction accuracy when used within plug-and-play methods.\nEmpirical results across diverse sensing matrices demonstrate that NPN priors\nconsistently enhance reconstruction fidelity in various imaging inverse\nproblems, such as compressive sensing, deblurring, super-resolution, computed\ntomography, and magnetic resonance imaging, with plug-and-play methods,\nunrolling networks, deep image prior, and diffusion models.",
      "authors": [
        "Roman Jacome",
        "Romario Gualdrón-Hurtado",
        "Leon Suarez",
        "Henry Arguello"
      ],
      "published": "2025-10-02T02:45:06Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01608v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出NPN方法，通过神经网络对感知矩阵零空间进行非线性投影，解决成像逆问题。该方法具有可解释性和灵活性，在压缩感知、去模糊、超分辨率、CT和MRI等多种成像任务中提升重建质量，并与现有重建框架兼容。",
      "order": 568
    },
    {
      "arxiv_id": "2510.01607v1",
      "title": "ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free\n  Human Demonstrations",
      "summary": "We present ActiveUMI, a framework for a data collection system that transfers\nin-the-wild human demonstrations to robots capable of complex bimanual\nmanipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized\ncontrollers that mirror the robot's end-effectors, bridging human-robot\nkinematics via precise pose alignment. To ensure mobility and data quality, we\nintroduce several key techniques, including immersive 3D model rendering, a\nself-contained wearable computer, and efficient calibration methods.\nActiveUMI's defining feature is its capture of active, egocentric perception.\nBy recording an operator's deliberate head movements via a head-mounted\ndisplay, our system learns the crucial link between visual attention and\nmanipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies\ntrained exclusively on ActiveUMI data achieve an average success rate of 70\\%\non in-distribution tasks and demonstrate strong generalization, retaining a\n56\\% success rate when tested on novel objects and in new environments. Our\nresults demonstrate that portable data collection systems, when coupled with\nlearned active perception, provide an effective and scalable pathway toward\ncreating generalizable and highly capable real-world robot policies.",
      "authors": [
        "Qiyuan Zeng",
        "Chengmeng Li",
        "Jude St. John",
        "Zhongyi Zhou",
        "Junjie Wen",
        "Guorui Feng",
        "Yichen Zhu",
        "Yi Xu"
      ],
      "published": "2025-10-02T02:44:21Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01607v1",
      "primary_area": "vla_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "ActiveUMI提出了一种通过VR遥操作采集人类演示数据并迁移到机器人双手机器人操控的框架。该系统通过头戴设备记录操作者的主动头部运动，学习视觉注意力与操作之间的关联，在六项复杂双手机器人任务中达到70%的平均成功率，并在新物体和新环境中保持56%的泛化成功率。",
      "order": 569
    },
    {
      "arxiv_id": "2510.01582v1",
      "title": "ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal\n  Reasoning for Vision Language Models",
      "summary": "We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the\ndevelopment of Vision Language Models (VLMs) with explicit reasoning\ncapabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,\nproviding structured thinking tokens and corresponding answers. Our synthetic\ndataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and\nKimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of\nthinking-answer sequences, creating a resource for training and evaluating\nmultimodal reasoning models. We capture the step-by-step reasoning process of\nVLMs and the final descriptive answers. Our goal with this dataset is to enable\nthe development of more robust VLMs while contributing to the broader\nunderstanding of multimodal reasoning mechanisms. The dataset and evaluation\nbenchmarks will be publicly available to aid research in reasoning/thinking\nmultimodal VLMs.",
      "authors": [
        "Krishna Teja Chitty-Venkata",
        "Murali Emani"
      ],
      "published": "2025-10-02T02:02:45Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01582v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "ImageNet-Think-250K是一个大规模合成多模态推理数据集，基于25万张ImageNet21k图像构建，包含结构化思维标记和对应答案。该数据集由GLM-4.1V和Kimi-VL两个先进视觉语言模型生成，每张图像配备两对思维-答案序列，旨在促进具有显式推理能力的视觉语言模型开发，推动多模态推理机制研究。",
      "order": 570
    },
    {
      "arxiv_id": "2510.01576v1",
      "title": "Guiding Multimodal Large Language Models with Blind and Low Vision\n  People Visual Questions for Proactive Visual Interpretations",
      "summary": "Multimodal large language models (MLLMs) have been integrated into visual\ninterpretation applications to support Blind and Low Vision (BLV) users because\nof their accuracy and ability to provide rich, human-like interpretations.\nHowever, these applications often default to comprehensive, lengthy\ndescriptions regardless of context. This leads to inefficient exchanges, as\nusers must go through irrelevant details rather than receiving the specific\ninformation they are likely to seek. To deliver more contextually-relevant\ninformation, we developed a system that draws on historical BLV users\nquestions. When given an image, our system identifies similar past visual\ncontexts from the VizWiz-LF dataset and uses the associated questions to guide\nthe MLLM generate descriptions more relevant to BLV users. An evaluation with\nthree human labelers who revised 92 context-aware and context-free descriptions\nshowed that context-aware descriptions anticipated and answered users'\nquestions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of\ncomparisons (50 out of 92). Our paper reviews, and data analysis are publicly\navailable in a Github repository at\nhttps://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .",
      "authors": [
        "Ricardo Gonzalez Penuela",
        "Felipe Arias-Russi",
        "Victor Capriles"
      ],
      "published": "2025-10-02T01:48:51Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01576v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究针对盲人和低视力用户开发了一种多模态大语言模型引导系统，通过分析历史视觉问题数据集，生成更具上下文相关性的图像描述，而非默认的冗长描述。评估显示76.1%的情境感知描述能预判并回答用户问题，54.4%的情况下更受偏好。",
      "order": 571
    },
    {
      "arxiv_id": "2510.01559v1",
      "title": "Consistent Assistant Domains Transformer for Source-free Domain\n  Adaptation",
      "summary": "Source-free domain adaptation (SFDA) aims to address the challenge of\nadapting to a target domain without accessing the source domain directly.\nHowever, due to the inaccessibility of source domain data, deterministic\ninvariable features cannot be obtained. Current mainstream methods primarily\nfocus on evaluating invariant features in the target domain that closely\nresemble those in the source domain, subsequently aligning the target domain\nwith the source domain. However, these methods are susceptible to hard samples\nand influenced by domain bias. In this paper, we propose a Consistent Assistant\nDomains Transformer for SFDA, abbreviated as CADTrans, which solves the issue\nby constructing invariable feature representations of domain consistency.\nConcretely, we develop an assistant domain module for CADTrans to obtain\ndiversified representations from the intermediate aggregated global attentions,\nwhich addresses the limitation of existing methods in adequately representing\ndiversity. Based on assistant and target domains, invariable feature\nrepresentations are obtained by multiple consistent strategies, which can be\nused to distinguish easy and hard samples. Finally, to align the hard samples\nto the corresponding easy samples, we construct a conditional multi-kernel max\nmean discrepancy (CMK-MMD) strategy to distinguish between samples of the same\ncategory and those of different categories. Extensive experiments are conducted\non various benchmarks such as Office-31, Office-Home, VISDA-C, and\nDomainNet-126, proving the significant performance improvements achieved by our\nproposed approaches. Code is available at\nhttps://github.com/RoryShao/CADTrans.git.",
      "authors": [
        "Renrong Shao",
        "Wei Zhang",
        "Kangyang Luo",
        "Qin Li",
        "and Jun Wang"
      ],
      "published": "2025-10-02T01:07:45Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01559v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出CADTrans方法解决无源域自适应问题，通过构建辅助域模块获取多样化表示，采用多重一致性策略获得不变特征表示，并设计条件多核最大均值差异策略对齐困难样本，在多个基准测试中取得显著性能提升。",
      "order": 572
    },
    {
      "arxiv_id": "2510.01547v1",
      "title": "Robust Classification of Oral Cancer with Limited Training Data",
      "summary": "Oral cancer ranks among the most prevalent cancers globally, with a\nparticularly high mortality rate in regions lacking adequate healthcare access.\nEarly diagnosis is crucial for reducing mortality; however, challenges persist\ndue to limited oral health programs, inadequate infrastructure, and a shortage\nof healthcare practitioners. Conventional deep learning models, while\npromising, often rely on point estimates, leading to overconfidence and reduced\nreliability. Critically, these models require large datasets to mitigate\noverfitting and ensure generalizability, an unrealistic demand in settings with\nlimited training data. To address these issues, we propose a hybrid model that\ncombines a convolutional neural network (CNN) with Bayesian deep learning for\noral cancer classification using small training sets. This approach employs\nvariational inference to enhance reliability through uncertainty\nquantification. The model was trained on photographic color images captured by\nsmartphones and evaluated on three distinct test datasets. The proposed method\nachieved 94% accuracy on a test dataset with a distribution similar to that of\nthe training data, comparable to traditional CNN performance. Notably, for\nreal-world photographic image data, despite limitations and variations\ndiffering from the training dataset, the proposed model demonstrated superior\ngeneralizability, achieving 88% accuracy on diverse datasets compared to 72.94%\nfor traditional CNNs, even with a smaller dataset. Confidence analysis revealed\nthat the model exhibits low uncertainty (high confidence) for correctly\nclassified samples and high uncertainty (low confidence) for misclassified\nsamples. These results underscore the effectiveness of Bayesian inference in\ndata-scarce environments in enhancing early oral cancer diagnosis by improving\nmodel reliability and generalizability.",
      "authors": [
        "Akshay Bhagwan Sonawane",
        "Lena D. Swamikannan",
        "Lakshman Tamil"
      ],
      "published": "2025-10-02T00:40:53Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01547v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出一种结合CNN与贝叶斯深度学习的混合模型，用于小样本口腔癌分类。通过变分推理进行不确定性量化，在智能手机拍摄的彩色图像上实现94%的准确率，并在分布外数据上展现出优于传统CNN的泛化能力（88% vs 72.94%），有效提升了数据稀缺环境下的口腔癌早期诊断可靠性。",
      "order": 573
    },
    {
      "arxiv_id": "2510.01546v1",
      "title": "Growing Visual Generative Capacity for Pre-Trained MLLMs",
      "summary": "Multimodal large language models (MLLMs) extend the success of language\nmodels to visual understanding, and recent efforts have sought to build unified\nMLLMs that support both understanding and generation. However, constructing\nsuch models remains challenging: hybrid approaches combine continuous\nembeddings with diffusion or flow-based objectives, producing high-quality\nimages but breaking the autoregressive paradigm, while pure autoregressive\napproaches unify text and image prediction over discrete visual tokens but\noften face trade-offs between semantic alignment and pixel-level fidelity. In\nthis work, we present Bridge, a pure autoregressive unified MLLM that augments\npre-trained visual understanding models with generative ability through a\nMixture-of-Transformers architecture, enabling both image understanding and\ngeneration within a single next-token prediction framework. To further improve\nvisual generation fidelity, we propose a semantic-to-pixel discrete\nrepresentation that integrates compact semantic tokens with fine-grained pixel\ntokens, achieving strong language alignment and precise description of visual\ndetails with only a 7.9% increase in sequence length. Extensive experiments\nacross diverse multimodal benchmarks demonstrate that Bridge achieves\ncompetitive or superior results in both understanding and generation\nbenchmarks, while requiring less training data and reduced training time\ncompared to prior unified MLLMs.",
      "authors": [
        "Hanyu Wang",
        "Jiaming Han",
        "Ziyan Yang",
        "Qi Zhao",
        "Shanchuan Lin",
        "Xiangyu Yue",
        "Abhinav Shrivastava",
        "Zhenheng Yang",
        "Hao Chen"
      ],
      "published": "2025-10-02T00:40:02Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01546v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Bridge模型，一种纯自回归统一多模态大语言模型，通过混合Transformer架构增强预训练视觉理解模型的生成能力。该模型采用语义到像素的离散表示方法，在仅增加7.9%序列长度的情况下实现语言对齐和视觉细节精确描述，在多项多模态基准测试中取得优异性能。",
      "order": 574
    },
    {
      "arxiv_id": "2510.01540v1",
      "title": "Towards Better Optimization For Listwise Preference in Diffusion Models",
      "summary": "Reinforcement learning from human feedback (RLHF) has proven effectiveness\nfor aligning text-to-image (T2I) diffusion models with human preferences.\nAlthough Direct Preference Optimization (DPO) is widely adopted for its\ncomputational efficiency and avoidance of explicit reward modeling, its\napplications to diffusion models have primarily relied on pairwise preferences.\nThe precise optimization of listwise preferences remains largely unaddressed.\nIn practice, human feedback on image preferences often contains implicit ranked\ninformation, which conveys more precise human preferences than pairwise\ncomparisons. In this work, we propose Diffusion-LPO, a simple and effective\nframework for Listwise Preference Optimization in diffusion models with\nlistwise data. Given a caption, we aggregate user feedback into a ranked list\nof images and derive a listwise extension of the DPO objective under the\nPlackett-Luce model. Diffusion-LPO enforces consistency across the entire\nranking by encouraging each sample to be preferred over all of its lower-ranked\nalternatives. We empirically demonstrate the effectiveness of Diffusion-LPO\nacross various tasks, including text-to-image generation, image editing, and\npersonalized preference alignment. Diffusion-LPO consistently outperforms\npairwise DPO baselines on visual quality and preference alignment.",
      "authors": [
        "Jiamu Bai",
        "Xin Yu",
        "Meilong Xu",
        "Weitao Lu",
        "Xin Pan",
        "Kiwan Maeng",
        "Daniel Kifer",
        "Jian Wang",
        "Yu Wang"
      ],
      "published": "2025-10-02T00:26:37Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01540v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Diffusion-LPO框架，针对扩散模型中的列表偏好优化问题，通过将用户反馈聚合成图像排序列表并在Plackett-Luce模型下扩展DPO目标，实现在文本到图像生成、图像编辑和个性化偏好对齐等任务中优于传统成对DPO方法的性能表现。",
      "order": 575
    },
    {
      "arxiv_id": "2510.01532v1",
      "title": "MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised\n  Histopathology Segmentation",
      "summary": "In semi-supervised segmentation, capturing meaningful semantic structures\nfrom unlabeled data is essential. This is particularly challenging in\nhistopathology image analysis, where objects are densely distributed. To\naddress this issue, we propose a semi-supervised segmentation framework\ndesigned to robustly identify and preserve relevant topological features. Our\nmethod leverages multiple perturbed predictions obtained through stochastic\ndropouts and temporal training snapshots, enforcing topological consistency\nacross these varied outputs. This consistency mechanism helps distinguish\nbiologically meaningful structures from transient and noisy artifacts. A key\nchallenge in this process is to accurately match the corresponding topological\nfeatures across the predictions in the absence of ground truth. To overcome\nthis, we introduce a novel matching strategy that integrates spatial overlap\nwith global structural alignment, minimizing discrepancies among predictions.\nExtensive experiments demonstrate that our approach effectively reduces\ntopological errors, resulting in more robust and accurate segmentations\nessential for reliable downstream analysis. Code is available at\n\\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.",
      "authors": [
        "Meilong Xu",
        "Xiaoling Hu",
        "Shahira Abousamra",
        "Chen Li",
        "Chao Chen"
      ],
      "published": "2025-10-02T00:08:28Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01532v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "提出MATCH半监督分割框架，通过多扰动预测和拓扑一致性机制，在组织病理学图像中鲁棒识别生物结构，减少拓扑误差，提升分割准确性。",
      "order": 576
    },
    {
      "arxiv_id": "2510.01524v1",
      "title": "WALT: Web Agents that Learn Tools",
      "summary": "Web agents promise to automate complex browser tasks, but current methods\nremain brittle -- relying on step-by-step UI interactions and heavy LLM\nreasoning that break under dynamic layouts and long horizons. Humans, by\ncontrast, exploit website-provided functionality through high-level operations\nlike search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),\na framework that reverse-engineers latent website functionality into reusable\ninvocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust\nimplementations of automations already designed into websites -- spanning\ndiscovery (search, filter, sort), communication (post, comment, upvote), and\ncontent management (create, edit, delete). Tools abstract away low-level\nexecution: instead of reasoning about how to click and type, agents simply call\nsearch(query) or create(listing). This shifts the computational burden from\nfragile step-by-step reasoning to reliable tool invocation. On VisualWebArena\nand WebArena, WALT achieves higher success with fewer steps and less\nLLM-dependent reasoning, establishing a robust and generalizable paradigm for\nbrowser automation.",
      "authors": [
        "Viraj Prabhu",
        "Yutong Dai",
        "Matthew Fernandez",
        "Jing Gu",
        "Krithika Ramakrishnan",
        "Yanqi Luo",
        "Silvio Savarese",
        "Caiming Xiong",
        "Junnan Li",
        "Zeyuan Chen",
        "Ran Xu"
      ],
      "published": "2025-10-01T23:41:47Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01524v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "WALT提出了一种网络代理框架，通过逆向工程将网站功能转化为可复用的工具，使代理能够直接调用高级操作（如搜索、筛选、排序），而非依赖脆弱的逐步UI交互。该方法在VisualWebArena和WebArena基准测试中实现了更高成功率、更少步骤和更低的大模型依赖，为浏览器自动化提供了稳健且可泛化的解决方案。",
      "order": 577
    },
    {
      "arxiv_id": "2510.01513v1",
      "title": "From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods\n  for Multimodal Content Analysis and Understanding",
      "summary": "Analysis of multi-modal content can be tricky, computationally expensive, and\nrequire a significant amount of engineering efforts. Lots of work with\npre-trained models on static data is out there, yet fusing these opensource\nmodels and methods with complex data such as videos is relatively challenging.\nIn this paper, we present a framework that enables efficiently prototyping\npipelines for multi-modal content analysis. We craft a candidate recipe for a\npipeline, marrying a set of pre-trained models, to convert videos into a\ntemporal semi-structured data format. We translate this structure further to a\nframe-level indexed knowledge graph representation that is query-able and\nsupports continual learning, enabling the dynamic incorporation of new\ndomain-specific knowledge through an interactive medium.",
      "authors": [
        "Basem Rizk",
        "Joel Walsh",
        "Mark Core",
        "Benjamin Nye"
      ],
      "published": "2025-10-01T23:20:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01513v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一个多模态内容分析框架，通过整合预训练模型将视频转换为时序半结构化数据，并进一步构建可查询的帧级索引知识图谱，支持持续学习和动态融入领域知识。",
      "order": 578
    },
    {
      "arxiv_id": "2510.01502v1",
      "title": "Aligning Video Models with Human Social Judgments via Behavior-Guided\n  Fine-Tuning",
      "summary": "Humans intuitively perceive complex social signals in visual scenes, yet it\nremains unclear whether state-of-the-art AI models encode the same similarity\nstructure. We study (Q1) whether modern video and language models capture\nhuman-perceived similarity in social videos, and (Q2) how to instill this\nstructure into models using human behavioral data. To address this, we\nintroduce a new benchmark of over 49,000 odd-one-out similarity judgments on\n250 three-second video clips of social interactions, and discover a modality\ngap: despite the task being visual, caption-based language embeddings align\nbetter with human similarity than any pretrained video model. We close this gap\nby fine-tuning a TimeSformer video model on these human judgments with our\nnovel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning\npairwise distances to human similarity. This fine-tuning protocol yields\nsignificantly improved alignment with human perceptions on held-out videos in\nterms of both explained variance and odd-one-out triplet accuracy. Variance\npartitioning shows that the fine-tuned video model increases shared variance\nwith language embeddings and explains additional unique variance not captured\nby the language model. Finally, we test transfer via linear probes and find\nthat human-similarity fine-tuning strengthens the encoding of social-affective\nattributes (intimacy, valence, dominance, communication) relative to the\npretrained baseline. Overall, our findings highlight a gap in pretrained video\nmodels' social recognition and demonstrate that behavior-guided fine-tuning\nshapes video representations toward human social perception.",
      "authors": [
        "Kathy Garcia",
        "Leyla Isik"
      ],
      "published": "2025-10-01T22:29:55Z",
      "primary_category": "q-bio.NC",
      "arxiv_url": "https://arxiv.org/abs/2510.01502v1",
      "primary_area": "video_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过人类社交判断数据微调视频模型，填补了预训练视频模型在社交认知方面的差距。构建了包含49,000多个相似性判断的新基准，发现语言嵌入比视频模型更符合人类感知。采用混合三元组-RSA目标和LoRA微调方法，显著提升了视频模型与人类社交感知的对齐度，并增强了社交情感属性的编码能力。",
      "order": 579
    },
    {
      "arxiv_id": "2510.01498v1",
      "title": "AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA\n  Imaging",
      "summary": "While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic\naneurysms (AAA), the required iodinated contrast agents pose significant risks,\nincluding nephrotoxicity, patient allergies, and environmental harm. To reduce\ncontrast agent use, recent deep learning methods have focused on generating\nsynthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a\nmulti-stage pipeline that first generates images and then performs\nsegmentation, which leads to error accumulation and fails to leverage shared\nsemantic and anatomical structures. To address this, we propose a unified deep\nlearning framework that generates synthetic CECT images from NCCT scans while\nsimultaneously segmenting the aortic lumen and thrombus. Our approach\nintegrates conditional diffusion models (CDM) with multi-task learning,\nenabling end-to-end joint optimization of image synthesis and anatomical\nsegmentation. Unlike previous multitask diffusion models, our approach requires\nno initial predictions (e.g., a coarse segmentation mask), shares both encoder\nand decoder parameters across tasks, and employs a semi-supervised training\nstrategy to learn from scans with missing segmentation labels, a common\nconstraint in real-world clinical data. We evaluated our method on a cohort of\n264 patients, where it consistently outperformed state-of-the-art single-task\nand multi-stage models. For image synthesis, our model achieved a PSNR of 25.61\ndB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,\nit improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus\nDice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to\nmore accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm\nfrom 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to\nnnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.",
      "authors": [
        "Yuxuan Ou",
        "Ning Bi",
        "Jiazhen Pan",
        "Jiancheng Yang",
        "Boliang Yu",
        "Usama Zidan",
        "Regent Lee",
        "Vicente Grau"
      ],
      "published": "2025-10-01T22:19:27Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01498v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "AortaDiff提出一种统一的多任务扩散框架，从非对比CT扫描生成合成对比增强CT图像并同时分割主动脉腔和血栓。该方法结合条件扩散模型与多任务学习，在264名患者数据上验证，在图像合成质量和解剖分割精度上均优于现有单任务和多阶段模型。",
      "order": 580
    },
    {
      "arxiv_id": "2510.01478v1",
      "title": "Purrception: Variational Flow Matching for Vector-Quantized Image\n  Generation",
      "summary": "We introduce Purrception, a variational flow matching approach for\nvector-quantized image generation that provides explicit categorical\nsupervision while maintaining continuous transport dynamics. Our method adapts\nVariational Flow Matching to vector-quantized latents by learning categorical\nposteriors over codebook indices while computing velocity fields in the\ncontinuous embedding space. This combines the geometric awareness of continuous\nmethods with the discrete supervision of categorical approaches, enabling\nuncertainty quantification over plausible codes and temperature-controlled\ngeneration. We evaluate Purrception on ImageNet-1k 256x256 generation. Training\nconverges faster than both continuous flow matching and discrete flow matching\nbaselines while achieving competitive FID scores with state-of-the-art models.\nThis demonstrates that Variational Flow Matching can effectively bridge\ncontinuous transport and discrete supervision for improved training efficiency\nin image generation.",
      "authors": [
        "Răzvan-Andrei Matişan",
        "Vincent Tao Hu",
        "Grigory Bartosh",
        "Björn Ommer",
        "Cees G. M. Snoek",
        "Max Welling",
        "Jan-Willem van de Meent",
        "Mohammad Mahdi Derakhshani",
        "Floor Eijkelboom"
      ],
      "published": "2025-10-01T21:41:30Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01478v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "Purrception是一种用于矢量量化图像生成的变分流匹配方法，通过在连续嵌入空间学习速度场的同时学习码书索引的类别后验，将连续方法的几何感知与离散监督相结合。该方法在ImageNet-1k 256×256生成任务上训练收敛速度优于连续和离散流匹配基线，同时达到竞争力的FID分数。",
      "order": 581
    },
    {
      "arxiv_id": "2510.01454v1",
      "title": "Data Selection for Fine-tuning Vision Language Models via Cross Modal\n  Alignment Trajectories",
      "summary": "Data-efficient learning aims to eliminate redundancy in large training\ndatasets by training models on smaller subsets of the most informative\nexamples. While data selection has been extensively explored for vision models\nand large language models (LLMs), it remains underexplored for Large\nVision-Language Models (LVLMs). Notably, none of existing methods can\noutperform random selection at different subset sizes. In this work, we propose\nthe first principled method for data-efficient instruction tuning of LVLMs. We\nprove that examples with similar cross-modal attention matrices during\ninstruction tuning have similar gradients. Thus, they influence model\nparameters in a similar manner and convey the same information to the model\nduring training. Building on this insight, we propose XMAS, which clusters\nexamples based on the trajectories of the top singular values of their\nattention matrices obtained from fine-tuning a small proxy LVLM. By sampling a\nbalanced subset from these clusters, XMAS effectively removes redundancy in\nlarge-scale LVLM training data. Extensive experiments show that XMAS can\ndiscard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while\nfully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and\nspeeding up its training by 1.2x. This is 30% more data reduction compared to\nthe best baseline for LLaVA-665k. The project's website can be found at\nhttps://bigml-cs-ucla.github.io/XMAS-project-page/.",
      "authors": [
        "Nilay Naharas",
        "Dang Nguyen",
        "Nesihan Bulut",
        "Mohammadhossein Bateni",
        "Vahab Mirrokni",
        "Baharan Mirzasoleiman"
      ],
      "published": "2025-10-01T20:47:29Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01454v1",
      "primary_area": "vla_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出XMAS方法，通过分析跨模态注意力矩阵轨迹进行数据选择，首次实现视觉语言模型的高效指令调优。该方法可删除50%-85%训练数据而不损失性能，训练速度提升1.2倍，在10个下游任务上保持LLaVA-1.5-7B模型性能。",
      "order": 582
    },
    {
      "arxiv_id": "2510.01448v1",
      "title": "GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of\n  Geographic Embeddings",
      "summary": "Worldwide visual geo-localization seeks to determine the geographic location\nof an image anywhere on Earth using only its visual content. Learned\nrepresentations of geography for visual geo-localization remain an active\nresearch topic despite much progress. We formulate geo-localization as aligning\nthe visual representation of the query image with a learned geographic\nrepresentation. Our novel geographic representation explicitly models the world\nas a hierarchy of geographic embeddings. Additionally, we introduce an approach\nto efficiently fuse the appearance features of the query image with its\nsemantic segmentation map, forming a robust visual representation. Our main\nexperiments demonstrate improved all-time bests in 22 out of 25 metrics\nmeasured across five benchmark datasets compared to prior state-of-the-art\n(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional\nablation studies support the claim that these gains are primarily driven by the\ncombination of geographic and visual representations.",
      "authors": [
        "Angel Daruna",
        "Nicholas Meegan",
        "Han-Pang Chiu",
        "Supun Samarasekera",
        "Rakesh Kumar"
      ],
      "published": "2025-10-01T20:39:48Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01448v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "GeoSURGE提出了一种新颖的地理定位方法，通过层次化地理嵌入和语义分割特征融合，在视觉地理定位任务中实现了22/25项指标的最优性能，超越了现有SOTA方法和大型视觉语言模型。",
      "order": 583
    },
    {
      "arxiv_id": "2510.01432v1",
      "title": "On the Role of Domain Experts in Creating Effective Tutoring Systems",
      "summary": "The role that highly curated knowledge, provided by domain experts, could\nplay in creating effective tutoring systems is often overlooked within the AI\nfor education community. In this paper, we highlight this topic by discussing\ntwo ways such highly curated expert knowledge could help in creating novel\neducational systems. First, we will look at how one could use explainable AI\n(XAI) techniques to automatically create lessons. Most existing XAI methods are\nprimarily aimed at debugging AI systems. However, we will discuss how one could\nuse expert specified rules about solving specific problems along with novel XAI\ntechniques to automatically generate lessons that could be provided to\nlearners. Secondly, we will see how an expert specified curriculum for learning\na target concept can help develop adaptive tutoring systems, that can not only\nprovide a better learning experience, but could also allow us to use more\nefficient algorithms to create these systems. Finally, we will highlight the\nimportance of such methods using a case study of creating a tutoring system for\npollinator identification, where such knowledge could easily be elicited from\nexperts.",
      "authors": [
        "Sarath Sreedharan",
        "Kelsey Sikes",
        "Nathaniel Blanchard",
        "Lisa Mason",
        "Nikhil Krishnaswamy",
        "Jill Zarestky"
      ],
      "published": "2025-10-01T20:12:57Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01432v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "education_ai",
      "tldr_zh": "本文探讨领域专家知识在构建智能教学系统中的关键作用，提出利用可解释AI技术自动生成课程内容，并通过专家设计的课程体系开发自适应教学系统，以传粉昆虫识别教学为例验证了方法的有效性。",
      "order": 584
    },
    {
      "arxiv_id": "2510.01407v1",
      "title": "Ultra-Efficient Decoding for End-to-End Neural Compression and\n  Reconstruction",
      "summary": "Image compression and reconstruction are crucial for various digital\napplications. While contemporary neural compression methods achieve impressive\ncompression rates, the adoption of such technology has been largely hindered by\nthe complexity and large computational costs of the convolution-based decoders\nduring data reconstruction. To address the decoder bottleneck in neural\ncompression, we develop a new compression-reconstruction framework based on\nincorporating low-rank representation in an autoencoder with vector\nquantization. We demonstrated that performing a series of computationally\nefficient low-rank operations on the learned latent representation of images\ncan efficiently reconstruct the data with high quality. Our approach\ndramatically reduces the computational overhead in the decoding phase of neural\ncompression/reconstruction, essentially eliminating the decoder compute\nbottleneck while maintaining high fidelity of image outputs.",
      "authors": [
        "Ethan G. Rogers",
        "Cheng Wang"
      ],
      "published": "2025-10-01T19:42:59Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01407v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于低秩表示和向量量化的神经压缩重建框架，通过高效的低秩运算显著降低解码阶段计算成本，在保持图像高质量重建的同时基本消除解码器计算瓶颈。",
      "order": 585
    },
    {
      "arxiv_id": "2510.01399v1",
      "title": "DisCo: Reinforcement with Diversity Constraints for Multi-Human\n  Generation",
      "summary": "State-of-the-art text-to-image models excel at realism but collapse on\nmulti-human prompts - duplicating faces, merging identities, and miscounting\nindividuals. We introduce DisCo (Reinforcement with Diversity Constraints), the\nfirst RL-based framework to directly optimize identity diversity in multi-human\ngeneration. DisCo fine-tunes flow-matching models via Group-Relative Policy\nOptimization (GRPO) with a compositional reward that (i) penalizes intra-image\nfacial similarity, (ii) discourages cross-sample identity repetition, (iii)\nenforces accurate person counts, and (iv) preserves visual fidelity through\nhuman preference scores. A single-stage curriculum stabilizes training as\ncomplexity scales, requiring no extra annotations. On the DiverseHumans\nTestset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global\nIdentity Spread - surpassing both open-source and proprietary methods (e.g.,\nGemini, GPT-Image) while maintaining competitive perceptual quality. Our\nresults establish DisCo as a scalable, annotation-free solution that resolves\nthe long-standing identity crisis in generative models and sets a new benchmark\nfor compositional multi-human generation.",
      "authors": [
        "Shubhankar Borse",
        "Farzad Farhadzadeh",
        "Munawar Hayat",
        "Fatih Porikli"
      ],
      "published": "2025-10-01T19:28:51Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01399v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "DisCo提出首个基于强化学习的多人生成框架，通过多样性约束优化身份多样性。该方法使用组相对策略优化和组合奖励函数，惩罚面部相似性、身份重复，确保准确人数统计并保持视觉质量。在DiverseHumans测试集上达到98.6%独特面部准确率，超越现有方法，解决了生成模型中长期存在的身份危机问题。",
      "order": 586
    },
    {
      "arxiv_id": "2510.01388v1",
      "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned\n  Navigation",
      "summary": "Robots must adapt to diverse human instructions and operate safely in\nunstructured, open-world environments. Recent Vision-Language models (VLMs)\noffer strong priors for grounding language and perception, but remain difficult\nto steer for navigation due to differences in action spaces and pretraining\nobjectives that hamper transferability to robotics tasks. Towards addressing\nthis, we introduce VENTURA, a vision-language navigation system that finetunes\ninternet-pretrained image diffusion models for path planning. Instead of\ndirectly predicting low-level actions, VENTURA generates a path mask (i.e. a\nvisual plan) in image space that captures fine-grained, context-aware\nnavigation behaviors. A lightweight behavior-cloning policy grounds these\nvisual plans into executable trajectories, yielding an interface that follows\nnatural language instructions to generate diverse robot behaviors. To scale\ntraining, we supervise on path masks derived from self-supervised tracking\nmodels paired with VLM-augmented captions, avoiding manual pixel-level\nannotation or highly engineered data collection setups. In extensive real-world\nevaluations, VENTURA outperforms state-of-the-art foundation model baselines on\nobject reaching, obstacle avoidance, and terrain preference tasks, improving\nsuccess rates by 33% and reducing collisions by 54% across both seen and unseen\nscenarios. Notably, we find that VENTURA generalizes to unseen combinations of\ndistinct tasks, revealing emergent compositional capabilities. Videos, code,\nand additional materials: https://venturapath.github.io",
      "authors": [
        "Arthur Zhang",
        "Xiangyun Meng",
        "Luca Calliari",
        "Dong-Ki Kim",
        "Shayegan Omidshafiei",
        "Joydeep Biswas",
        "Ali Agha",
        "Amirreza Shaban"
      ],
      "published": "2025-10-01T19:21:28Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01388v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "VENTURA是一种视觉语言导航系统，通过微调图像扩散模型生成路径掩码进行路径规划，结合轻量级行为克隆策略执行轨迹，在真实世界评估中相比基线方法成功率提升33%，碰撞率降低54%，并展现出对未见任务组合的泛化能力。",
      "order": 587
    },
    {
      "arxiv_id": "2510.01370v1",
      "title": "SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs",
      "summary": "We introduce Small PDE U-Net Solver (SPUS), a compact and efficient\nfoundation model (FM) designed as a unified neural operator for solving a wide\nrange of partial differential equations (PDEs). Unlike existing\nstate-of-the-art PDE FMs-primarily based on large complex transformer\narchitectures with high computational and parameter overhead-SPUS leverages a\nlightweight residual U-Net-based architecture that has been largely\nunderexplored as a foundation model architecture in this domain. To enable\neffective learning in this minimalist framework, we utilize a simple yet\npowerful auto-regressive pretraining strategy which closely replicates the\nbehavior of numerical solvers to learn the underlying physics. SPUS is\npretrained on a diverse set of fluid dynamics PDEs and evaluated across 6\nchallenging unseen downstream PDEs spanning various physical systems.\nExperimental results demonstrate that SPUS using residual U-Net based\narchitecture achieves state-of-the-art generalization on these downstream tasks\nwhile requiring significantly fewer parameters and minimal fine-tuning data,\nhighlighting its potential as a highly parameter-efficient FM for solving\ndiverse PDE systems.",
      "authors": [
        "Abu Bucker Siddik",
        "Diane Oyen",
        "Alexander Most",
        "Michal Kucer",
        "Ayan Biswas"
      ],
      "published": "2025-10-01T18:54:59Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01370v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "SPUS是一种轻量级参数高效的偏微分方程求解基础模型，采用残差U-Net架构和自回归预训练策略，在多种PDE任务上实现最先进泛化性能，显著减少参数需求和微调数据。",
      "order": 588
    },
    {
      "arxiv_id": "2510.01362v1",
      "title": "EvoStruggle: A Dataset Capturing the Evolution of Struggle across\n  Activities and Skill Levels",
      "summary": "The ability to determine when a person struggles during skill acquisition is\ncrucial for both optimizing human learning and enabling the development of\neffective assistive systems. As skills develop, the type and frequency of\nstruggles tend to change, and understanding this evolution is key to\ndetermining the user's current stage of learning. However, existing\nmanipulation datasets have not focused on how struggle evolves over time. In\nthis work, we collect a dataset for struggle determination, featuring 61.68\nhours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle\nsegments collected from 76 participants. The dataset includes 18 tasks grouped\ninto four diverse activities -- tying knots, origami, tangram puzzles, and\nshuffling cards, representing different task variations. In addition,\nparticipants repeated the same task five times to capture their evolution of\nskill. We define the struggle determination problem as a temporal action\nlocalization task, focusing on identifying and precisely localizing struggle\nsegments with start and end times. Experimental results show that Temporal\nAction Localization models can successfully learn to detect struggle cues, even\nwhen evaluated on unseen tasks or activities. The models attain an overall\naverage mAP of 34.56% when generalizing across tasks and 19.24% across\nactivities, indicating that struggle is a transferable concept across various\nskill-based tasks while still posing challenges for further improvement in\nstruggle detection. Our dataset is available at\nhttps://github.com/FELIXFENG2019/EvoStruggle.",
      "authors": [
        "Shijia Feng",
        "Michael Wray",
        "Walterio Mayol-Cuevas"
      ],
      "published": "2025-10-01T18:41:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01362v1",
      "primary_area": "video_models",
      "secondary_focus": "tech_reports",
      "application_domain": "education_ai",
      "tldr_zh": "EvoStruggle数据集包含61.68小时视频、2,793个视频片段和5,385个标注的时间段，记录了76名参与者在18项任务中的技能学习过程。该数据集通过时间动作定位模型分析学习过程中的困难演变，在跨任务和跨活动测试中分别达到34.56%和19.24%的平均mAP，表明困难检测是可迁移的概念，对优化人类学习和开发辅助系统具有重要意义。",
      "order": 589
    },
    {
      "arxiv_id": "2510.01361v1",
      "title": "An Efficient Quality Metric for Video Frame Interpolation Based on\n  Motion-Field Divergence",
      "summary": "Video frame interpolation is a fundamental tool for temporal video\nenhancement, but existing quality metrics struggle to evaluate the perceptual\nimpact of interpolation artefacts effectively. Metrics like PSNR, SSIM and\nLPIPS ignore temporal coherence. State-of-the-art quality metrics tailored\ntowards video frame interpolation, like FloLPIPS, have been developed but\nsuffer from computational inefficiency that limits their practical application.\nWe present $\\text{PSNR}_{\\text{DIV}}$, a novel full-reference quality metric\nthat enhances PSNR through motion divergence weighting, a technique adapted\nfrom archival film restoration where it was developed to detect temporal\ninconsistencies. Our approach highlights singularities in motion fields which\nis then used to weight image errors. Evaluation on the BVI-VFI dataset (180\nsequences across multiple frame rates, resolutions and interpolation methods)\nshows $\\text{PSNR}_{\\text{DIV}}$ achieves statistically significant\nimprovements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while\nbeing 2.5$\\times$ faster and using 4$\\times$ less memory. Performance remains\nconsistent across all content categories and are robust to the motion estimator\nused. The efficiency and accuracy of $\\text{PSNR}_{\\text{DIV}}$ enables fast\nquality evaluation and practical use as a loss function for training neural\nnetworks for video frame interpolation tasks. An implementation of our metric\nis available at www.github.com/conalld/psnr-div.",
      "authors": [
        "Conall Daly",
        "Darren Ramsook",
        "Anil Kokaram"
      ],
      "published": "2025-10-01T18:40:38Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.01361v1",
      "primary_area": "video_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于运动场散度的视频帧插值质量评估指标PSNR_DIV，通过运动发散加权改进PSNR，在BVI-VFI数据集上相比FloLPIPS提升0.09皮尔逊相关系数，同时速度提升2.5倍、内存占用减少4倍，适用于神经网络训练损失函数。",
      "order": 590
    },
    {
      "arxiv_id": "2510.01347v1",
      "title": "Image Generation Based on Image Style Extraction",
      "summary": "Image generation based on text-to-image generation models is a task with\npractical application scenarios that fine-grained styles cannot be precisely\ndescribed and controlled in natural language, while the guidance information of\nstylized reference images is difficult to be directly aligned with the textual\nconditions of traditional textual guidance generation. This study focuses on\nhow to maximize the generative capability of the pretrained generative model,\nby obtaining fine-grained stylistic representations from a single given\nstylistic reference image, and injecting the stylistic representations into the\ngenerative body without changing the structural framework of the downstream\ngenerative model, so as to achieve fine-grained controlled stylized image\ngeneration. In this study, we propose a three-stage training style\nextraction-based image generation method, which uses a style encoder and a\nstyle projection layer to align the style representations with the textual\nrepresentations to realize fine-grained textual cue-based style guide\ngeneration. In addition, this study constructs the Style30k-captions dataset,\nwhose samples contain a triad of images, style labels, and text descriptions,\nto train the style encoder and style projection layer in this experiment.",
      "authors": [
        "Shuochen Chang"
      ],
      "published": "2025-10-01T18:23:09Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01347v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出一种基于风格提取的三阶段图像生成方法，通过风格编码器和投影层将参考图像的细粒度风格表征与文本表征对齐，实现基于文本提示的精细化风格控制图像生成，并构建了包含图像-风格标签-文本描述三元组的Style30k-captions数据集。",
      "order": 591
    },
    {
      "arxiv_id": "2510.01339v1",
      "title": "LVTINO: LAtent Video consisTency INverse sOlver for High Definition\n  Video Restoration",
      "summary": "Computational imaging methods increasingly rely on powerful generative\ndiffusion models to tackle challenging image restoration tasks. In particular,\nstate-of-the-art zero-shot image inverse solvers leverage distilled\ntext-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy\nand perceptual quality with high computational efficiency. However, extending\nthese advances to high-definition video restoration remains a significant\nchallenge, due to the need to recover fine spatial detail while capturing\nsubtle temporal dependencies. Consequently, methods that naively apply\nimage-based LDM priors on a frame-by-frame basis often result in temporally\ninconsistent reconstructions. We address this challenge by leveraging recent\nadvances in Video Consistency Models (VCMs), which distill video latent\ndiffusion models into fast generators that explicitly capture temporal\ncausality. Building on this foundation, we propose LVTINO, the first zero-shot\nor plug-and-play inverse solver for high definition video restoration with\npriors encoded by VCMs. Our conditioning mechanism bypasses the need for\nautomatic differentiation and achieves state-of-the-art video reconstruction\nquality with only a few neural function evaluations, while ensuring strong\nmeasurement consistency and smooth temporal transitions across frames.\nExtensive experiments on a diverse set of video inverse problems show\nsignificant perceptual improvements over current state-of-the-art methods that\napply image LDMs frame by frame, establishing a new benchmark in both\nreconstruction fidelity and computational efficiency.",
      "authors": [
        "Alessio Spagnoletti",
        "Andrés Almansa",
        "Marcelo Pereyra"
      ],
      "published": "2025-10-01T18:10:08Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01339v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "LVTINO是一种基于视频一致性模型的零样本高清视频修复方法，通过利用视频潜在扩散模型的蒸馏技术，在保持测量一致性和时间平滑性的同时，显著提升了视频重建质量与计算效率。",
      "order": 592
    },
    {
      "arxiv_id": "2510.01186v1",
      "title": "IMAGEdit: Let Any Subject Transform",
      "summary": "In this paper, we present IMAGEdit, a training-free framework for any number\nof video subject editing that manipulates the appearances of multiple\ndesignated subjects while preserving non-target regions, without finetuning or\nretraining. We achieve this by providing robust multimodal conditioning and\nprecise mask sequences through a prompt-guided multimodal alignment module and\na prior-based mask retargeting module. We first leverage large models'\nunderstanding and generation capabilities to produce multimodal information and\nmask motion sequences for multiple subjects across various types. Then, the\nobtained prior mask sequences are fed into a pretrained mask-driven video\ngeneration model to synthesize the edited video. With strong generalization\ncapability, IMAGEdit remedies insufficient prompt-side multimodal conditioning\nand overcomes mask boundary entanglement in videos with any number of subjects,\nthereby significantly expanding the applicability of video editing. More\nimportantly, IMAGEdit is compatible with any mask-driven video generation\nmodel, significantly improving overall performance. Extensive experiments on\nour newly constructed multi-subject benchmark MSVBench verify that IMAGEdit\nconsistently surpasses state-of-the-art methods. Code, models, and datasets are\npublicly available at https://github.com/XWH-A/IMAGEdit.",
      "authors": [
        "Fei Shen",
        "Weihao Xu",
        "Rui Yan",
        "Dong Zhang",
        "Xiangbo Shu",
        "Jinhui Tang"
      ],
      "published": "2025-10-01T17:59:56Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01186v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "IMAGEdit是一种无需训练的通用视频编辑框架，通过多模态对齐模块和基于先验的掩码重定向模块，实现对任意数量视频主体的外观编辑，同时保持非目标区域不变。该方法兼容各类掩码驱动视频生成模型，在新建的多主体基准MSVBench上表现优于现有技术。",
      "order": 593
    },
    {
      "arxiv_id": "2510.01183v1",
      "title": "EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory",
      "summary": "Humans possess a remarkable ability to mentally explore and replay 3D\nenvironments they have previously experienced. Inspired by this mental process,\nwe present EvoWorld: a world model that bridges panoramic video generation with\nevolving 3D memory to enable spatially consistent long-horizon exploration.\nGiven a single panoramic image as input, EvoWorld first generates future video\nframes by leveraging a video generator with fine-grained view control, then\nevolves the scene's 3D reconstruction using a feedforward plug-and-play\ntransformer, and finally synthesizes futures by conditioning on geometric\nreprojections from this evolving explicit 3D memory. Unlike prior\nstate-of-the-arts that synthesize videos only, our key insight lies in\nexploiting this evolving 3D reconstruction as explicit spatial guidance for the\nvideo generation process, projecting the reconstructed geometry onto target\nviewpoints to provide rich spatial cues that significantly enhance both visual\nrealism and geometric consistency. To evaluate long-range exploration\ncapabilities, we introduce the first comprehensive benchmark spanning synthetic\noutdoor environments, Habitat indoor scenes, and challenging real-world\nscenarios, with particular emphasis on loop-closure detection and spatial\ncoherence over extended trajectories. Extensive experiments demonstrate that\nour evolving 3D memory substantially improves visual fidelity and maintains\nspatial scene coherence compared to existing approaches, representing a\nsignificant advance toward long-horizon spatially consistent world modeling.",
      "authors": [
        "Jiahao Wang",
        "Luoxin Ye",
        "TaiMing Lu",
        "Junfei Xiao",
        "Jiahan Zhang",
        "Yuxiang Guo",
        "Xijun Liu",
        "Rama Chellappa",
        "Cheng Peng",
        "Alan Yuille",
        "Jieneng Chen"
      ],
      "published": "2025-10-01T17:59:38Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01183v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "EvoWorld提出了一种结合全景视频生成与演化3D记忆的世界模型，通过显式3D重建为视频生成提供空间引导，显著提升了长时程探索中的视觉真实感和几何一致性。该方法在合成户外、室内及真实场景的基准测试中表现出色，代表了空间一致世界建模的重要进展。",
      "order": 594
    },
    {
      "arxiv_id": "2510.01176v1",
      "title": "Audio Driven Real-Time Facial Animation for Social Telepresence",
      "summary": "We present an audio-driven real-time system for animating photorealistic 3D\nfacial avatars with minimal latency, designed for social interactions in\nvirtual reality for anyone. Central to our approach is an encoder model that\ntransforms audio signals into latent facial expression sequences in real time,\nwhich are then decoded as photorealistic 3D facial avatars. Leveraging the\ngenerative capabilities of diffusion models, we capture the rich spectrum of\nfacial expressions necessary for natural communication while achieving\nreal-time performance (<15ms GPU time). Our novel architecture minimizes\nlatency through two key innovations: an online transformer that eliminates\ndependency on future inputs and a distillation pipeline that accelerates\niterative denoising into a single step. We further address critical design\nchallenges in live scenarios for processing continuous audio signals\nframe-by-frame while maintaining consistent animation quality. The versatility\nof our framework extends to multimodal applications, including semantic\nmodalities such as emotion conditions and multimodal sensors with head-mounted\neye cameras on VR headsets. Experimental results demonstrate significant\nimprovements in facial animation accuracy over existing offline\nstate-of-the-art baselines, achieving 100 to 1000 times faster inference speed.\nWe validate our approach through live VR demonstrations and across various\nscenarios such as multilingual speeches.",
      "authors": [
        "Jiye Lee",
        "Chenghui Li",
        "Linh Tran",
        "Shih-En Wei",
        "Jason Saragih",
        "Alexander Richard",
        "Hanbyul Joo",
        "Shaojie Bai"
      ],
      "published": "2025-10-01T17:57:05Z",
      "primary_category": "cs.GR",
      "arxiv_url": "https://arxiv.org/abs/2510.01176v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "提出基于音频驱动的实时3D面部动画系统，采用扩散模型将语音信号转换为逼真面部表情，通过在线Transformer和蒸馏技术实现毫秒级延迟，适用于VR社交临场感场景，推理速度比现有方法快100-1000倍。",
      "order": 595
    },
    {
      "arxiv_id": "2510.01174v1",
      "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
      "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.",
      "authors": [
        "Yanzhe Chen",
        "Kevin Qinghong Lin",
        "Mike Zheng Shou"
      ],
      "published": "2025-10-01T17:56:48Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01174v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "education_ai",
      "tldr_zh": "Code2Video提出一种以代码为中心的教育视频生成框架，通过三个协作智能体（规划器、编码器、评审器）将教学内容转换为可执行的Python代码并生成专业教育视频。该方法在MMMC基准测试中表现优异，视频质量接近人工制作教程，相比直接代码生成提升40%效果。",
      "order": 596
    },
    {
      "arxiv_id": "2510.01173v1",
      "title": "EditTrack: Detecting and Attributing AI-assisted Image Editing",
      "summary": "In this work, we formulate and study the problem of image-editing detection\nand attribution: given a base image and a suspicious image, detection seeks to\ndetermine whether the suspicious image was derived from the base image using an\nAI editing model, while attribution further identifies the specific editing\nmodel responsible. Existing methods for detecting and attributing AI-generated\nimages are insufficient for this problem, as they focus on determining whether\nan image was AI-generated/edited rather than whether it was edited from a\nparticular base image. To bridge this gap, we propose EditTrack, the first\nframework for this image-editing detection and attribution problem. Building on\nfour key observations about the editing process, EditTrack introduces a novel\nre-editing strategy and leverages carefully designed similarity metrics to\ndetermine whether a suspicious image originates from a base image and, if so,\nby which model. We evaluate EditTrack on five state-of-the-art editing models\nacross six datasets, demonstrating that it consistently achieves accurate\ndetection and attribution, significantly outperforming five baselines.",
      "authors": [
        "Zhengyuan Jiang",
        "Yuyang Zhang",
        "Moyang Guo",
        "Neil Zhenqiang Gong"
      ],
      "published": "2025-10-01T17:56:35Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01173v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "EditTrack是首个检测和归因AI辅助图像编辑的框架，通过重新编辑策略和相似度度量，判断可疑图像是否源自特定基础图像及使用的编辑模型，在多个数据集上显著优于现有方法。",
      "order": 597
    },
    {
      "arxiv_id": "2510.01126v1",
      "title": "Strategic Fusion of Vision Language Models: Shapley-Credited\n  Context-Aware Dawid-Skene for Multi-Label Tasks in Autonomous Driving",
      "summary": "Large vision-language models (VLMs) are increasingly used in\nautonomous-vehicle (AV) stacks, but hallucination limits their reliability in\nsafety-critical pipelines. We present Shapley-credited Context-Aware\nDawid-Skene with Agreement, a game-theoretic fusion method for multi-label\nunderstanding of ego-view dashcam video. It learns per-model, per-label,\ncontext-conditioned reliabilities from labelled history and, at inference,\nconverts each model's report into an agreement-guardrailed log-likelihood ratio\nthat is combined with a contextual prior and a public reputation state updated\nvia Shapley-based team credit. The result is calibrated, thresholdable\nposteriors that (i) amplify agreement among reliable models, (ii) preserve\nuniquely correct single-model signals, and (iii) adapt to drift. To specialise\ngeneral VLMs, we curate 1,000 real-world dashcam clips with structured\nannotations (scene description, manoeuvre recommendation, rationale) via an\nautomatic pipeline that fuses HDD ground truth, vehicle kinematics, and YOLOv11\n+ BoT-SORT tracking, guided by a three-step chain-of-thought prompt; three\nheterogeneous VLMs are then fine-tuned with LoRA. We evaluate with Hamming\ndistance, Micro-Macro-F1, and average per-video latency. Empirically, the\nproposed method achieves a 23% reduction in Hamming distance, 55% improvement\nin Macro-F1, and 47% improvement in Micro-F1 when comparing with the best\nsingle model, supporting VLM fusion as a calibrated, interpretable, and robust\ndecision-support component for AV pipelines.",
      "authors": [
        "Yuxiang Feng",
        "Keyang Zhang",
        "Hassane Ouchouid",
        "Ashwil Kaniamparambil",
        "Ioannis Souflas",
        "Panagiotis Angeloudis"
      ],
      "published": "2025-10-01T17:14:11Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01126v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "提出基于Shapley值的上下文感知Dawid-Skene融合方法，通过博弈论框架集成多个视觉语言模型，在自动驾驶多标签任务中实现23%汉明距离降低和55%宏F1提升，有效缓解模型幻觉问题并保持决策可解释性。",
      "order": 598
    },
    {
      "arxiv_id": "2510.01119v1",
      "title": "Instant4D: 4D Gaussian Splatting in Minutes",
      "summary": "Dynamic view synthesis has seen significant advances, yet reconstructing\nscenes from uncalibrated, casual video remains challenging due to slow\noptimization and complex parameter estimation. In this work, we present\nInstant4D, a monocular reconstruction system that leverages native 4D\nrepresentation to efficiently process casual video sequences within minutes,\nwithout calibrated cameras or depth sensors. Our method begins with geometric\nrecovery through deep visual SLAM, followed by grid pruning to optimize scene\nrepresentation. Our design significantly reduces redundancy while maintaining\ngeometric integrity, cutting model size to under 10% of its original footprint.\nTo handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian\nrepresentation, achieving a 30x speed-up and reducing training time to within\ntwo minutes, while maintaining competitive performance across several\nbenchmarks. Our method reconstruct a single video within 10 minutes on the\nDycheck dataset or for a typical 200-frame video. We further apply our model to\nin-the-wild videos, showcasing its generalizability. Our project website is\npublished at https://instant4d.github.io/.",
      "authors": [
        "Zhanpeng Luo",
        "Haoxi Ran",
        "Li Lu"
      ],
      "published": "2025-10-01T17:07:21Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01119v1",
      "primary_area": "video_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "Instant4D是一种单目4D重建系统，通过深度视觉SLAM和网格剪枝技术，在无需标定相机或深度传感器的情况下，能在10分钟内从普通视频重建动态场景。该方法将模型大小压缩至原尺寸10%以下，训练速度提升30倍，在多个基准测试中保持竞争力。",
      "order": 599
    },
    {
      "arxiv_id": "2510.01061v1",
      "title": "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced\n  Wasserstein Distance for Variance Reduction",
      "summary": "Distribution matching is central to many vision and graphics tasks, where the\nwidely used Wasserstein distance is too costly to compute for high dimensional\ndistributions. The Sliced Wasserstein Distance (SWD) offers a scalable\nalternative, yet its Monte Carlo estimator suffers from high variance,\nresulting in noisy gradients and slow convergence. We introduce Reservoir SWD\n(ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively\nretain informative projection directions in optimization steps, resulting in\nstable gradients while remaining unbiased. Experiments on synthetic benchmarks\nand real-world tasks such as color correction and diffusion guidance show that\nReSWD consistently outperforms standard SWD and other variance reduction\nbaselines. Project page: https://reservoirswd.github.io/",
      "authors": [
        "Mark Boss",
        "Andreas Engelhardt",
        "Simon Donné",
        "Varun Jampani"
      ],
      "published": "2025-10-01T16:01:17Z",
      "primary_category": "cs.GR",
      "arxiv_url": "https://arxiv.org/abs/2510.01061v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "提出ReSWD方法，结合加权储层采样与切片Wasserstein距离，通过自适应保留优化过程中的关键投影方向来降低方差，在颜色校正和扩散引导等任务中表现优于标准SWD。",
      "order": 600
    },
    {
      "arxiv_id": "2510.01049v1",
      "title": "KeySG: Hierarchical Keyframe-Based 3D Scene Graphs",
      "summary": "In recent years, 3D scene graphs have emerged as a powerful world\nrepresentation, offering both geometric accuracy and semantic richness.\nCombining 3D scene graphs with large language models enables robots to reason,\nplan, and navigate in complex human-centered environments. However, current\napproaches for constructing 3D scene graphs are semantically limited to a\npredefined set of relationships, and their serialization in large environments\ncan easily exceed an LLM's context window. We introduce KeySG, a framework that\nrepresents 3D scenes as a hierarchical graph consisting of floors, rooms,\nobjects, and functional elements, where nodes are augmented with multi-modal\ninformation extracted from keyframes selected to optimize geometric and visual\ncoverage. The keyframes allow us to efficiently leverage VLM to extract scene\ninformation, alleviating the need to explicitly model relationship edges\nbetween objects, enabling more general, task-agnostic reasoning and planning.\nOur approach can process complex and ambiguous queries while mitigating the\nscalability issues associated with large scene graphs by utilizing a\nhierarchical retrieval-augmented generation (RAG) pipeline to extract relevant\ncontext from the graph. Evaluated across four distinct benchmarks -- including\n3D object segmentation and complex query retrieval -- KeySG outperforms prior\napproaches on most metrics, demonstrating its superior semantic richness and\nefficiency.",
      "authors": [
        "Abdelrhman Werby",
        "Dennis Rotondi",
        "Fabio Scaparro",
        "Kai O. Arras"
      ],
      "published": "2025-10-01T15:53:27Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01049v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "KeySG提出了一种基于关键帧的分层3D场景图框架，通过构建包含楼层、房间、物体和功能元素的多层次图结构，结合视觉语言模型提取多模态信息。该方法采用分层检索增强生成管道处理复杂查询，在四个基准测试中表现出优越的语义丰富性和效率，解决了传统3D场景图关系预定义限制和规模扩展问题。",
      "order": 601
    },
    {
      "arxiv_id": "2510.01047v1",
      "title": "Authentic Discrete Diffusion Model",
      "summary": "We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally\nredefines prior pseudo-discrete approaches by preserving core diffusion\ncharacteristics directly in the one-hot space through a suite of coordinated\nmechanisms. Unlike conventional \"pseudo\" discrete diffusion (PDD) methods, ADD\nreformulates the diffusion input by directly using float-encoded one-hot class\ndata, without relying on diffusing in the continuous latent spaces or masking\npolicies. At its core, a timestep-conditioned cross-entropy loss is introduced\nbetween the diffusion model's outputs and the original one-hot labels. This\nsynergistic design establishes a bridge between discriminative and generative\nlearning. Our experiments demonstrate that ADD not only achieves superior\nperformance on classification tasks compared to the baseline, but also exhibits\nexcellent text generation capabilities on Image captioning. Extensive ablations\nvalidate the measurable gains of each component.",
      "authors": [
        "Xiao Li",
        "Jiaqi Zhang",
        "Shuxiang Zhang",
        "Tianshui Chen",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "published": "2025-10-01T15:51:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01047v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出真实离散扩散(ADD)框架，通过在一热空间中直接保留核心扩散特性，重新定义离散扩散方法。该框架使用浮点编码的一热类别数据作为输入，引入时间步条件交叉熵损失，在分类任务上表现优于基线，并在图像描述生成中展现出优秀文本生成能力。",
      "order": 602
    },
    {
      "arxiv_id": "2510.01038v1",
      "title": "Activation-Deactivation: A General Framework for Robust Post-hoc\n  Explainable AI",
      "summary": "Black-box explainability methods are popular tools for explaining the\ndecisions of image classifiers. A major drawback of these tools is their\nreliance on mutants obtained by occluding parts of the input, leading to\nout-of-distribution images. This raises doubts about the quality of the\nexplanations. Moreover, choosing an appropriate occlusion value often requires\ndomain knowledge. In this paper we introduce a novel forward-pass paradigm\nActivation-Deactivation (AD), which removes the effects of occluded input\nfeatures from the model's decision-making by switching off the parts of the\nmodel that correspond to the occlusions. We introduce ConvAD, a drop-in\nmechanism that can be easily added to any trained Convolutional Neural Network\n(CNN), and which implements the AD paradigm. This leads to more robust\nexplanations without any additional training or fine-tuning. We prove that the\nConvAD mechanism does not change the decision-making process of the network. We\nprovide experimental evaluation across several datasets and model\narchitectures. We compare the quality of AD-explanations with explanations\nachieved using a set of masking values, using the proxies of robustness, size,\nand confidence drop-off. We observe a consistent improvement in robustness of\nAD explanations (up to 62.5%) compared to explanations obtained with\nocclusions, demonstrating that ConvAD extracts more robust explanations without\nthe need for domain knowledge.",
      "authors": [
        "Akchunya Chanchal",
        "David A. Kelly",
        "Hana Chockler"
      ],
      "published": "2025-10-01T15:42:58Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01038v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出激活-失活(AD)框架，通过关闭模型中与遮挡输入特征对应的部分来改进黑盒可解释AI方法。提出的ConvAD机制可轻松集成到任何训练好的CNN中，无需额外训练即可生成更鲁棒的解释，实验显示其鲁棒性比传统遮挡方法提升高达62.5%。",
      "order": 603
    },
    {
      "arxiv_id": "2510.01031v1",
      "title": "Secure and reversible face anonymization with diffusion models",
      "summary": "Face images processed by computer vision algorithms contain sensitive\npersonal information that malicious actors can capture without consent. These\nprivacy and security risks highlight the need for effective face anonymization\nmethods. Current methods struggle to propose a good trade-off between a secure\nscheme with high-quality image generation and reversibility for later person\nauthentication. Diffusion-based approaches produce high-quality anonymized\nimages but lack the secret key mechanism to ensure that only authorized parties\ncan reverse the process. In this paper, we introduce, to our knowledge, the\nfirst secure, high-quality reversible anonymization method based on a diffusion\nmodel. We propose to combine the secret key with the latent faces\nrepresentation of the diffusion model. To preserve identity-irrelevant\nfeatures, generation is constrained by a facial mask, maintaining high-quality\nimages. By using a deterministic forward and backward diffusion process, our\napproach enforces that the original face can be recovered with the correct\nsecret key. We also show that the proposed method produces anonymized faces\nthat are less visually similar to the original faces, compared to other\nprevious work.",
      "authors": [
        "Pol Labarbarie",
        "Vincent Itier",
        "William Puech"
      ],
      "published": "2025-10-01T15:37:20Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01031v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出首个基于扩散模型的安全、高质量可逆人脸匿名化方法，通过结合密钥与潜在人脸表示，在保持图像质量的同时实现仅授权方可恢复原始人脸的安全机制。",
      "order": 604
    },
    {
      "arxiv_id": "2510.01014v1",
      "title": "Towards Adversarial Training under Hyperspectral Images",
      "summary": "Recent studies have revealed that hyperspectral classification models based\non deep learning are highly vulnerable to adversarial attacks, which pose\nsignificant security risks. Although several approaches have attempted to\nenhance adversarial robustness by modifying network architectures, these\nmethods often rely on customized designs that limit scalability and fail to\ndefend effectively against strong attacks. To address these challenges, we\nintroduce adversarial training to the hyperspectral domain, which is widely\nregarded as one of the most effective defenses against adversarial attacks.\nThrough extensive empirical analyses, we demonstrate that while adversarial\ntraining does enhance robustness across various models and datasets,\nhyperspectral data introduces unique challenges not seen in RGB images.\nSpecifically, we find that adversarial noise and the non-smooth nature of\nadversarial examples can distort or eliminate important spectral semantic\ninformation. To mitigate this issue, we employ data augmentation techniques and\npropose a novel hyperspectral adversarial training method, termed AT-RA. By\nincreasing the diversity of spectral information and ensuring spatial\nsmoothness, AT-RA preserves and corrects spectral semantics in hyperspectral\nimages. Experimental results show that AT-RA improves adversarial robustness by\n21.34% against AutoAttack and 18.78% against PGD-50 while boosting benign\naccuracy by 2.68%.",
      "authors": [
        "Weihua Zhang",
        "Chengze Jiang",
        "Jie Gui",
        "Lu Dong"
      ],
      "published": "2025-10-01T15:19:39Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01014v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对高光谱图像分类模型易受对抗攻击的问题，首次将对抗训练引入高光谱领域。研究发现高光谱数据存在独特挑战，对抗噪声会破坏光谱语义信息。为此提出AT-RA方法，通过数据增强和空间平滑处理，在提升对抗鲁棒性（AutoAttack提升21.34%，PGD-50提升18.78%）的同时提高良性准确率2.68%。",
      "order": 605
    },
    {
      "arxiv_id": "2510.01010v1",
      "title": "ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image\n  Reasoning",
      "summary": "The rapid advancement of text-to-image (T2I) models has increased the need\nfor reliable human preference modeling, a demand further amplified by recent\nprogress in reinforcement learning for preference alignment. However, existing\napproaches typically quantify the quality of a generated image using a single\nscalar, limiting their ability to provide comprehensive and interpretable\nfeedback on image quality. To address this, we introduce ImageDoctor, a unified\nmulti-aspect T2I model evaluation framework that assesses image quality across\nfour complementary dimensions: plausibility, semantic alignment, aesthetics,\nand overall quality. ImageDoctor also provides pixel-level flaw indicators in\nthe form of heatmaps, which highlight misaligned or implausible regions, and\ncan be used as a dense reward for T2I model preference alignment. Inspired by\nthe diagnostic process, we improve the detail sensitivity and reasoning\ncapability of ImageDoctor by introducing a \"look-think-predict\" paradigm, where\nthe model first localizes potential flaws, then generates reasoning, and\nfinally concludes the evaluation with quantitative scores. Built on top of a\nvision-language model and trained through a combination of supervised\nfine-tuning and reinforcement learning, ImageDoctor demonstrates strong\nalignment with human preference across multiple datasets, establishing its\neffectiveness as an evaluation metric. Furthermore, when used as a reward model\nfor preference tuning, ImageDoctor significantly improves generation quality --\nachieving an improvement of 10% over scalar-based reward models.",
      "authors": [
        "Yuxiang Guo",
        "Jiang Liu",
        "Ze Wang",
        "Hao Chen",
        "Ximeng Sun",
        "Yang Zhao",
        "Jialian Wu",
        "Xiaodong Yu",
        "Zicheng Liu",
        "Emad Barsoum"
      ],
      "published": "2025-10-01T15:15:55Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01010v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "ImageDoctor是一个多维度文本到图像生成评估框架，通过'观察-思考-预测'范式在四个维度（合理性、语义对齐、美学、整体质量）评估图像质量，并提供像素级缺陷热力图，作为强化学习的密集奖励信号，相比标量奖励模型提升10%生成质量。",
      "order": 606
    },
    {
      "arxiv_id": "2510.01009v1",
      "title": "POVQA: Preference-Optimized Video Question Answering with Rationales for\n  Data Efficiency",
      "summary": "Video Question Answering (VQA) with Large Vision Language Models (LVLMs) has\ngained significant traction in research ever since the Flamingo was introduced\nby Deepmind. Recent advancements in large context/long video question answering\nhave allowed VQA tasks to have context window of 1500+ frames. However, this\nonly leads to 50 seconds of video footage without losing any significant\ninformation. We introduce POVQA, a data-efficient pipeline that compresses each\nsecond of video into a single temporally pooled image (via motion blur and\nweighted averaging variants) and then align LVLMs with lightweight supervision.\nConcretely, we build 1 fps input sources using Blend Blur with Last Frame,\nWeighted Average, Exponential and Ramp pooling and fine-tune QWEN-2.5-VL 7B\nwith supervised two turn target including reasoning and final answer. We apply\nSupervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) on our\nnovel dataset ReasonVQA consisting of 12 movies with 239 human annotated\nquestion-answer with reasoning prompts. On our ReasonVQA dataset, this method\ndramatically improves performance over pooled baselines: F1 score improves from\n0.212 to 0.543, BLEU-4 from 0.031 to 0.291, and ROUGE-L from 0.196 to 0.528.\nRationale quality also significantly increases. Cross-evaluation of SFT + DPO\non various pooling functions show that the gains persist regardless of the\npooling scheme used at train or test time, indicating strong robustness on\nsummarization of temporal evidence. Similar observations were made on zero-shot\nin TVQA.",
      "authors": [
        "Ashim Dahal",
        "Ankit Ghimire",
        "Saydul Akbar Murad",
        "Nick Rahimi"
      ],
      "published": "2025-10-01T15:15:36Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01009v1",
      "primary_area": "vla_models",
      "secondary_focus": "['long_context', 'reasoning', 'training_optimization']",
      "application_domain": "general_purpose",
      "tldr_zh": "POVQA提出一种数据高效的视频问答方法，通过时间池化将每秒视频压缩为单帧，结合监督微调和直接偏好优化，在ReasonVQA数据集上显著提升性能指标，并证明在不同池化方案下均保持鲁棒性。",
      "order": 607
    },
    {
      "arxiv_id": "2510.01004v1",
      "title": "TextCAM: Explaining Class Activation Map with Text",
      "summary": "Deep neural networks (DNNs) have achieved remarkable success across domains\nbut remain difficult to interpret, limiting their trustworthiness in\nhigh-stakes applications. This paper focuses on deep vision models, for which a\ndominant line of explainability methods are Class Activation Mapping (CAM) and\nits variants working by highlighting spatial regions that drive predictions. We\nfigure out that CAM provides little semantic insight into what attributes\nunderlie these activations. To address this limitation, we propose TextCAM, a\nnovel explanation framework that enriches CAM with natural languages. TextCAM\ncombines the precise spatial localization of CAM with the semantic alignment of\nvision-language models (VLMs). Specifically, we derive channel-level semantic\nrepresentations using CLIP embeddings and linear discriminant analysis, and\naggregate them with CAM weights to produce textual descriptions of salient\nvisual evidence. This yields explanations that jointly specify where the model\nattends and what visual attributes likely support its decision. We further\nextend TextCAM to generate feature channels into semantically coherent groups,\nenabling more fine-grained visual-textual explanations. Experiments on\nImageNet, CLEVR, and CUB demonstrate that TextCAM produces faithful and\ninterpretable rationales that improve human understanding, detect spurious\ncorrelations, and preserve model fidelity.",
      "authors": [
        "Qiming Zhao",
        "Xingjian Li",
        "Xiaoyu Cao",
        "Xiaolong Wu",
        "Min Xu"
      ],
      "published": "2025-10-01T15:11:14Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01004v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "TextCAM是一种新颖的可解释性框架，通过将传统类激活图(CAM)与视觉语言模型结合，生成包含空间定位和语义描述的解释。该方法利用CLIP嵌入和线性判别分析获取通道级语义表示，与CAM权重聚合产生文本描述，在ImageNet等数据集上验证了其能提供忠实可理解的决策依据，检测虚假相关性并保持模型保真度。",
      "order": 608
    },
    {
      "arxiv_id": "2510.00996v2",
      "title": "SoftCFG: Uncertainty-guided Stable Guidance for Visual Autoregressive\n  Model",
      "summary": "Autoregressive (AR) models have emerged as powerful tools for image\ngeneration by modeling images as sequences of discrete tokens. While\nClassifier-Free Guidance (CFG) has been adopted to improve conditional\ngeneration, its application in AR models faces two key issues: guidance\ndiminishing, where the conditional-unconditional gap quickly vanishes as\ndecoding progresses, and over-guidance, where strong conditions distort visual\ncoherence. To address these challenges, we propose SoftCFG, an\nuncertainty-guided inference method that distributes adaptive perturbations\nacross all tokens in the sequence. The key idea behind SoftCFG is to let each\ngenerated token contribute certainty-weighted guidance, ensuring that the\nsignal persists across steps while resolving conflicts between text guidance\nand visual context. To further stabilize long-sequence generation, we introduce\nStep Normalization, which bounds cumulative perturbations of SoftCFG. Our\nmethod is training-free, model-agnostic, and seamlessly integrates with\nexisting AR pipelines. Experiments show that SoftCFG significantly improves\nimage quality over standard CFG and achieves state-of-the-art FID on ImageNet\n256*256 among autoregressive models.",
      "authors": [
        "Dongli Xu",
        "Aleksei Tiulpin",
        "Matthew B. Blaschko"
      ],
      "published": "2025-10-01T15:04:00Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00996v2",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "SoftCFG是一种针对自回归视觉模型提出的不确定性引导推理方法，通过自适应扰动分布解决传统分类器自由引导中的引导衰减和过度引导问题。该方法采用步长归一化技术稳定长序列生成，无需额外训练且与现有AR流程兼容，在ImageNet 256×256上实现了自回归模型中最优的FID指标。",
      "order": 609
    },
    {
      "arxiv_id": "2510.00993v1",
      "title": "Visual Self-Refinement for Autoregressive Models",
      "summary": "Autoregressive models excel in sequential modeling and have proven to be\neffective for vision-language data. However, the spatial nature of visual\nsignals conflicts with the sequential dependencies of next-token prediction,\nleading to suboptimal results. This work proposes a plug-and-play refinement\nmodule to enhance the complex spatial correspondence modeling within the\ngenerated visual sequence. This module operates as a post-pretraining step to\njointly refine all generated tokens of autoregressive model, enhancing\nvision-language modeling under a shared sequential prediction framework. By\nleveraging global context and relationship across the tokens, our method\nmitigates the error accumulation issue within the sequential generation.\nExperiments demonstrate that the proposed method improves the generation\nquality, enhancing the model's ability to produce semantically consistent\nresults.",
      "authors": [
        "Jiamian Wang",
        "Ziqi Zhou",
        "Chaithanya Kumar Mummadi",
        "Sohail Dianat",
        "Majid Rabbani",
        "Raghuveer Rao",
        "Chen Qiu",
        "Zhiqiang Tao"
      ],
      "published": "2025-10-01T15:03:32Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00993v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种即插即用的视觉自优化模块，用于增强自回归模型在视觉序列生成中的空间对应关系建模。该模块作为后预训练步骤，通过利用全局上下文和token间关系，联合优化所有生成token，缓解序列生成中的错误累积问题，提升视觉语言模型的语义一致性生成质量。",
      "order": 610
    },
    {
      "arxiv_id": "2510.00978v1",
      "title": "A Scene is Worth a Thousand Features: Feed-Forward Camera Localization\n  from a Collection of Image Features",
      "summary": "Visually localizing an image, i.e., estimating its camera pose, requires\nbuilding a scene representation that serves as a visual map. The representation\nwe choose has direct consequences towards the practicability of our system.\nEven when starting from mapping images with known camera poses,\nstate-of-the-art approaches still require hours of mapping time in the worst\ncase, and several minutes in the best. This work raises the question whether we\ncan achieve competitive accuracy much faster. We introduce FastForward, a\nmethod that creates a map representation and relocalizes a query image\non-the-fly in a single feed-forward pass. At the core, we represent multiple\nmapping images as a collection of features anchored in 3D space. FastForward\nutilizes these mapping features to predict image-to-scene correspondences for\nthe query image, enabling the estimation of its camera pose. We couple\nFastForward with image retrieval and achieve state-of-the-art accuracy when\ncompared to other approaches with minimal map preparation time. Furthermore,\nFastForward demonstrates robust generalization to unseen domains, including\nchallenging large-scale outdoor environments.",
      "authors": [
        "Axel Barroso-Laguna",
        "Tommaso Cavallari",
        "Victor Adrian Prisacariu",
        "Eric Brachmann"
      ],
      "published": "2025-10-01T14:52:12Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00978v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出FastForward方法，通过单次前向传播实现相机定位，将多张地图图像表示为3D空间中的特征集合，利用这些特征预测查询图像与场景的对应关系，在最小地图准备时间内达到最先进精度，并能泛化到未见过的户外环境。",
      "order": 611
    },
    {
      "arxiv_id": "2510.00974v1",
      "title": "JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for\n  Image Generation",
      "summary": "Modern Text-to-Image (T2I) generation increasingly relies on token-centric\narchitectures that are trained with self-supervision, yet effectively fusing\ntext with visual tokens remains a challenge. We propose \\textbf{JEPA-T}, a\nunified multimodal framework that encodes images and captions into discrete\nvisual and textual tokens, processed by a joint-embedding predictive\nTransformer. To enhance fusion, we incorporate cross-attention after the\nfeature predictor for conditional denoising while maintaining a task-agnostic\nbackbone. Additionally, raw texts embeddings are injected prior to the flow\nmatching loss to improve alignment during training. During inference, the same\nnetwork performs both class-conditional and free-text image generation by\niteratively denoising visual tokens conditioned on text. Evaluations on\nImageNet-1K demonstrate that JEPA-T achieves strong data efficiency,\nopen-vocabulary generalization, and consistently outperforms non-fusion and\nlate-fusion baselines. Our approach shows that late architectural fusion\ncombined with objective-level alignment offers an effective balance between\nconditioning strength and backbone generality in token-based T2I.The code is\nnow available: https://github.com/justin-herry/JEPA-T.git",
      "authors": [
        "Siheng Wan",
        "Zhengtao Yao",
        "Zhengdao Li",
        "Junhao Dong",
        "Yanshu Li",
        "Yikai Li",
        "Linshan Li",
        "Haoyan Xu",
        "Yijiang Li",
        "Zhikang Dong",
        "Huacan Wang",
        "Jifeng Shen"
      ],
      "published": "2025-10-01T14:51:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00974v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "JEPA-T提出了一种联合嵌入预测架构，通过文本融合改进图像生成。该方法将图像和文本编码为离散标记，使用联合嵌入预测Transformer处理，并在特征预测后加入交叉注意力机制增强融合。在ImageNet-1K上的评估显示，该模型在数据效率、开放词汇泛化方面表现优异，优于非融合和后期融合基线。",
      "order": 612
    },
    {
      "arxiv_id": "2510.00948v1",
      "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution",
      "summary": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR.",
      "authors": [
        "Ziqing Zhang",
        "Kai Liu",
        "Zheng Chen",
        "Xi Li",
        "Yucong Chen",
        "Bingnan Duan",
        "Linghe Kong",
        "Yulun Zhang"
      ],
      "published": "2025-10-01T14:21:45Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00948v1",
      "primary_area": "video_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "InfVSR提出自回归单步扩散范式，突破传统视频超分辨率处理长序列的限制。通过因果DiT架构和扩散过程蒸馏，实现无限长度视频的高效超分，在保持时空一致性的同时提速58倍，并建立了专门的长视频评估基准。",
      "order": 613
    },
    {
      "arxiv_id": "2510.00936v1",
      "title": "Looking Alike From Far to Near: Enhancing Cross-Resolution\n  Re-Identification via Feature Vector Panning",
      "summary": "In surveillance scenarios, varying camera distances cause significant\ndifferences among pedestrian image resolutions, making it hard to match\nlow-resolution (LR) images with high-resolution (HR) counterparts, limiting the\nperformance of Re-Identification (ReID) tasks. Most existing Cross-Resolution\nReID (CR-ReID) methods rely on super-resolution (SR) or joint learning for\nfeature compensation, which increases training and inference complexity and has\nreached a performance bottleneck in recent studies. Inspired by semantic\ndirections in the word embedding space, we empirically discover that semantic\ndirections implying resolution differences also emerge in the feature space of\nReID, and we substantiate this finding from a statistical perspective using\nCanonical Correlation Analysis and Pearson Correlation Analysis. Based on this\ninteresting finding, we propose a lightweight and effective Vector Panning\nFeature Alignment (VPFA) framework, which conducts CR-ReID from a novel\nperspective of modeling the resolution-specific feature discrepancy. Extensive\nexperimental results on multiple CR-ReID benchmarks show that our method\nsignificantly outperforms previous state-of-the-art baseline models while\nobtaining higher efficiency, demonstrating the effectiveness and superiority of\nour model based on the new finding in this paper.",
      "authors": [
        "Zanwu Liu",
        "Chao Yuan",
        "Bo Li",
        "Xiaowei Zhang",
        "Guanglin Niu"
      ],
      "published": "2025-10-01T14:15:39Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00936v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种轻量级的向量平移特征对齐(VPFA)框架，通过建模分辨率特征差异解决跨分辨率行人重识别问题。研究发现ReID特征空间中存在类似词嵌入的语义方向，提出无需超分辨率的特征补偿方法，在多个基准测试中显著超越现有方法并提高效率。",
      "order": 614
    },
    {
      "arxiv_id": "2510.00929v2",
      "title": "Equivariant Splitting: Self-supervised learning from incomplete data",
      "summary": "Self-supervised learning for inverse problems allows to train a\nreconstruction network from noise and/or incomplete data alone. These methods\nhave the potential of enabling learning-based solutions when obtaining\nground-truth references for training is expensive or even impossible. In this\npaper, we propose a new self-supervised learning strategy devised for the\nchallenging setting where measurements are observed via a single incomplete\nobservation model. We introduce a new definition of equivariance in the context\nof reconstruction networks, and show that the combination of self-supervised\nsplitting losses and equivariant reconstruction networks results in the same\nminimizer in expectation as the one of a supervised loss. Through a series of\nexperiments on image inpainting, accelerated magnetic resonance imaging, and\ncompressive sensing, we demonstrate that the proposed loss achieves\nstate-of-the-art performance in settings with highly rank-deficient forward\nmodels.",
      "authors": [
        "Victor Sechaud",
        "Jérémy Scanvic",
        "Quentin Barthélemy",
        "Patrice Abry",
        "Julián Tachella"
      ],
      "published": "2025-10-01T14:08:17Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00929v2",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "medical_ai",
      "tldr_zh": "提出一种新的自监督学习策略——等变分裂，针对单次不完整观测模型的逆问题重建。通过结合自监督分裂损失和等变重建网络，在图像修复、加速磁共振成像和压缩感知等任务中实现最先进性能，特别适用于高度秩不足的前向模型场景。",
      "order": 615
    },
    {
      "arxiv_id": "2510.00910v1",
      "title": "PAL-Net: A Point-Wise CNN with Patch-Attention for 3D Facial Landmark\n  Localization",
      "summary": "Manual annotation of anatomical landmarks on 3D facial scans is a\ntime-consuming and expertise-dependent task, yet it remains critical for\nclinical assessments, morphometric analysis, and craniofacial research. While\nseveral deep learning methods have been proposed for facial landmark\nlocalization, most focus on pseudo-landmarks or require complex input\nrepresentations, limiting their clinical applicability. This study presents a\nfully automated deep learning pipeline (PAL-Net) for localizing 50 anatomical\nlandmarks on stereo-photogrammetry facial models. The method combines coarse\nalignment, region-of-interest filtering, and an initial approximation of\nlandmarks with a patch-based pointwise CNN enhanced by attention mechanisms.\nTrained and evaluated on 214 annotated scans from healthy adults, PAL-Net\nachieved a mean localization error of 3.686 mm and preserves relevant\nanatomical distances with a 2.822 mm average error, comparable to\nintra-observer variability. To assess generalization, the model was further\nevaluated on 700 subjects from the FaceScape dataset, achieving a point-wise\nerror of 0.41\\,mm and a distance-wise error of 0.38\\,mm. Compared to existing\nmethods, PAL-Net offers a favorable trade-off between accuracy and\ncomputational cost. While performance degrades in regions with poor mesh\nquality (e.g., ears, hairline), the method demonstrates consistent accuracy\nacross most anatomical regions. PAL-Net generalizes effectively across datasets\nand facial regions, outperforming existing methods in both point-wise and\nstructural evaluations. It provides a lightweight, scalable solution for\nhigh-throughput 3D anthropometric analysis, with potential to support clinical\nworkflows and reduce reliance on manual annotation. Source code can be found at\nhttps://github.com/Ali5hadman/PAL-Net-A-Point-Wise-CNN-with-Patch-Attention",
      "authors": [
        "Ali Shadman Yazdi",
        "Annalisa Cappella",
        "Benedetta Baldini",
        "Riccardo Solazzo",
        "Gianluca Tartaglia",
        "Chiarella Sforza",
        "Giuseppe Baselli"
      ],
      "published": "2025-10-01T13:52:35Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00910v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "PAL-Net提出了一种结合粗对齐、感兴趣区域过滤和基于补丁的点式CNN注意力机制的3D面部标志点定位方法。在214个健康成人扫描数据上训练评估，平均定位误差3.686毫米，在FaceScape数据集上泛化误差0.41毫米，为临床3D人脸测量分析提供了轻量级自动化解决方案。",
      "order": 616
    },
    {
      "arxiv_id": "2510.00902v1",
      "title": "Intuitions of Machine Learning Researchers about Transfer Learning for\n  Medical Image Classification",
      "summary": "Transfer learning is crucial for medical imaging, yet the selection of source\ndatasets - which can impact the generalizability of algorithms, and thus\npatient outcomes - often relies on researchers' intuition rather than\nsystematic principles. This study investigates these decisions through a\ntask-based survey with machine learning practitioners. Unlike prior work that\nbenchmarks models and experimental setups, we take a human-centered HCI\nperspective on how practitioners select source datasets. Our findings indicate\nthat choices are task-dependent and influenced by community practices, dataset\nproperties, and computational (data embedding), or perceived visual or semantic\nsimilarity. However, similarity ratings and expected performance are not always\naligned, challenging a traditional \"more similar is better\" view. Participants\noften used ambiguous terminology, which suggests a need for clearer definitions\nand HCI tools to make them explicit and usable. By clarifying these heuristics,\nthis work provides practical insights for more systematic source selection in\ntransfer learning.",
      "authors": [
        "Yucheng Lu",
        "Hubert Dariusz Zając",
        "Veronika Cheplygina",
        "Amelia Jiménez-Sánchez"
      ],
      "published": "2025-10-01T13:44:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00902v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究通过任务型调查探讨机器学习研究者在医学图像分类中选择迁移学习源数据集时的直觉决策。研究发现选择具有任务依赖性，受社区实践、数据集特性和计算因素影响，相似性评分与预期性能并不总一致，挑战了传统'越相似越好'观点。研究揭示了需要更清晰定义和HCI工具来支持系统化的源数据集选择。",
      "order": 617
    },
    {
      "arxiv_id": "2510.01298v1",
      "title": "MorphGen: Controllable and Morphologically Plausible Generative\n  Cell-Imaging",
      "summary": "Simulating in silico cellular responses to interventions is a promising\ndirection to accelerate high-content image-based assays, critical for advancing\ndrug discovery and gene editing. To support this, we introduce MorphGen, a\nstate-of-the-art diffusion-based generative model for fluorescent microscopy\nthat enables controllable generation across multiple cell types and\nperturbations. To capture biologically meaningful patterns consistent with\nknown cellular morphologies, MorphGen is trained with an alignment loss to\nmatch its representations to the phenotypic embeddings of OpenPhenom, a\nstate-of-the-art biological foundation model. Unlike prior approaches that\ncompress multichannel stains into RGB images -- thus sacrificing\norganelle-specific detail -- MorphGen generates the complete set of fluorescent\nchannels jointly, preserving per-organelle structures and enabling a\nfine-grained morphological analysis that is essential for biological\ninterpretation. We demonstrate biological consistency with real images via\nCellProfiler features, and MorphGen attains an FID score over $35\\%$ lower than\nthe prior state-of-the-art MorphoDiff, which only generates RGB images for a\nsingle cell type. Code is available at https://github.com/czi-ai/MorphGen.",
      "authors": [
        "Berker Demirel",
        "Marco Fumero",
        "Theofanis Karaletsos",
        "Francesco Locatello"
      ],
      "published": "2025-10-01T13:34:29Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.01298v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "alignment",
      "application_domain": "medical_ai",
      "tldr_zh": "MorphGen是一种先进的基于扩散模型的荧光显微镜图像生成方法，通过对齐损失与生物基础模型OpenPhenom的表型嵌入匹配，实现跨细胞类型和干预的可控生成。与先前将多通道染色压缩为RGB图像的方法不同，MorphGen联合生成完整的荧光通道，保留细胞器细节，FID分数比现有最佳方法降低35%以上，适用于药物发现和基因编辑研究。",
      "order": 618
    },
    {
      "arxiv_id": "2510.00882v1",
      "title": "AI-CNet3D: An Anatomically-Informed Cross-Attention Network with\n  Multi-Task Consistency Fine-tuning for 3D Glaucoma Classification",
      "summary": "Glaucoma is a progressive eye disease that leads to optic nerve damage,\ncausing irreversible vision loss if left untreated. Optical coherence\ntomography (OCT) has become a crucial tool for glaucoma diagnosis, offering\nhigh-resolution 3D scans of the retina and optic nerve. However, the\nconventional practice of condensing information from 3D OCT volumes into 2D\nreports often results in the loss of key structural details. To address this,\nwe propose a novel hybrid deep learning model that integrates cross-attention\nmechanisms into a 3D convolutional neural network (CNN), enabling the\nextraction of critical features from the superior and inferior hemiretinas, as\nwell as from the optic nerve head (ONH) and macula, within OCT volumes. We\nintroduce Channel Attention REpresentations (CAREs) to visualize\ncross-attention outputs and leverage them for consistency-based multi-task\nfine-tuning, aligning them with Gradient-Weighted Class Activation Maps\n(Grad-CAMs) from the CNN's final convolutional layer to enhance performance,\ninterpretability, and anatomical coherence. We have named this model AI-CNet3D\n(AI-`See'-Net3D) to reflect its design as an Anatomically-Informed\nCross-attention Network operating on 3D data. By dividing the volume along two\naxes and applying cross-attention, our model enhances glaucoma classification\nby capturing asymmetries between the hemiretinal regions while integrating\ninformation from the optic nerve head and macula. We validate our approach on\ntwo large datasets, showing that it outperforms state-of-the-art attention and\nconvolutional models across all key metrics. Finally, our model is\ncomputationally efficient, reducing the parameter count by one-hundred--fold\ncompared to other attention mechanisms while maintaining high diagnostic\nperformance and comparable GFLOPS.",
      "authors": [
        "Roshan Kenia",
        "Anfei Li",
        "Rishabh Srivastava",
        "Kaveri A. Thakoor"
      ],
      "published": "2025-10-01T13:30:55Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00882v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "提出AI-CNet3D模型，一种结合交叉注意力机制的3D卷积神经网络，用于青光眼分类。通过分割视网膜上下区域并应用交叉注意力捕捉不对称特征，整合视神经头和黄斑区信息，在保持高诊断性能的同时将参数量减少百倍，在两个大型数据集上超越现有最优模型。",
      "order": 619
    },
    {
      "arxiv_id": "2510.00862v1",
      "title": "Gather-Scatter Mamba: Accelerating Propagation with Efficient State\n  Space Model",
      "summary": "State Space Models (SSMs)-most notably RNNs-have historically played a\ncentral role in sequential modeling. Although attention mechanisms such as\nTransformers have since dominated due to their ability to model global context,\ntheir quadratic complexity and limited scalability make them less suited for\nlong sequences. Video super-resolution (VSR) methods have traditionally relied\non recurrent architectures to propagate features across frames. However, such\napproaches suffer from well-known issues including vanishing gradients, lack of\nparallelism, and slow inference speed. Recent advances in selective SSMs like\nMamba offer a compelling alternative: by enabling input-dependent state\ntransitions with linear-time complexity, Mamba mitigates these issues while\nmaintaining strong long-range modeling capabilities. Despite this potential,\nMamba alone struggles to capture fine-grained spatial dependencies due to its\ncausal nature and lack of explicit context aggregation. To address this, we\npropose a hybrid architecture that combines shifted window self-attention for\nspatial context aggregation with Mamba-based selective scanning for efficient\ntemporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an\nalignment-aware mechanism that warps features toward a center anchor frame\nwithin the temporal window before Mamba propagation and scatters them back\nafterward, effectively reducing occlusion artifacts and ensuring effective\nredistribution of aggregated information across all frames. The official\nimplementation is provided at: https://github.com/Ko-Lani/GSMamba.",
      "authors": [
        "Hyun-kyu Ko",
        "Youbin Kim",
        "Jihyeon Park",
        "Dongheok Park",
        "Gyeongjin Kang",
        "Wonjun Cho",
        "Hyung Yi",
        "Eunbyung Park"
      ],
      "published": "2025-10-01T13:11:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00862v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Gather-Scatter Mamba (GSMamba)混合架构，结合移位窗口自注意力与Mamba选择性扫描，通过特征对齐机制解决视频超分辨率中的时空依赖建模问题，在保持线性复杂度的同时提升长序列处理性能。",
      "order": 620
    },
    {
      "arxiv_id": "2510.00855v1",
      "title": "Can World Models Benefit VLMs for World Dynamics?",
      "summary": "Trained on internet-scale video data, generative world models are\nincreasingly recognized as powerful world simulators that can generate\nconsistent and plausible dynamics over structure, motion, and physics. This\nraises a natural question: with the advent of strong video foundational models,\nmight they supplant conventional vision encoder paradigms for general-purpose\nmultimodal understanding? While recent studies have begun to explore the\npotential of world models on common vision tasks, these explorations typically\nlack a systematic investigation of generic, multimodal tasks. In this work, we\nstrive to investigate the capabilities when world model priors are transferred\ninto Vision-Language Models: we re-purpose a video diffusion model as a\ngenerative encoder to perform a single denoising step and treat the resulting\nlatents as a set of visual embedding. We empirically investigate this class of\nmodels, which we refer to as World-Language Models (WorldLMs), and we find that\ngenerative encoders can capture latents useful for downstream understanding\nthat show distinctions from conventional encoders. Naming our best-performing\nvariant Dynamic Vision Aligner (DyVA), we further discover that this method\nsignificantly enhances spatial reasoning abilities and enables single-image\nmodels to perform multi-frame reasoning. Through the curation of a suite of\nvisual reasoning tasks, we find DyVA to surpass both open-source and\nproprietary baselines, achieving state-of-the-art or comparable performance. We\nattribute these gains to WorldLM's inherited motion-consistency internalization\nfrom video pre-training. Finally, we systematically explore extensive model\ndesigns to highlight promising directions for future work. We hope our study\ncan pave the way for a new family of VLMs that leverage priors from world\nmodels and are on a promising path towards generalist vision learners.",
      "authors": [
        "Kevin Zhang",
        "Kuangzhi Ge",
        "Xiaowei Chi",
        "Renrui Zhang",
        "Shaojun Shi",
        "Zhen Dong",
        "Sirui Han",
        "Shanghang Zhang"
      ],
      "published": "2025-10-01T13:07:05Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00855v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探索将世界模型先验知识融入视觉语言模型，提出WorldLM框架及最佳变体DyVA。通过将视频扩散模型重构为生成式编码器，该方法显著提升了空间推理能力，使单图像模型具备多帧推理功能，在视觉推理任务中达到领先性能。",
      "order": 621
    },
    {
      "arxiv_id": "2510.00837v1",
      "title": "Feature Identification for Hierarchical Contrastive Learning",
      "summary": "Hierarchical classification is a crucial task in many applications, where\nobjects are organized into multiple levels of categories. However, conventional\nclassification approaches often neglect inherent inter-class relationships at\ndifferent hierarchy levels, thus missing important supervisory signals. Thus,\nwe propose two novel hierarchical contrastive learning (HMLC) methods. The\nfirst, leverages a Gaussian Mixture Model (G-HMLC) and the second uses an\nattention mechanism to capture hierarchy-specific features (A-HMLC), imitating\nhuman processing. Our approach explicitly models inter-class relationships and\nimbalanced class distribution at higher hierarchy levels, enabling fine-grained\nclustering across all hierarchy levels. On the competitive CIFAR100 and\nModelNet40 datasets, our method achieves state-of-the-art performance in linear\nevaluation, outperforming existing hierarchical contrastive learning methods by\n2 percentage points in terms of accuracy. The effectiveness of our approach is\nbacked by both quantitative and qualitative results, highlighting its potential\nfor applications in computer vision and beyond.",
      "authors": [
        "Julius Ott",
        "Nastassia Vysotskaya",
        "Huawei Sun",
        "Lorenzo Servadei",
        "Robert Wille"
      ],
      "published": "2025-10-01T12:46:47Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00837v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出两种层次对比学习方法(G-HMLC和A-HMLC)，通过高斯混合模型和注意力机制捕捉层次化特征关系，在CIFAR100和ModelNet40数据集上实现最优性能，准确率提升2个百分点。",
      "order": 622
    },
    {
      "arxiv_id": "2510.00820v1",
      "title": "NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image\n  Super-Resolution",
      "summary": "Most recent real-world image super-resolution (Real-ISR) methods employ\npre-trained text-to-image (T2I) diffusion models to synthesize the high-quality\nimage either from random Gaussian noise, which yields realistic results but is\nslow due to iterative denoising, or directly from the input low-quality image,\nwhich is efficient but at the price of lower output quality. These approaches\ntrain ControlNet or LoRA modules while keeping the pre-trained model fixed,\nwhich often introduces over-enhanced artifacts and hallucinations, suffering\nfrom the robustness to inputs of varying degradations. Recent visual\nautoregressive (AR) models, such as pre-trained Infinity, can provide strong\nT2I generation capabilities while offering superior efficiency by using the\nbitwise next-scale prediction strategy. Building upon next-scale prediction, we\nintroduce a robust Real-ISR framework, namely Next-Scale Autoregressive\nModeling (NSARM). Specifically, we train NSARM in two stages: a transformation\nnetwork is first trained to map the input low-quality image to preliminary\nscales, followed by an end-to-end full-model fine-tuning. Such a comprehensive\nfine-tuning enhances the robustness of NSARM in Real-ISR tasks without\ncompromising its generative capability. Extensive quantitative and qualitative\nevaluations demonstrate that as a pure AR model, NSARM achieves superior visual\nresults over existing Real-ISR methods while maintaining a fast inference\nspeed. Most importantly, it demonstrates much higher robustness to the quality\nof input images, showing stronger generalization performance. Project page:\nhttps://github.com/Xiangtaokong/NSARM",
      "authors": [
        "Xiangtao Kong",
        "Rongyuan Wu",
        "Shuaizheng Liu",
        "Lingchen Sun",
        "Lei Zhang"
      ],
      "published": "2025-10-01T12:29:58Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00820v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "NSARM提出了一种基于自回归模型的真实图像超分辨率框架，采用两阶段训练策略：先训练转换网络将低质量图像映射到初步尺度，再进行端到端全模型微调。相比现有方法，NSARM在保持快速推理的同时，实现了更优的视觉效果和对输入图像质量更高的鲁棒性。",
      "order": 623
    },
    {
      "arxiv_id": "2510.00818v1",
      "title": "PhraseStereo: The First Open-Vocabulary Stereo Image Segmentation\n  Dataset",
      "summary": "Understanding how natural language phrases correspond to specific regions in\nimages is a key challenge in multimodal semantic segmentation. Recent advances\nin phrase grounding are largely limited to single-view images, neglecting the\nrich geometric cues available in stereo vision. For this, we introduce\nPhraseStereo, the first novel dataset that brings phrase-region segmentation to\nstereo image pairs. PhraseStereo builds upon the PhraseCut dataset by\nleveraging GenStereo to generate accurate right-view images from existing\nsingle-view data, enabling the extension of phrase grounding into the stereo\ndomain. This new setting introduces unique challenges and opportunities for\nmultimodal learning, particularly in leveraging depth cues for more precise and\ncontext-aware grounding. By providing stereo image pairs with aligned\nsegmentation masks and phrase annotations, PhraseStereo lays the foundation for\nfuture research at the intersection of language, vision, and 3D perception,\nencouraging the development of models that can reason jointly over semantics\nand geometry. The PhraseStereo dataset will be released online upon acceptance\nof this work.",
      "authors": [
        "Thomas Campagnolo",
        "Ezio Malis",
        "Philippe Martinet",
        "Gaetan Bahl"
      ],
      "published": "2025-10-01T12:29:24Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00818v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "PhraseStereo是首个面向立体图像对的开源短语分割数据集，通过GenStereo技术将单视图的PhraseCut数据集扩展为立体视觉数据，为语言-视觉-3D感知的交叉研究提供基础，推动模型在语义与几何联合推理方面的发展。",
      "order": 624
    },
    {
      "arxiv_id": "2510.00808v1",
      "title": "What You See is What You Ask: Evaluating Audio Descriptions",
      "summary": "Audio descriptions (ADs) narrate important visual details in movies, enabling\nBlind and Low Vision (BLV) users to understand narratives and appreciate visual\ndetails. Existing works in automatic AD generation mostly focus on few-second\ntrimmed clips, and evaluate them by comparing against a single ground-truth\nreference AD. However, writing ADs is inherently subjective. Through alignment\nand analysis of two independent AD tracks for the same movies, we quantify the\nsubjectivity in when and whether to describe, and what and how to highlight.\nThus, we show that working with trimmed clips is inadequate. We propose ADQA, a\nQA benchmark that evaluates ADs at the level of few-minute long, coherent video\nsegments, testing whether they would help BLV users understand the story and\nappreciate visual details. ADQA features visual appreciation (VA) questions\nabout visual facts and narrative understanding (NU) questions based on the\nplot. Through ADQA, we show that current AD generation methods lag far behind\nhuman-authored ADs. We conclude with several recommendations for future work\nand introduce a public leaderboard for benchmarking.",
      "authors": [
        "Divy Kala",
        "Eshika Khandelwal",
        "Makarand Tapaswi"
      ],
      "published": "2025-10-01T12:14:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00808v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ADQA基准，用于评估音频描述在长视频片段中的表现，通过量化描述主观性并分析视觉欣赏与叙事理解问题，发现当前自动生成方法远不及人工创作，为盲人和低视力用户提供更好的观影体验。",
      "order": 625
    },
    {
      "arxiv_id": "2510.00806v1",
      "title": "From Seeing to Predicting: A Vision-Language Framework for Trajectory\n  Forecasting and Controlled Video Generation",
      "summary": "Current video generation models produce physically inconsistent motion that\nviolates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework for\nphysics-aware image-to-video generation. First, we employ a Vision Language\nModel to predict coarse-grained motion trajectories that maintain consistency\nwith real-world physics. Second, these trajectories guide video generation\nthrough attention-based mechanisms for fine-grained motion refinement. We build\na trajectory prediction dataset based on video tracking data with realistic\nmotion patterns. Experiments on UCF-101 and MSR-VTT demonstrate that\nTrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of\n545 on UCF-101 and 539 on MSR-VTT.",
      "authors": [
        "Fan Yang",
        "Zhiyang Chen",
        "Yousong Zhu",
        "Xin Li",
        "Jinqiao Wang"
      ],
      "published": "2025-10-01T12:11:36Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00806v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "提出TrajVLM-Gen两阶段框架，通过视觉语言模型预测符合物理规律的运动轨迹，并基于注意力机制引导视频生成，在UCF-101和MSR-VTT数据集上取得优于现有方法的FVD分数。",
      "order": 626
    },
    {
      "arxiv_id": "2510.00797v1",
      "title": "Solar PV Installation Potential Assessment on Building Facades Based on\n  Vision and Language Foundation Models",
      "summary": "Building facades represent a significant untapped resource for solar energy\ngeneration in dense urban environments, yet assessing their photovoltaic (PV)\npotential remains challenging due to complex geometries and semantic com\nponents. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), an\nautomated framework that transforms street-view photographs into quantitative\nPV deployment assessments. The approach combines com puter vision and\nartificial intelligence techniques to address three key challenges: perspective\ndistortion correction, semantic understanding of facade elements, and spatial\nreasoning for PV layout optimization. Our four-stage pipeline processes images\nthrough geometric rectification, zero-shot semantic segmentation, Large\nLanguage Model (LLM) guided spatial reasoning, and energy simulation.\nValidation across 80 buildings in four countries demonstrates ro bust\nperformance with mean area estimation errors of 6.2% &#177; 2.8% compared to\nexpert annotations. The auto mated assessment requires approximately 100\nseconds per building, a substantial gain in efficiency over manual methods.\nSimulated energy yield predictions confirm the method's reliability and\napplicability for regional poten tial studies, urban energy planning, and\nbuilding-integrated photovoltaic (BIPV) deployment. Code is available at:\nhttps:github.com/CodeAXu/Solar-PV-Installation",
      "authors": [
        "Ruyu Liu",
        "Dongxu Zhuang",
        "Jianhua Zhang",
        "Arega Getaneh Abate",
        "Per Sieverts Nielsen",
        "Ben Wang",
        "Xiufeng Liu"
      ],
      "published": "2025-10-01T11:51:28Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00797v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出SF-SPA框架，利用视觉与语言基础模型自动评估建筑立面光伏安装潜力。通过几何校正、零样本语义分割、LLM空间推理和能源模拟四阶段流程，在80栋建筑验证中实现6.2%面积估计误差，为城市能源规划提供高效解决方案。",
      "order": 627
    },
    {
      "arxiv_id": "2510.00796v1",
      "title": "MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically\n  Equivalent Prompts",
      "summary": "Recent advances in text-to-image (T2I) models, especially diffusion-based\narchitectures, have significantly improved the visual quality of generated\nimages. However, these models continue to struggle with a critical limitation:\nmaintaining semantic consistency when input prompts undergo minor linguistic\nvariations. Despite being logically equivalent, such prompt pairs often yield\nmisaligned or semantically inconsistent images, exposing a lack of robustness\nin reasoning and generalisation. To address this, we propose MetaLogic, a novel\nevaluation framework that detects T2I misalignment without relying on ground\ntruth images. MetaLogic leverages metamorphic testing, generating image pairs\nfrom prompts that differ grammatically but are semantically identical. By\ndirectly comparing these image pairs, the framework identifies inconsistencies\nthat signal failures in preserving the intended meaning, effectively diagnosing\nrobustness issues in the model's logic understanding. Unlike existing\nevaluation methods that compare a generated image to a single prompt, MetaLogic\nevaluates semantic equivalence between paired images, offering a scalable,\nground-truth-free approach to identifying alignment failures. It categorises\nthese alignment errors (e.g., entity omission, duplication, positional\nmisalignment) and surfaces counterexamples that can be used for model debugging\nand refinement. We evaluate MetaLogic across multiple state-of-the-art T2I\nmodels and reveal consistent robustness failures across a range of logical\nconstructs. We find that even the SOTA text-to-image models like Flux.dev and\nDALLE-3 demonstrate a 59 percent and 71 percent misalignment rate,\nrespectively. Our results show that MetaLogic is not only efficient and\nscalable, but also effective in uncovering fine-grained logical inconsistencies\nthat are overlooked by existing evaluation metrics.",
      "authors": [
        "Yifan Shen",
        "Yangyang Shu",
        "Hye-young Paik",
        "Yulei Sui"
      ],
      "published": "2025-10-01T11:51:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00796v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "MetaLogic提出了一种无需真实图像标注的文生图模型鲁棒性评估框架，通过生成语法不同但语义等价的提示词对，检测模型在逻辑一致性方面的缺陷。该框架能识别实体遗漏、重复、位置错位等对齐错误，在主流模型上发现高达59%-71%的语义失配率。",
      "order": 628
    },
    {
      "arxiv_id": "2510.00773v1",
      "title": "Uncertainty-Aware Concept Bottleneck Models with Enhanced\n  Interpretability",
      "summary": "In the context of image classification, Concept Bottleneck Models (CBMs)\nfirst embed images into a set of human-understandable concepts, followed by an\nintrinsically interpretable classifier that predicts labels based on these\nintermediate representations. While CBMs offer a semantically meaningful and\ninterpretable classification pipeline, they often sacrifice predictive\nperformance compared to end-to-end convolutional neural networks. Moreover, the\npropagation of uncertainty from concept predictions to final label decisions\nremains underexplored. In this paper, we propose a novel uncertainty-aware and\ninterpretable classifier for the second stage of CBMs. Our method learns a set\nof binary class-level concept prototypes and uses the distances between\npredicted concept vectors and each class prototype as both a classification\nscore and a measure of uncertainty. These prototypes also serve as\ninterpretable classification rules, indicating which concepts should be present\nin an image to justify a specific class prediction. The proposed framework\nenhances both interpretability and robustness by enabling conformal prediction\nfor uncertain or outlier inputs based on their deviation from the learned\nbinary class-level concept prototypes.",
      "authors": [
        "Haifei Zhang",
        "Patrick Barry",
        "Eduardo Brandao"
      ],
      "published": "2025-10-01T11:11:18Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00773v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种不确定性感知的概念瓶颈模型，通过构建二元类别概念原型，将概念向量与原型距离同时用于分类评分和不确定性度量，在保持可解释性的同时提升了图像分类的鲁棒性。",
      "order": 629
    },
    {
      "arxiv_id": "2510.00769v1",
      "title": "ZQBA: Zero Query Black-box Adversarial Attack",
      "summary": "Current black-box adversarial attacks either require multiple queries or\ndiffusion models to produce adversarial samples that can impair the target\nmodel performance. However, these methods require training a surrogate loss or\ndiffusion models to produce adversarial samples, which limits their\napplicability in real-world settings. Thus, we propose a Zero Query Black-box\nAdversarial (ZQBA) attack that exploits the representations of Deep Neural\nNetworks (DNNs) to fool other networks. Instead of requiring thousands of\nqueries to produce deceiving adversarial samples, we use the feature maps\nobtained from a DNN and add them to clean images to impair the classification\nof a target model. The results suggest that ZQBA can transfer the adversarial\nsamples to different models and across various datasets, namely CIFAR and Tiny\nImageNet. The experiments also show that ZQBA is more effective than\nstate-of-the-art black-box attacks with a single query, while maintaining the\nimperceptibility of perturbations, evaluated both quantitatively (SSIM) and\nqualitatively, emphasizing the vulnerabilities of employing DNNs in real-world\ncontexts. All the source code is available at\nhttps://github.com/Joana-Cabral/ZQBA.",
      "authors": [
        "Joana C. Costa",
        "Tiago Roxo",
        "Hugo Proença",
        "Pedro R. M. Inácio"
      ],
      "published": "2025-10-01T11:00:53Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00769v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "ZQBA提出一种零查询黑盒对抗攻击方法，通过利用深度神经网络的特征表示生成对抗样本，无需多次查询或训练替代模型。该方法将源DNN的特征图叠加到干净图像上，可跨模型和数据集（CIFAR、Tiny ImageNet）实现攻击，在单次查询下优于现有技术，同时保持扰动不可感知性，揭示了DNN在实际应用中的安全漏洞。",
      "order": 630
    },
    {
      "arxiv_id": "2510.00766v1",
      "title": "Multi-Objective Task-Aware Predictor for Image-Text Alignment",
      "summary": "Evaluating image-text alignment while reflecting human preferences across\nmultiple aspects is a significant issue for the development of reliable\nvision-language applications. It becomes especially crucial in real-world\nscenarios where multiple valid descriptions exist depending on contexts or user\nneeds. However, research progress is hindered by the lack of comprehensive\nbenchmarks and existing evaluation predictors lacking at least one of these key\nproperties: (1) Alignment with human judgments, (2) Long-sequence processing,\n(3) Inference efficiency, and (4) Applicability to multi-objective scoring. To\naddress these challenges, we propose a plug-and-play architecture to build a\nrobust predictor, MULTI-TAP (Multi-Objective Task-Aware Predictor), capable of\nboth multi and single-objective scoring. MULTI-TAP can produce a single overall\nscore, utilizing a reward head built on top of a large vision-language model\n(LVLMs). We show that MULTI-TAP is robust in terms of application to different\nLVLM architectures, achieving significantly higher performance than existing\nmetrics and even on par with the GPT-4o-based predictor, G-VEval, with a\nsmaller size (7-8B). By training a lightweight ridge regression layer on the\nfrozen hidden states of a pre-trained LVLM, MULTI-TAP can produce fine-grained\nscores for multiple human-interpretable objectives. MULTI-TAP performs better\nthan VisionREWARD, a high-performing multi-objective reward model, in both\nperformance and efficiency on multi-objective benchmarks and our newly released\ntext-image-to-text dataset, EYE4ALL. Our new dataset, consisting of\nchosen/rejected human preferences (EYE4ALLPref) and human-annotated\nfine-grained scores across seven dimensions (EYE4ALLMulti), can serve as a\nfoundation for developing more accessible AI systems by capturing the\nunderlying preferences of users, including blind and low-vision (BLV)\nindividuals.",
      "authors": [
        "Eunki Kim",
        "Na Min An",
        "James Thorne",
        "Hyunjung Shim"
      ],
      "published": "2025-10-01T10:55:33Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00766v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MULTI-TAP，一种用于图像-文本对齐评估的多目标任务感知预测器。该模型基于大型视觉语言模型构建，能够生成单目标或多目标评分，在性能上超越现有指标并与GPT-4o相当，同时发布包含人类偏好标注的新数据集EYE4ALL，支持开发更易访问的AI系统。",
      "order": 631
    },
    {
      "arxiv_id": "2510.00745v1",
      "title": "Defect Segmentation in OCT scans of ceramic parts for non-destructive\n  inspection using deep learning",
      "summary": "Non-destructive testing (NDT) is essential in ceramic manufacturing to ensure\nthe quality of components without compromising their integrity. In this\ncontext, Optical Coherence Tomography (OCT) enables high-resolution internal\nimaging, revealing defects such as pores, delaminations, or inclusions. This\npaper presents an automatic defect detection system based on Deep Learning\n(DL), trained on OCT images with manually segmented annotations. A neural\nnetwork based on the U-Net architecture is developed, evaluating multiple\nexperimental configurations to enhance its performance. Post-processing\ntechniques enable both quantitative and qualitative evaluation of the\npredictions. The system shows an accurate behavior of 0.979 Dice Score,\noutperforming comparable studies. The inference time of 18.98 seconds per\nvolume supports its viability for detecting inclusions, enabling more\nefficient, reliable, and automated quality control.",
      "authors": [
        "Andrés Laveda-Martínez",
        "Natalia P. García-de-la-Puente",
        "Fernando García-Torres",
        "Niels Møller Israelsen",
        "Ole Bang",
        "Dominik Brouczek",
        "Niels Benson",
        "Adrián Colomer",
        "Valery Naranjo"
      ],
      "published": "2025-10-01T10:30:24Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00745v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出基于U-Net架构的深度学习系统，用于陶瓷部件OCT扫描中的缺陷分割。通过手动标注的OCT图像训练，在孔隙、分层和夹杂物检测上达到0.979 Dice分数，推理时间18.98秒/体积，为无损检测提供自动化质量控制方案。",
      "order": 632
    },
    {
      "arxiv_id": "2510.00728v1",
      "title": "Extreme Blind Image Restoration via Prompt-Conditioned Information\n  Bottleneck",
      "summary": "Blind Image Restoration (BIR) methods have achieved remarkable success but\nfalter when faced with Extreme Blind Image Restoration (EBIR), where inputs\nsuffer from severe, compounded degradations beyond their training scope.\nDirectly learning a mapping from extremely low-quality (ELQ) to high-quality\n(HQ) images is challenging due to the massive domain gap, often leading to\nunnatural artifacts and loss of detail. To address this, we propose a novel\nframework that decomposes the intractable ELQ-to-HQ restoration process. We\nfirst learn a projector that maps an ELQ image onto an intermediate,\nless-degraded LQ manifold. This intermediate image is then restored to HQ using\na frozen, off-the-shelf BIR model. Our approach is grounded in information\ntheory; we provide a novel perspective of image restoration as an Information\nBottleneck problem and derive a theoretically-driven objective to train our\nprojector. This loss function effectively stabilizes training by balancing a\nlow-quality reconstruction term with a high-quality prior-matching term. Our\nframework enables Look Forward Once (LFO) for inference-time prompt refinement,\nand supports plug-and-play strengthening of existing image restoration models\nwithout need for finetuning. Extensive experiments under severe degradation\nregimes provide a thorough analysis of the effectiveness of our work.",
      "authors": [
        "Hongeun Kim",
        "Bryan Sangwoo Kim",
        "Jong Chul Ye"
      ],
      "published": "2025-10-01T10:13:27Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00728v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "提出一种基于信息瓶颈理论的极端盲图像恢复框架，通过将严重退化图像投影到中间低质量流形，再利用现成BIR模型恢复，解决了极端退化场景下的图像恢复难题。",
      "order": 633
    },
    {
      "arxiv_id": "2510.00725v1",
      "title": "DEAP DIVE: Dataset Investigation with Vision transformers for EEG\n  evaluation",
      "summary": "Accurately predicting emotions from brain signals has the potential to\nachieve goals such as improving mental health, human-computer interaction, and\naffective computing. Emotion prediction through neural signals offers a\npromising alternative to traditional methods, such as self-assessment and\nfacial expression analysis, which can be subjective or ambiguous. Measurements\nof the brain activity via electroencephalogram (EEG) provides a more direct and\nunbiased data source. However, conducting a full EEG is a complex,\nresource-intensive process, leading to the rise of low-cost EEG devices with\nsimplified measurement capabilities. This work examines how subsets of EEG\nchannels from the DEAP dataset can be used for sufficiently accurate emotion\nprediction with low-cost EEG devices, rather than fully equipped\nEEG-measurements. Using Continuous Wavelet Transformation to convert EEG data\ninto scaleograms, we trained a vision transformer (ViT) model for emotion\nclassification. The model achieved over 91,57% accuracy in predicting 4\nquadrants (high/low per arousal and valence) with only 12 measuring points\n(also referred to as channels). Our work shows clearly, that a significant\nreduction of input channels yields high results compared to state-of-the-art\nresults of 96,9% with 32 channels. Training scripts to reproduce our code can\nbe found here:\nhttps://gitlab.kit.edu/kit/aifb/ATKS/public/AutoSMiLeS/DEAP-DIVE.",
      "authors": [
        "Annemarie Hoffsommer",
        "Helen Schneider",
        "Svetlana Pavlitska",
        "J. Marius Zöllner"
      ],
      "published": "2025-10-01T10:07:07Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00725v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出DEAP DIVE方法，利用视觉变换器(ViT)分析脑电图(EEG)信号进行情绪分类。通过连续小波变换将EEG数据转换为尺度图，仅使用12个测量通道即可实现91.57%的四象限情绪预测准确率，显著降低了传统32通道系统的复杂度，为低成本EEG设备在心理健康和人机交互等医疗AI应用提供了可行方案。",
      "order": 634
    },
    {
      "arxiv_id": "2510.00723v1",
      "title": "Deep learning motion correction of quantitative stress perfusion\n  cardiovascular magnetic resonance",
      "summary": "Background: Quantitative stress perfusion cardiovascular magnetic resonance\n(CMR) is a powerful tool for assessing myocardial ischemia. Motion correction\nis essential for accurate pixel-wise mapping but traditional registration-based\nmethods are slow and sensitive to acquisition variability, limiting robustness\nand scalability.\n  Methods: We developed an unsupervised deep learning-based motion correction\npipeline that replaces iterative registration with efficient one-shot\nestimation. The method corrects motion in three steps and uses robust principal\ncomponent analysis to reduce contrast-related effects. It aligns the perfusion\nseries and auxiliary images (arterial input function and proton\ndensity-weighted series). Models were trained and validated on multivendor data\nfrom 201 patients, with 38 held out for testing. Performance was assessed via\ntemporal alignment and quantitative perfusion values, compared to a previously\npublished registration-based method.\n  Results: The deep learning approach significantly improved temporal\nsmoothness of time-intensity curves (p<0.001). Myocardial alignment (Dice =\n0.92 (0.04) and 0.91 (0.05)) was comparable to the baseline and superior to\nbefore registration (Dice = 0.80 (0.09), p<0.001). Perfusion maps showed\nreduced motion, with lower standard deviation in the myocardium (0.52 (0.39)\nml/min/g) compared to baseline (0.55 (0.44) ml/min/g). Processing time was\nreduced 15-fold.\n  Conclusion: This deep learning pipeline enables fast, robust motion\ncorrection for stress perfusion CMR, improving accuracy across dynamic and\nauxiliary images. Trained on multivendor data, it generalizes across sequences\nand may facilitate broader clinical adoption of quantitative perfusion imaging.",
      "authors": [
        "Noortje I. P. Schueler",
        "Nathan C. K. Wong",
        "Richard J. Crawley",
        "Josien P. W. Pluim",
        "Amedeo Chiribiri",
        "Cian M. Scannell"
      ],
      "published": "2025-10-01T09:59:48Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00723v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出一种基于无监督深度学习的运动校正方法，用于定量应力灌注心血管磁共振成像。该方法通过三步校正流程和鲁棒主成分分析，替代传统迭代配准，在保持心肌对齐精度的同时将处理时间缩短15倍，显著提升时间强度曲线平滑度，有望促进定量灌注成像的临床推广应用。",
      "order": 635
    },
    {
      "arxiv_id": "2510.01296v1",
      "title": "From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic\n  Resonance Imaging: A Review",
      "summary": "Deep learning-based 3-dimensional (3D) shape reconstruction from\n2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly\nimportant in medical disease diagnosis, treatment planning, and computational\nmodeling. This review surveys the methodological landscape of 3D MRI\nreconstruction, focusing on 4 primary approaches: point cloud, mesh-based,\nshape-aware, and volumetric models. For each category, we analyze the current\nstate-of-the-art techniques, their methodological foundation, limitations, and\napplications across anatomical structures. We provide an extensive overview\nranging from cardiac to neurological to lung imaging. We also focus on the\nclinical applicability of models to diseased anatomy, and the influence of\ntheir training and testing data. We examine publicly available datasets,\ncomputational demands, and evaluation metrics. Finally, we highlight the\nemerging research directions including multimodal integration and\ncross-modality frameworks. This review aims to provide researchers with a\nstructured overview of current 3D reconstruction methodologies to identify\nopportunities for advancing deep learning towards more robust, generalizable,\nand clinically impactful solutions.",
      "authors": [
        "Emma McMillian",
        "Abhirup Banerjee",
        "Alfonso Bueno-Orovio"
      ],
      "published": "2025-10-01T09:57:29Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01296v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "本文综述了基于深度学习的2D磁共振图像到3D形状重建方法，涵盖点云、网格、形状感知和体积模型四大技术路线，分析了各方法在心脏、神经、肺部等解剖结构的应用现状、临床价值、数据集及评估指标，并展望了多模态融合等未来研究方向。",
      "order": 636
    },
    {
      "arxiv_id": "2510.00705v1",
      "title": "Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs",
      "summary": "Multimodal Large Language Models (MLLMs) often struggle with fine-grained\nperception, such as identifying small objects in high-resolution images or\nfinding key moments in long videos. Existing works typically rely on\ncomplicated, task-specific fine-tuning, which limits their generalizability and\nincreases model complexity. In this work, we propose an effective,\ntraining-free framework that uses an MLLM's intrinsic uncertainty as a\nproactive guidance signal. Our core insight is that a model's output entropy\ndecreases when presented with relevant visual information. We introduce a\nunified mechanism that scores candidate visual inputs by response uncertainty,\nenabling the model to autonomously focus on the most salient data. We apply\nthis simple principle to three complex visual tasks: Visual Search, Long Video\nUnderstanding, and Temporal Grounding, allowing off-the-shelf MLLMs to achieve\nperformance competitive with specialized, fine-tuned methods. Our work\nvalidates that harnessing intrinsic uncertainty is a powerful, general strategy\nfor enhancing fine-grained multimodal performance.",
      "authors": [
        "Sanghwan Kim",
        "Rui Xiao",
        "Stephan Alaniz",
        "Yongqin Xian",
        "Zeynep Akata"
      ],
      "published": "2025-10-01T09:20:51Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00705v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种无需训练的多模态大语言模型框架，利用模型内在不确定性作为引导信号，通过响应熵值评估视觉输入相关性，在视觉搜索、长视频理解和时序定位等复杂任务中实现与专用微调方法相媲美的性能。",
      "order": 637
    },
    {
      "arxiv_id": "2510.00701v1",
      "title": "Graph Integrated Multimodal Concept Bottleneck Model",
      "summary": "With growing demand for interpretability in deep learning, especially in high\nstakes domains, Concept Bottleneck Models (CBMs) address this by inserting\nhuman understandable concepts into the prediction pipeline, but they are\ngenerally single modal and ignore structured concept relationships. To overcome\nthese limitations, we present MoE-SGT, a reasoning driven framework that\naugments CBMs with a structure injecting Graph Transformer and a Mixture of\nExperts (MoE) module. We construct answer-concept and answer-question graphs\nfor multimodal inputs to explicitly model the structured relationships among\nconcepts. Subsequently, we integrate Graph Transformer to capture multi level\ndependencies, addressing the limitations of traditional Concept Bottleneck\nModels in modeling concept interactions. However, it still encounters\nbottlenecks in adapting to complex concept patterns. Therefore, we replace the\nfeed forward layers with a Mixture of Experts (MoE) module, enabling the model\nto have greater capacity in learning diverse concept relationships while\ndynamically allocating reasoning tasks to different sub experts, thereby\nsignificantly enhancing the model's adaptability to complex concept reasoning.\nMoE-SGT achieves higher accuracy than other concept bottleneck networks on\nmultiple datasets by modeling structured relationships among concepts and\nutilizing a dynamic expert selection mechanism.",
      "authors": [
        "Jiakai Lin",
        "Jinchang Zhang",
        "Guoyu Lu"
      ],
      "published": "2025-10-01T09:18:38Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00701v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "提出MoE-SGT框架，通过图Transformer和混合专家模块增强概念瓶颈模型，显式建模概念间结构化关系，提升复杂概念推理能力，在多个数据集上实现更高准确率。",
      "order": 638
    },
    {
      "arxiv_id": "2510.00695v2",
      "title": "HAMLET: Switch your Vision-Language-Action Model into a History-Aware\n  Policy",
      "summary": "Inherently, robotic manipulation tasks are history-dependent: leveraging past\ncontext could be beneficial. However, most existing Vision-Language-Action\nmodels (VLAs) have been designed without considering this aspect, i.e., they\nrely solely on the current observation, ignoring preceding context. In this\npaper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the\nhistorical context during action prediction. Specifically, we introduce moment\ntokens that compactly encode perceptual information at each timestep. Their\nrepresentations are initialized with time-contrastive learning, allowing them\nto better capture temporally distinctive aspects. Next, we employ a lightweight\nmemory module that integrates the moment tokens across past timesteps into\nmemory features, which are then leveraged for action prediction. Through\nempirical evaluation, we show that HAMLET successfully transforms a\nstate-of-the-art VLA into a history-aware policy, especially demonstrating\nsignificant improvements on long-horizon tasks that require historical context.\nIn particular, on top of GR00T N1.5, HAMLET achieves an average success rate of\n76.4% on history-dependent real-world tasks, surpassing the baseline\nperformance by 47.2%. Furthermore, HAMLET pushes prior art performance from\n64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on\nLIBERO, highlighting its effectiveness even under generic robot-manipulation\nbenchmarks.",
      "authors": [
        "Myungkyu Koo",
        "Daewon Choi",
        "Taeyoung Kim",
        "Kyungmin Lee",
        "Changyeon Kim",
        "Younggyo Seo",
        "Jinwoo Shin"
      ],
      "published": "2025-10-01T09:15:52Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00695v2",
      "primary_area": "vla_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "HAMLET提出了一种可扩展框架，将视觉-语言-动作模型转换为历史感知策略。通过引入紧凑编码感知信息的时刻标记和轻量级记忆模块，有效整合历史上下文进行动作预测。在GR00T N1.5上对历史依赖任务的完成率达76.4%，较基线提升47.2%，在RoboCasa Kitchen和LIBERO基准测试中也显著提升性能。",
      "order": 639
    },
    {
      "arxiv_id": "2510.00683v1",
      "title": "ProtoMask: Segmentation-Guided Prototype Learning",
      "summary": "XAI gained considerable importance in recent years. Methods based on\nprototypical case-based reasoning have shown a promising improvement in\nexplainability. However, these methods typically rely on additional post-hoc\nsaliency techniques to explain the semantics of learned prototypes. Multiple\ncritiques have been raised about the reliability and quality of such\ntechniques. For this reason, we study the use of prominent image segmentation\nfoundation models to improve the truthfulness of the mapping between embedding\nand input space. We aim to restrict the computation area of the saliency map to\na predefined semantic image patch to reduce the uncertainty of such\nvisualizations. To perceive the information of an entire image, we use the\nbounding box from each generated segmentation mask to crop the image. Each mask\nresults in an individual input in our novel model architecture named ProtoMask.\nWe conduct experiments on three popular fine-grained classification datasets\nwith a wide set of metrics, providing a detailed overview on explainability\ncharacteristics. The comparison with other popular models demonstrates\ncompetitive performance and unique explainability features of our model.\nhttps://github.com/uos-sis/quanproto",
      "authors": [
        "Steffen Meinert",
        "Philipp Schlinge",
        "Nils Strodthoff",
        "Martin Atzmueller"
      ],
      "published": "2025-10-01T09:07:24Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00683v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "ProtoMask提出了一种基于分割引导的原型学习方法，通过图像分割基础模型改进可解释AI中原型与输入空间的映射真实性。该模型利用分割掩码裁剪图像区域作为独立输入，在细粒度分类任务中展现出竞争力的性能和独特的可解释性特征。",
      "order": 640
    },
    {
      "arxiv_id": "2510.00681v1",
      "title": "Adaptive Event Stream Slicing for Open-Vocabulary Event-Based Object\n  Detection via Vision-Language Knowledge Distillation",
      "summary": "Event cameras offer advantages in object detection tasks due to high-speed\nresponse, low latency, and robustness to motion blur. However, event cameras\nlack texture and color information, making open-vocabulary detection\nparticularly challenging. Current event-based detection methods are typically\ntrained on predefined categories, limiting their ability to generalize to novel\nobjects, where encountering previously unseen objects is common.\nVision-language models (VLMs) have enabled open-vocabulary object detection in\nRGB images. However, the modality gap between images and event streams makes it\nineffective to directly transfer CLIP to event data, as CLIP was not designed\nfor event streams. To bridge this gap, we propose an event-image knowledge\ndistillation framework that leverages CLIP's semantic understanding to achieve\nopen-vocabulary object detection on event data. Instead of training CLIP\ndirectly on event streams, we use image frames as inputs to a teacher model,\nguiding the event-based student model to learn CLIP's rich visual\nrepresentations. Through spatial attention-based distillation, the student\nnetwork learns meaningful visual features directly from raw event inputs while\ninheriting CLIP's broad visual knowledge. Furthermore, to prevent information\nloss due to event data segmentation, we design a hybrid spiking neural network\n(SNN) and convolutional neural network (CNN) framework. Unlike fixed-group\nevent segmentation methods, which often discard crucial temporal information,\nour SNN adaptively determines the optimal event segmentation moments, ensuring\nthat key temporal features are extracted. The extracted event features are then\nprocessed by CNNs for object detection.",
      "authors": [
        "Jinchang Zhang",
        "Zijun Li",
        "Jiakai Lin",
        "Guoyu Lu"
      ],
      "published": "2025-10-01T09:03:30Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00681v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于视觉-语言知识蒸馏的自适应事件流切片方法，用于解决事件相机在开放词汇目标检测中的挑战。通过教师-学生框架，利用CLIP模型的语义理解能力指导事件数据学习，结合脉冲神经网络自适应确定事件分割时机，保留关键时序特征，实现无需预定义类别的通用目标检测。",
      "order": 641
    },
    {
      "arxiv_id": "2510.00667v1",
      "title": "Beyond one-hot encoding? Journey into compact encoding for large\n  multi-class segmentation",
      "summary": "This work presents novel methods to reduce computational and memory\nrequirements for medical image segmentation with a large number of classes. We\ncuriously observe challenges in maintaining state-of-the-art segmentation\nperformance with all of the explored options. Standard learning-based methods\ntypically employ one-hot encoding of class labels. The computational complexity\nand memory requirements thus increase linearly with the number of classes. We\npropose a family of binary encoding approaches instead of one-hot encoding to\nreduce the computational complexity and memory requirements to logarithmic in\nthe number of classes. In addition to vanilla binary encoding, we investigate\nthe effects of error-correcting output codes (ECOCs), class weighting,\nhard/soft decoding, class-to-codeword assignment, and label embedding trees. We\napply the methods to the use case of whole brain parcellation with 108 classes\nbased on 3D MRI images. While binary encodings have proven efficient in\nso-called extreme classification problems in computer vision, we faced\nchallenges in reaching state-of-the-art segmentation quality with binary\nencodings. Compared to one-hot encoding (Dice Similarity Coefficient (DSC) =\n82.4 (2.8)), we report reduced segmentation performance with the binary\nsegmentation approaches, achieving DSCs in the range from 39.3 to 73.8.\nInformative negative results all too often go unpublished. We hope that this\nwork inspires future research of compact encoding strategies for large\nmulti-class segmentation tasks.",
      "authors": [
        "Aaron Kujawa",
        "Thomas Booth",
        "Tom Vercauteren"
      ],
      "published": "2025-10-01T08:53:39Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00667v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究探索了用于大规模多类医学图像分割的紧凑编码方法，提出用二进制编码替代传统one-hot编码以降低计算和内存需求。在108类全脑分割任务中，虽然二进制编码将复杂度从线性降至对数级，但分割性能（DSC 39.3-73.8）未能达到one-hot编码水平（DSC 82.4），为未来紧凑编码研究提供了重要的负面结果参考。",
      "order": 642
    },
    {
      "arxiv_id": "2510.00666v1",
      "title": "A Geometric Unification of Generative AI with Manifold-Probabilistic\n  Projection Models",
      "summary": "The foundational premise of generative AI for images is the assumption that\nimages are inherently low-dimensional objects embedded within a\nhigh-dimensional space. Additionally, it is often implicitly assumed that\nthematic image datasets form smooth or piecewise smooth manifolds. Common\napproaches overlook the geometric structure and focus solely on probabilistic\nmethods, approximating the probability distribution through universal\napproximation techniques such as the kernel method. In some generative models,\nthe low dimensional nature of the data manifest itself by the introduction of a\nlower dimensional latent space. Yet, the probability distribution in the latent\nor the manifold coordinate space is considered uninteresting and is predefined\nor considered uniform. This study unifies the geometric and probabilistic\nperspectives by providing a geometric framework and a kernel-based\nprobabilistic method simultaneously. The resulting framework demystifies\ndiffusion models by interpreting them as a projection mechanism onto the\nmanifold of ``good images''. This interpretation leads to the construction of a\nnew deterministic model, the Manifold-Probabilistic Projection Model (MPPM),\nwhich operates in both the representation (pixel) space and the latent space.\nWe demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion\nModel (LDM) across various datasets, achieving superior results in terms of\nimage restoration and generation.",
      "authors": [
        "Leah Bar",
        "Liron Mor Yosef",
        "Shai Zucker",
        "Neta Shoham",
        "Inbar Seroussi",
        "Nir Sochen"
      ],
      "published": "2025-10-01T08:50:30Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00666v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出几何框架统一生成式AI的几何与概率视角，将扩散模型解释为向'优质图像'流形的投影机制，并构建新型确定性模型MPPM。实验表明潜在MPPM在图像修复和生成任务上优于潜在扩散模型。",
      "order": 643
    },
    {
      "arxiv_id": "2510.00665v2",
      "title": "Multi-Domain Brain Vessel Segmentation Through Feature Disentanglement",
      "summary": "The intricate morphology of brain vessels poses significant challenges for\nautomatic segmentation models, which usually focus on a single imaging\nmodality. However, accurately treating brain-related conditions requires a\ncomprehensive understanding of the cerebrovascular tree, regardless of the\nspecific acquisition procedure. Our framework effectively segments brain\narteries and veins in various datasets through image-to-image translation while\navoiding domain-specific model design and data harmonization between the source\nand the target domain. This is accomplished by employing disentanglement\ntechniques to independently manipulate different image properties, allowing\nthem to move from one domain to another in a label-preserving manner.\nSpecifically, we focus on manipulating vessel appearances during adaptation\nwhile preserving spatial information, such as shapes and locations, which are\ncrucial for correct segmentation. Our evaluation effectively bridges large and\nvaried domain gaps across medical centers, image modalities, and vessel types.\nAdditionally, we conduct ablation studies on the optimal number of required\nannotations and other architectural choices. The results highlight our\nframework's robustness and versatility, demonstrating the potential of domain\nadaptation methodologies to perform cerebrovascular image segmentation in\nmultiple scenarios accurately. Our code is available at\nhttps://github.com/i-vesseg/MultiVesSeg.",
      "authors": [
        "Francesco Galati",
        "Daniele Falcetta",
        "Rosa Cortese",
        "Ferran Prados",
        "Ninon Burgos",
        "Maria A. Zuluaga"
      ],
      "published": "2025-10-01T08:48:11Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00665v2",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出一种通过特征解耦实现多领域脑血管分割的框架，利用图像到图像转换技术在不同数据集上分割脑动脉和静脉，无需针对特定领域设计模型或进行数据协调。该方法通过独立操控图像属性实现跨域迁移，同时保留对分割至关重要的空间信息，在多个医学中心、成像模态和血管类型上展现出强大的适应性和鲁棒性。",
      "order": 644
    },
    {
      "arxiv_id": "2510.00664v1",
      "title": "Batch-CAM: Introduction to better reasoning in convolutional deep\n  learning models",
      "summary": "Understanding the inner workings of deep learning models is crucial for\nadvancing artificial intelligence, particularly in high-stakes fields such as\nhealthcare, where accurate explanations are as vital as precision. This paper\nintroduces Batch-CAM, a novel training paradigm that fuses a batch\nimplementation of the Grad-CAM algorithm with a prototypical reconstruction\nloss. This combination guides the model to focus on salient image features,\nthereby enhancing its performance across classification tasks. Our results\ndemonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and\nimage reconstruction quality while reducing training and inference times. By\nensuring models learn from evidence-relevant information,this approach makes a\nrelevant contribution to building more transparent, explainable, and\ntrustworthy AI systems.",
      "authors": [
        "Giacomo Ignesti",
        "Davide Moroni",
        "Massimo Martinelli"
      ],
      "published": "2025-10-01T08:47:00Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00664v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出Batch-CAM训练范式，融合批量Grad-CAM算法与原型重建损失，引导模型聚焦关键图像特征，在提升分类精度的同时优化图像重建质量并缩短训练/推理时间，为构建透明可信的AI系统（尤其适用于医疗等高风险领域）提供新思路。",
      "order": 645
    },
    {
      "arxiv_id": "2510.00660v1",
      "title": "Unsupervised Unfolded rPCA (U2-rPCA): Deep Interpretable Clutter\n  Filtering for Ultrasound Microvascular Imaging",
      "summary": "High-sensitivity clutter filtering is a fundamental step in ultrasound\nmicrovascular imaging. Singular value decomposition (SVD) and robust principal\ncomponent analysis (rPCA) are the main clutter filtering strategies. However,\nboth strategies are limited in feature modeling and tissue-blood flow\nseparation for high-quality microvascular imaging. Recently, deep\nlearning-based clutter filtering has shown potential in more thoroughly\nseparating tissue and blood flow signals. However, the existing supervised\nfilters face the challenges of interpretability and lack of in-vitro and\nin-vivo ground truths. While the interpretability issue can be addressed by\nalgorithm deep unfolding, the training ground truth remains unsolved. To this\nend, this paper proposes an unsupervised unfolded rPCA (U2-rPCA) method that\npreserves mathematical interpretability and is insusceptible to learning\nlabels. Specifically, U2-rPCA is unfolded from an iteratively reweighted least\nsquares (IRLS) rPCA baseline with intrinsic low-rank and sparse regularization.\nA sparse-enhancement unit is added to the network to strengthen its capability\nto capture the sparse micro-flow signals. U2-rPCA is like an adaptive filter\nthat is trained with part of the image sequence and then used for the following\nframes. Experimental validations on a in-silico dataset and public in-vivo\ndatasets demonstrated the outperformance of U2-rPCA when compared with the\nSVD-based method, the rPCA baseline, and another deep learning-based filter.\nParticularly, the proposed method improved the contrastto-noise ratio (CNR) of\nthe power Doppler image by 2 dB to 10 dB when compared with other methods.\nFurthermore, the effectiveness of the building modules of U2-rPCA was validated\nthrough ablation studies.",
      "authors": [
        "Huaying Li",
        "Liansheng Wang",
        "Yinran Chen"
      ],
      "published": "2025-10-01T08:39:58Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00660v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出无监督展开式鲁棒主成分分析(U2-rPCA)方法，用于超声微血管成像中的杂波滤波。该方法通过深度展开技术保持数学可解释性，无需监督标签，在合成和真实数据集上验证显示其性能优于传统SVD、rPCA及现有深度学习方法，尤其将功率多普勒图像的对比噪声比提升2-10dB。",
      "order": 646
    },
    {
      "arxiv_id": "2510.00658v1",
      "title": "Align Your Tangent: Training Better Consistency Models via\n  Manifold-Aligned Tangents",
      "summary": "With diffusion and flow matching models achieving state-of-the-art generating\nperformance, the interest of the community now turned to reducing the inference\ntime without sacrificing sample quality. Consistency Models (CMs), which are\ntrained to be consistent on diffusion or probability flow ordinary differential\nequation (PF-ODE) trajectories, enable one or two-step flow or diffusion\nsampling. However, CMs typically require prolonged training with large batch\nsizes to obtain competitive sample quality. In this paper, we examine the\ntraining dynamics of CMs near convergence and discover that CM tangents -- CM\noutput update directions -- are quite oscillatory, in the sense that they move\nparallel to the data manifold, not towards the manifold. To mitigate\noscillatory tangents, we propose a new loss function, called the manifold\nfeature distance (MFD), which provides manifold-aligned tangents that point\ntoward the data manifold. Consequently, our method -- dubbed Align Your Tangent\n(AYT) -- can accelerate CM training by orders of magnitude and even out-perform\nthe learned perceptual image patch similarity metric (LPIPS). Furthermore, we\nfind that our loss enables training with extremely small batch sizes without\ncompromising sample quality. Code: https://github.com/1202kbs/AYT",
      "authors": [
        "Beomsu Kim",
        "Byunghee Cha",
        "Jong Chul Ye"
      ],
      "published": "2025-10-01T08:35:18Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00658v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种名为'对齐切线'(AYT)的新方法，通过流形特征距离损失函数解决一致性模型训练中切线振荡问题，使更新方向指向数据流形，大幅加速训练速度并提升样本质量，即使使用极小批次也能保持性能。",
      "order": 647
    },
    {
      "arxiv_id": "2510.00654v1",
      "title": "Weakly Supervised Cloud Detection Combining Spectral Features and\n  Multi-Scale Deep Network",
      "summary": "Clouds significantly affect the quality of optical satellite images, which\nseriously limits their precise application. Recently, deep learning has been\nwidely applied to cloud detection and has achieved satisfactory results.\nHowever, the lack of distinctive features in thin clouds and the low quality of\ntraining samples limit the cloud detection accuracy of deep learning methods,\nleaving space for further improvements. In this paper, we propose a weakly\nsupervised cloud detection method that combines spectral features and\nmulti-scale scene-level deep network (SpecMCD) to obtain highly accurate\npixel-level cloud masks. The method first utilizes a progressive training\nframework with a multi-scale scene-level dataset to train the multi-scale\nscene-level cloud detection network. Pixel-level cloud probability maps are\nthen obtained by combining the multi-scale probability maps and cloud thickness\nmap based on the characteristics of clouds in dense cloud coverage and large\ncloud-area coverage images. Finally, adaptive thresholds are generated based on\nthe differentiated regions of the scene-level cloud masks at different scales\nand combined with distance-weighted optimization to obtain binary cloud masks.\nTwo datasets, WDCD and GF1MS-WHU, comprising a total of 60 Gaofen-1\nmultispectral (GF1-MS) images, were used to verify the effectiveness of the\nproposed method. Compared to the other weakly supervised cloud detection\nmethods such as WDCD and WSFNet, the F1-score of the proposed SpecMCD method\nshows an improvement of over 7.82%, highlighting the superiority and potential\nof the SpecMCD method for cloud detection under different cloud coverage\nconditions.",
      "authors": [
        "Shaocong Zhu",
        "Zhiwei Li",
        "Xinghua Li",
        "Huanfeng Shen"
      ],
      "published": "2025-10-01T08:32:49Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00654v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种结合光谱特征与多尺度深度网络的弱监督云检测方法(SpecMCD)，通过渐进式训练框架生成多尺度概率图，结合云厚度图和自适应阈值优化，在GF1-MS卫星图像上实现像素级云掩码检测，相比现有方法F1分数提升超过7.82%。",
      "order": 648
    },
    {
      "arxiv_id": "2510.00652v1",
      "title": "OTTER: Open-Tagging via Text-Image Representation for Multi-modal\n  Understanding",
      "summary": "We introduce OTTER, a unified open-set multi-label tagging framework that\nharmonizes the stability of a curated, predefined category set with the\nadaptability of user-driven open tags. OTTER is built upon a large-scale,\nhierarchically organized multi-modal dataset, collected from diverse online\nrepositories and annotated through a hybrid pipeline combining automated\nvision-language labeling with human refinement. By leveraging a multi-head\nattention architecture, OTTER jointly aligns visual and textual representations\nwith both fixed and open-set label embeddings, enabling dynamic and\nsemantically consistent tagging. OTTER consistently outperforms competitive\nbaselines on two benchmark datasets: it achieves an overall F1 score of 0.81 on\nOtter and 0.75 on Favorite, surpassing the next-best results by margins of 0.10\nand 0.02, respectively. OTTER attains near-perfect performance on open-set\nlabels, with F1 of 0.99 on Otter and 0.97 on Favorite, while maintaining\ncompetitive accuracy on predefined labels. These results demonstrate OTTER's\neffectiveness in bridging closed-set consistency with open-vocabulary\nflexibility for multi-modal tagging applications.",
      "authors": [
        "Jieer Ouyang",
        "Xiaoneng Xiang",
        "Zheng Wang",
        "Yangkai Ding"
      ],
      "published": "2025-10-01T08:31:19Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00652v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "OTTER是一种统一开放集多标签标注框架，结合了预定义类别稳定性与用户驱动开放标签的适应性。基于大规模分层多模态数据集，采用多头注意力架构对齐视觉文本表示与固定/开放集标签嵌入，在基准测试中F1分数达0.81和0.75，开放集标签性能接近完美。",
      "order": 649
    },
    {
      "arxiv_id": "2510.00651v1",
      "title": "FIN: Fast Inference Network for Map Segmentation",
      "summary": "Multi-sensor fusion in autonomous vehicles is becoming more common to offer a\nmore robust alternative for several perception tasks. This need arises from the\nunique contribution of each sensor in collecting data: camera-radar fusion\noffers a cost-effective solution by combining rich semantic information from\ncameras with accurate distance measurements from radar, without incurring\nexcessive financial costs or overwhelming data processing requirements. Map\nsegmentation is a critical task for enabling effective vehicle behaviour in its\nenvironment, yet it continues to face significant challenges in achieving high\naccuracy and meeting real-time performance requirements. Therefore, this work\npresents a novel and efficient map segmentation architecture, using cameras and\nradars, in the \\acrfull{bev} space. Our model introduces a real-time map\nsegmentation architecture considering aspects such as high accuracy, per-class\nbalancing, and inference time. To accomplish this, we use an advanced loss set\ntogether with a new lightweight head to improve the perception results. Our\nresults show that, with these modifications, our approach achieves results\ncomparable to large models, reaching 53.5 mIoU, while also setting a new\nbenchmark for inference time, improving it by 260\\% over the strongest baseline\nmodels.",
      "authors": [
        "Ruan Bispo",
        "Tim Brophy",
        "Reenu Mohandas",
        "Anthony Scanlan",
        "Ciarán Eising"
      ],
      "published": "2025-10-01T08:29:59Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00651v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出了一种用于自动驾驶地图分割的快速推理网络FIN，采用相机-雷达多传感器融合技术，在鸟瞰图空间实现实时高精度分割。通过先进的损失函数和轻量级头部设计，在保持53.5 mIoU高精度的同时，推理速度比基线模型提升260%。",
      "order": 650
    },
    {
      "arxiv_id": "2510.00635v1",
      "title": "Erased, But Not Forgotten: Erased Rectified Flow Transformers Still\n  Remain Unsafe Under Concept Attack",
      "summary": "Recent advances in text-to-image (T2I) diffusion models have enabled\nimpressive generative capabilities, but they also raise significant safety\nconcerns due to the potential to produce harmful or undesirable content. While\nconcept erasure has been explored as a mitigation strategy, most existing\napproaches and corresponding attack evaluations are tailored to Stable\nDiffusion (SD) and exhibit limited effectiveness when transferred to\nnext-generation rectified flow transformers such as Flux. In this work, we\npresent ReFlux, the first concept attack method specifically designed to assess\nthe robustness of concept erasure in the latest rectified flow-based T2I\nframework. Our approach is motivated by the observation that existing concept\nerasure techniques, when applied to Flux, fundamentally rely on a phenomenon\nknown as attention localization. Building on this insight, we propose a simple\nyet effective attack strategy that specifically targets this property. At its\ncore, a reverse-attention optimization strategy is introduced to effectively\nreactivate suppressed signals while stabilizing attention. This is further\nreinforced by a velocity-guided dynamic that enhances the robustness of concept\nreactivation by steering the flow matching process, and a\nconsistency-preserving objective that maintains the global layout and preserves\nunrelated content. Extensive experiments consistently demonstrate the\neffectiveness and efficiency of the proposed attack method, establishing a\nreliable benchmark for evaluating the robustness of concept erasure strategies\nin rectified flow transformers.",
      "authors": [
        "Nanxiang Jiang",
        "Zhaoxin Fan",
        "Enhan Kang",
        "Daiheng Gao",
        "Yun Zhou",
        "Yanxia Chang",
        "Zheng Zhu",
        "Yeying Jin",
        "Wenjun Wu"
      ],
      "published": "2025-10-01T08:12:07Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00635v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ReFlux攻击方法，专门针对新一代整流流变换器（如Flux）的概念擦除技术进行评估。研究发现现有擦除方法依赖注意力定位机制，因此设计了反向注意力优化策略，结合速度引导动态和一致性保持目标，有效重新激活被抑制的有害概念，为评估整流流变换器的概念擦除鲁棒性建立了可靠基准。",
      "order": 651
    },
    {
      "arxiv_id": "2510.00634v1",
      "title": "LAKAN: Landmark-assisted Adaptive Kolmogorov-Arnold Network for Face\n  Forgery Detection",
      "summary": "The rapid development of deepfake generation techniques necessitates robust\nface forgery detection algorithms. While methods based on Convolutional Neural\nNetworks (CNNs) and Transformers are effective, there is still room for\nimprovement in modeling the highly complex and non-linear nature of forgery\nartifacts. To address this issue, we propose a novel detection method based on\nthe Kolmogorov-Arnold Network (KAN). By replacing fixed activation functions\nwith learnable splines, our KAN-based approach is better suited to this\nchallenge. Furthermore, to guide the network's focus towards critical facial\nareas, we introduce a Landmark-assisted Adaptive Kolmogorov-Arnold Network\n(LAKAN) module. This module uses facial landmarks as a structural prior to\ndynamically generate the internal parameters of the KAN, creating an\ninstance-specific signal that steers a general-purpose image encoder towards\nthe most informative facial regions with artifacts. This core innovation\ncreates a powerful combination between geometric priors and the network's\nlearning process. Extensive experiments on multiple public datasets show that\nour proposed method achieves superior performance.",
      "authors": [
        "Jiayao Jiang",
        "Siran Peng",
        "Bin Liu",
        "Qi Chu",
        "Nenghai Yu"
      ],
      "published": "2025-10-01T08:10:38Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00634v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出LAKAN方法，结合地标引导与自适应Kolmogorov-Arnold网络，通过可学习样条函数和面部关键点先验，提升深度伪造人脸检测性能，在多个数据集上表现优异。",
      "order": 652
    },
    {
      "arxiv_id": "2510.00633v1",
      "title": "Virtual Fashion Photo-Shoots: Building a Large-Scale Garment-Lookbook\n  Dataset",
      "summary": "Fashion image generation has so far focused on narrow tasks such as virtual\ntry-on, where garments appear in clean studio environments. In contrast,\neditorial fashion presents garments through dynamic poses, diverse locations,\nand carefully crafted visual narratives. We introduce the task of virtual\nfashion photo-shoot, which seeks to capture this richness by transforming\nstandardized garment images into contextually grounded editorial imagery. To\nenable this new direction, we construct the first large-scale dataset of\ngarment-lookbook pairs, bridging the gap between e-commerce and fashion media.\nBecause such pairs are not readily available, we design an automated retrieval\npipeline that aligns garments across domains, combining visual-language\nreasoning with object-level localization. We construct a dataset with three\ngarment-lookbook pair accuracy levels: high quality (10,000 pairs), medium\nquality (50,000 pairs), and low quality (300,000 pairs). This dataset offers a\nfoundation for models that move beyond catalog-style generation and toward\nfashion imagery that reflects creativity, atmosphere, and storytelling.",
      "authors": [
        "Yannick Hauri",
        "Luca A. Lanzendörfer",
        "Till Aczel"
      ],
      "published": "2025-10-01T08:05:05Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00633v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出虚拟时尚摄影任务，构建首个大规模服装-画册配对数据集，通过跨域检索管道连接电商与时尚媒体，支持创意时尚图像生成。",
      "order": 653
    },
    {
      "arxiv_id": "2510.00624v1",
      "title": "UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs",
      "summary": "Adversarial training turns out to be the key to one-step generation,\nespecially for Generative Adversarial Network (GAN) and diffusion model\ndistillation. Yet in practice, GAN training hardly converges properly and\nstruggles in mode collapse. In this work, we quantitatively analyze the extent\nof Nash equilibrium in GAN training, and conclude that redundant shortcuts by\ninputting condition in $D$ disables meaningful knowledge extraction. We thereby\npropose to employ an unconditional discriminator (UCD), in which $D$ is\nenforced to extract more comprehensive and robust features with no condition\ninjection. In this way, $D$ is able to leverage better knowledge to supervise\n$G$, which promotes Nash equilibrium in GAN literature. Theoretical guarantee\non compatibility with vanilla GAN theory indicates that UCD can be implemented\nin a plug-in manner. Extensive experiments confirm the significant performance\nimprovements with high efficiency. For instance, we achieved \\textbf{1.47 FID}\non the ImageNet-64 dataset, surpassing StyleGAN-XL and several state-of-the-art\none-step diffusion models. The code will be made publicly available.",
      "authors": [
        "Mengfei Xia",
        "Nan Xue",
        "Jiapeng Zhu",
        "Yujun Shen"
      ],
      "published": "2025-10-01T07:58:33Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00624v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出无条件判别器(UCD)方法，通过移除判别器的条件输入来避免冗余捷径，促进GAN训练达到纳什均衡。理论证明与原始GAN兼容，实验在ImageNet-64上取得1.47 FID的优异性能，超越StyleGAN-XL等先进模型。",
      "order": 654
    },
    {
      "arxiv_id": "2510.00618v1",
      "title": "Robust Context-Aware Object Recognition",
      "summary": "In visual recognition, both the object of interest (referred to as\nforeground, FG, for simplicity) and its surrounding context (background, BG)\nplay an important role. However, standard supervised learning often leads to\nunintended over-reliance on the BG, known as shortcut learning of spurious\ncorrelations, limiting model robustness in real-world deployment settings. In\nthe literature, the problem is mainly addressed by suppressing the BG,\nsacrificing context information for improved generalization.\n  We propose RCOR -- Robust Context-Aware Object Recognition -- the first\napproach that jointly achieves robustness and context-awareness without\ncompromising either. RCOR treats localization as an integral part of\nrecognition to decouple object-centric and context-aware modelling, followed by\na robust, non-parametric fusion. It improves the performance of both supervised\nmodels and VLM on datasets with both in-domain and out-of-domain BG, even\nwithout fine-tuning. The results confirm that localization before recognition\nis now possible even in complex scenes as in ImageNet-1k.",
      "authors": [
        "Klara Janouskova",
        "Cristian Gavrus",
        "Jiri Matas"
      ],
      "published": "2025-10-01T07:45:38Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00618v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出RCOR方法，通过将定位作为识别的重要组成部分，实现对象中心建模与上下文感知的分离，并采用鲁棒的非参数融合，在保持上下文感知的同时提升模型鲁棒性，无需微调即可在ImageNet-1k等复杂场景中改善监督模型和VLM的性能。",
      "order": 655
    },
    {
      "arxiv_id": "2510.00604v1",
      "title": "Disentangling Foreground and Background for vision-Language Navigation\n  via Online Augmentation",
      "summary": "Following language instructions, vision-language navigation (VLN) agents are\ntasked with navigating unseen environments. While augmenting multifaceted\nvisual representations has propelled advancements in VLN, the significance of\nforeground and background in visual observations remains underexplored.\nIntuitively, foreground regions provide semantic cues, whereas the background\nencompasses spatial connectivity information. Inspired on this insight, we\npropose a Consensus-driven Online Feature Augmentation strategy (COFA) with\nalternative foreground and background features to facilitate the navigable\ngeneralization. Specifically, we first leverage semantically-enhanced landmark\nidentification to disentangle foreground and background as candidate augmented\nfeatures. Subsequently, a consensus-driven online augmentation strategy\nencourages the agent to consolidate two-stage voting results on feature\npreferences according to diverse instructions and navigational locations.\nExperiments on REVERIE and R2R demonstrate that our online\nforeground-background augmentation boosts the generalization of baseline and\nattains state-of-the-art performance.",
      "authors": [
        "Yunbo Xu",
        "Xuesong Zhang",
        "Jia Li",
        "Zhenzhen Hu",
        "Richang Hong"
      ],
      "published": "2025-10-01T07:32:36Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00604v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于在线增强的前景-背景解耦方法COFA，用于提升视觉语言导航的泛化能力。通过语义增强地标识别分离前景语义线索和背景空间信息，采用共识驱动的在线增强策略整合特征偏好，在REVERIE和R2R数据集上达到先进性能。",
      "order": 656
    },
    {
      "arxiv_id": "2510.00603v1",
      "title": "LVLMs as inspectors: an agentic framework for category-level structural\n  defect annotation",
      "summary": "Automated structural defect annotation is essential for ensuring\ninfrastructure safety while minimizing the high costs and inefficiencies of\nmanual labeling. A novel agentic annotation framework, Agent-based Defect\nPattern Tagger (ADPT), is introduced that integrates Large Vision-Language\nModels (LVLMs) with a semantic pattern matching module and an iterative\nself-questioning refinement mechanism. By leveraging optimized domain-specific\nprompting and a recursive verification process, ADPT transforms raw visual data\ninto high-quality, semantically labeled defect datasets without any manual\nsupervision. Experimental results demonstrate that ADPT achieves up to 98%\naccuracy in distinguishing defective from non-defective images, and 85%-98%\nannotation accuracy across four defect categories under class-balanced\nsettings, with 80%-92% accuracy on class-imbalanced datasets. The framework\noffers a scalable and cost-effective solution for high-fidelity dataset\nconstruction, providing strong support for downstream tasks such as transfer\nlearning and domain adaptation in structural damage assessment.",
      "authors": [
        "Sheng Jiang",
        "Yuanmin Ning",
        "Bingxi Huang",
        "Peiyin Chen",
        "Zhaohui Chen"
      ],
      "published": "2025-10-01T07:31:42Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00603v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "提出ADPT框架，利用大型视觉语言模型结合语义模式匹配和迭代自问优化机制，实现无需人工监督的结构缺陷自动标注，在缺陷检测中准确率达98%，为基础设施安全评估提供可扩展的解决方案。",
      "order": 657
    },
    {
      "arxiv_id": "2510.00600v1",
      "title": "Hybrid Training for Vision-Language-Action Models",
      "summary": "Using Large Language Models to produce intermediate thoughts, a.k.a.\nChain-of-thought (CoT), before providing an answer has been a successful recipe\nfor solving complex language tasks. In robotics, similar embodied CoT\nstrategies, generating thoughts before actions, have also been shown to lead to\nimproved performance when using Vision-Language-Action models (VLAs). As these\ntechniques increase the length of the model's generated outputs to include the\nthoughts, the inference time is negatively affected. Delaying an agent's\nactions in real-world executions, as in robotic manipulation settings, strongly\naffects the usability of a method, as tasks require long sequences of actions.\nHowever, is the generation of long chains-of-thought a strong prerequisite for\nachieving performance improvements? In this work, we explore the idea of Hybrid\nTraining (HyT), a framework that enables VLAs to learn from thoughts and\nbenefit from the associated performance gains, while enabling the possibility\nto leave out CoT generation during inference. Furthermore, by learning to\nconditionally predict a diverse set of outputs, HyT supports flexibility at\ninference time, enabling the model to either predict actions directly, generate\nthoughts or follow instructions. We evaluate the proposed method in a series of\nsimulated benchmarks and real-world experiments.",
      "authors": [
        "Pietro Mazzaglia",
        "Cansu Sancaktar",
        "Markus Peschl",
        "Daniel Dijkman"
      ],
      "published": "2025-10-01T07:27:15Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00600v1",
      "primary_area": "vla_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出混合训练框架HyT，使视觉-语言-动作模型能在训练时学习思维链推理，但在推理时无需生成思维链即可保持性能提升，解决了机器人任务中长思维链导致推理延迟的问题，在仿真和真实实验中验证了有效性。",
      "order": 658
    },
    {
      "arxiv_id": "2510.00592v1",
      "title": "Multi-level Dynamic Style Transfer for NeRFs",
      "summary": "As the application of neural radiance fields (NeRFs) in various 3D vision\ntasks continues to expand, numerous NeRF-based style transfer techniques have\nbeen developed. However, existing methods typically integrate style statistics\ninto the original NeRF pipeline, often leading to suboptimal results in both\ncontent preservation and artistic stylization. In this paper, we present\nmulti-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that\nreengineers the NeRF pipeline specifically for stylization and incorporates an\ninnovative dynamic style injection module. Particularly, we propose a\nmulti-level feature adaptor that helps generate a multi-level feature grid\nrepresentation from the content radiance field, effectively capturing the\nmulti-scale spatial structure of the scene. In addition, we present a dynamic\nstyle injection module that learns to extract relevant style features and\nadaptively integrates them into the content patterns. The stylized multi-level\nfeatures are then transformed into the final stylized view through our proposed\nmulti-level cascade decoder. Furthermore, we extend our 3D style transfer\nmethod to support omni-view style transfer using 3D style references. Extensive\nexperiments demonstrate that MDS-NeRF achieves outstanding performance for 3D\nstyle transfer, preserving multi-scale spatial structures while effectively\ntransferring stylistic characteristics.",
      "authors": [
        "Zesheng Li",
        "Shuaibo Li",
        "Wei Ma",
        "Jianwei Guo",
        "Hongbin Zha"
      ],
      "published": "2025-10-01T07:19:27Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00592v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出MDS-NeRF方法，通过重构NeRF流程并引入动态风格注入模块，实现多层级3D风格迁移。该方法采用多层级特征适配器捕捉场景多尺度空间结构，通过级联解码器生成风格化视图，支持全视角3D风格迁移，在保持内容结构和风格特征方面表现优异。",
      "order": 659
    },
    {
      "arxiv_id": "2510.00585v1",
      "title": "U-DFA: A Unified DINOv2-Unet with Dual Fusion Attention for\n  Multi-Dataset Medical Segmentation",
      "summary": "Accurate medical image segmentation plays a crucial role in overall diagnosis\nand is one of the most essential tasks in the diagnostic pipeline. CNN-based\nmodels, despite their extensive use, suffer from a local receptive field and\nfail to capture the global context. A common approach that combines CNNs with\ntransformers attempts to bridge this gap but fails to effectively fuse the\nlocal and global features. With the recent emergence of VLMs and foundation\nmodels, they have been adapted for downstream medical imaging tasks; however,\nthey suffer from an inherent domain gap and high computational cost. To this\nend, we propose U-DFA, a unified DINOv2-Unet encoder-decoder architecture that\nintegrates a novel Local-Global Fusion Adapter (LGFA) to enhance segmentation\nperformance. LGFA modules inject spatial features from a CNN-based Spatial\nPattern Adapter (SPA) module into frozen DINOv2 blocks at multiple stages,\nenabling effective fusion of high-level semantic and spatial features. Our\nmethod achieves state-of-the-art performance on the Synapse and ACDC datasets\nwith only 33\\% of the trainable model parameters. These results demonstrate\nthat U-DFA is a robust and scalable framework for medical image segmentation\nacross multiple modalities.",
      "authors": [
        "Zulkaif Sajjad",
        "Furqan Shaukat",
        "Junaid Mir"
      ],
      "published": "2025-10-01T07:06:49Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.00585v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "U-DFA提出了一种统一的DINOv2-Unet架构，通过新型局部-全局融合适配器(LGFA)有效结合CNN的空间特征与DINOv2的语义特征，在医学图像分割任务中仅用33%可训练参数即在Synapse和ACDC数据集上达到最先进性能。",
      "order": 660
    },
    {
      "arxiv_id": "2510.00584v1",
      "title": "Color Models in Image Processing: A Review and Experimental Comparison",
      "summary": "Color representation is essential in computer vision and human-computer\ninteraction. There are multiple color models available. The choice of a\nsuitable color model is critical for various applications. This paper presents\na review of color models and spaces, analyzing their theoretical foundations,\ncomputational properties, and practical applications. We explore traditional\nmodels such as RGB, CMYK, and YUV, perceptually uniform spaces like CIELAB and\nCIELUV, and fuzzy-based approaches as well. Additionally, we conduct a series\nof experiments to evaluate color models from various perspectives, like device\ndependency, chromatic consistency, and computational complexity. Our\nexperimental results reveal gaps in existing color models and show that the HS*\nfamily is the most aligned with human perception. The review also identifies\nkey strengths and limitations of different models and outlines open challenges\nand future directions This study provides a reference for researchers in image\nprocessing, perceptual computing, digital media, and any other color-related\nfield.",
      "authors": [
        "Muragul Muratbekova",
        "Nuray Toganas",
        "Ayan Igali",
        "Maksat Shagyrov",
        "Elnara Kadyrgali",
        "Adilet Yerkin",
        "Pakizar Shamoi"
      ],
      "published": "2025-10-01T07:06:02Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00584v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本文系统综述了图像处理中的颜色模型，分析了RGB、CMYK、YUV等传统模型、CIELAB/CIELUV感知均匀空间及模糊方法的理论基础与计算特性。通过设备依赖性、色彩一致性和计算复杂度实验，发现HS*系列最符合人类视觉感知，并指出了现有模型的局限性与未来研究方向。",
      "order": 661
    },
    {
      "arxiv_id": "2510.00578v1",
      "title": "Arbitrary Generative Video Interpolation",
      "summary": "Video frame interpolation (VFI), which generates intermediate frames from\ngiven start and end frames, has become a fundamental function in video\ngeneration applications. However, existing generative VFI methods are\nconstrained to synthesize a fixed number of intermediate frames, lacking the\nflexibility to adjust generated frame rates or total sequence duration. In this\nwork, we present ArbInterp, a novel generative VFI framework that enables\nefficient interpolation at any timestamp and of any length. Specifically, to\nsupport interpolation at any timestamp, we propose the Timestamp-aware Rotary\nPosition Embedding (TaRoPE), which modulates positions in temporal RoPE to\nalign generated frames with target normalized timestamps. This design enables\nfine-grained control over frame timestamps, addressing the inflexibility of\nfixed-position paradigms in prior work. For any-length interpolation, we\ndecompose long-sequence generation into segment-wise frame synthesis. We\nfurther design a novel appearance-motion decoupled conditioning strategy: it\nleverages prior segment endpoints to enforce appearance consistency and\ntemporal semantics to maintain motion coherence, ensuring seamless\nspatiotemporal transitions across segments. Experimentally, we build\ncomprehensive benchmarks for multi-scale frame interpolation (2x to 32x) to\nassess generalizability across arbitrary interpolation factors. Results show\nthat ArbInterp outperforms prior methods across all scenarios with higher\nfidelity and more seamless spatiotemporal continuity. Project website:\nhttps://mcg-nju.github.io/ArbInterp-Web/.",
      "authors": [
        "Guozhen Zhang",
        "Haiguang Wang",
        "Chunyu Wang",
        "Yuan Zhou",
        "Qinglin Lu",
        "Limin Wang"
      ],
      "published": "2025-10-01T06:57:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00578v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "ArbInterp是一种创新的生成式视频插帧框架，支持任意时间点和任意长度的帧插值。通过时间感知旋转位置编码实现精确时间控制，采用外观-运动解耦条件策略保证跨片段时空连续性，在2x至32x多尺度插值中均优于现有方法。",
      "order": 662
    },
    {
      "arxiv_id": "2510.00570v1",
      "title": "Adaptive Shared Experts with LoRA-Based Mixture of Experts for\n  Multi-Task Learning",
      "summary": "Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task\nlearning (MTL). However, existing MoE-MTL methods often rely on single-task\npretrained backbones and suffer from redundant adaptation and inefficient\nknowledge sharing during the transition from single-task to multi-task learning\n(STL to MTL). To address these limitations, we propose adaptive shared experts\n(ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are\nassigned router-computed gating weights jointly normalized with sparse experts.\nThis design facilitates STL to MTL transition, enhances expert specialization,\nand cooperation. Furthermore, we incorporate fine-grained experts by increasing\nthe number of LoRA experts while proportionally reducing their rank, enabling\nmore effective knowledge sharing under a comparable parameter budget. Extensive\nexperiments on the PASCAL-Context benchmark, under unified training settings,\ndemonstrate that ASE consistently improves performance across diverse\nconfigurations and validates the effectiveness of fine-grained designs for MTL.",
      "authors": [
        "Minghao Yang",
        "Ren Togo",
        "Guang Li",
        "Takahiro Ogawa",
        "Miki Haseyama"
      ],
      "published": "2025-10-01T06:49:19Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00570v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出基于LoRA的自适应共享专家(ASE)方法，通过路由器计算的门控权重联合归一化共享专家与稀疏专家，改进多任务学习中专家专业化与协作，并在PASCAL-Context基准测试中验证了其有效性。",
      "order": 663
    },
    {
      "arxiv_id": "2510.00561v1",
      "title": "Assessing Foundation Models for Mold Colony Detection with Limited\n  Training Data",
      "summary": "The process of quantifying mold colonies on Petri dish samples is of critical\nimportance for the assessment of indoor air quality, as high colony counts can\nindicate potential health risks and deficiencies in ventilation systems.\nConventionally the automation of such a labor-intensive process, as well as\nother tasks in microbiology, relies on the manual annotation of large datasets\nand the subsequent extensive training of models like YoloV9. To demonstrate\nthat exhaustive annotation is not a prerequisite anymore when tackling a new\nvision task, we compile a representative dataset of 5000 Petri dish images\nannotated with bounding boxes, simulating both a traditional data collection\napproach as well as few-shot and low-shot scenarios with well curated subsets\nwith instance level masks. We benchmark three vision foundation models against\ntraditional baselines on task specific metrics, reflecting realistic real-world\nrequirements. Notably, MaskDINO attains near-parity with an extensively trained\nYoloV9 model while finetuned only on 150 images, retaining competitive\nperformance with as few as 25 images, still being reliable on $\\approx$ 70% of\nthe samples. Our results show that data-efficient foundation models can match\ntraditional approaches with only a fraction of the required data, enabling\nearlier development and faster iterative improvement of automated\nmicrobiological systems with a superior upper-bound performance than\ntraditional models would achieve.",
      "authors": [
        "Henrik Pichler",
        "Janis Keuper",
        "Matthew Copping"
      ],
      "published": "2025-10-01T06:25:45Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00561v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究评估了基础模型在少量训练数据下检测培养皿霉菌菌落的能力。通过构建5000张标注图像数据集，在少样本场景下对比三种视觉基础模型与传统方法。结果显示MaskDINO仅用150张图像微调即可达到与充分训练的YoloV9相近性能，25张图像时仍能保持约70%的可靠性，证明基础模型能以极少数据实现传统方法同等效果，推动微生物检测系统高效开发。",
      "order": 664
    },
    {
      "arxiv_id": "2510.00547v1",
      "title": "Forestpest-YOLO: A High-Performance Detection Framework for Small\n  Forestry Pests",
      "summary": "Detecting agricultural pests in complex forestry environments using remote\nsensing imagery is fundamental for ecological preservation, yet it is severely\nhampered by practical challenges. Targets are often minuscule, heavily\noccluded, and visually similar to the cluttered background, causing\nconventional object detection models to falter due to the loss of fine-grained\nfeatures and an inability to handle extreme data imbalance. To overcome these\nobstacles, this paper introduces Forestpest-YOLO, a detection framework\nmeticulously optimized for the nuances of forestry remote sensing. Building\nupon the YOLOv8 architecture, our framework introduces a synergistic trio of\ninnovations. We first integrate a lossless downsampling module, SPD-Conv, to\nensure that critical high-resolution details of small targets are preserved\nthroughout the network. This is complemented by a novel cross-stage feature\nfusion block, CSPOK, which dynamically enhances multi-scale feature\nrepresentation while suppressing background noise. Finally, we employ\nVarifocalLoss to refine the training objective, compelling the model to focus\non high-quality and hard-to-classify samples. Extensive experiments on our\nchallenging, self-constructed ForestPest dataset demonstrate that\nForestpest-YOLO achieves state-of-the-art performance, showing marked\nimprovements in detecting small, occluded pests and significantly outperforming\nestablished baseline models.",
      "authors": [
        "Aoduo Li",
        "Peikai Lin",
        "Jiancheng Li",
        "Zhen Zhang",
        "Shiting Wu",
        "Zexiao Liang",
        "Zhifa Jiang"
      ],
      "published": "2025-10-01T06:06:40Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00547v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Forestpest-YOLO框架，针对林业遥感图像中的小型害虫检测难题进行优化。该框架基于YOLOv8架构，引入SPD-Conv无损下采样模块、CSPOK跨阶段特征融合块和VarifocalLoss损失函数，在自建ForestPest数据集上实现了对小尺寸、遮挡害虫的先进检测性能。",
      "order": 665
    },
    {
      "arxiv_id": "2510.00527v1",
      "title": "Cascaded Diffusion Framework for Probabilistic Coarse-to-Fine Hand Pose\n  Estimation",
      "summary": "Deterministic models for 3D hand pose reconstruction, whether single-staged\nor cascaded, struggle with pose ambiguities caused by self-occlusions and\ncomplex hand articulations. Existing cascaded approaches refine predictions in\na coarse-to-fine manner but remain deterministic and cannot capture pose\nuncertainties. Recent probabilistic methods model pose distributions yet are\nrestricted to single-stage estimation, which often fails to produce accurate 3D\nreconstructions without refinement. To address these limitations, we propose a\ncoarse-to-fine cascaded diffusion framework that combines probabilistic\nmodeling with cascaded refinement. The first stage is a joint diffusion model\nthat samples diverse 3D joint hypotheses, and the second stage is a Mesh Latent\nDiffusion Model (Mesh LDM) that reconstructs a 3D hand mesh conditioned on a\njoint sample. By training Mesh LDM with diverse joint hypotheses in a learned\nlatent space, our framework learns distribution-aware joint-mesh relationships\nand robust hand priors. Furthermore, the cascaded design mitigates the\ndifficulty of directly mapping 2D images to dense 3D poses, enhancing accuracy\nthrough sequential refinement. Experiments on FreiHAND and HO3Dv2 demonstrate\nthat our method achieves state-of-the-art performance while effectively\nmodeling pose distributions.",
      "authors": [
        "Taeyun Woo",
        "Jinah Park",
        "Tae-Kyun Kim"
      ],
      "published": "2025-10-01T05:19:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00527v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种级联扩散框架，通过两阶段概率建模解决3D手部姿态估计中的模糊性问题：首阶段使用关节扩散模型生成多样3D关节假设，次阶段通过网格隐扩散模型重建手部网格。该方法在FreiHAND和HO3Dv2数据集上实现最先进性能，有效建模姿态分布并提升重建精度。",
      "order": 666
    },
    {
      "arxiv_id": "2510.00523v1",
      "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
      "summary": "Multimodal representation learning models have demonstrated successful\noperation across complex tasks, and the integration of vision-language models\n(VLMs) has further enabled embedding models with instruction-following\ncapabilities. However, existing embedding models lack visual-interactive\ncapabilities to specify regions of interest from users (e.g., point, bounding\nbox, mask), which have been explored in generative models to broaden their\nhuman-interactive applicability. Equipping embedding models with visual\ninteractions not only would unlock new applications with localized grounding of\nuser intent, which remains unexplored, but also enable the models to learn\nentity-level information within images to complement their global\nrepresentations for conventional embedding tasks. In this paper, we propose a\nnovel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends\nthe capabilities of the segmentation model and the vision-language model to the\nrealm of representation learning. In VIRTUE, the segmentation model can process\nvisual prompts that pinpoint specific regions within an image, thereby enabling\nthe embedder to handle complex and ambiguous scenarios more precisely. To\nevaluate the visual-interaction ability of VIRTUE, we introduce a large-scale\nSegmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples\nthat aims to retrieve the text caption by jointly considering the entity with a\nspecific object and image scene. VIRTUE consistently achieves a\nstate-of-the-art performance with significant improvements across 36 universal\nMMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
      "authors": [
        "Wei-Yao Wang",
        "Kazuya Tateishi",
        "Qiyu Wu",
        "Shusuke Takahashi",
        "Yuki Mitsufuji"
      ],
      "published": "2025-10-01T05:11:54Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00523v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "VIRTUE提出了一种视觉交互式文本-图像通用嵌入模型，通过整合分割模型和视觉语言模型，支持用户通过点选、边界框等交互方式指定图像兴趣区域，在36个多模态任务和5个视觉交互检索任务中实现显著性能提升。",
      "order": 667
    },
    {
      "arxiv_id": "2510.00520v1",
      "title": "CardioBench: Do Echocardiography Foundation Models Generalize Beyond the\n  Lab?",
      "summary": "Foundation models (FMs) are reshaping medical imaging, yet their application\nin echocardiography remains limited. While several echocardiography-specific\nFMs have recently been introduced, no standardized benchmark exists to evaluate\nthem. Echocardiography poses unique challenges, including noisy acquisitions,\nhigh frame redundancy, and limited public datasets. Most existing solutions\nevaluate on private data, restricting comparability. To address this, we\nintroduce CardioBench, a comprehensive benchmark for echocardiography FMs.\nCardioBench unifies eight publicly available datasets into a standardized suite\nspanning four regression and five classification tasks, covering functional,\nstructural, diagnostic, and view recognition endpoints. We evaluate several\nleading FM, including cardiac-specific, biomedical, and general-purpose\nencoders, under consistent zero-shot, probing, and alignment protocols. Our\nresults highlight complementary strengths across model families: temporal\nmodeling is critical for functional regression, retrieval provides robustness\nunder distribution shift, and domain-specific text encoders capture\nphysiologically meaningful axes. General-purpose encoders transfer strongly and\noften close the gap with probing, but struggle with fine-grained distinctions\nlike view classification and subtle pathology recognition. By releasing\npreprocessing, splits, and public evaluation pipelines, CardioBench establishes\na reproducible reference point and offers actionable insights to guide the\ndesign of future echocardiography foundation models.",
      "authors": [
        "Darya Taratynova",
        "Ahmed Aly",
        "Numan Saeed",
        "Mohammad Yaqub"
      ],
      "published": "2025-10-01T05:09:48Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00520v1",
      "primary_area": "video_models",
      "secondary_focus": "alignment",
      "application_domain": "medical_ai",
      "tldr_zh": "CardioBench是首个针对超声心动图基础模型的标准化基准测试，整合了8个公开数据集，涵盖4个回归和5个分类任务。研究评估了心脏专用、生物医学和通用编码器在零样本、探测和对齐协议下的表现，发现时序建模对功能回归至关重要，检索增强在分布偏移下具有鲁棒性，而通用编码器在细粒度病理识别方面存在局限。",
      "order": 668
    },
    {
      "arxiv_id": "2510.00515v1",
      "title": "Efficient Multi-modal Large Language Models via Progressive Consistency\n  Distillation",
      "summary": "Visual tokens consume substantial computational resources in multi-modal\nlarge models (MLLMs), significantly compromising their efficiency. Recent works\nhave attempted to improve efficiency by compressing visual tokens during\ntraining, either through modifications to model components or by introducing\nadditional parameters. However, they often overlook the increased learning\ndifficulty caused by such compression, as the model's parameter space struggles\nto quickly adapt to the substantial perturbations in the feature space induced\nby token compression. In this work, we propose to develop Efficient MLLMs via\nProgressive Consistency Distillation (EPIC), a progressive learning framework.\nSpecifically, by decomposing the feature space perturbations introduced by\ntoken compression along the token-wise and layer-wise dimensions, we introduce\ntoken consistency distillation and layer consistency distillation,\nrespectively, aiming to reduce the training difficulty by leveraging guidance\nfrom a teacher model and following a progressive learning trajectory. Extensive\nexperiments demonstrate the superior effectiveness, robustness, and\ngeneralization capabilities of our proposed framework.",
      "authors": [
        "Zichen Wen",
        "Shaobo Wang",
        "Yufa Zhou",
        "Junyuan Zhang",
        "Qintong Zhang",
        "Yifeng Gao",
        "Zhaorun Chen",
        "Bin Wang",
        "Weijia Li",
        "Conghui He",
        "Linfeng Zhang"
      ],
      "published": "2025-10-01T04:56:40Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00515v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出EPIC框架，通过渐进一致性蒸馏解决多模态大模型中视觉令牌压缩导致的训练困难问题。该方法从令牌和层级两个维度分解特征空间扰动，分别引入令牌一致性蒸馏和层级一致性蒸馏，利用教师模型指导并遵循渐进学习轨迹，显著提升了模型效率、鲁棒性和泛化能力。",
      "order": 669
    },
    {
      "arxiv_id": "2510.00506v1",
      "title": "Affordance-Guided Diffusion Prior for 3D Hand Reconstruction",
      "summary": "How can we reconstruct 3D hand poses when large portions of the hand are\nheavily occluded by itself or by objects? Humans often resolve such ambiguities\nby leveraging contextual knowledge -- such as affordances, where an object's\nshape and function suggest how the object is typically grasped. Inspired by\nthis observation, we propose a generative prior for hand pose refinement guided\nby affordance-aware textual descriptions of hand-object interactions (HOI). Our\nmethod employs a diffusion-based generative model that learns the distribution\nof plausible hand poses conditioned on affordance descriptions, which are\ninferred from a large vision-language model (VLM). This enables the refinement\nof occluded regions into more accurate and functionally coherent hand poses.\nExtensive experiments on HOGraspNet, a 3D hand-affordance dataset with severe\nocclusions, demonstrate that our affordance-guided refinement significantly\nimproves hand pose estimation over both recent regression methods and\ndiffusion-based refinement lacking contextual reasoning.",
      "authors": [
        "Naru Suzuki",
        "Takehiko Ohkawa",
        "Tatsuro Banno",
        "Jihyun Lee",
        "Ryosuke Furuta",
        "Yoichi Sato"
      ],
      "published": "2025-10-01T04:36:11Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00506v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于功能感知引导的扩散先验方法，用于解决严重遮挡下的3D手部重建问题。通过结合视觉语言模型推断的手-物体交互功能描述，利用扩散模型生成符合功能逻辑的合理手部姿态，在严重遮挡场景下显著提升了手部姿态估计的准确性。",
      "order": 670
    },
    {
      "arxiv_id": "2510.00505v1",
      "title": "A Fast and Precise Method for Searching Rectangular Tumor Regions in\n  Brain MR Images",
      "summary": "Purpose: To develop a fast and precise method for searching rectangular\nregions in brain tumor images. Methods: The authors propose a new method for\nsearching rectangular tumor regions in brain MR images. The proposed method\nconsisted of a segmentation network and a fast search method with a\nuser-controllable search metric. As the segmentation network, the U-Net whose\nencoder was replaced by the EfficientNet was used. In the fast search method,\nsummed-area tables were used for accelerating sums of voxels in rectangular\nregions. Use of the summed-area tables enabled exhaustive search of the 3D\noffset (3D full search). The search metric was designed for giving priority to\ncubes over oblongs, and assigning better values for higher tumor fractions even\nif they exceeded target tumor fractions. The proposed computation and metric\nwere compared with those used in a conventional method using the Brain Tumor\nImage Segmentation dataset. Results: When the 3D full search was used, the\nproposed computation (8 seconds) was 100-500 times faster than the conventional\ncomputation (11-40 minutes). When the user-controllable parts of the search\nmetrics were changed variously, the tumor fractions of the proposed metric were\nhigher than those of the conventional metric. In addition, the conventional\nmetric preferred oblongs whereas the proposed metric preferred cubes.\nConclusion: The proposed method is promising for implementing fast and precise\nsearch of rectangular tumor regions, which is useful for brain tumor diagnosis\nusing MRI systems. The proposed computation reduced processing times of the 3D\nfull search, and the proposed metric improved the quality of the assigned\nrectangular tumor regions.",
      "authors": [
        "Hidenori Takeshima",
        "Shuki Maruyama"
      ],
      "published": "2025-10-01T04:35:52Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.00505v1",
      "primary_area": "vla_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出一种快速精确的脑部MR图像矩形肿瘤区域搜索方法，结合改进的U-Net分割网络和基于积分图的快速搜索算法，相比传统方法速度提升100-500倍，且更偏好立方体区域而非细长区域，有助于提升脑肿瘤MRI诊断效率。",
      "order": 671
    },
    {
      "arxiv_id": "2510.00500v1",
      "title": "Relative-Absolute Fusion: Rethinking Feature Extraction in Image-Based\n  Iterative Method Selection for Solving Sparse Linear Systems",
      "summary": "Iterative method selection is crucial for solving sparse linear systems\nbecause these methods inherently lack robustness. Though image-based selection\napproaches have shown promise, their feature extraction techniques might encode\ndistinct matrices into identical image representations, leading to the same\nselection and suboptimal method. In this paper, we introduce RAF\n(Relative-Absolute Fusion), an efficient feature extraction technique to\nenhance image-based selection approaches. By simultaneously extracting and\nfusing image representations as relative features with corresponding numerical\nvalues as absolute features, RAF achieves comprehensive matrix representations\nthat prevent feature ambiguity across distinct matrices, thus improving\nselection accuracy and unlocking the potential of image-based selection\napproaches. We conducted comprehensive evaluations of RAF on SuiteSparse and\nour developed BMCMat (Balanced Multi-Classification Matrix dataset),\ndemonstrating solution time reductions of 0.08s-0.29s for sparse linear\nsystems, which is 5.86%-11.50% faster than conventional image-based selection\napproaches and achieves state-of-the-art (SOTA) performance. BMCMat is\navailable at https://github.com/zkqq/BMCMat.",
      "authors": [
        "Kaiqi Zhang",
        "Mingguan Yang",
        "Dali Chang",
        "Chun Chen",
        "Yuxiang Zhang",
        "Kexun He",
        "Jing Zhao"
      ],
      "published": "2025-10-01T04:33:23Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00500v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出RAF（相对-绝对融合）特征提取技术，通过融合图像相对特征与数值绝对特征，解决稀疏线性系统求解中图像化方法选择时不同矩阵产生相同图像表示的问题。在SuiteSparse和BMCMat数据集上的实验表明，该方法比传统图像选择方法快5.86%-11.50%，达到最先进性能。",
      "order": 672
    },
    {
      "arxiv_id": "2510.00495v2",
      "title": "Normal-Abnormal Guided Generalist Anomaly Detection",
      "summary": "Generalist Anomaly Detection (GAD) aims to train a unified model on an\noriginal domain that can detect anomalies in new target domains. Previous GAD\nmethods primarily use only normal samples as references, overlooking the\nvaluable information contained in anomalous samples that are often available in\nreal-world scenarios. To address this limitation, we propose a more practical\napproach: normal-abnormal-guided generalist anomaly detection, which leverages\nboth normal and anomalous samples as references to guide anomaly detection\nacross diverse domains. We introduce the Normal-Abnormal Generalist Learning\n(NAGL) framework, consisting of two key components: Residual Mining (RM) and\nAnomaly Feature Learning (AFL). RM extracts abnormal patterns from\nnormal-abnormal reference residuals to establish transferable anomaly\nrepresentations, while AFL adaptively learns anomaly features in query images\nthrough residual mapping to identify instance-aware anomalies. Our approach\neffectively utilizes both normal and anomalous references for more accurate and\nefficient cross-domain anomaly detection. Extensive experiments across multiple\nbenchmarks demonstrate that our method significantly outperforms existing GAD\napproaches. This work represents the first to adopt a mixture of normal and\nabnormal samples as references in generalist anomaly detection. The code and\ndatasets are available at https://github.com/JasonKyng/NAGL.",
      "authors": [
        "Yuexin Wang",
        "Xiaolei Wang",
        "Yizheng Gong",
        "Jimin Xiao"
      ],
      "published": "2025-10-01T04:27:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00495v2",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种实用的通用异常检测方法NAGL，首次同时利用正常和异常样本作为参考，通过残差挖掘和异常特征学习组件，实现跨域异常检测的显著性能提升。",
      "order": 673
    },
    {
      "arxiv_id": "2510.00483v1",
      "title": "MathSticks: A Benchmark for Visual Symbolic Compositional Reasoning with\n  Matchstick Puzzles",
      "summary": "We introduce \\textsc{MathSticks}, a benchmark for Visual Symbolic\nCompositional Reasoning (VSCR), which unifies visual perception, symbolic\nmanipulation, and arithmetic consistency. Each task presents an incorrect\nmatchstick equation that must be corrected by moving one or two sticks under\nstrict conservation rules. The benchmark includes both text-guided and purely\nvisual settings, systematically covering digit scale, move complexity, solution\nmultiplicity, and operator variation, with 1.4M generated instances and a\ncurated test set. Evaluations of 14 vision--language models reveal substantial\nlimitations: closed-source models succeed only on simple cases, open-source\nmodels fail in the visual regime, while humans exceed 90\\% accuracy. These\nfindings establish \\textsc{MathSticks} as a rigorous testbed for advancing\ncompositional reasoning across vision and symbols. Our code and dataset are\npublicly available at https://github.com/Yuheng2000/MathSticks.",
      "authors": [
        "Yuheng Ji",
        "Huajie Tan",
        "Cheng Chi",
        "Yijie Xu",
        "Yuting Zhao",
        "Enshen Zhou",
        "Huaihai Lyu",
        "Pengwei Wang",
        "Zhongyuan Wang",
        "Shanghang Zhang",
        "Xiaolong Zheng"
      ],
      "published": "2025-10-01T04:04:54Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00483v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "reasoning",
      "application_domain": "education_ai",
      "tldr_zh": "MathSticks是一个视觉符号组合推理基准，通过火柴棒拼图任务评估模型在视觉感知、符号操作和算术一致性方面的能力。该基准包含140万生成实例和精选测试集，评估显示现有视觉语言模型在此类组合推理任务上存在显著局限，而人类准确率超过90%。",
      "order": 674
    },
    {
      "arxiv_id": "2510.00475v1",
      "title": "Diagnosing Shortcut-Induced Rigidity in Continual Learning: The\n  Einstellung Rigidity Index (ERI)",
      "summary": "Deep neural networks frequently exploit shortcut features, defined as\nincidental correlations between inputs and labels without causal meaning.\nShortcut features undermine robustness and reduce reliability under\ndistribution shifts. In continual learning (CL), the consequences of shortcut\nexploitation can persist and intensify: weights inherited from earlier tasks\nbias representation reuse toward whatever features most easily satisfied prior\nlabels, mirroring the cognitive Einstellung effect, a phenomenon where past\nhabits block optimal solutions. Whereas catastrophic forgetting erodes past\nskills, shortcut-induced rigidity throttles the acquisition of new ones. We\nintroduce the Einstellung Rigidity Index (ERI), a compact diagnostic that\ndisentangles genuine transfer from cue-inflated performance using three\ninterpretable facets: (i) Adaptation Delay (AD), (ii) Performance Deficit (PD),\nand (iii) Relative Suboptimal Feature Reliance (SFR_rel). On a two-phase\nCIFAR-100 CL benchmark with a deliberately spurious magenta patch in Phase 2,\nwe evaluate Naive fine-tuning (SGD), online Elastic Weight Consolidation\n(EWC_on), Dark Experience Replay (DER++), Gradient Projection Memory (GPM), and\nDeep Generative Replay (DGR). Across these continual learning methods, we\nobserve that CL methods reach accuracy thresholds earlier than a Scratch-T2\nbaseline (negative AD) but achieve slightly lower final accuracy on patched\nshortcut classes (positive PD). Masking the patch improves accuracy for CL\nmethods while slightly reducing Scratch-T2, yielding negative SFR_rel. This\npattern indicates the patch acted as a distractor for CL models in this setting\nrather than a helpful shortcut.",
      "authors": [
        "Kai Gu",
        "Weishi Shi"
      ],
      "published": "2025-10-01T03:52:40Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00475v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Einstellung刚性指数(ERI)，用于诊断持续学习中的捷径特征诱导刚性现象。通过三个可解释维度(适应延迟、性能赤字、相对次优特征依赖)量化模型对非因果捷径特征的依赖程度，在CIFAR-100基准测试中发现持续学习方法易受虚假特征干扰，揭示了类似认知定势效应的学习刚性问题。",
      "order": 675
    },
    {
      "arxiv_id": "2510.00467v1",
      "title": "Rehearsal-free and Task-free Online Continual Learning With Contrastive\n  Prompt",
      "summary": "The main challenge of continual learning is \\textit{catastrophic forgetting}.\nBecause of processing data in one pass, online continual learning (OCL) is one\nof the most difficult continual learning scenarios. To address catastrophic\nforgetting in OCL, some existing studies use a rehearsal buffer to store\nsamples and replay them in the later learning process, other studies do not\nstore samples but assume a sequence of learning tasks so that the task\nidentities can be explored. However, storing samples may raise data security or\nprivacy concerns and it is not always possible to identify the boundaries\nbetween learning tasks in one pass of data processing. It motivates us to\ninvestigate rehearsal-free and task-free OCL (F2OCL). By integrating prompt\nlearning with an NCM classifier, this study has effectively tackled\ncatastrophic forgetting without storing samples and without usage of task\nboundaries or identities. The extensive experimental results on two benchmarks\nhave demonstrated the effectiveness of the proposed method.",
      "authors": [
        "Aopeng Wang",
        "Ke Deng",
        "Yongli Ren",
        "Jun Luo"
      ],
      "published": "2025-10-01T03:39:29Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00467v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出一种无需排练缓冲区和任务标识的在线持续学习方法，通过结合提示学习和最近类均值分类器，有效解决了灾难性遗忘问题，在两个基准测试中验证了方法的有效性。",
      "order": 676
    },
    {
      "arxiv_id": "2510.00458v1",
      "title": "VLOD-TTA: Test-Time Adaptation of Vision-Language Object Detectors",
      "summary": "Vision-language object detectors (VLODs) such as YOLO-World and Grounding\nDINO achieve impressive zero-shot recognition by aligning region proposals with\ntext representations. However, their performance often degrades under domain\nshift. We introduce VLOD-TTA, a test-time adaptation (TTA) framework for VLODs\nthat leverages dense proposal overlap and image-conditioned prompt scores.\nFirst, an IoU-weighted entropy objective is proposed that concentrates\nadaptation on spatially coherent proposal clusters and reduces confirmation\nbias from isolated boxes. Second, image-conditioned prompt selection is\nintroduced, which ranks prompts by image-level compatibility and fuses the most\ninformative prompts with the detector logits. Our benchmarking across diverse\ndistribution shifts -- including stylized domains, driving scenes, low-light\nconditions, and common corruptions -- shows the effectiveness of our method on\ntwo state-of-the-art VLODs, YOLO-World and Grounding DINO, with consistent\nimprovements over the zero-shot and TTA baselines. Code :\nhttps://github.com/imatif17/VLOD-TTA",
      "authors": [
        "Atif Belal",
        "Heitor R. Medeiros",
        "Marco Pedersoli",
        "Eric Granger"
      ],
      "published": "2025-10-01T03:17:56Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00458v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "VLOD-TTA提出了一种视觉语言目标检测器的测试时自适应框架，通过IoU加权熵目标和图像条件提示选择，在域偏移场景下显著提升YOLO-World和Grounding DINO等模型的性能。",
      "order": 677
    },
    {
      "arxiv_id": "2510.00454v1",
      "title": "Measuring and Controlling the Spectral Bias for Self-Supervised Image\n  Denoising",
      "summary": "Current self-supervised denoising methods for paired noisy images typically\ninvolve mapping one noisy image through the network to the other noisy image.\nHowever, after measuring the spectral bias of such methods using our proposed\nImage Pair Frequency-Band Similarity, it suffers from two practical\nlimitations. Firstly, the high-frequency structural details in images are not\npreserved well enough. Secondly, during the process of fitting high\nfrequencies, the network learns high-frequency noise from the mapped noisy\nimages. To address these challenges, we introduce a Spectral Controlling\nnetwork (SCNet) to optimize self-supervised denoising of paired noisy images.\nFirst, we propose a selection strategy to choose frequency band components for\nnoisy images, to accelerate the convergence speed of training. Next, we present\na parameter optimization method that restricts the learning ability of\nconvolutional kernels to high-frequency noise using the Lipschitz constant,\nwithout changing the network structure. Finally, we introduce the Spectral\nSeparation and low-rank Reconstruction module (SSR module), which separates\nnoise and high-frequency details through frequency domain separation and\nlow-rank space reconstruction, to retain the high-frequency structural details\nof images. Experiments performed on synthetic and real-world datasets verify\nthe effectiveness of SCNet.",
      "authors": [
        "Wang Zhang",
        "Huaqiu Li",
        "Xiaowan Hu",
        "Tao Jiang",
        "Zikang Chen",
        "Haoqian Wang"
      ],
      "published": "2025-10-01T03:07:05Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00454v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出SCNet方法解决自监督图像去噪中的频谱偏差问题。通过频率带选择策略加速训练收敛，利用Lipschitz常数限制卷积核学习高频噪声的能力，并设计频谱分离与低秩重建模块保留图像高频细节。在合成和真实数据集上验证了有效性。",
      "order": 678
    },
    {
      "arxiv_id": "2510.00438v1",
      "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal\n  Integration",
      "summary": "Diffusion Transformer has shown remarkable abilities in generating\nhigh-fidelity videos, delivering visually coherent frames and rich details over\nextended durations. However, existing video generation models still fall short\nin subject-consistent video generation due to an inherent difficulty in parsing\nprompts that specify complex spatial relationships, temporal logic, and\ninteractions among multiple subjects. To address this issue, we propose\nBindWeave, a unified framework that handles a broad range of subject-to-video\nscenarios from single-subject cases to complex multi-subject scenes with\nheterogeneous entities. To bind complex prompt semantics to concrete visual\nsubjects, we introduce an MLLM-DiT framework in which a pretrained multimodal\nlarge language model performs deep cross-modal reasoning to ground entities and\ndisentangle roles, attributes, and interactions, yielding subject-aware hidden\nstates that condition the diffusion transformer for high-fidelity\nsubject-consistent video generation. Experiments on the OpenS2V benchmark\ndemonstrate that our method achieves superior performance across subject\nconsistency, naturalness, and text relevance in generated videos, outperforming\nexisting open-source and commercial models.",
      "authors": [
        "Zhaoyang Li",
        "Dongjun Qian",
        "Kai Su",
        "Qishuai Diao",
        "Xiangyang Xia",
        "Chang Liu",
        "Wenfei Yang",
        "Tianzhu Zhang",
        "Zehuan Yuan"
      ],
      "published": "2025-10-01T02:41:11Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00438v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "BindWeave提出了一种通过多模态大语言模型与扩散Transformer结合的框架，解决现有视频生成模型在复杂多主体场景下主题一致性的难题。该方法通过跨模态推理解析提示中的空间关系、时间逻辑和主体交互，在OpenS2V基准测试中在主题一致性、自然度和文本相关性方面优于现有模型。",
      "order": 679
    },
    {
      "arxiv_id": "2510.00434v1",
      "title": "On-the-Fly Data Augmentation via Gradient-Guided and Sample-Aware\n  Influence Estimation",
      "summary": "Data augmentation has been widely employed to improve the generalization of\ndeep neural networks. Most existing methods apply fixed or random\ntransformations. However, we find that sample difficulty evolves along with the\nmodel's generalization capabilities in dynamic training environments. As a\nresult, applying uniform or stochastic augmentations, without accounting for\nsuch dynamics, can lead to a mismatch between augmented data and the model's\nevolving training needs, ultimately degrading training effectiveness. To\naddress this, we introduce SADA, a Sample-Aware Dynamic Augmentation that\nperforms on-the-fly adjustment of augmentation strengths based on each sample's\nevolving influence on model optimization. Specifically, we estimate each\nsample's influence by projecting its gradient onto the accumulated model update\ndirection and computing the temporal variance within a local training window.\nSamples with low variance, indicating stable and consistent influence, are\naugmented more strongly to emphasize diversity, while unstable samples receive\nmilder transformations to preserve semantic fidelity and stabilize learning.\nOur method is lightweight, which does not require auxiliary models or policy\ntuning. It can be seamlessly integrated into existing training pipelines as a\nplug-and-play module. Experiments across various benchmark datasets and model\narchitectures show consistent improvements of SADA, including +7.3\\% on\nfine-grained tasks and +4.3\\% on long-tailed datasets, highlighting the\nmethod's effectiveness and practicality.",
      "authors": [
        "Suorong Yang",
        "Jie Zong",
        "Lihang Wang",
        "Ziheng Qin",
        "Hai Gan",
        "Pengfei Zhou",
        "Kai Wang",
        "Yang You",
        "Furao Shen"
      ],
      "published": "2025-10-01T02:26:52Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00434v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出SADA方法，一种基于梯度引导和样本感知影响估计的动态数据增强技术。通过计算样本梯度在模型更新方向上的投影及其时间方差，自适应调整增强强度：对影响稳定的样本增强更强以增加多样性，对不稳定样本增强较弱以保持语义保真度。该方法无需额外模型或策略调优，可作为即插即用模块集成到现有训练流程中，在细粒度任务和长尾数据集上分别提升7.3%和4.3%。",
      "order": 680
    },
    {
      "arxiv_id": "2510.00430v1",
      "title": "Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model\n  Alignment",
      "summary": "Despite the recent progress, reinforcement learning (RL)-based fine-tuning of\ndiffusion models often struggles with generalization, composability, and\nrobustness against reward hacking. Recent studies have explored prompt\nrefinement as a modular alternative, but most adopt a feed-forward approach\nthat applies a single refined prompt throughout the entire sampling trajectory,\nthereby failing to fully leverage the sequential nature of reinforcement\nlearning. To address this, here we introduce PromptLoop, a plug-and-play RL\nframework that incorporates latent feedback into step-wise prompt refinement.\nRather than modifying diffusion model weights, a multimodal large language\nmodel (MLLM) is trained with RL to iteratively update prompts based on\nintermediate latent states of diffusion models. This design achieves a\nstructural analogy to the Diffusion RL approach, while retaining the\nflexibility and generality of prompt-based alignment. Extensive experiments\nacross diverse reward functions and diffusion backbones demonstrate that\nPromptLoop (i) achieves effective reward optimization, (ii) generalizes\nseamlessly to unseen models, (iii) composes orthogonally with existing\nalignment methods, and (iv) mitigates over-optimization and reward hacking.",
      "authors": [
        "Suhyeon Lee",
        "Jong Chul Ye"
      ],
      "published": "2025-10-01T02:18:58Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00430v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出PromptLoop框架，通过潜在反馈实现逐步提示优化，解决扩散模型对齐中的泛化性和鲁棒性问题。该方法利用多模态大语言模型基于扩散中间状态迭代更新提示，无需修改模型权重，在多种奖励函数和扩散骨干网络上均表现出色。",
      "order": 681
    },
    {
      "arxiv_id": "2510.00416v1",
      "title": "Domain-Specialized Interactive Segmentation Framework for Meningioma\n  Radiotherapy Planning",
      "summary": "Precise delineation of meningiomas is crucial for effective radiotherapy (RT)\nplanning, directly influencing treatment efficacy and preservation of adjacent\nhealthy tissues. While automated deep learning approaches have demonstrated\nconsiderable potential, achieving consistently accurate clinical segmentation\nremains challenging due to tumor heterogeneity. Interactive Medical Image\nSegmentation (IMIS) addresses this challenge by integrating advanced AI\ntechniques with clinical input. However, generic segmentation tools, despite\nwidespread applicability, often lack the specificity required for clinically\ncritical and disease-specific tasks like meningioma RT planning. To overcome\nthese limitations, we introduce Interactive-MEN-RT, a dedicated IMIS tool\nspecifically developed for clinician-assisted 3D meningioma segmentation in RT\nworkflows. The system incorporates multiple clinically relevant interaction\nmethods, including point annotations, bounding boxes, lasso tools, and\nscribbles, enhancing usability and clinical precision. In our evaluation\ninvolving 500 contrast-enhanced T1-weighted MRI scans from the BraTS 2025\nMeningioma RT Segmentation Challenge, Interactive-MEN-RT demonstrated\nsubstantial improvement compared to other segmentation methods, achieving Dice\nsimilarity coefficients of up to 77.6\\% and Intersection over Union scores of\n64.8\\%. These results emphasize the need for clinically tailored segmentation\nsolutions in critical applications such as meningioma RT planning. The code is\npublicly available at: https://github.com/snuh-rad-aicon/Interactive-MEN-RT",
      "authors": [
        "Junhyeok Lee",
        "Han Jang",
        "Kyu Sung Choi"
      ],
      "published": "2025-10-01T01:57:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00416v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本文提出Interactive-MEN-RT，一种专用于脑膜瘤放疗规划的交互式医学图像分割框架。该系统整合多种临床交互方法（点标注、边界框、套索工具等），在BraTS 2025数据集上达到77.6% Dice系数和64.8% IoU，显著优于通用分割工具，强调临床定制化解决方案在关键医疗应用中的必要性。",
      "order": 682
    },
    {
      "arxiv_id": "2510.00413v1",
      "title": "PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents",
      "summary": "Graphical User Interface (GUI) agents powered by Multimodal Large Language\nModels (MLLMs) promise human-like interaction with software applications, yet\nlong-horizon tasks remain challenging due to memory limitations. Existing\napproaches either truncate history or rely on simple textual summaries, which\nrisk losing critical information when past visual details become necessary for\nfuture decisions. In this paper, we propose \\textbf{PAL-UI} (\\textbf{P}lanning\nwith \\textbf{A}ctive \\textbf{L}ook-back), a novel framework that enables GUI\nagents to adaptively retrieve past observations when required. PAL-UI combines\na dual-level summarization agent, capturing both observation-level cues and\naction-level outcomes, with a dedicated retrieval tool that allows the agent to\nrecall specific historical screenshots during planning. We curate a step-level\ninstruction dataset of 8.6K samples from mobile GUI navigation trajectories and\ntrain \\textbf{PAL-UI-3B} and \\textbf{PAL-UI-7B} models based on Qwen2.5-VL.\nExtensive experiments demonstrate that PAL-UI significantly outperforms\nbaseline models and prior methods in mobile GUI navigation tasks, even under\ndata-efficient settings. Moreover, PAL-UI exhibits strong cross-domain\ngeneralization, achieving notable improvements in web navigation without\nadditional training. Our work highlights the potential of active memory\nretrieval for long-horizon planning capabilities of vision-based GUI agents.",
      "authors": [
        "Zikang Liu",
        "Junyi Li",
        "Wayne Xin Zhao",
        "Dawei Gao",
        "Yaliang Li",
        "Ji-rong Wen"
      ],
      "published": "2025-10-01T01:48:39Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00413v1",
      "primary_area": "vla_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "PAL-UI是一种新型视觉GUI代理框架，通过主动回溯机制解决长时任务中的记忆限制问题。该框架采用双层级摘要代理和专用检索工具，能够自适应调用历史屏幕截图，在移动GUI导航任务中显著优于基线模型，并展现出强大的跨领域泛化能力。",
      "order": 683
    },
    {
      "arxiv_id": "2510.00411v2",
      "title": "Does Bigger Mean Better? Comparitive Analysis of CNNs and Biomedical\n  Vision Language Modles in Medical Diagnosis",
      "summary": "The accurate interpretation of chest radiographs using automated methods is a\ncritical task in medical imaging. This paper presents a comparative analysis\nbetween a supervised lightweight Convolutional Neural Network (CNN) and a\nstate-of-the-art, zero-shot medical Vision-Language Model (VLM), BiomedCLIP,\nacross two distinct diagnostic tasks: pneumonia detection on the PneumoniaMNIST\nbenchmark and tuberculosis detection on the Shenzhen TB dataset. Our\nexperiments show that supervised CNNs serve as highly competitive baselines in\nboth cases. While the default zero-shot performance of the VLM is lower, we\ndemonstrate that its potential can be unlocked via a simple yet crucial remedy:\ndecision threshold calibration. By optimizing the classification threshold on a\nvalidation set, the performance of BiomedCLIP is significantly boosted across\nboth datasets. For pneumonia detection, calibration enables the zero-shot VLM\nto achieve a superior F1-score of 0.8841, surpassing the supervised CNN's\n0.8803. For tuberculosis detection, calibration dramatically improves the\nF1-score from 0.4812 to 0.7684, bringing it close to the supervised baseline's\n0.7834. This work highlights a key insight: proper calibration is essential for\nleveraging the full diagnostic power of zero-shot VLMs, enabling them to match\nor even outperform efficient, task-specific supervised models.",
      "authors": [
        "Ran Tong",
        "Jiaqi Liu",
        "Su Liu",
        "Jiexi Xu",
        "Lanruo Wang",
        "Tong Wang"
      ],
      "published": "2025-10-01T01:46:09Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00411v2",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本文比较了轻量级CNN与BiomedCLIP视觉语言模型在肺炎和结核病诊断中的表现。研究发现，通过决策阈值校准，零射VLM在肺炎检测中F1分数达0.8841超越CNN的0.8803，在结核病检测中从0.4812提升至0.7684接近CNN的0.7834，表明适当校准可使零射模型媲美专用监督模型。",
      "order": 684
    },
    {
      "arxiv_id": "2510.00406v1",
      "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified\n  Rewards in World Simulators",
      "summary": "Vision-Language-Action (VLA) models enable embodied decision-making but rely\nheavily on imitation learning, leading to compounding errors and poor\nrobustness under distribution shift. Reinforcement learning (RL) can mitigate\nthese issues yet typically demands costly real-world interactions or suffers\nfrom sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning\nframework that leverages a data-driven world model as a controllable simulator.\nTrained from real interaction data, the simulator predicts future visual\nobservations conditioned on actions, allowing policy rollouts with dense,\ntrajectory-level rewards derived from goal-achieving references. This design\ndelivers an efficient and action-aligned learning signal, drastically lowering\nsample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses\nstrong supervised baselines and achieves greater efficiency than\nsimulator-based RL. Moreover, it exhibits strong robustness under perturbed\nconditions, sustaining stable task execution. Our results establish\nworld-model-based RFT as a practical post-training paradigm to enhance the\ngeneralization and robustness of VLA models. For more details, please refer to\nhttps://vla-rft.github.io/.",
      "authors": [
        "Hengtao Li",
        "Pengxiang Ding",
        "Runze Suo",
        "Yihao Wang",
        "Zirui Ge",
        "Dongyuan Zang",
        "Kexian Yu",
        "Mingyang Sun",
        "Hongyin Zhang",
        "Donglin Wang",
        "Weihua Su"
      ],
      "published": "2025-10-01T01:33:10Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00406v1",
      "primary_area": "vla_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "VLA-RFT提出一种基于世界模型的强化微调框架，通过数据驱动的可控模拟器生成密集奖励信号，仅需不到400步微调即可超越监督基线，显著提升VLA模型的泛化能力和鲁棒性。",
      "order": 685
    },
    {
      "arxiv_id": "2510.00405v1",
      "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy\n  Observations",
      "summary": "Reliable trajectory prediction from an ego-centric perspective is crucial for\nrobotic navigation in human-centric environments. However, existing methods\ntypically assume idealized observation histories, failing to account for the\nperceptual artifacts inherent in first-person vision, such as occlusions, ID\nswitches, and tracking drift. This discrepancy between training assumptions and\ndeployment reality severely limits model robustness. To bridge this gap, we\nintroduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,\nfirst-person visual histories in clean, bird's-eye-view future trajectories,\nenabling robust learning under realistic perceptual constraints. Building on\nthis benchmark, we propose BiFlow, a dual-stream flow matching model that\nconcurrently denoises historical observations and forecasts future motion by\nleveraging a shared latent representation. To better model agent intent, BiFlow\nincorporates our EgoAnchor mechanism, which conditions the prediction decoder\non distilled historical features via feature modulation. Extensive experiments\nshow that BiFlow achieves state-of-the-art performance, reducing minADE and\nminFDE by 10-15% on average and demonstrating superior robustness. We\nanticipate that our benchmark and model will provide a critical foundation for\ndeveloping trajectory forecasting systems truly resilient to the challenges of\nreal-world, ego-centric perception.",
      "authors": [
        "Jiayi Liu",
        "Jiaming Zhou",
        "Ke Ye",
        "Kun-Yu Lin",
        "Allan Wang",
        "Junwei Liang"
      ],
      "published": "2025-10-01T01:30:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00405v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出EgoTraj-Bench基准测试和BiFlow双流匹配模型，解决第一人称视角下轨迹预测的噪声观测问题。通过共享潜在表征同时去噪历史观测和预测未来运动，引入EgoAnchor机制建模智能体意图，在真实感知约束下实现最先进的轨迹预测性能。",
      "order": 686
    },
    {
      "arxiv_id": "2510.00392v1",
      "title": "A Deep Learning Pipeline for Epilepsy Genomic Analysis Using GPT-2 XL\n  and NVIDIA H100",
      "summary": "Epilepsy is a chronic neurological condition characterized by recurrent\nseizures, with global prevalence estimated at 50 million people worldwide.\nWhile progress in high-throughput sequencing has allowed for broad-based\ntranscriptomic profiling of brain tissues, the deciphering of these highly\ncomplex datasets remains one of the challenges. To address this issue, in this\npaper we propose a new analysis pipeline that integrates the power of deep\nlearning strategies with GPU-acceleration computation for investigating Gene\nexpression patterns in epilepsy. Specifically, our proposed approach employs\nGPT-2 XL, a transformer-based Large Language Model (LLM) with 1.5 billion\nparameters for genomic sequence analysis over the latest NVIDIA H100 Tensor\nCore GPUs based on Hopper architecture. Our proposed method enables efficient\npreprocessing of RNA sequence data, gene sequence encoding, and subsequent\npattern identification. We conducted experiments on two epilepsy datasets\nincluding GEO accession GSE264537 and GSE275235. The obtained results reveal\nseveral significant transcriptomic modifications, including reduced hippocampal\nastrogliosis after ketogenic diet treatment as well as restored\nexcitatory-inhibitory signaling equilibrium in zebrafish epilepsy model.\nMoreover, our results highlight the effectiveness of leveraging LLMs in\ncombination with advanced hardware acceleration for transcriptomic\ncharacterization in neurological diseases.",
      "authors": [
        "Muhammad Omer Latif",
        "Hayat Ullah",
        "Muhammad Ali Shafique",
        "Zhihua Dong"
      ],
      "published": "2025-10-01T01:07:35Z",
      "primary_category": "q-bio.GN",
      "arxiv_url": "https://arxiv.org/abs/2510.00392v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出一种结合GPT-2 XL大语言模型与NVIDIA H100 GPU的深度学习流程，用于癫痫基因组分析。该方法能高效处理RNA序列数据并进行基因模式识别，在癫痫数据集实验中成功揭示了海马区星形胶质细胞增生减少和兴奋-抑制信号平衡恢复等关键转录组变化，证明了大语言模型结合硬件加速在神经系统疾病转录组表征中的有效性。",
      "order": 687
    },
    {
      "arxiv_id": "2510.00376v1",
      "title": "Discrete Wavelet Transform as a Facilitator for Expressive Latent Space\n  Representation in Variational Autoencoders in Satellite Imagery",
      "summary": "Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the\ncomputational complexity of pixel-space diffusion by operating within a\ncompressed latent space constructed by Variational Autoencoders (VAEs),\ndemonstrating significant advantages in Remote Sensing (RS) applications.\nThough numerous studies enhancing LDMs have been conducted, investigations\nexplicitly targeting improvements within the intrinsic latent space remain\nscarce. This paper proposes an innovative perspective, utilizing the Discrete\nWavelet Transform (DWT) to enhance the VAE's latent space representation,\ndesigned for satellite imagery. The proposed method, ExpDWT-VAE, introduces\ndual branches: one processes spatial domain input through convolutional\noperations, while the other extracts and processes frequency-domain features\nvia 2D Haar wavelet decomposition, convolutional operation, and inverse DWT\nreconstruction. These branches merge to create an integrated spatial-frequency\nrepresentation, further refined through convolutional and diagonal Gaussian\nmapping into a robust latent representation. We utilize a new satellite imagery\ndataset housed by the TerraFly mapping system to validate our method.\nExperimental results across several performance metrics highlight the efficacy\nof the proposed method at enhancing latent space representation.",
      "authors": [
        "Arpan Mahara",
        "Md Rezaul Karim Khan",
        "Naphtali Rishe",
        "Wenjia Wang",
        "Seyed Masoud Sadjadi"
      ],
      "published": "2025-10-01T00:49:41Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00376v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ExpDWT-VAE方法，通过离散小波变换增强变分自编码器的潜在空间表示，结合空间域和频域特征处理，在卫星影像数据集上验证了改进的潜在空间表达能力。",
      "order": 688
    },
    {
      "arxiv_id": "2510.02312v1",
      "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
      "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
      "authors": [
        "Anna Kuzina",
        "Maciej Pioro",
        "Paul N. Whatmough",
        "Babak Ehteshami Bejnordi"
      ],
      "published": "2025-10-02T17:59:51Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02312v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "KaVa提出首个通过压缩KV缓存蒸馏实现潜在推理的框架，将教师模型的KV轨迹知识蒸馏到潜在推理学生模型中，在保持推理效率的同时显著提升自然语言推理性能，解决了潜在推理缺乏监督信号的关键问题。",
      "order": 689
    },
    {
      "arxiv_id": "2510.02311v1",
      "title": "Inferring Dynamic Physical Properties from Video Foundation Models",
      "summary": "We study the task of predicting dynamic physical properties from videos. More\nspecifically, we consider physical properties that require temporal information\nto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,\nand dynamic friction of an object sliding on a surface. To this end, we make\nthe following contributions: (i) We collect a new video dataset for each\nphysical property, consisting of synthetic training and testing splits, as well\nas a real split for real world evaluation. (ii) We explore three ways to infer\nthe physical property from videos: (a) an oracle method where we supply the\nvisual cues that intrinsically reflect the property using classical computer\nvision techniques; (b) a simple read out mechanism using a visual prompt and\ntrainable prompt vector for cross-attention on pre-trained video generative and\nself-supervised models; and (c) prompt strategies for Multi-modal Large\nLanguage Models (MLLMs). (iii) We show that video foundation models trained in\na generative or self-supervised manner achieve a similar performance, though\nbehind that of the oracle, and MLLMs are currently inferior to the other\nmodels, though their performance can be improved through suitable prompting.",
      "authors": [
        "Guanqi Zhan",
        "Xianzheng Ma",
        "Weidi Xie",
        "Andrew Zisserman"
      ],
      "published": "2025-10-02T17:59:50Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02311v1",
      "primary_area": "video_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究探索从视频中推断动态物理属性的方法，包括弹性、粘度和动摩擦。构建了合成与真实视频数据集，比较了三种推断方法：基于传统视觉的预言机方法、预训练视频模型的提示机制和多模态大语言模型。结果显示生成式和自监督视频基础模型表现相近，多模态大模型当前效果较差但可通过提示优化提升。",
      "order": 690
    },
    {
      "arxiv_id": "2510.02308v1",
      "title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient\n  Orthogonalization",
      "summary": "Estimating the tangent spaces of a data manifold is a fundamental problem in\ndata analysis. The standard approach, Local Principal Component Analysis\n(LPCA), struggles in high-noise settings due to a critical trade-off in\nchoosing the neighborhood size. Selecting an optimal size requires prior\nknowledge of the geometric and noise characteristics of the data that are often\nunavailable. In this paper, we propose a spectral method, Laplacian Eigenvector\nGradient Orthogonalization (LEGO), that utilizes the global structure of the\ndata to guide local tangent space estimation. Instead of relying solely on\nlocal neighborhoods, LEGO estimates the tangent space at each data point by\northogonalizing the gradients of low-frequency eigenvectors of the graph\nLaplacian. We provide two theoretical justifications of our method. First, a\ndifferential geometric analysis on a tubular neighborhood of a manifold shows\nthat gradients of the low-frequency Laplacian eigenfunctions of the tube align\nclosely with the manifold's tangent bundle, while an eigenfunction with high\ngradient in directions orthogonal to the manifold lie deeper in the spectrum.\nSecond, a random matrix theoretic analysis also demonstrates that low-frequency\neigenvectors are robust to sub-Gaussian noise. Through comprehensive\nexperiments, we demonstrate that LEGO yields tangent space estimates that are\nsignificantly more robust to noise than those from LPCA, resulting in marked\nimprovements in downstream tasks such as manifold learning, boundary detection,\nand local intrinsic dimension estimation.",
      "authors": [
        "Dhruv Kohli",
        "Sawyer J. Robertson",
        "Gal Mishne",
        "Alexander Cloninger"
      ],
      "published": "2025-10-02T17:59:45Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02308v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出LEGO方法，通过拉普拉斯特征向量梯度正交化，利用数据全局结构指导局部切空间估计，解决了传统LPCA在高噪声环境下邻域大小选择的难题。理论分析和实验表明，该方法在流形学习、边界检测等任务中具有更强的噪声鲁棒性。",
      "order": 691
    },
    {
      "arxiv_id": "2510.02305v1",
      "title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is\n  Geometry Adaptive",
      "summary": "Diffusion models have achieved state-of-the-art performance, demonstrating\nremarkable generalisation capabilities across diverse domains. However, the\nmechanisms underpinning these strong capabilities remain only partially\nunderstood. A leading conjecture, based on the manifold hypothesis, attributes\nthis success to their ability to adapt to low-dimensional geometric structure\nwithin the data. This work provides evidence for this conjecture, focusing on\nhow such phenomena could result from the formulation of the learning problem\nthrough score matching. We inspect the role of implicit regularisation by\ninvestigating the effect of smoothing minimisers of the empirical score\nmatching objective. Our theoretical and empirical results confirm that\nsmoothing the score function -- or equivalently, smoothing in the log-density\ndomain -- produces smoothing tangential to the data manifold. In addition, we\nshow that the manifold along which the diffusion model generalises can be\ncontrolled by choosing an appropriate smoothing.",
      "authors": [
        "Tyler Farghly",
        "Peter Potaptchik",
        "Samuel Howard",
        "George Deligiannidis",
        "Jakiw Pidstrigach"
      ],
      "published": "2025-10-02T17:59:39Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02305v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究验证了扩散模型成功的关键在于其对数据流形几何结构的自适应能力。通过分析分数匹配学习问题，发现对数密度域平滑处理能产生沿数据流形的切向平滑，且可通过选择不同平滑方式控制模型泛化的流形结构。",
      "order": 692
    },
    {
      "arxiv_id": "2510.02302v1",
      "title": "Knowledge Distillation Detection for Open-weights Models",
      "summary": "We propose the task of knowledge distillation detection, which aims to\ndetermine whether a student model has been distilled from a given teacher,\nunder a practical setting where only the student's weights and the teacher's\nAPI are available. This problem is motivated by growing concerns about model\nprovenance and unauthorized replication through distillation. To address this\ntask, we introduce a model-agnostic framework that combines data-free input\nsynthesis and statistical score computation for detecting distillation. Our\napproach is applicable to both classification and generative models.\nExperiments on diverse architectures for image classification and text-to-image\ngeneration show that our method improves detection accuracy over the strongest\nbaselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image\ngeneration. The code is available at\nhttps://github.com/shqii1j/distillation_detection.",
      "authors": [
        "Qin Shi",
        "Amber Yijia Zheng",
        "Qifan Song",
        "Raymond A. Yeh"
      ],
      "published": "2025-10-02T17:59:14Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02302v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出知识蒸馏检测任务，旨在判断学生模型是否从给定教师模型蒸馏而来。开发了一种模型无关框架，结合无数据输入合成和统计评分方法，适用于分类和生成模型。实验显示在多个数据集上检测准确率显著提升。",
      "order": 693
    },
    {
      "arxiv_id": "2510.02300v1",
      "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
      "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.",
      "authors": [
        "Runqian Wang",
        "Yilun Du"
      ],
      "published": "2025-10-02T17:59:06Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02300v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出平衡匹配(EqM)生成建模框架，摒弃传统扩散/流模型中的非平衡时间条件动态，转而学习隐式能量景观的平衡梯度。该方法在推理时采用基于优化的采样过程，通过梯度下降在学习的景观上生成样本，支持可调步长、自适应优化器和计算。EqM在ImageNet 256×256上达到1.90 FID，超越现有扩散/流模型，并能自然处理图像去噪、OOD检测和图像合成等任务。",
      "order": 694
    },
    {
      "arxiv_id": "2510.02297v1",
      "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
      "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
      "authors": [
        "Wentao Zhang",
        "Yang Young Lu",
        "Yuntian Deng"
      ],
      "published": "2025-10-02T17:59:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02297v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出交互式训练框架，通过控制服务器实现人机协同实时干预神经网络训练过程，支持动态调整超参数、训练数据和模型检查点，在三个案例中证明其能提升训练稳定性、降低超参数敏感性并增强适应性。",
      "order": 695
    },
    {
      "arxiv_id": "2510.02296v1",
      "title": "Continual Personalization for Diffusion Models",
      "summary": "Updating diffusion models in an incremental setting would be practical in\nreal-world applications yet computationally challenging. We present a novel\nlearning strategy of Concept Neuron Selection (CNS), a simple yet effective\napproach to perform personalization in a continual learning scheme. CNS\nuniquely identifies neurons in diffusion models that are closely related to the\ntarget concepts. In order to mitigate catastrophic forgetting problems while\npreserving zero-shot text-to-image generation ability, CNS finetunes concept\nneurons in an incremental manner and jointly preserves knowledge learned of\nprevious concepts. Evaluation of real-world datasets demonstrates that CNS\nachieves state-of-the-art performance with minimal parameter adjustments,\noutperforming previous methods in both single and multi-concept personalization\nworks. CNS also achieves fusion-free operation, reducing memory storage and\nprocessing time for continual personalization.",
      "authors": [
        "Yu-Chien Liao",
        "Jr-Jen Chen",
        "Chi-Pin Huang",
        "Ci-Siang Lin",
        "Meng-Lin Wu",
        "Yu-Chiang Frank Wang"
      ],
      "published": "2025-10-02T17:58:56Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02296v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出概念神经元选择(CNS)方法，通过在扩散模型中识别与目标概念相关的神经元进行增量微调，实现持续个性化学习。该方法能有效缓解灾难性遗忘问题，保持零样本文本到图像生成能力，同时减少内存占用和处理时间，在单概念和多概念个性化任务中均达到最先进性能。",
      "order": 696
    },
    {
      "arxiv_id": "2510.02295v1",
      "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
      "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.",
      "authors": [
        "Enxin Song",
        "Wenhao Chai",
        "Shusheng Yang",
        "Ethan Armand",
        "Xiaojun Shan",
        "Haiyang Xu",
        "Jianwen Xie",
        "Zhuowen Tu"
      ],
      "published": "2025-10-02T17:58:54Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02295v1",
      "primary_area": "vla_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "VideoNSA通过原生稀疏注意力机制增强视频语言模型，在Qwen2.5-VL基础上采用硬件感知混合注意力方案：文本保持稠密注意力，视频使用稀疏注意力。该方法在21.6万视频指令数据集上端到端训练，支持128K令牌长视频理解，在时序推理和空间基准测试中优于令牌压缩和训练无关稀疏基线，并揭示了全局-局部注意力分配优化等关键发现。",
      "order": 697
    },
    {
      "arxiv_id": "2510.02291v1",
      "title": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling",
      "summary": "We study the problem of posterior sampling using pretrained discrete\ndiffusion foundation models, aiming to recover images from noisy measurements\nwithout retraining task-specific models. While diffusion models have achieved\nremarkable success in generative modeling, most advances rely on continuous\nGaussian diffusion. In contrast, discrete diffusion offers a unified framework\nfor jointly modeling categorical data such as text and images. Beyond\nunification, discrete diffusion provides faster inference, finer control, and\nprincipled training-free Bayesian inference, making it particularly well-suited\nfor posterior sampling. However, existing approaches to discrete diffusion\nposterior sampling face severe challenges: derivative-free guidance yields\nsparse signals, continuous relaxations limit applicability, and split Gibbs\nsamplers suffer from the curse of dimensionality. To overcome these\nlimitations, we introduce Anchored Posterior Sampling (APS) for masked\ndiffusion foundation models, built on two key innovations -- quantized\nexpectation for gradient-like guidance in discrete embedding space, and\nanchored remasking for adaptive decoding. Our approach achieves\nstate-of-the-art performance among discrete diffusion samplers across linear\nand nonlinear inverse problems on the standard benchmarks. We further\ndemonstrate the benefits of our approach in training-free stylization and\ntext-guided editing.",
      "authors": [
        "Litu Rout",
        "Andreas Lugmayr",
        "Yasamin Jafarian",
        "Srivatsan Varadharajan",
        "Constantine Caramanis",
        "Sanjay Shakkottai",
        "Ira Kemelmacher-Shlizerman"
      ],
      "published": "2025-10-02T17:58:37Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02291v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出锚定后验采样(APS)方法，针对离散扩散基础模型的后验采样问题。通过量化期望和锚定重掩码两项创新技术，在离散嵌入空间实现梯度式引导和自适应解码，在线性和非线性逆问题中达到最先进性能，支持无需训练的风格化和文本引导编辑。",
      "order": 698
    },
    {
      "arxiv_id": "2510.02286v1",
      "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
      "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
      "authors": [
        "Ruohao Guo",
        "Afshin Oroojlooy",
        "Roshan Sridhar",
        "Miguel Ballesteros",
        "Alan Ritter",
        "Dan Roth"
      ],
      "published": "2025-10-02T17:57:05Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02286v1",
      "primary_area": "text_models",
      "secondary_focus": "dialogue_systems",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出DialTree-RPO框架，结合强化学习与树搜索，自主发现多轮对话攻击策略，在10个目标模型上攻击成功率比现有方法提升25.9%，有效应对大语言模型在多轮交互中的安全漏洞。",
      "order": 699
    },
    {
      "arxiv_id": "2510.02284v1",
      "title": "Learning to Generate Object Interactions with Physics-Guided Video\n  Diffusion",
      "summary": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.",
      "authors": [
        "David Romero",
        "Ariana Bermudez",
        "Hao Li",
        "Fabio Pizzati",
        "Ivan Laptev"
      ],
      "published": "2025-10-02T17:56:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02284v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出KineMask方法，通过物理引导的视频扩散模型实现逼真的物体交互生成。该方法采用两阶段训练策略，结合单张图像和指定物体速度生成未来交互视频，在合成场景和真实场景中均显著提升了物体交互的物理合理性，并支持低层运动控制与高层文本条件的有效结合。",
      "order": 700
    },
    {
      "arxiv_id": "2510.02282v1",
      "title": "VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning\n  MLLMs and RL",
      "summary": "With the rapid advancement of AI-generated videos, there is an urgent need\nfor effective detection tools to mitigate societal risks such as misinformation\nand reputational harm. In addition to accurate classification, it is essential\nthat detection models provide interpretable explanations to ensure transparency\nfor regulators and end users. To address these challenges, we introduce\nVidGuard-R1, the first video authenticity detector that fine-tunes a\nmulti-modal large language model (MLLM) using group relative policy\noptimization (GRPO). Our model delivers both highly accurate judgments and\ninsightful reasoning. We curate a challenging dataset of 140k real and\nAI-generated videos produced by state-of-the-art generation models, carefully\ndesigning the generation process to maximize discrimination difficulty. We then\nfine-tune Qwen-VL using GRPO with two specialized reward models that target\ntemporal artifacts and generation complexity. Extensive experiments demonstrate\nthat VidGuard-R1 achieves state-of-the-art zero-shot performance on existing\nbenchmarks, with additional training pushing accuracy above 95%. Case studies\nfurther show that VidGuard-R1 produces precise and interpretable rationales\nbehind its predictions. The code is publicly available at\nhttps://VidGuard-R1.github.io.",
      "authors": [
        "Kyoungjun Park",
        "Yifan Yang",
        "Juheon Yi",
        "Shicheng Zheng",
        "Yifei Shen",
        "Dongqi Han",
        "Caihua Shan",
        "Muhammad Muaz",
        "Lili Qiu"
      ],
      "published": "2025-10-02T17:55:37Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02282v1",
      "primary_area": "video_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "VidGuard-R1是首个通过多模态大语言模型与强化学习相结合的视频真实性检测系统，采用GRPO优化方法和专门设计的奖励模型，在14万真实与AI生成视频数据集上训练，零样本检测准确率领先，并能提供可解释的检测推理过程。",
      "order": 701
    },
    {
      "arxiv_id": "2510.02279v1",
      "title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods\n  for Natural Language Generation",
      "summary": "Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings.",
      "authors": [
        "Mykyta Ielanskyi",
        "Kajetan Schweighofer",
        "Lukas Aichberger",
        "Sepp Hochreiter"
      ],
      "published": "2025-10-02T17:54:09Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02279v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对自然语言生成中不确定性估计方法的评估缺陷，指出常用近似正确性函数存在显著分歧，导致方法排名不可靠。提出使用多种风险指标增强评估鲁棒性，在问答任务中通过集成多个LLM评判变体减少评估偏差，并探索结构化任务及分布外检测任务。最后引入Elo评分对不确定性估计方法进行客观总结。",
      "order": 702
    },
    {
      "arxiv_id": "2510.02278v1",
      "title": "Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks",
      "summary": "Traffic forecasting on road networks is a complex task of significant\npractical importance that has recently attracted considerable attention from\nthe machine learning community, with spatiotemporal graph neural networks\n(GNNs) becoming the most popular approach. The proper evaluation of traffic\nforecasting methods requires realistic datasets, but current publicly available\nbenchmarks have significant drawbacks, including the absence of information\nabout road connectivity for road graph construction, limited information about\nroad properties, and a relatively small number of road segments that falls\nshort of real-world applications. Further, current datasets mostly contain\ninformation about intercity highways with sparsely located sensors, while city\nroad networks arguably present a more challenging forecasting task due to much\ndenser roads and more complex urban traffic patterns. In this work, we provide\na more complete, realistic, and challenging benchmark for traffic forecasting\nby releasing datasets representing the road networks of two major cities, with\nthe largest containing almost 100,000 road segments (more than a 10-fold\nincrease relative to existing datasets). Our datasets contain rich road\nfeatures and provide fine-grained data about both traffic volume and traffic\nspeed, allowing for building more holistic traffic forecasting systems. We show\nthat most current implementations of neural spatiotemporal models for traffic\nforecasting have problems scaling to datasets of our size. To overcome this\nissue, we propose an alternative approach to neural traffic forecasting that\nuses a GNN without a dedicated module for temporal sequence processing, thus\nachieving much better scalability, while also demonstrating stronger\nforecasting performance. We hope our datasets and modeling insights will serve\nas a valuable resource for research in traffic forecasting.",
      "authors": [
        "Fedor Velikonivtsev",
        "Oleg Platonov",
        "Gleb Bazhenov",
        "Liudmila Prokhorenkova"
      ],
      "published": "2025-10-02T17:53:51Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02278v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一个更完整、真实且具有挑战性的城市交通预测基准数据集，包含两个主要城市的路网数据，最大规模达10万路段。针对现有时空图神经网络扩展性问题，提出一种无需专用时序处理模块的GNN替代方案，在保证预测性能的同时显著提升可扩展性。",
      "order": 703
    },
    {
      "arxiv_id": "2510.02274v1",
      "title": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps",
      "summary": "Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information.",
      "authors": [
        "Kyoungjun Park",
        "Yifan Yang",
        "Changhan Ge",
        "Lili Qiu",
        "Shiqi Jiang"
      ],
      "published": "2025-10-02T17:50:22Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02274v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "Diffusion^2是一种基于扩散模型的射频信号传播预测方法，利用3D点云数据模拟从Wi-Fi到毫米波等多种频率的射频信号传播。该方法通过RF-3D编码器捕捉3D几何特征，采用多尺度嵌入模拟信号传播过程，在合成和真实测量数据上相比现有方法误差仅1.9 dB且速度快27倍。",
      "order": 704
    },
    {
      "arxiv_id": "2510.02265v1",
      "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement\n  Learning",
      "summary": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime.",
      "authors": [
        "Yalin E. Sagduyu",
        "Tugba Erpek",
        "Kemal Davaslioglu",
        "Sastry Kompella"
      ],
      "published": "2025-10-02T17:44:38Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02265v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究如何利用强化学习对抗反应式动态干扰攻击，通过Q学习和深度Q网络自适应调整传输功率、调制方式和信道选择，在未知信道条件和干扰策略的情况下维持高传输速率。",
      "order": 705
    },
    {
      "arxiv_id": "2510.02264v1",
      "title": "Paving the Way Towards Kinematic Assessment Using Monocular Video: A\n  Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose\n  Estimators Against Inertial Sensors in Daily Living Activities",
      "summary": "Advances in machine learning and wearable sensors offer new opportunities for\ncapturing and analyzing human movement outside specialized laboratories.\nAccurate assessment of human movement under real-world conditions is essential\nfor telemedicine, sports science, and rehabilitation. This preclinical\nbenchmark compares monocular video-based 3D human pose estimation models with\ninertial measurement units (IMUs), leveraging the VIDIMU dataset containing a\ntotal of 13 clinically relevant daily activities which were captured using both\ncommodity video cameras and five IMUs. During this initial study only healthy\nsubjects were recorded, so results cannot be generalized to pathological\ncohorts. Joint angles derived from state-of-the-art deep learning frameworks\n(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA\nBodyTrack) were evaluated against joint angles computed from IMU data using\nOpenSim inverse kinematics following the Human3.6M dataset format with 17\nkeypoints. Among them, MotionAGFormer demonstrated superior performance,\nachieving the lowest overall RMSE ($9.27\\deg \\pm 4.80\\deg$) and MAE ($7.86\\deg\n\\pm 4.18\\deg$), as well as the highest Pearson correlation ($0.86 \\pm 0.15$)\nand the highest coefficient of determination $R^{2}$ ($0.67 \\pm 0.28$). The\nresults reveal that both technologies are viable for out-of-the-lab kinematic\nassessment. However, they also highlight key trade-offs between video- and\nsensor-based approaches including costs, accessibility, and precision. This\nstudy clarifies where off-the-shelf video models already provide clinically\npromising kinematics in healthy adults and where they lag behind IMU-based\nestimates while establishing valuable guidelines for researchers and clinicians\nseeking to develop robust, cost-effective, and user-friendly solutions for\ntelehealth and remote patient monitoring.",
      "authors": [
        "Mario Medrano-Paredes",
        "Carmen Fernández-González",
        "Francisco-Javier Díaz-Pernas",
        "Hichem Saoudi",
        "Javier González-Alonso",
        "Mario Martínez-Zarzuela"
      ],
      "published": "2025-10-02T17:44:31Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02264v1",
      "primary_area": "video_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究比较了基于单目视频的3D人体姿态估计模型与惯性传感器在日常生活活动中的性能，使用VIDIMU数据集评估了四种深度学习框架。MotionAGFormer表现最佳，整体RMSE为9.27°±4.80°，皮尔逊相关系数达0.86±0.15。研究揭示了视频与传感器方案在成本、可及性和精度间的权衡，为远程医疗和患者监测提供了实用指南。",
      "order": 706
    },
    {
      "arxiv_id": "2510.02263v1",
      "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
      "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.",
      "authors": [
        "Yuxiao Qu",
        "Anikait Singh",
        "Yoonho Lee",
        "Amrith Setlur",
        "Ruslan Salakhutdinov",
        "Chelsea Finn",
        "Aviral Kumar"
      ],
      "published": "2025-10-02T17:44:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02263v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "RLAD提出一种双智能体强化学习框架，通过训练抽象生成器和解决方案生成器来提升大语言模型的推理能力。该方法让模型先提出自然语言描述的推理抽象概念，再利用这些抽象指导问题求解，有效改善结构化探索和泛化性能，在复杂推理任务中表现优于传统思维链方法。",
      "order": 707
    },
    {
      "arxiv_id": "2510.02259v1",
      "title": "Transformers Discover Molecular Structure Without Graph Priors",
      "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinates$\\unicode{x2013}$without predefined graphs or physical\npriors$\\unicode{x2013}$can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatterns$\\unicode{x2013}$such as attention weights that decay inversely with\ninteratomic distance$\\unicode{x2013}$and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.",
      "authors": [
        "Tobias Kreiman",
        "Yutong Bai",
        "Fadi Atieh",
        "Elizabeth Weaver",
        "Eric Qu",
        "Aditi S. Krishnapriyan"
      ],
      "published": "2025-10-02T17:42:10Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02259v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究挑战了分子机器学习中图神经网络(GNN)的主导地位，证明未经修改的Transformer仅凭笛卡尔坐标即可学习分子能量和力，无需预设图结构或物理先验。在OMol25数据集上，Transformer达到与先进等变GNN相当的精度，并自适应学习物理一致模式(如注意力权重随原子距离衰减)，展现标准化、可扩展分子建模架构的潜力。",
      "order": 708
    },
    {
      "arxiv_id": "2510.02253v1",
      "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
      "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
      "authors": [
        "Zihan Zhou",
        "Shilin Lu",
        "Shuli Leng",
        "Shaocong Zhang",
        "Zhuming Lian",
        "Xinlei Yu",
        "Adams Wai-Kin Kong"
      ],
      "published": "2025-10-02T17:39:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02253v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "DragFlow是首个利用FLUX强大先验进行拖拽式图像编辑的框架，通过区域监督和仿射变换解决DiT特征结构化不足的问题，集成个性化适配器保持主体一致性，使用MLLM解决任务歧义，在多个基准测试中超越现有方法。",
      "order": 709
    },
    {
      "arxiv_id": "2510.02250v1",
      "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
      "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
      "authors": [
        "Gonzalo Gonzalez-Pumariega",
        "Vincent Tu",
        "Chih-Lun Lee",
        "Jiachen Yang",
        "Ang Li",
        "Xin Eric Wang"
      ],
      "published": "2025-10-02T17:37:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02250v1",
      "primary_area": "vla_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Behavior Best-of-N (bBoN)方法，通过生成多个执行轨迹并使用行为描述进行选择，显著提升计算机使用代理的可靠性和成功率。在OSWorld基准测试中达到69.9%的新SOTA，接近人类72%的水平，并在跨操作系统任务中展现强大泛化能力，证明规模化代理在具备结构化轨迹理解时的非凡效果。",
      "order": 710
    },
    {
      "arxiv_id": "2510.02249v1",
      "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process.",
      "authors": [
        "Tianyi Jiang",
        "Yi Bin",
        "Yujuan Ding",
        "Kainian Zhu",
        "Fei Ma",
        "Jingkuan Song",
        "Heng Tao Shen"
      ],
      "published": "2025-10-02T17:36:50Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02249v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种名为'简要探索后决策'的新推理范式，通过累积熵调控机制解决大语言模型过度思考问题。该方法利用标记熵累积平均值动态确定最优推理终止点，在数学基准测试中显著减少71%的响应长度，同时保持问题解决能力。",
      "order": 711
    },
    {
      "arxiv_id": "2510.02245v1",
      "title": "ExGRPO: Learning to Reason from Experience",
      "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.",
      "authors": [
        "Runzhe Zhan",
        "Yafu Li",
        "Zhi Wang",
        "Xiaoye Qu",
        "Dongrui Liu",
        "Jing Shao",
        "Derek F. Wong",
        "Yu Cheng"
      ],
      "published": "2025-10-02T17:31:30Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02245v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出ExGRPO框架，通过分析推理经验的价值指标（正确性和熵），优化大语言模型的强化学习过程。该方法在数学和通用推理基准上平均提升3.5/7.6分，显著提高训练效率和稳定性。",
      "order": 712
    },
    {
      "arxiv_id": "2510.02239v1",
      "title": "Drop-Muon: Update Less, Converge Faster",
      "summary": "Conventional wisdom in deep learning optimization dictates updating all\nlayers at every step-a principle followed by all recent state-of-the-art\noptimizers such as Muon. In this work, we challenge this assumption, showing\nthat full-network updates can be fundamentally suboptimal, both in theory and\nin practice. We introduce a non-Euclidean Randomized Progressive Training\nmethod-Drop-Muon-a simple yet powerful framework that updates only a subset of\nlayers per step according to a randomized schedule, combining the efficiency of\nprogressive training with layer-specific non-Euclidean updates for top-tier\nperformance. We provide rigorous convergence guarantees under both layer-wise\nsmoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and\nstochastic gradient settings, marking the first such results for progressive\ntraining in the stochastic and non-smooth regime. Our cost analysis further\nreveals that full-network updates are not optimal unless a very specific\nrelationship between layer smoothness constants holds. Through controlled CNN\nexperiments, we empirically demonstrate that Drop-Muon consistently outperforms\nfull-network Muon, achieving the same accuracy up to $1.4\\times$ faster in\nwall-clock time. Together, our results suggest a shift in how large-scale\nmodels can be efficiently trained, challenging the status quo and offering a\nhighly efficient, theoretically grounded alternative to full-network updates.",
      "authors": [
        "Kaja Gruntkowska",
        "Yassine Maziane",
        "Zheng Qu",
        "Peter Richtárik"
      ],
      "published": "2025-10-02T17:28:55Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02239v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Drop-Muon优化方法，挑战传统深度学习需每步更新所有层的做法，通过随机选择部分层进行非欧几里得更新，在理论和实验上证明能比全网络更新快1.4倍达到相同精度，为大规模模型训练提供高效替代方案。",
      "order": 713
    },
    {
      "arxiv_id": "2510.02236v1",
      "title": "PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed\n  Slice Mobility Attacks",
      "summary": "Network Slices (NSs) are virtual networks operating over a shared physical\ninfrastructure, each designed to meet specific application requirements while\nmaintaining consistent Quality of Service (QoS). In Fifth Generation (5G)\nnetworks, User Equipment (UE) can connect to and seamlessly switch between\nmultiple NSs to access diverse services. However, this flexibility, known as\nInter-Slice Switching (ISS), introduces a potential vulnerability that can be\nexploited to launch Distributed Slice Mobility (DSM) attacks, a form of\nDistributed Denial of Service (DDoS) attack. To secure 5G networks and their\nNSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an\nanomaly detection solution that leverages Positive Unlabeled Learning (PUL) and\nincorporates a combination of Long Short-Term Memory Autoencoders and K-Means\nclustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership\nProject (3GPP) key performance indicators and performance measurement counters\nas features for its machine learning models to detect DSM attack variants while\nmaintaining robustness in the presence of contaminated training data. When\nevaluated on data collected from our 5G testbed based on the open-source\nfree5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;\nPUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training\ndatasets with 10% to 40% attack contamination, consistently outperforming its\ncounterpart Inter-Slice Defender and other PUL based solutions combining\nOne-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.",
      "authors": [
        "Ricardo Misael Ayala Molina",
        "Hyame Assem Alameddine",
        "Makan Pourzandi",
        "Chadi Assi"
      ],
      "published": "2025-10-02T17:24:17Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02236v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出PUL-Inter-slice Defender，一种基于正样本无标签学习的异常检测方案，用于防御5G网络中的分布式切片移动攻击。该方法结合LSTM自编码器和K-Means聚类，利用3GPP性能指标，在攻击污染数据下实现超过98.50%的F1分数，显著优于现有方案。",
      "order": 714
    },
    {
      "arxiv_id": "2510.02228v1",
      "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity",
      "summary": "Scaling laws play a central role in the success of Large Language Models\n(LLMs), enabling the prediction of model performance relative to compute\nbudgets prior to training. While Transformers have been the dominant\narchitecture, recent alternatives such as xLSTM offer linear complexity with\nrespect to context length while remaining competitive in the billion-parameter\nregime. We conduct a comparative investigation on the scaling behavior of\nTransformers and xLSTM along the following lines, providing insights to guide\nfuture model design and deployment. First, we study the scaling behavior for\nxLSTM in compute-optimal and over-training regimes using both IsoFLOP and\nparametric fit approaches on a wide range of model sizes (80M-7B) and number of\ntraining tokens (2B-2T). Second, we examine the dependence of optimal model\nsizes on context length, a pivotal aspect that was largely ignored in previous\nwork. Finally, we analyze inference-time scaling characteristics. Our findings\nreveal that in typical LLM training and inference scenarios, xLSTM scales\nfavorably compared to Transformers. Importantly, xLSTM's advantage widens as\ntraining and inference contexts grow.",
      "authors": [
        "Maximilian Beck",
        "Kajetan Schweighofer",
        "Sebastian Böck",
        "Sebastian Lehner",
        "Sepp Hochreiter"
      ],
      "published": "2025-10-02T17:14:34Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02228v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究比较了Transformer与xLSTM的扩展规律，发现xLSTM在保持线性时间复杂度的同时，在大规模训练和长上下文推理场景下展现出优于Transformer的扩展性能，为未来模型设计提供了重要参考。",
      "order": 715
    },
    {
      "arxiv_id": "2510.02227v1",
      "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.",
      "authors": [
        "Xiaoyang Yuan",
        "Yujuan Ding",
        "Yi Bin",
        "Wenqi Shao",
        "Jinyu Cai",
        "Jingkuan Song",
        "Yang Yang",
        "Hengtao Shen"
      ],
      "published": "2025-10-02T17:14:00Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02227v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出自适应多引导策略优化(AMPO)框架，通过动态调用多个教师模型指导，仅在策略模型生成错误时提供引导，增强大语言模型的长链推理能力。该方法在数学推理任务上提升4.3%，分布外任务提升12.2%，显著提高推理多样性和性能。",
      "order": 716
    },
    {
      "arxiv_id": "2510.02226v1",
      "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models",
      "summary": "Recent advances in generative video models have enabled the creation of\nhigh-quality videos based on natural language prompts. However, these models\nfrequently lack fine-grained temporal control, meaning they do not allow users\nto specify when particular visual elements should appear within a generated\nsequence. In this work, we introduce TempoControl, a method that allows for\ntemporal alignment of visual concepts during inference, without requiring\nretraining or additional supervision. TempoControl utilizes cross-attention\nmaps, a key component of text-to-video diffusion models, to guide the timing of\nconcepts through a novel optimization approach. Our method steers attention\nusing three complementary principles: aligning its temporal shape with a\ncontrol signal (via correlation), amplifying it where visibility is needed (via\nenergy), and maintaining spatial focus (via entropy). TempoControl allows\nprecise control over timing while ensuring high video quality and diversity. We\ndemonstrate its effectiveness across various video generation applications,\nincluding temporal reordering for single and multiple objects, as well as\naction and audio-aligned generation.",
      "authors": [
        "Shira Schiber",
        "Ofir Lindenbaum",
        "Idan Schwartz"
      ],
      "published": "2025-10-02T17:13:35Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02226v1",
      "primary_area": "video_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "TempoControl是一种无需重新训练即可实现文本到视频生成模型时间对齐的方法，通过优化交叉注意力图来精确控制视觉概念在视频序列中的出现时机，确保视频质量和多样性。",
      "order": 717
    },
    {
      "arxiv_id": "2510.02224v1",
      "title": "Efficiently Generating Correlated Sample Paths from Multi-step Time\n  Series Foundation Models",
      "summary": "Many time series applications require access to multi-step forecast\ntrajectories in the form of sample paths. Recently, time series foundation\nmodels have leveraged multi-step lookahead predictions to improve the quality\nand efficiency of multi-step forecasts. However, these models only predict\nindependent marginal distributions for each time step, rather than a full joint\npredictive distribution. To generate forecast sample paths with realistic\ncorrelation structures, one typically resorts to autoregressive sampling, which\ncan be extremely expensive. In this paper, we present a copula-based approach\nto efficiently generate accurate, correlated sample paths from existing\nmulti-step time series foundation models in one forward pass. Our copula-based\napproach generates correlated sample paths orders of magnitude faster than\nautoregressive sampling, and it yields improved sample path quality by\nmitigating the snowballing error phenomenon.",
      "authors": [
        "Ethan Baron",
        "Boris Oreshkin",
        "Ruijun Ma",
        "Hanyu Zhang",
        "Kari Torkkola",
        "Michael W. Mahoney",
        "Andrew Gordon Wilson",
        "Tatiana Konstantinova"
      ],
      "published": "2025-10-02T17:08:58Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02224v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "financial_ai",
      "tldr_zh": "本文提出一种基于copula的方法，能够从多步时间序列基础模型中高效生成具有相关性的样本路径，相比自回归采样速度提升数个数量级，并通过缓解误差累积现象提高路径质量。",
      "order": 718
    },
    {
      "arxiv_id": "2510.02218v1",
      "title": "Quantum Fisher information matrices from Rényi relative entropies",
      "summary": "Quantum generalizations of the Fisher information are important in quantum\ninformation science, with applications in high energy and condensed matter\nphysics and in quantum estimation theory, machine learning, and optimization.\nOne can derive a quantum generalization of the Fisher information matrix in a\nnatural way as the Hessian matrix arising in a Taylor expansion of a smooth\ndivergence. Such an approach is appealing for quantum information theorists,\ngiven the ubiquity of divergences in quantum information theory. In contrast to\nthe classical case, there is not a unique quantum generalization of the Fisher\ninformation matrix, similar to how there is not a unique quantum generalization\nof the relative entropy or the R\\'enyi relative entropy. In this paper, I\nderive information matrices arising from the log-Euclidean, $\\alpha$-$z$, and\ngeometric R\\'enyi relative entropies, with the main technical tool for doing so\nbeing the method of divided differences for calculating matrix derivatives.\nInterestingly, for all non-negative values of the R\\'enyi parameter $\\alpha$,\nthe log-Euclidean R\\'enyi relative entropy leads to the Kubo-Mori information\nmatrix, and the geometric R\\'enyi relative entropy leads to the\nright-logarithmic derivative Fisher information matrix. Thus, the resulting\ninformation matrices obey the data-processing inequality for all non-negative\nvalues of the R\\'enyi parameter $\\alpha$ even though the original quantities do\nnot. Additionally, I derive and establish basic properties of $\\alpha$-$z$\ninformation matrices resulting from the $\\alpha$-$z$ R\\'enyi relative\nentropies. For parameterized thermal states, I establish formulas for their\n$\\alpha$-$z$ information matrices and hybrid quantum-classical algorithms for\nestimating them, with applications in quantum Boltzmann machine learning.",
      "authors": [
        "Mark M. Wilde"
      ],
      "published": "2025-10-02T17:02:48Z",
      "primary_category": "quant-ph",
      "arxiv_url": "https://arxiv.org/abs/2510.02218v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文从Rényi相对熵推导量子Fisher信息矩阵，提出log-Euclidean、α-z和几何Rényi相对熵对应的信息矩阵，证明其满足数据处理不等式，并针对参数化热态开发量子-经典混合算法，应用于量子玻尔兹曼机器学习。",
      "order": 719
    },
    {
      "arxiv_id": "2510.02216v1",
      "title": "Diffusion Transformers for Imputation: Statistical Efficiency and\n  Uncertainty Quantification",
      "summary": "Imputation methods play a critical role in enhancing the quality of practical\ntime-series data, which often suffer from pervasive missing values. Recently,\ndiffusion-based generative imputation methods have demonstrated remarkable\nsuccess compared to autoregressive and conventional statistical approaches.\nDespite their empirical success, the theoretical understanding of how well\ndiffusion-based models capture complex spatial and temporal dependencies\nbetween the missing values and observed ones remains limited. Our work\naddresses this gap by investigating the statistical efficiency of conditional\ndiffusion transformers for imputation and quantifying the uncertainty in\nmissing values. Specifically, we derive statistical sample complexity bounds\nbased on a novel approximation theory for conditional score functions using\ntransformers, and, through this, construct tight confidence regions for missing\nvalues. Our findings also reveal that the efficiency and accuracy of imputation\nare significantly influenced by the missing patterns. Furthermore, we validate\nthese theoretical insights through simulation and propose a mixed-masking\ntraining strategy to enhance the imputation performance.",
      "authors": [
        "Zeqi Ye",
        "Minshuo Chen"
      ],
      "published": "2025-10-02T17:00:18Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02216v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究针对扩散变换器在时间序列数据填补中的统计效率与不确定性量化问题，提出了基于条件得分函数近似理论的样本复杂度边界，构建了缺失值的紧致置信区域，并揭示了缺失模式对填补性能的影响，同时通过混合掩码训练策略验证了理论发现。",
      "order": 720
    },
    {
      "arxiv_id": "2510.02215v1",
      "title": "C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale\n  Recommendation Systems",
      "summary": "Training large-scale recommendation models under a single global objective\nimplicitly assumes homogeneity across user populations. However, real-world\ndata are composites of heterogeneous cohorts with distinct conditional\ndistributions. As models increase in scale and complexity and as more data is\nused for training, they become dominated by central distribution patterns,\nneglecting head and tail regions. This imbalance limits the model's learning\nability and can result in inactive attention weights or dead neurons. In this\npaper, we reveal how the attention mechanism can play a key role in\nfactorization machines for shared embedding selection, and propose to address\nthis challenge by analyzing the substructures in the dataset and exposing those\nwith strong distributional contrast through auxiliary learning. Unlike previous\nresearch, which heuristically applies weighted labels or multi-task heads to\nmitigate such biases, we leverage partially conflicting auxiliary labels to\nregularize the shared representation. This approach customizes the learning\nprocess of attention layers to preserve mutual information with minority\ncohorts while improving global performance. We evaluated C2AL on massive\nproduction datasets with billions of data points each for six SOTA models.\nExperiments show that the factorization machine is able to capture fine-grained\nuser-ad interactions using the proposed method, achieving up to a 0.16%\nreduction in normalized entropy overall and delivering gains exceeding 0.30% on\ntargeted minority cohorts.",
      "authors": [
        "Mertcan Cokbas",
        "Ziteng Liu",
        "Zeyi Tao",
        "Chengkai Zhang",
        "Elder Veliz",
        "Qin Huang",
        "Ellie Wen",
        "Huayu Li",
        "Qiang Jin",
        "Murat Duman",
        "Benjamin Au",
        "Guy Lebanon",
        "Sagar Chordia"
      ],
      "published": "2025-10-02T17:00:17Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02215v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出C2AL方法，通过队列对比辅助学习解决大规模推荐系统中的数据异质性问题。该方法利用分布对比的辅助标签正则化共享表示，优化注意力机制在因子分解机中的嵌入选择，在保持少数群体信息的同时提升全局性能，在数十亿数据点的生产环境中验证了有效性。",
      "order": 721
    },
    {
      "arxiv_id": "2510.02212v1",
      "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via\n  Reinforcement Learning",
      "summary": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks.",
      "authors": [
        "Hanyang Zhao",
        "Dawen Liang",
        "Wenpin Tang",
        "David Yao",
        "Nathan Kallus"
      ],
      "published": "2025-10-02T16:57:24Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02212v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "DiFFPO提出了一种基于强化学习的统一训练框架，用于优化掩码扩散大语言模型的推理能力。该方法通过离策略RL训练代理策略，结合两阶段似然近似和重要性采样校正，提升样本效率和任务性能。同时训练高效采样器，让模型自适应分配推理阈值，在减少函数评估次数的同时提高准确性，在数学和规划任务上展现了优越性能。",
      "order": 722
    },
    {
      "arxiv_id": "2510.02209v1",
      "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
      "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
      "authors": [
        "Yanxu Chen",
        "Zijun Yao",
        "Yantao Liu",
        "Jin Ye",
        "Jianing Yu",
        "Lei Hou",
        "Juanzi Li"
      ],
      "published": "2025-10-02T16:54:57Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02209v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "financial_ai",
      "tldr_zh": "本文提出StockBench基准，评估大语言模型在真实股票交易环境中的表现。研究发现多数LLM代理难以超越简单买入持有策略，但部分模型展现出更高收益和风险管理潜力，表明静态金融知识与实际交易策略存在差距。",
      "order": 723
    },
    {
      "arxiv_id": "2510.02208v1",
      "title": "Measurement-Guided Consistency Model Sampling for Inverse Problems",
      "summary": "Diffusion models have become powerful generative priors for solving inverse\nimaging problems, but their reliance on slow multi-step sampling limits\npractical deployment. Consistency models address this bottleneck by enabling\nhigh-quality generation in a single or only a few steps, yet their direct\nadaptation to inverse problems is underexplored. In this paper, we present a\nmodified consistency sampling approach tailored for inverse problem\nreconstruction: the sampler's stochasticity is guided by a\nmeasurement-consistency mechanism tied to the measurement operator, which\nenforces fidelity to the acquired measurements while retaining the efficiency\nof consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom\ndatasets demonstrate consistent improvements in perceptual and pixel-level\nmetrics, including Fr\\'echet Inception Distance, Kernel Inception Distance,\npeak signal-to-noise ratio, and structural similarity index measure, compared\nto baseline consistency sampling, yielding competitive or superior\nreconstructions with only a handful of steps.",
      "authors": [
        "Amirreza Tanevardi",
        "Pooria Abbas Rad Moghadam",
        "Sajjad Amini"
      ],
      "published": "2025-10-02T16:53:07Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.02208v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种针对逆问题的改进一致性采样方法，通过测量一致性机制引导采样随机性，在保持测量保真度的同时实现高效生成。在Fashion-MNIST和LSUN Bedroom数据集上的实验表明，该方法仅需少量步骤即可获得优于基线的一致性重建效果，在感知质量和像素级指标上均有提升。",
      "order": 724
    },
    {
      "arxiv_id": "2510.02206v1",
      "title": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling",
      "summary": "Sequence-to-sequence models have become central in Artificial Intelligence,\nparticularly following the introduction of the transformer architecture. While\ninitially developed for Natural Language Processing, these models have\ndemonstrated utility across domains, including Computer Vision. Such models\nrequire mechanisms to exchange information along the time dimension, typically\nusing recurrent or self-attention layers. However, self-attention scales\nquadratically with sequence length, limiting its practicality for very long\nsequences.\n  We introduce Poolformer, a sequence-to-sequence model that replaces\nself-attention with recurrent layers and incorporates pooling operations to\nreduce sequence length. Poolformer is defined recursively using SkipBlocks,\nwhich contain residual blocks, a down-pooling layer, a nested SkipBlock, an\nup-pooling layer, and additional residual blocks. We conduct extensive\nexperiments to support our architectural choices.\n  Our results show that pooling greatly accelerates training, improves\nperceptual metrics (FID and IS), and prevents overfitting. Our experiments also\nsuggest that long-range dependencies are handled by deep layers, while shallow\nlayers take care of short-term features.\n  Evaluated on raw audio, which naturally features long sequence lengths,\nPoolformer outperforms state-of-the-art models such as SaShiMi and Mamba.\nFuture directions include applications to text and vision, as well as\nmulti-modal scenarios, where a Poolformer-based LLM could effectively process\ndense representations of images and videos.",
      "authors": [
        "Daniel Gallo Fernández"
      ],
      "published": "2025-10-02T16:52:45Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02206v1",
      "primary_area": "audio_models",
      "secondary_focus": "long_context",
      "application_domain": "general_purpose",
      "tldr_zh": "Poolformer是一种序列到序列模型，用循环层和池化操作替代自注意力机制，解决了长序列建模中二次复杂度问题。在原始音频处理任务中表现优于Sashimi和Mamba等先进模型，通过池化加速训练、提升感知指标并防止过拟合。",
      "order": 725
    },
    {
      "arxiv_id": "2510.02202v1",
      "title": "Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet\n  Challenge 2025",
      "summary": "Objective: Chagas disease is a parasitic infection that is endemic to South\nAmerica, Central America, and, more recently, the U.S., primarily transmitted\nby insects. Chronic Chagas disease can cause cardiovascular diseases and\ndigestive problems. Serological testing capacities for Chagas disease are\nlimited, but Chagas cardiomyopathy often manifests in ECGs, providing an\nopportunity to prioritize patients for testing and treatment. Approach: The\nGeorge B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic\napproaches for identifying Chagas disease from electrocardiograms (ECGs). Main\nresults: This Challenge provides multiple innovations. First, we leveraged\nseveral datasets with labels from patient reports and serological testing,\nprovided a large dataset with weak labels and smaller datasets with strong\nlabels. Second, we augmented the data to support model robustness and\ngeneralizability to unseen data sources. Third, we applied an evaluation metric\nthat captured the local serological testing capacity for Chagas disease to\nframe the machine learning problem as a triage task. Significance: Over 630\nparticipants from 111 teams submitted over 1300 entries during the Challenge,\nrepresenting diverse approaches from academia and industry worldwide.",
      "authors": [
        "Matthew A. Reyna",
        "Zuzana Koscova",
        "Jan Pavlus",
        "Soheil Saghafi",
        "James Weigle",
        "Andoni Elola",
        "Salman Seyedi",
        "Kiersten Campbell",
        "Qiao Li",
        "Ali Bahrami Rad",
        "Antônio H. Ribeiro",
        "Antonio Luiz P. Ribeiro",
        "Reza Sameni",
        "Gari D. Clifford"
      ],
      "published": "2025-10-02T16:50:36Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02202v1",
      "primary_area": "audio_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "乔治·B·穆迪PhysioNet挑战赛2025聚焦于利用心电图检测恰加斯病，通过整合多源数据集（含强弱标签）、数据增强技术和考虑当地检测能力的评估指标，吸引了全球630多名参与者开发算法模型，旨在优化患者筛查和治疗优先级排序。",
      "order": 726
    },
    {
      "arxiv_id": "2510.02194v1",
      "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language\n  Models",
      "summary": "Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control.",
      "authors": [
        "Yuhao Sun",
        "Zhuoer Xu",
        "Shiwen Cui",
        "Kun Yang",
        "Lingyun Yu",
        "Yongdong Zhang",
        "Hongtao Xie"
      ],
      "published": "2025-10-02T16:43:33Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02194v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "UpSafe°C提出了一种通过安全感知升级增强大语言模型安全性的统一框架。该方法识别安全关键层并将其升级为稀疏混合专家结构，引入安全温度机制实现推理时安全性与实用性的动态权衡控制，在保持通用能力的同时显著提升对有害内容和越狱攻击的防御能力。",
      "order": 727
    },
    {
      "arxiv_id": "2510.02189v1",
      "title": "Hybrid Physics-ML Framework for Pan-Arctic Permafrost Infrastructure\n  Risk at Record 2.9-Million Observation Scale",
      "summary": "Arctic warming threatens over 100 billion in permafrost-dependent\ninfrastructure across Northern territories, yet existing risk assessment\nframeworks lack spatiotemporal validation, uncertainty quantification, and\noperational decision-support capabilities. We present a hybrid physics-machine\nlearning framework integrating 2.9 million observations from 171,605 locations\n(2005-2021) combining permafrost fraction data with climate reanalysis. Our\nstacked ensemble model (Random Forest + Histogram Gradient Boosting + Elastic\nNet) achieves R2=0.980 (RMSE=5.01 pp) with rigorous spatiotemporal\ncross-validation preventing data leakage. To address machine learning\nlimitations in extrapolative climate scenarios, we develop a hybrid approach\ncombining learned climate-permafrost relationships (60%) with physical\npermafrost sensitivity models (40%, -10 pp/C). Under RCP8.5 forcing (+5C over\n10 years), we project mean permafrost fraction decline of -20.3 pp (median:\n-20.0 pp), with 51.5% of Arctic Russia experiencing over 20 percentage point\nloss. Infrastructure risk classification identifies 15% high-risk zones (25%\nmedium-risk) with spatially explicit uncertainty maps. Our framework represents\nthe largest validated permafrost ML dataset globally, provides the first\noperational hybrid physics-ML forecasting system for Arctic infrastructure, and\ndelivers open-source tools enabling probabilistic permafrost projections for\nengineering design codes and climate adaptation planning. The methodology is\ngeneralizable to other permafrost regions and demonstrates how hybrid\napproaches can overcome pure data-driven limitations in climate change\napplications.",
      "authors": [
        "Boris Kriuk"
      ],
      "published": "2025-10-02T16:38:36Z",
      "primary_category": "stat.ML",
      "arxiv_url": "https://arxiv.org/abs/2510.02189v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出了一种混合物理-机器学习框架，整合290万观测数据评估泛北极永久冻土基础设施风险。通过集成随机森林、直方图梯度提升和弹性网络模型，实现了R²=0.980的高精度预测。该框架结合60%机器学习学习的气候-冻土关系和40%物理敏感性模型，预测在RCP8.5情景下永久冻土层平均减少20.3个百分点，并识别出15%的高风险区域。这是全球最大的验证冻土机器学习数据集，为北极工程设计和气候适应提供开源工具。",
      "order": 728
    },
    {
      "arxiv_id": "2510.02187v1",
      "title": "High-Fidelity Speech Enhancement via Discrete Audio Tokens",
      "summary": "Recent autoregressive transformer-based speech enhancement (SE) methods have\nshown promising results by leveraging advanced semantic understanding and\ncontextual modeling of speech. However, these approaches often rely on complex\nmulti-stage pipelines and low sampling rate codecs, limiting them to narrow and\ntask-specific speech enhancement. In this work, we introduce DAC-SE1, a\nsimplified language model-based SE framework leveraging discrete\nhigh-resolution audio representations; DAC-SE1 preserves fine-grained acoustic\ndetails while maintaining semantic coherence. Our experiments show that DAC-SE1\nsurpasses state-of-the-art autoregressive SE methods on both objective\nperceptual metrics and in a MUSHRA human evaluation. We release our codebase\nand model checkpoints to support further research in scalable, unified, and\nhigh-quality speech enhancement.",
      "authors": [
        "Luca A. Lanzendörfer",
        "Frédéric Berdoz",
        "Antonis Asonitis",
        "Roger Wattenhofer"
      ],
      "published": "2025-10-02T16:38:05Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.02187v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出DAC-SE1，一种基于离散音频令牌的简化语言模型语音增强框架，通过高分辨率音频表示在保持语义连贯性的同时保留精细声学细节，在客观感知指标和人类评估中均超越现有自回归方法。",
      "order": 729
    },
    {
      "arxiv_id": "2510.02186v1",
      "title": "GeoPurify: A Data-Efficient Geometric Distillation Framework for\n  Open-Vocabulary 3D Segmentation",
      "summary": "Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to\n3D semantic segmentation expose a persistent trade-off. Directly projecting 2D\nfeatures into 3D yields noisy and fragmented predictions, whereas enforcing\ngeometric coherence necessitates costly training pipelines and large-scale\nannotated 3D data. We argue that this limitation stems from the dominant\nsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with\n3D geometric structure. The geometric cues are not eliminated during the\n2D-to-3D transfer but remain latent within the noisy and view-aggregated\nfeatures. To exploit this property, we propose GeoPurify that applies a small\nStudent Affinity Network to purify 2D VLM-generated 3D point features using\ngeometric priors distilled from a 3D self-supervised teacher model. During\ninference, we devise a Geometry-Guided Pooling module to further denoise the\npoint cloud and ensure the semantic and structural consistency. Benefiting from\nlatent geometric information and the learned affinity network, GeoPurify\neffectively mitigates the trade-off and achieves superior data efficiency.\nExtensive experiments on major 3D benchmarks demonstrate that GeoPurify\nachieves or surpasses state-of-the-art performance while utilizing only about\n1.5% of the training data. Our codes and checkpoints are available at\n[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).",
      "authors": [
        "Weijia Dou",
        "Xu Zhang",
        "Yi Bin",
        "Jian Liu",
        "Bo Peng",
        "Guoqing Wang",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "published": "2025-10-02T16:37:56Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02186v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "GeoPurify提出了一种数据高效的几何蒸馏框架，用于解决开放词汇3D分割中2D视觉语言模型特征向3D转换时的噪声和几何不一致问题。该方法通过学生亲和网络利用3D自监督教师模型的几何先验来纯化点特征，并在推理时采用几何引导池化确保语义结构一致性，仅需约1.5%训练数据即可达到或超越现有最优性能。",
      "order": 730
    },
    {
      "arxiv_id": "2510.02182v1",
      "title": "Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex\n  with Mutual Information-Guided Diffusion",
      "summary": "Understanding how neural populations in higher visual areas encode\nobject-centered visual information remains a central challenge in computational\nneuroscience. Prior works have investigated representational alignment between\nartificial neural networks and the visual cortex. Nevertheless, these findings\nare indirect and offer limited insights to the structure of neural populations\nthemselves. Similarly, decoding-based methods have quantified semantic features\nfrom neural populations but have not uncovered their underlying organizations.\nThis leaves open a scientific question: \"how feature-specific visual\ninformation is distributed across neural populations in higher visual areas,\nand whether it is organized into structured, semantically meaningful\nsubspaces.\" To tackle this problem, we present MIG-Vis, a method that leverages\nthe generative power of diffusion models to visualize and validate the\nvisual-semantic attributes encoded in neural latent subspaces. Our method first\nuses a variational autoencoder to infer a group-wise disentangled neural latent\nsubspace from neural populations. Subsequently, we propose a mutual information\n(MI)-guided diffusion synthesis procedure to visualize the specific\nvisual-semantic features encoded by each latent group. We validate MIG-Vis on\nmulti-session neural spiking datasets from the inferior temporal (IT) cortex of\ntwo macaques. The synthesized results demonstrate that our method identifies\nneural latent groups with clear semantic selectivity to diverse visual\nfeatures, including object pose, inter-category transformations, and\nintra-class content. These findings provide direct, interpretable evidence of\nstructured semantic representation in the higher visual cortex and advance our\nunderstanding of its encoding principles.",
      "authors": [
        "Yule Wang",
        "Joseph Yu",
        "Chengrui Li",
        "Weihan Li",
        "Anqi Wu"
      ],
      "published": "2025-10-02T16:33:40Z",
      "primary_category": "q-bio.NC",
      "arxiv_url": "https://arxiv.org/abs/2510.02182v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出MIG-Vis方法，利用扩散模型生成能力可视化高阶视觉皮层神经群体的语义选择性。通过变分自编码器推断解耦的神经潜在子空间，结合互信息引导的扩散合成技术，在猕猴颞下皮层数据中识别出对物体姿态、类别转换等视觉特征具有明确选择性的神经群组，为视觉皮层语义表征结构提供了直接证据。",
      "order": 731
    },
    {
      "arxiv_id": "2510.02180v1",
      "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement\n  Learning",
      "summary": "Inverse Reinforcement Learning aims to recover reward models from expert\ndemonstrations, but traditional methods yield \"black-box\" models that are\ndifficult to interpret and debug. In this work, we introduce GRACE (Generating\nRewards As CodE), a method for using Large Language Models within an\nevolutionary search to reverse-engineer an interpretable, code-based reward\nfunction directly from expert trajectories. The resulting reward function is\nexecutable code that can be inspected and verified. We empirically validate\nGRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns\nhighly accurate rewards, even in complex, multi-task settings. Further, we\ndemonstrate that the resulting reward leads to strong policies, compared to\nboth competitive Imitation Learning and online RL approaches with ground-truth\nrewards. Finally, we show that GRACE is able to build complex reward APIs in\nmulti-task setups.",
      "authors": [
        "Silvia Sapora",
        "Devon Hjelm",
        "Alexander Toshev",
        "Omar Attia",
        "Bogdan Mazoure"
      ],
      "published": "2025-10-02T16:31:39Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02180v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "GRACE框架利用大语言模型通过进化搜索从专家轨迹中逆向生成可解释的代码化奖励函数，解决了传统逆强化学习模型难以解释的问题。该方法在BabyAI和AndroidWorld基准测试中表现出色，能生成可执行验证的奖励代码，并在多任务环境中构建复杂奖励API。",
      "order": 732
    },
    {
      "arxiv_id": "2510.02174v1",
      "title": "Flatness-Aware Stochastic Gradient Langevin Dynamics",
      "summary": "Generalization in deep learning is closely tied to the pursuit of flat minima\nin the loss landscape, yet classical Stochastic Gradient Langevin Dynamics\n(SGLD) offers no mechanism to bias its dynamics toward such low-curvature\nsolutions. This work introduces Flatness-Aware Stochastic Gradient Langevin\nDynamics (fSGLD), designed to efficiently and provably seek flat minima in\nhigh-dimensional nonconvex optimization problems. At each iteration, fSGLD uses\nthe stochastic gradient evaluated at parameters perturbed by isotropic Gaussian\nnoise, commonly referred to as Random Weight Perturbation (RWP), thereby\noptimizing a randomized-smoothing objective that implicitly captures curvature\ninformation. Leveraging these properties, we prove that the invariant measure\nof fSGLD stays close to a stationary measure concentrated on the global\nminimizers of a loss function regularized by the Hessian trace whenever the\ninverse temperature and the scale of random weight perturbation are properly\ncoupled. This result provides a rigorous theoretical explanation for the\nbenefits of random weight perturbation. In particular, we establish\nnon-asymptotic convergence guarantees in Wasserstein distance with the best\nknown rate and derive an excess-risk bound for the Hessian-trace regularized\nobjective. Extensive experiments on noisy-label and large-scale vision tasks,\nin both training-from-scratch and fine-tuning settings, demonstrate that fSGLD\nachieves superior or comparable generalization and robustness to baseline\nalgorithms while maintaining the computational cost of SGD, about half that of\nSAM. Hessian-spectrum analysis further confirms that fSGLD converges to\nsignificantly flatter minima.",
      "authors": [
        "Stefano Bruno",
        "Youngsik Hwang",
        "Jaehyeon An",
        "Sotirios Sabanis",
        "Dong-Young Lim"
      ],
      "published": "2025-10-02T16:24:46Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02174v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出平坦感知随机梯度朗之万动力学(fSGLD)，通过随机权重扰动机制在非凸优化中高效寻找平坦极小值。理论证明其稳态分布集中于Hessian迹正则化的全局最小解，实验显示在噪声标签和大规模视觉任务中取得优异泛化性能，计算成本仅相当于SGD的一半。",
      "order": 733
    },
    {
      "arxiv_id": "2510.02173v1",
      "title": "Learning to Reason for Hallucination Span Detection",
      "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
      "authors": [
        "Hsuan Su",
        "Ting-Yao Hu",
        "Hema Swetha Koppula",
        "Kundan Krishna",
        "Hadi Pouransari",
        "Cheng-Yu Hsieh",
        "Cem Koc",
        "Joseph Yitan Cheng",
        "Oncel Tuzel",
        "Raviteja Vemulapalli"
      ],
      "published": "2025-10-02T16:24:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02173v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出RL4HS强化学习框架，通过链式思维推理和跨度级奖励函数检测大语言模型生成的幻觉内容。相比传统二分类方法，该方法能精确定位幻觉片段，在RAGTruth基准测试中优于预训练模型和监督微调。",
      "order": 734
    },
    {
      "arxiv_id": "2510.02162v1",
      "title": "NoMod: A Non-modular Attack on Module Learning With Errors",
      "summary": "The advent of quantum computing threatens classical public-key cryptography,\nmotivating NIST's adoption of post-quantum schemes such as those based on the\nModule Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a\nhybrid white-box cryptanalytic method that circumvents the challenge of\nmodeling modular reduction by treating wrap-arounds as statistical corruption\nand casting secret recovery as robust linear estimation. Our approach combines\noptimized lattice preprocessing--including reduced-vector saving and algebraic\namplification--with robust estimators trained via Tukey's Biweight loss.\nExperiments show NoMod achieves full recovery of binary secrets for dimension\n$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful\nrecovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =\n(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous\nrepository https://anonymous.4open.science/r/NoMod-3BD4.",
      "authors": [
        "Cristian Bassotto",
        "Ermes Franch",
        "Marina Krček",
        "Stjepan Picek"
      ],
      "published": "2025-10-02T16:12:13Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.02162v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "NoMod攻击是一种针对模块学习错误问题的非模块化密码分析方法，通过将模运算包装视为统计噪声，将密钥恢复转化为鲁棒线性估计问题。该方法结合优化的格预处理技术和Tukey双权重损失训练的鲁棒估计器，实验显示能成功恢复CRYSTALS-Kyber等后量子密码方案中的稀疏密钥。",
      "order": 735
    },
    {
      "arxiv_id": "2510.02161v1",
      "title": "Comparing Contrastive and Triplet Loss in Audio-Visual Embedding:\n  Intra-Class Variance and Greediness Analysis",
      "summary": "Contrastive loss and triplet loss are widely used objectives in deep metric\nlearning, yet their effects on representation quality remain insufficiently\nunderstood. We present a theoretical and empirical comparison of these losses,\nfocusing on intra- and inter-class variance and optimization behavior (e.g.,\ngreedy updates). Through task-specific experiments with consistent settings on\nsynthetic data and real datasets-MNIST, CIFAR-10-it is shown that triplet loss\npreserves greater variance within and across classes, supporting finer-grained\ndistinctions in the learned representations. In contrast, contrastive loss\ntends to compact intra-class embeddings, which may obscure subtle semantic\ndifferences. To better understand their optimization dynamics, By examining\nloss-decay rate, active ratio, and gradient norm, we find that contrastive loss\ndrives many small updates early on, while triplet loss produces fewer but\nstronger updates that sustain learning on hard examples. Finally, across both\nclassification and retrieval tasks on MNIST, CIFAR-10, CUB-200, and CARS196\ndatasets, our results consistently show that triplet loss yields superior\nperformance, which suggests using triplet loss for detail retention and\nhard-sample focus, and contrastive loss for smoother, broad-based embedding\nrefinement.",
      "authors": [
        "Donghuo Zeng"
      ],
      "published": "2025-10-02T16:11:46Z",
      "primary_category": "cs.MM",
      "arxiv_url": "https://arxiv.org/abs/2510.02161v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文通过理论和实验对比了对比损失与三元组损失在深度度量学习中的表现。研究发现三元组损失能保留更大的类内和类间方差，支持更细粒度的表征区分，而对比损失倾向于压缩类内嵌入，可能掩盖细微语义差异。在优化动态方面，对比损失早期产生多个小更新，三元组损失则产生较少但更强的更新，持续学习困难样本。在多个数据集上的实验表明，三元组损失在分类和检索任务中表现更优，建议在需要细节保留和困难样本关注时使用三元组损失，在需要平滑、广泛的嵌入优化时使用对比损失。",
      "order": 736
    },
    {
      "arxiv_id": "2510.02149v1",
      "title": "Reinforcement Learning with Action-Triggered Observations",
      "summary": "We study reinforcement learning problems where state observations are\nstochastically triggered by actions, a constraint common in many real-world\napplications. This framework is formulated as Action-Triggered Sporadically\nTraceable Markov Decision Processes (ATST-MDPs), where each action has a\nspecified probability of triggering a state observation. We derive tailored\nBellman optimality equations for this framework and introduce the\naction-sequence learning paradigm in which agents commit to executing a\nsequence of actions until the next observation arrives. Under the linear MDP\nassumption, value-functions are shown to admit linear representations in an\ninduced action-sequence feature map. Leveraging this structure, we propose\noff-policy estimators with statistical error guarantees for such feature maps\nand introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered\nsettings. ST-LSVI-UCB achieves regret $\\widetilde\nO(\\sqrt{Kd^3(1-\\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the\nfeature dimension, and $\\gamma$ the discount factor (per-step episode\nnon-termination probability). Crucially, this work establishes the theoretical\nfoundation for learning with sporadic, action-triggered observations while\ndemonstrating that efficient learning remains feasible under such observation\nconstraints.",
      "authors": [
        "Alexander Ryabchenko",
        "Wenlong Mou"
      ],
      "published": "2025-10-02T16:00:50Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02149v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究动作触发观测的强化学习问题，提出ATST-MDPs框架及动作序列学习范式。在线性MDP假设下，证明了价值函数的线性表示特性，并开发了具有统计误差保证的离策略估计器ST-LSVI-UCB算法，实现了次线性遗憾界，为稀疏观测环境下的高效学习奠定了理论基础。",
      "order": 737
    },
    {
      "arxiv_id": "2510.02148v1",
      "title": "Policy Gradient Guidance Enables Test Time Control",
      "summary": "We introduce Policy Gradient Guidance (PGG), a simple extension of\nclassifier-free guidance from diffusion models to classical policy gradient\nmethods. PGG augments the policy gradient with an unconditional branch and\ninterpolates conditional and unconditional branches, yielding a test-time\ncontrol knob that modulates behavior without retraining. We provide a\ntheoretical derivation showing that the additional normalization term vanishes\nunder advantage estimation, leading to a clean guided policy gradient update.\nEmpirically, we evaluate PGG on discrete and continuous control benchmarks. We\nfind that conditioning dropout-central to diffusion guidance-offers gains in\nsimple discrete tasks and low sample regimes, but dropout destabilizes\ncontinuous control. Training with modestly larger guidance ($\\gamma>1$)\nconsistently improves stability, sample efficiency, and controllability. Our\nresults show that guidance, previously confined to diffusion policies, can be\nadapted to standard on-policy methods, opening new directions for controllable\nonline reinforcement learning.",
      "authors": [
        "Jianing Qi",
        "Hao Tang",
        "Zhigang Zhu"
      ],
      "published": "2025-10-02T16:00:35Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02148v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出策略梯度引导(PGG)，将扩散模型中的无分类器引导方法扩展到经典策略梯度算法。PGG通过添加无条件分支并插值条件/无条件分支，实现无需重新训练的行为调控。理论证明优势估计下归一化项可消除，实验表明适度引导训练能提升稳定性、样本效率和可控性，为在线强化学习开辟新方向。",
      "order": 738
    },
    {
      "arxiv_id": "2510.02143v1",
      "title": "How to Find Fantastic Papers: Self-Rankings as a Powerful Predictor of\n  Scientific Impact Beyond Peer Review",
      "summary": "Peer review in academic research aims not only to ensure factual correctness\nbut also to identify work of high scientific potential that can shape future\nresearch directions. This task is especially critical in fast-moving fields\nsuch as artificial intelligence (AI), yet it has become increasingly difficult\ngiven the rapid growth of submissions. In this paper, we investigate an\nunderexplored measure for identifying high-impact research: authors' own\nrankings of their multiple submissions to the same AI conference. Grounded in\ngame-theoretic reasoning, we hypothesize that self-rankings are informative\nbecause authors possess unique understanding of their work's conceptual depth\nand long-term promise. To test this hypothesis, we conducted a large-scale\nexperiment at a leading AI conference, where 1,342 researchers self-ranked\ntheir 2,592 submissions by perceived quality. Tracking outcomes over more than\na year, we found that papers ranked highest by their authors received twice as\nmany citations as their lowest-ranked counterparts; self-rankings were\nespecially effective at identifying highly cited papers (those with over 150\ncitations). Moreover, we showed that self-rankings outperformed peer review\nscores in predicting future citation counts. Our results remained robust after\naccounting for confounders such as preprint posting time and self-citations.\nTogether, these findings demonstrate that authors' self-rankings provide a\nreliable and valuable complement to peer review for identifying and elevating\nhigh-impact research in AI.",
      "authors": [
        "Buxin Su",
        "Natalie Collina",
        "Garrett Wen",
        "Didong Li",
        "Kyunghyun Cho",
        "Jianqing Fan",
        "Bingxin Zhao",
        "Weijie Su"
      ],
      "published": "2025-10-02T15:50:21Z",
      "primary_category": "stat.AP",
      "arxiv_url": "https://arxiv.org/abs/2510.02143v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究通过大规模实验发现，作者对自身多篇投稿的排名能有效预测论文的科学影响力。在AI顶会中，作者自评最高的论文引用量是最低的两倍，且自评比同行评审更能准确预测未来引用量。这为快速发展的AI领域提供了一种补充同行评审的有效方法。",
      "order": 739
    },
    {
      "arxiv_id": "2510.02142v1",
      "title": "Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution\n  reaction case study",
      "summary": "Efficient and inexpensive energy storage is essential for accelerating the\nadoption of renewable energy and ensuring a stable supply, despite fluctuations\nin sources such as wind and solar. Electrocatalysts play a key role in hydrogen\nenergy storage (HES), allowing the energy to be stored as hydrogen. However,\nthe development of affordable and high-performance catalysts for this process\nremains a significant challenge. We introduce Catalyst GFlowNet, a generative\nmodel that leverages machine learning-based predictors of formation and\nadsorption energy to design crystal surfaces that act as efficient catalysts.\nWe demonstrate the performance of the model through a proof-of-concept\napplication to the hydrogen evolution reaction, a key reaction in HES, for\nwhich we successfully identified platinum as the most efficient known catalyst.\nIn future work, we aim to extend this approach to the oxygen evolution\nreaction, where current optimal catalysts are expensive metal oxides, and open\nthe search space to discover new materials. This generative modeling framework\noffers a promising pathway for accelerating the search for novel and efficient\ncatalysts.",
      "authors": [
        "Lena Podina",
        "Christina Humer",
        "Alexandre Duval",
        "Victor Schmidt",
        "Ali Ramlaoui",
        "Shahana Chatterjee",
        "Yoshua Bengio",
        "Alex Hernandez-Garcia",
        "David Rolnick",
        "Félix Therrien"
      ],
      "published": "2025-10-02T15:49:39Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02142v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Catalyst GFlowNet生成模型，利用机器学习预测形成能和吸附能来设计高效电催化剂晶体表面。以氢析出反应为案例研究，成功识别铂为最优催化剂，为加速新型催化剂发现提供了有前景的生成建模框架。",
      "order": 740
    },
    {
      "arxiv_id": "2510.02139v1",
      "title": "BioinfoMCP: A Unified Platform Enabling MCP Interfaces in Agentic\n  Bioinformatics",
      "summary": "Bioinformatics tools are essential for complex computational biology tasks,\nyet their integration with emerging AI-agent frameworks is hindered by\nincompatible interfaces, heterogeneous input-output formats, and inconsistent\nparameter conventions. The Model Context Protocol (MCP) provides a standardized\nframework for tool-AI communication, but manually converting hundreds of\nexisting and rapidly growing specialized bioinformatics tools into\nMCP-compliant servers is labor-intensive and unsustainable. Here, we present\nBioinfoMCP, a unified platform comprising two components: BioinfoMCP Converter,\nwhich automatically generates robust MCP servers from tool documentation using\nlarge language models, and BioinfoMCP Benchmark, which systematically validates\nthe reliability and versatility of converted tools across diverse computational\ntasks. We present a platform of 38 MCP-converted bioinformatics tools,\nextensively validated to show that 94.7% successfully executed complex\nworkflows across three widely used AI-agent platforms. By removing technical\nbarriers to AI automation, BioinfoMCP enables natural-language interaction with\nsophisticated bioinformatics analyses without requiring extensive programming\nexpertise, offering a scalable path to intelligent, interoperable computational\nbiology.",
      "authors": [
        "Florensia Widjaja",
        "Zhangtianyi Chen",
        "Juexiao Zhou"
      ],
      "published": "2025-10-02T15:47:59Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.02139v1",
      "primary_area": "text_models",
      "secondary_focus": "tech_reports",
      "application_domain": "medical_ai",
      "tldr_zh": "BioinfoMCP是一个统一平台，通过自动转换生物信息学工具为MCP兼容服务器，解决了AI代理框架与生物信息工具集成中的接口不兼容问题。该平台包含转换器和基准测试两大组件，已成功转换38种工具，94.7%能在主流AI平台上执行复杂工作流，为计算生物学提供了可扩展的智能互操作解决方案。",
      "order": 741
    },
    {
      "arxiv_id": "2510.02133v1",
      "title": "FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic\n  Documents for Training Document Understanding Models",
      "summary": "Developing document understanding models at enterprise scale requires large,\ndiverse, and well-annotated datasets spanning a wide range of document types.\nHowever, collecting such data is prohibitively expensive due to privacy\nconstraints, legal restrictions, and the sheer volume of manual annotation\nneeded - costs that can scale into millions of dollars. We introduce FlexDoc, a\nscalable synthetic data generation framework that combines Stochastic Schemas\nand Parameterized Sampling to produce realistic, multilingual semi-structured\ndocuments with rich annotations. By probabilistically modeling layout patterns,\nvisual structure, and content variability, FlexDoc enables the controlled\ngeneration of diverse document variants at scale. Experiments on Key\nInformation Extraction (KIE) tasks demonstrate that FlexDoc-generated data\nimproves the absolute F1 Score by up to 11% when used to augment real datasets,\nwhile reducing annotation effort by over 90% compared to traditional\nhard-template methods. The solution is in active deployment, where it has\naccelerated the development of enterprise-grade document understanding models\nwhile significantly reducing data acquisition and annotation costs.",
      "authors": [
        "Karan Dua",
        "Hitesh Laxmichand Patel",
        "Puneet Mittal",
        "Ranjeet Gupta",
        "Amit Agarwal",
        "Praneet Pabolu",
        "Srikant Panda",
        "Hansa Meghwani",
        "Graham Horwood",
        "Fahad Shah"
      ],
      "published": "2025-10-02T15:42:35Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02133v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "FlexDoc是一个可扩展的合成数据生成框架，通过随机模式和参数化采样技术，能够大规模生成多语言半结构化文档及丰富标注。该方法在关键信息提取任务中可将F1分数提升高达11%，同时相比传统硬模板方法减少90%以上的标注工作量，显著降低企业级文档理解模型的数据获取成本。",
      "order": 742
    },
    {
      "arxiv_id": "2510.02120v1",
      "title": "VarCoNet: A variability-aware self-supervised framework for functional\n  connectome extraction from resting-state fMRI",
      "summary": "Accounting for inter-individual variability in brain function is key to\nprecision medicine. Here, by considering functional inter-individual\nvariability as meaningful data rather than noise, we introduce VarCoNet, an\nenhanced self-supervised framework for robust functional connectome (FC)\nextraction from resting-state fMRI (rs-fMRI) data. VarCoNet employs\nself-supervised contrastive learning to exploit inherent functional\ninter-individual variability, serving as a brain function encoder that\ngenerates FC embeddings readily applicable to downstream tasks even in the\nabsence of labeled data. Contrastive learning is facilitated by a novel\naugmentation strategy based on segmenting rs-fMRI signals. At its core,\nVarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-series\nprocessing, enhanced with a robust Bayesian hyperparameter optimization. Our\nVarCoNet framework is evaluated on two downstream tasks: (i) subject\nfingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii)\nautism spectrum disorder (ASD) classification, using rs-fMRI data from the\nABIDE I and ABIDE II datasets. Using different brain parcellations, our\nextensive testing against state-of-the-art methods, including 13 deep learning\nmethods, demonstrates VarCoNet's superiority, robustness, interpretability, and\ngeneralizability. Overall, VarCoNet provides a versatile and robust framework\nfor FC analysis in rs-fMRI.",
      "authors": [
        "Charalampos Lamprou",
        "Aamna Alshehhi",
        "Leontios J. Hadjileontiadis",
        "Mohamed L. Seghier"
      ],
      "published": "2025-10-02T15:29:17Z",
      "primary_category": "cs.NE",
      "arxiv_url": "https://arxiv.org/abs/2510.02120v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "VarCoNet是一种变异性感知的自监督框架，通过对比学习利用个体间功能变异性，结合1D-CNN-Transformer编码器和贝叶斯超参数优化，从静息态fMRI数据中提取功能连接组，在人类连接组项目和自闭症分类任务中表现出优越性能。",
      "order": 743
    },
    {
      "arxiv_id": "2510.02119v1",
      "title": "Non-Asymptotic Analysis of Data Augmentation for Precision Matrix\n  Estimation",
      "summary": "This paper addresses the problem of inverse covariance (also known as\nprecision matrix) estimation in high-dimensional settings. Specifically, we\nfocus on two classes of estimators: linear shrinkage estimators with a target\nproportional to the identity matrix, and estimators derived from data\naugmentation (DA). Here, DA refers to the common practice of enriching a\ndataset with artificial samples--typically generated via a generative model or\nthrough random transformations of the original data--prior to model fitting.\nFor both classes of estimators, we derive estimators and provide concentration\nbounds for their quadratic error. This allows for both method comparison and\nhyperparameter tuning, such as selecting the optimal proportion of artificial\nsamples. On the technical side, our analysis relies on tools from random matrix\ntheory. We introduce a novel deterministic equivalent for generalized resolvent\nmatrices, accommodating dependent samples with specific structure. We support\nour theoretical results with numerical experiments.",
      "authors": [
        "Lucas Morisset",
        "Adrien Hardy",
        "Alain Durmus"
      ],
      "published": "2025-10-02T15:28:14Z",
      "primary_category": "stat.ML",
      "arxiv_url": "https://arxiv.org/abs/2510.02119v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究高维环境下逆协方差矩阵估计问题，重点分析线性收缩估计器和数据增强估计器。通过随机矩阵理论工具，推导了估计器的二次误差集中界，为方法比较和超参数调优提供理论依据，并通过数值实验验证结果。",
      "order": 744
    },
    {
      "arxiv_id": "2510.02117v1",
      "title": "DAG DECORation: Continuous Optimization for Structure Learning under\n  Hidden Confounding",
      "summary": "We study structure learning for linear Gaussian SEMs in the presence of\nlatent confounding. Existing continuous methods excel when errors are\nindependent, while deconfounding-first pipelines rely on pervasive factor\nstructure or nonlinearity. We propose \\textsc{DECOR}, a single likelihood-based\nand fully differentiable estimator that jointly learns a DAG and a correlated\nnoise model. Our theory gives simple sufficient conditions for global parameter\nidentifiability: if the mixed graph is bow free and the noise covariance has a\nuniform eigenvalue margin, then the map from $(\\B,\\OmegaMat)$ to the\nobservational covariance is injective, so both the directed structure and the\nnoise are uniquely determined. The estimator alternates a smooth-acyclic graph\nupdate with a convex noise update and can include a light bow complementarity\npenalty or a post hoc reconciliation step. On synthetic benchmarks that vary\nconfounding density, graph density, latent rank, and dimension with $n<p$,\n\\textsc{DECOR} matches or outperforms strong baselines and is especially robust\nwhen confounding is non-pervasive, while remaining competitive under\npervasiveness.",
      "authors": [
        "Samhita Pal",
        "James O'quinn",
        "Kaveh Aryan",
        "Heather Pua",
        "James P. Long",
        "Amir Asiaee"
      ],
      "published": "2025-10-02T15:23:30Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02117v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出DECOR方法，针对存在潜在混杂因素的线性高斯结构方程模型，通过联合学习有向无环图和相关噪声模型，在非普遍混杂情况下表现优异。该方法基于可微优化，理论证明在弓形自由图和噪声协方差均匀特征值边界条件下可实现全局参数识别。",
      "order": 745
    },
    {
      "arxiv_id": "2510.02116v1",
      "title": "Ensemble Threshold Calibration for Stable Sensitivity Control",
      "summary": "Precise recall control is critical in large-scale spatial conflation and\nentity-matching tasks, where missing even a few true matches can break\ndownstream analytics, while excessive manual review inflates cost. Classical\nconfidence-interval cuts such as Clopper-Pearson or Wilson provide lower bounds\non recall, but they routinely overshoot the target by several percentage points\nand exhibit high run-to-run variance under skewed score distributions. We\npresent an end-to-end framework that achieves exact recall with sub-percent\nvariance over tens of millions of geometry pairs, while remaining TPU-friendly.\nOur pipeline starts with an equigrid bounding-box filter and compressed sparse\nrow (CSR) candidate representation, reducing pair enumeration by two orders of\nmagnitude. A deterministic xxHash bootstrap sample trains a lightweight neural\nranker; its scores are propagated to all remaining pairs via a single forward\npass and used to construct a reproducible, score-decile-stratified calibration\nset. Four complementary threshold estimators - Clopper-Pearson, Jeffreys,\nWilson, and an exact quantile - are aggregated via inverse-variance weighting,\nthen fused across nine independent subsamples. This ensemble reduces threshold\nvariance compared to any single method. Evaluated on two real cadastral\ndatasets (approximately 6.31M and 67.34M pairs), our approach consistently hits\na recall target within a small error, decreases redundant verifications\nrelative to other calibrations, and runs end-to-end on a single TPU v3 core.",
      "authors": [
        "John N. Daras"
      ],
      "published": "2025-10-02T15:22:28Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02116v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种集成阈值校准框架，用于在大规模空间匹配任务中实现精确的召回率控制。通过结合边界框过滤、轻量级神经排序器和四种阈值估计器的集成方法，在数千万几何对数据上实现亚百分比方差的稳定召回控制，相比传统方法显著减少冗余验证并保持TPU兼容性。",
      "order": 746
    },
    {
      "arxiv_id": "2510.02115v1",
      "title": "Hybrid Deep Learning Modeling Approach to Predict Natural Gas\n  Consumption of Home Subscribers on Limited Data",
      "summary": "Today, natural gas, as a clean fuel and the best alternative to crude oil,\ncovers a significant part of global demand. Iran is one of the largest\ncountries with energy resources and in terms of gas is the second-largest\ncountry in the world. But, due to the increase in population and energy\nconsumption, it faces problems such as pressure drops and gas outages yearly in\ncold seasons and therefore it is necessary to control gas consumption,\nespecially in the residential sector, which has the largest share in Iran. This\nstudy aims to analyze and predict gas consumption for residential customers in\nZanjan province, Iran, using machine learning models, including LSTM, GRU, and\na hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and\nmeteorology data collected over six years, from 2017 to 2022. The models were\ntrained and evaluated based on their ability to accurately predict consumption\npatterns. The results indicate that the hybrid BiLSTM-XGBoost model\noutperformed the other models in terms of accuracy, with lower Root Mean\nSquared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean\nPercentage Error (MPE). Additionally, the Hybrid model demonstrated robust\nperformance, particularly in scenarios with limited data. The findings suggest\nthat machine learning approaches, particularly hybrid models, can be\neffectively utilized to manage and predict gas consumption, contributing to\nmore efficient resource management and reducing seasonal shortages. This study\nhighlights the importance of incorporating geographical and climatic factors in\npredictive modeling, as these significantly influence gas usage across\ndifferent regions.",
      "authors": [
        "Milad Firoozeh",
        "Nader Dashti",
        "Mohammad Ali Hatefi"
      ],
      "published": "2025-10-02T15:22:19Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02115v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究针对伊朗赞詹省住宅用户的天然气消费预测问题，提出了一种混合BiLSTM-XGBoost深度学习模型。在六年消费与气象数据上，该模型相比LSTM和GRU模型表现出更优的预测精度（RMSE、MAPE和MPE指标更低），尤其在数据有限场景下稳健性突出。研究表明结合地理气候因素的混合机器学习方法可有效提升能源管理效率，缓解季节性短缺问题。",
      "order": 747
    },
    {
      "arxiv_id": "2510.02110v1",
      "title": "SoundReactor: Frame-level Online Video-to-Audio Generation",
      "summary": "Prevailing Video-to-Audio (V2A) generation models operate offline, assuming\nan entire video sequence or chunks of frames are available beforehand. This\ncritically limits their use in interactive applications such as live content\ncreation and emerging generative world models. To address this gap, we\nintroduce the novel task of frame-level online V2A generation, where a model\nautoregressively generates audio from video without access to future video\nframes. Furthermore, we propose SoundReactor, which, to the best of our\nknowledge, is the first simple yet effective framework explicitly tailored for\nthis task. Our design enforces end-to-end causality and targets low per-frame\nlatency with audio-visual synchronization. Our model's backbone is a\ndecoder-only causal transformer over continuous audio latents. For vision\nconditioning, it leverages grid (patch) features extracted from the smallest\nvariant of the DINOv2 vision encoder, which are aggregated into a single token\nper frame to maintain end-to-end causality and efficiency. The model is trained\nthrough a diffusion pre-training followed by consistency fine-tuning to\naccelerate the diffusion head decoding. On a benchmark of diverse gameplay\nvideos from AAA titles, our model successfully generates semantically and\ntemporally aligned, high-quality full-band stereo audio, validated by both\nobjective and human evaluations. Furthermore, our model achieves low per-frame\nwaveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on\n30FPS, 480p videos using a single H100. Demo samples are available at\nhttps://koichi-saito-sony.github.io/soundreactor/.",
      "authors": [
        "Koichi Saito",
        "Julian Tanke",
        "Christian Simon",
        "Masato Ishii",
        "Kazuki Shimada",
        "Zachary Novack",
        "Zhi Zhong",
        "Akio Hayakawa",
        "Takashi Shibuya",
        "Yuki Mitsufuji"
      ],
      "published": "2025-10-02T15:18:00Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.02110v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "SoundReactor是首个帧级在线视频到音频生成框架，采用因果变换器架构，通过扩散预训练和一致性微调实现低延迟、高质量的实时音频生成，在游戏视频上验证了音视频同步效果。",
      "order": 748
    },
    {
      "arxiv_id": "2510.02107v1",
      "title": "PENEX: AdaBoost-Inspired Neural Network Regularization",
      "summary": "AdaBoost sequentially fits so-called weak learners to minimize an exponential\nloss, which penalizes mislabeled data points more severely than other loss\nfunctions like cross-entropy. Paradoxically, AdaBoost generalizes well in\npractice as the number of weak learners grows. In the present work, we\nintroduce Penalized Exponential Loss (PENEX), a new formulation of the\nmulti-class exponential loss that is theoretically grounded and, in contrast to\nthe existing formulation, amenable to optimization via first-order methods. We\ndemonstrate both empirically and theoretically that PENEX implicitly maximizes\nmargins of data points. Also, we show that gradient increments on PENEX\nimplicitly parameterize weak learners in the boosting framework. Across\ncomputer vision and language tasks, we show that PENEX exhibits a regularizing\neffect often better than established methods with similar computational cost.\nOur results highlight PENEX's potential as an AdaBoost-inspired alternative for\neffective training and fine-tuning of deep neural networks.",
      "authors": [
        "Klaus-Rudolf Kladny",
        "Bernhard Schölkopf",
        "Michael Muehlebach"
      ],
      "published": "2025-10-02T15:13:02Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02107v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出PENEX（惩罚指数损失），一种受AdaBoost启发的多分类指数损失函数，理论上有良好基础且适合一阶优化方法。研究表明PENEX能隐式最大化数据边界，梯度增量可参数化提升框架中的弱学习器。在计算机视觉和语言任务中，PENEX展现出优于现有方法的正则化效果，是深度神经网络训练和微调的有效替代方案。",
      "order": 749
    },
    {
      "arxiv_id": "2510.02096v1",
      "title": "Learning Model Representations Using Publicly Available Model Hubs",
      "summary": "The weights of neural networks have emerged as a novel data modality, giving\nrise to the field of weight space learning. A central challenge in this area is\nthat learning meaningful representations of weights typically requires large,\ncarefully constructed collections of trained models, typically referred to as\nmodel zoos. These model zoos are often trained ad-hoc, requiring large\ncomputational resources, constraining the learned weight space representations\nin scale and flexibility. In this work, we drop this requirement by training a\nweight space learning backbone on arbitrary models downloaded from large,\nunstructured model repositories such as Hugging Face. Unlike curated model\nzoos, these repositories contain highly heterogeneous models: they vary in\narchitecture and dataset, and are largely undocumented. To address the\nmethodological challenges posed by such heterogeneity, we propose a new weight\nspace backbone designed to handle unstructured model populations. We\ndemonstrate that weight space representations trained on models from Hugging\nFace achieve strong performance, often outperforming backbones trained on\nlaboratory-generated model zoos. Finally, we show that the diversity of the\nmodel weights in our training set allows our weight space model to generalize\nto unseen data modalities. By demonstrating that high-quality weight space\nrepresentations can be learned in the wild, we show that curated model zoos are\nnot indispensable, thereby overcoming a strong limitation currently faced by\nthe weight space learning community.",
      "authors": [
        "Damian Falk",
        "Konstantin Schürholt",
        "Konstantinos Tzevelekakis",
        "Léo Meynent",
        "Damian Borth"
      ],
      "published": "2025-10-02T15:04:31Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02096v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出一种直接从Hugging Face等公开模型库中学习权重空间表示的新方法，无需依赖精心构建的模型动物园。通过设计能处理异构模型的新型权重空间骨干网络，在多样化的公开模型上训练出的表示性能优异，甚至超越实验室生成模型，并能泛化到未见过的数据模态，突破了权重空间学习领域对定制模型库的依赖。",
      "order": 750
    },
    {
      "arxiv_id": "2510.02084v1",
      "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
      "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
      "authors": [
        "Kuiye Ding",
        "Fanda Fan",
        "Zheya Wang",
        "Hongxiao Li",
        "Yifan Wang",
        "Lei Wang",
        "Chunjie Luo",
        "Jianfeng Zhan"
      ],
      "published": "2025-10-02T14:50:50Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02084v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "KAIROS是一种非自回归时间序列预测框架，直接建模分段多峰分布，避免误差累积并实现实时推理。在大规模语料上训练后，在六个基准测试中展现出强大的零样本泛化能力，以较低推理成本达到与最先进基础模型相当的预测性能。",
      "order": 751
    },
    {
      "arxiv_id": "2510.02081v1",
      "title": "Fine-Tuning Flow Matching via Maximum Likelihood Estimation of\n  Reconstructions",
      "summary": "Flow Matching (FM) algorithm achieves remarkable results in generative tasks\nespecially in robotic manipulation. Building upon the foundations of diffusion\nmodels, the simulation-free paradigm of FM enables simple and efficient\ntraining, but inherently introduces a train-inference gap. Specifically, we\ncannot assess the model's output during the training phase. In contrast, other\ngenerative models including Variational Autoencoder (VAE), Normalizing Flow and\nGenerative Adversarial Networks (GANs) directly optimize on the reconstruction\nloss. Such a gap is particularly evident in scenarios that demand high\nprecision, such as robotic manipulation. Moreover, we show that FM's\nover-pursuit of straight predefined paths may introduce some serious problems\nsuch as stiffness into the system. These motivate us to fine-tune FM via\nMaximum Likelihood Estimation of reconstructions - an approach made feasible by\nFM's underlying smooth ODE formulation, in contrast to the stochastic\ndifferential equations (SDEs) used in diffusion models. This paper first\ntheoretically analyzes the relation between training loss and inference error\nin FM. Then we propose a method of fine-tuning FM via Maximum Likelihood\nEstimation of reconstructions, which includes both straightforward fine-tuning\nand residual-based fine-tuning approaches. Furthermore, through specifically\ndesigned architectures, the residual-based fine-tuning can incorporate the\ncontraction property into the model, which is crucial for the model's\nrobustness and interpretability. Experimental results in image generation and\nrobotic manipulation verify that our method reliably improves the inference\nperformance of FM.",
      "authors": [
        "Zhaoyi Li",
        "Jingtao Ding",
        "Yong Li",
        "Shihua Li"
      ],
      "published": "2025-10-02T14:49:47Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02081v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出通过重构的最大似然估计来微调流匹配算法，解决了训练-推理差距问题。理论分析了训练损失与推理误差的关系，提出直接微调和基于残差的微调方法，后者可引入收缩特性增强鲁棒性。在图像生成和机器人操控实验中验证了方法对推理性能的可靠提升。",
      "order": 752
    },
    {
      "arxiv_id": "2510.02073v1",
      "title": "Inferring Optical Tissue Properties from Photoplethysmography using\n  Hybrid Amortized Inference",
      "summary": "Smart wearables enable continuous tracking of established biomarkers such as\nheart rate, heart rate variability, and blood oxygen saturation via\nphotoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer\nphysiological information, as recent deep learning (DL) studies demonstrate.\nHowever, DL models often rely on features with unclear physiological meaning,\ncreating a tension between predictive power, clinical interpretability, and\nsensor design. We address this gap by introducing PPGen, a biophysical model\nthat relates PPG signals to interpretable physiological and optical parameters.\nBuilding on PPGen, we propose hybrid amortized inference (HAI), enabling fast,\nrobust, and scalable estimation of relevant physiological parameters from PPG\nsignals while correcting for model misspecification. In extensive in-silico\nexperiments, we show that HAI can accurately infer physiological parameters\nunder diverse noise and sensor conditions. Our results illustrate a path toward\nPPG models that retain the fidelity needed for DL-based features while\nsupporting clinical interpretation and informed hardware design.",
      "authors": [
        "Jens Behrmann",
        "Maria R. Cervera",
        "Antoine Wehenkel",
        "Andrew C. Miller",
        "Albert Cerussi",
        "Pranay Jain",
        "Vivek Venugopal",
        "Shijie Yan",
        "Guillermo Sapiro",
        "Luca Pegolotti",
        "Jörn-Henrik Jacobsen"
      ],
      "published": "2025-10-02T14:36:02Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02073v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出PPGen生物物理模型和混合摊销推理方法，从光电容积脉搏波信号中推断可解释的生理光学参数，解决了深度学习模型在医疗应用中预测能力与临床可解释性之间的平衡问题。",
      "order": 753
    },
    {
      "arxiv_id": "2510.02067v1",
      "title": "Adaptive Kernel Selection for Stein Variational Gradient Descent",
      "summary": "A central challenge in Bayesian inference is efficiently approximating\nposterior distributions. Stein Variational Gradient Descent (SVGD) is a popular\nvariational inference method which transports a set of particles to approximate\na target distribution. The SVGD dynamics are governed by a reproducing kernel\nHilbert space (RKHS) and are highly sensitive to the choice of the kernel\nfunction, which directly influences both convergence and approximation quality.\nThe commonly used median heuristic offers a simple approach for setting kernel\nbandwidths but lacks flexibility and often performs poorly, particularly in\nhigh-dimensional settings. In this work, we propose an alternative strategy for\nadaptively choosing kernel parameters over an abstract family of kernels.\nRecent convergence analyses based on the kernelized Stein discrepancy (KSD)\nsuggest that optimizing the kernel parameters by maximizing the KSD can improve\nperformance. Building on this insight, we introduce Adaptive SVGD (Ad-SVGD), a\nmethod that alternates between updating the particles via SVGD and adaptively\ntuning kernel bandwidths through gradient ascent on the KSD. We provide a\nsimplified theoretical analysis that extends existing results on minimizing the\nKSD for fixed kernels to our adaptive setting, showing convergence properties\nfor the maximal KSD over our kernel class. Our empirical results further\nsupport this intuition: Ad-SVGD consistently outperforms standard heuristics in\na variety of tasks.",
      "authors": [
        "Moritz Melcher",
        "Simon Weissmann",
        "Ashia C. Wilson",
        "Jakob Zech"
      ],
      "published": "2025-10-02T14:33:57Z",
      "primary_category": "stat.ML",
      "arxiv_url": "https://arxiv.org/abs/2510.02067v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出自适应Stein变分梯度下降(Ad-SVGD)方法，通过梯度上升优化核参数以最大化核化Stein差异，解决了传统中位数启发式在贝叶斯推理中核选择不灵活的问题。理论分析和实验表明该方法在多种任务中优于标准启发式方法。",
      "order": 754
    },
    {
      "arxiv_id": "2510.02060v1",
      "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly\n  Detection",
      "summary": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD.",
      "authors": [
        "Sanghyu Yoon",
        "Dongmin Kim",
        "Suhee Yoon",
        "Ye Seul Sim",
        "Seungdong Yoa",
        "Hye-Seung Cho",
        "Soonyoung Lee",
        "Hankook Lee",
        "Woohyung Lim"
      ],
      "published": "2025-10-02T14:28:45Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02060v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "ReTabAD是一个用于表格异常检测的基准数据集，通过恢复文本语义上下文来解决现有基准缺乏领域知识的问题。该工作提供20个包含结构化文本元数据的表格数据集，实现了包括传统方法、深度学习和LLM在内的先进异常检测算法，并提出了无需任务特定训练的零样本LLM框架。实验表明语义上下文能提升检测性能并增强可解释性。",
      "order": 755
    },
    {
      "arxiv_id": "2510.02056v1",
      "title": "Adaptive Heterogeneous Mixtures of Normalising Flows for Robust\n  Variational Inference",
      "summary": "Normalising-flow variational inference (VI) can approximate complex\nposteriors, yet single-flow models often behave inconsistently across\nqualitatively different distributions. We propose Adaptive Mixture Flow\nVariational Inference (AMF-VI), a heterogeneous mixture of complementary flows\n(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of\nindividual flows, and (ii) adaptive global weight estimation via\nlikelihood-driven updates, without per-sample gating or architectural changes.\nEvaluated on six canonical posterior families of banana, X-shape, two-moons,\nrings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower\nnegative log-likelihood than each single-flow baseline and delivers stable\ngains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),\nindicating improved robustness across shapes and modalities. The procedure is\nefficient and architecture-agnostic, incurring minimal overhead relative to\nstandard flow training, and demonstrates that adaptive mixtures of diverse\nflows provide a reliable route to robust VI across diverse posterior families\nwhilst preserving each expert's inductive bias.",
      "authors": [
        "Benjamin Wiriyapong",
        "Oktay Karakuş",
        "Kirill Sidorov"
      ],
      "published": "2025-10-02T14:25:29Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02056v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "提出自适应混合流变分推理(AMF-VI)，通过异质归一化流混合(MAF、RealNVP、RBIG)和两阶段训练策略，在六种典型后验分布上实现更稳健的变分推理，显著提升负对数似然和分布匹配指标。",
      "order": 756
    },
    {
      "arxiv_id": "2510.02050v1",
      "title": "Multidata Causal Discovery for Statistical Hurricane Intensity\n  Forecasting",
      "summary": "Improving statistical forecasts of Atlantic hurricane intensity is limited by\ncomplex nonlinear interactions and difficulty in identifying relevant\npredictors. Conventional methods prioritize correlation or fit, often\noverlooking confounding variables and limiting generalizability to unseen\ntropical storms. To address this, we leverage a multidata causal discovery\nframework with a replicated dataset based on Statistical Hurricane Intensity\nPrediction Scheme (SHIPS) using ERA5 meteorological reanalysis. We conduct\nmultiple experiments to identify and select predictors causally linked to\nhurricane intensity changes. We train multiple linear regression models to\ncompare causal feature selection with no selection, correlation, and random\nforest feature importance across five forecast lead times from 1 to 5 days (24\nto 120 hours). Causal feature selection consistently outperforms on unseen test\ncases, especially for lead times shorter than 3 days. The causal features\nprimarily include vertical shear, mid-tropospheric potential vorticity and\nsurface moisture conditions, which are physically significant yet often\nunderutilized in hurricane intensity predictions. Further, we build an extended\npredictor set (SHIPS+) by adding selected features to the standard SHIPS\npredictors. SHIPS+ yields increased short-term predictive skill at lead times\nof 24, 48, and 72 hours. Adding nonlinearity using multilayer perceptron\nfurther extends skill to longer lead times, despite our framework being purely\nregional and not requiring global forecast data. Operational SHIPS tests\nconfirm that three of the six added causally discovered predictors improve\nforecasts, with the largest gains at longer lead times. Our results demonstrate\nthat causal discovery improves hurricane intensity prediction and pave the way\ntoward more empirical forecasts.",
      "authors": [
        "Saranya Ganesh S.",
        "Frederick Iat-Hin Tam",
        "Milton S. Gomez",
        "Marie McGraw",
        "Mark DeMaria",
        "Kate Musgrave",
        "Jakob Runge",
        "Tom Beucler"
      ],
      "published": "2025-10-02T14:23:51Z",
      "primary_category": "stat.AP",
      "arxiv_url": "https://arxiv.org/abs/2510.02050v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究提出一种多数据因果发现框架，用于改进大西洋飓风强度统计预测。通过因果特征选择方法识别与飓风强度变化因果相关的预测因子（如垂直风切变、中层位势涡度和地表湿度），构建扩展预测因子集SHIPS+。实验表明，因果特征选择在短期预测（1-3天）中表现最优，线性回归和多层感知器模型均证实其能提升预测技能，且无需全球预报数据。",
      "order": 757
    },
    {
      "arxiv_id": "2510.02049v1",
      "title": "Mathematical Modeling and Convergence Analysis of Deep Neural Networks\n  with Dense Layer Connectivities in Deep Learning",
      "summary": "In deep learning, dense layer connectivity has become a key design principle\nin deep neural networks (DNNs), enabling efficient information flow and strong\nperformance across a range of applications. In this work, we model densely\nconnected DNNs mathematically and analyze their learning problems in the\ndeep-layer limit. For a broad applicability, we present our analysis in a\nframework setting of DNNs with densely connected layers and general non-local\nfeature transformations (with local feature transformations as special cases)\nwithin layers, which is called dense non-local (DNL) framework and includes\nstandard DenseNets and variants as special examples. In this formulation, the\ndensely connected networks are modeled as nonlinear integral equations, in\ncontrast to the ordinary differential equation viewpoint commonly adopted in\nprior works. We study the associated training problems from an optimal control\nperspective and prove convergence results from the network learning problem to\nits continuous-time counterpart. In particular, we show the convergence of\noptimal values and the subsequence convergence of minimizers, using a piecewise\nlinear extension and $\\Gamma$-convergence analysis. Our results provide a\nmathematical foundation for understanding densely connected DNNs and further\nsuggest that such architectures can offer stability of training deep models.",
      "authors": [
        "Jinshu Huang",
        "Haibin Su",
        "Xue-Cheng Tai",
        "Chunlin Wu"
      ],
      "published": "2025-10-02T14:22:51Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02049v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出密集非局部(DNL)框架，将密集连接深度神经网络建模为非线性积分方程，从最优控制角度分析训练问题，证明了网络学习问题到连续时间对应问题的收敛性，为理解密集连接DNN提供了数学基础。",
      "order": 758
    },
    {
      "arxiv_id": "2510.02048v1",
      "title": "Variational Secret Common Randomness Extraction",
      "summary": "This paper studies the problem of extracting common randomness (CR) or secret\nkeys from correlated random sources observed by two legitimate parties, Alice\nand Bob, through public discussion in the presence of an eavesdropper, Eve. We\npropose a practical two-stage CR extraction framework. In the first stage, the\nvariational probabilistic quantization (VPQ) step is introduced, where Alice\nand Bob employ probabilistic neural network (NN) encoders to map their\nobservations into discrete, nearly uniform random variables (RVs) with high\nagreement probability while minimizing information leakage to Eve. This is\nrealized through a variational learning objective combined with adversarial\ntraining. In the second stage, a secure sketch using code-offset construction\nreconciles the encoder outputs into identical secret keys, whose secrecy is\nguaranteed by the VPQ objective. As a representative application, we study\nphysical layer key (PLK) generation. Beyond the traditional methods, which rely\non the channel reciprocity principle and require two-way channel probing, thus\nsuffering from large protocol overhead and being unsuitable in high mobility\nscenarios, we propose a sensing-based PLK generation method for integrated\nsensing and communications (ISAC) systems, where paired range-angle (RA) maps\nmeasured at Alice and Bob serve as correlated sources. The idea is verified\nthrough both end-to-end simulations and real-world software-defined radio (SDR)\nmeasurements, including scenarios where Eve has partial knowledge about Bob's\nposition. The results demonstrate the feasibility and convincing performance of\nboth the proposed CR extraction framework and sensing-based PLK generation\nmethod.",
      "authors": [
        "Xinyang Li",
        "Vlad C. Andrei",
        "Peter J. Gu",
        "Yiqi Chen",
        "Ullrich J. Mönich",
        "Holger Boche"
      ],
      "published": "2025-10-02T14:22:21Z",
      "primary_category": "cs.IT",
      "arxiv_url": "https://arxiv.org/abs/2510.02048v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种两阶段变分秘密共同随机性提取框架：第一阶段采用变分概率量化，通过神经网络编码器将相关源映射为离散均匀随机变量，结合对抗训练最小化信息泄露；第二阶段使用安全草图实现密钥一致。应用于集成感知通信系统，提出基于感知的物理层密钥生成方法，通过端到端仿真和软件无线电实测验证了在移动场景下的可行性和优越性能。",
      "order": 759
    },
    {
      "arxiv_id": "2510.02043v1",
      "title": "Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers",
      "summary": "Pose estimation refers to tracking a human's full body posture, including\ntheir head, torso, arms, and legs. The problem is challenging in practical\nsettings where the number of body sensors are limited. Past work has shown\npromising results using conditional diffusion models, where the pose prediction\nis conditioned on both <location, rotation> measurements from the sensors.\nUnfortunately, nearly all these approaches generalize poorly across users,\nprimarly because location measurements are highly influenced by the body size\nof the user. In this paper, we formulate pose estimation as an inverse problem\nand design an algorithm capable of zero-shot generalization. Our idea utilizes\na pre-trained diffusion model and conditions it on rotational measurements\nalone; the priors from this model are then guided by a likelihood term, derived\nfrom the measured locations. Thus, given any user, our proposed InPose method\ngeneratively estimates the highly likely sequence of poses that best explains\nthe sparse on-body measurements.",
      "authors": [
        "Sahil Bhandary Karnoor",
        "Romit Roy Choudhury"
      ],
      "published": "2025-10-02T14:16:43Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02043v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出InPose方法，将人体姿态估计构建为逆问题，利用预训练扩散模型仅基于旋转测量进行条件生成，通过位置测量的似然项引导生成最符合稀疏传感器数据的姿态序列，实现零样本跨用户泛化。",
      "order": 760
    },
    {
      "arxiv_id": "2510.02017v1",
      "title": "FairContrast: Enhancing Fairness through Contrastive learning and\n  Customized Augmenting Methods on Tabular Data",
      "summary": "As AI systems become more embedded in everyday life, the development of fair\nand unbiased models becomes more critical. Considering the social impact of AI\nsystems is not merely a technical challenge but a moral imperative. As\nevidenced in numerous research studies, learning fair and robust\nrepresentations has proven to be a powerful approach to effectively debiasing\nalgorithms and improving fairness while maintaining essential information for\nprediction tasks. Representation learning frameworks, particularly those that\nutilize self-supervised and contrastive learning, have demonstrated superior\nrobustness and generalizability across various domains. Despite the growing\ninterest in applying these approaches to tabular data, the issue of fairness in\nthese learned representations remains underexplored. In this study, we\nintroduce a contrastive learning framework specifically designed to address\nbias and learn fair representations in tabular datasets. By strategically\nselecting positive pair samples and employing supervised and self-supervised\ncontrastive learning, we significantly reduce bias compared to existing\nstate-of-the-art contrastive learning models for tabular data. Our results\ndemonstrate the efficacy of our approach in mitigating bias with minimum\ntrade-off in accuracy and leveraging the learned fair representations in\nvarious downstream tasks.",
      "authors": [
        "Aida Tayebi",
        "Ali Khodabandeh Yalabadi",
        "Mehdi Yazdani-Jahromi",
        "Ozlem Ozmen Garibay"
      ],
      "published": "2025-10-02T13:43:53Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02017v1",
      "primary_area": "text_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "FairContrast提出了一种针对表格数据的对比学习框架，通过策略性选择正样本对和监督/自监督对比学习，在保持预测准确性的同时显著减少算法偏见，学习公平表征并应用于下游任务。",
      "order": 761
    },
    {
      "arxiv_id": "2510.02014v1",
      "title": "Normality Calibration in Semi-supervised Graph Anomaly Detection",
      "summary": "Graph anomaly detection (GAD) has attracted growing interest for its crucial\nability to uncover irregular patterns in broad applications. Semi-supervised\nGAD, which assumes a subset of annotated normal nodes available during\ntraining, is among the most widely explored application settings. However, the\nnormality learned by existing semi-supervised GAD methods is limited to the\nlabeled normal nodes, often inclining to overfitting the given patterns. These\ncan lead to high detection errors, such as high false positives. To overcome\nthis limitation, we propose GraphNC , a graph normality calibration framework\nthat leverages both labeled and unlabeled data to calibrate the normality from\na teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly\nscore and node representation spaces. GraphNC includes two main components,\nanomaly score distribution alignment (ScoreDA) and perturbation-based normality\nregularization (NormReg). ScoreDA optimizes the anomaly scores of our model by\naligning them with the score distribution yielded by the teacher model. Due to\naccurate scores in most of the normal nodes and part of the anomaly nodes in\nthe teacher model, the score alignment effectively pulls the anomaly scores of\nthe normal and abnormal classes toward the two ends, resulting in more\nseparable anomaly scores. Nevertheless, there are inaccurate scores from the\nteacher model. To mitigate the misleading by these scores, NormReg is designed\nto regularize the graph normality in the representation space, making the\nrepresentations of normal nodes more compact by minimizing a\nperturbation-guided consistency loss solely on the labeled nodes.",
      "authors": [
        "Guolei Zeng",
        "Hezhe Qiao",
        "Guoguo Ai",
        "Jinsong Guo",
        "Guansong Pang"
      ],
      "published": "2025-10-02T13:36:04Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02014v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "提出GraphNC框架解决半监督图异常检测中的过拟合问题，通过异常分数分布对齐和扰动正则化校准节点正常性，在分数和表示空间联合优化，提升检测准确性。",
      "order": 762
    },
    {
      "arxiv_id": "2510.02009v1",
      "title": "ShapeGen3DCP: A Deep Learning Framework for Layer Shape Prediction in 3D\n  Concrete Printing",
      "summary": "This work introduces ShapeGen3DCP, a deep learning framework for fast and\naccurate prediction of filament cross-sectional geometry in 3D Concrete\nPrinting (3DCP). The method is based on a neural network architecture that\ntakes as input both material properties in the fluid state (density, yield\nstress, plastic viscosity) and process parameters (nozzle diameter, nozzle\nheight, printing and flow velocities) to directly predict extruded layer\nshapes. To enhance generalization, some inputs are reformulated into\ndimensionless parameters that capture underlying physical principles. Predicted\ngeometries are compactly represented using Fourier descriptors, which enforce\nsmooth, closed, and symmetric profiles while reducing the prediction task to a\nsmall set of coefficients. The training dataset was synthetically generated\nusing a well-established Particle Finite Element (PFEM) model of 3DCP,\novercoming the scarcity of experimental data. Validation against diverse\nnumerical and experimental cases shows strong agreement, confirming the\nframework's accuracy and reliability. This opens the way to practical uses\nranging from pre-calibration of print settings, minimizing or even eliminating\ntrial-and-error adjustments, to toolpath optimization for more advanced\ndesigns. Looking ahead, coupling the framework with simulations and sensor\nfeedback could enable closed-loop digital twins for 3DCP, driving real-time\nprocess optimization, defect detection, and adaptive control of printing\nparameters.",
      "authors": [
        "Giacomo Rizzieri",
        "Federico Lanteri",
        "Liberato Ferrara",
        "Massimiliano Cremonesi"
      ],
      "published": "2025-10-02T13:30:20Z",
      "primary_category": "cs.CE",
      "arxiv_url": "https://arxiv.org/abs/2510.02009v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "ShapeGen3DCP是一个深度学习框架，用于3D混凝土打印中丝状截面几何形状的快速准确预测。该模型结合材料特性和工艺参数，通过傅里叶描述符紧凑表示预测形状，基于合成数据进行训练，验证显示与实验数据高度一致，可用于打印参数预校准和路径优化。",
      "order": 763
    },
    {
      "arxiv_id": "2510.01988v1",
      "title": "PepCompass: Navigating peptide embedding spaces using Riemannian\n  Geometry",
      "summary": "Antimicrobial peptide discovery is challenged by the astronomical size of\npeptide space and the relative scarcity of active peptides. Generative models\nprovide continuous latent \"maps\" of peptide space, but conventionally ignore\ndecoder-induced geometry and rely on flat Euclidean metrics, rendering\nexploration and optimization distorted and inefficient. Prior manifold-based\nremedies assume fixed intrinsic dimensionality, which critically fails in\npractice for peptide data. Here, we introduce PepCompass, a geometry-aware\nframework for peptide exploration and optimization. At its core, we define a\nUnion of $\\kappa$-Stable Riemannian Manifolds $\\mathbb{M}^{\\kappa}$, a family\nof decoder-induced manifolds that captures local geometry while ensuring\ncomputational stability. We propose two local exploration methods: Second-Order\nRiemannian Brownian Efficient Sampling, which provides a convergent\nsecond-order approximation to Riemannian Brownian motion, and Mutation\nEnumeration in Tangent Space, which reinterprets tangent directions as discrete\namino-acid substitutions. Combining these yields Local Enumeration Bayesian\nOptimization (LE-BO), an efficient algorithm for local activity optimization.\nFinally, we introduce Potential-minimizing Geodesic Search (PoGS), which\ninterpolates between prototype embeddings along property-enriched geodesics,\nbiasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro\nvalidation confirms the effectiveness of PepCompass: PoGS yields four novel\nseeds, and subsequent optimization with LE-BO discovers 25 highly active\npeptides with broad-spectrum activity, including against resistant bacterial\nstrains. These results demonstrate that geometry-informed exploration provides\na powerful new paradigm for antimicrobial peptide design.",
      "authors": [
        "Marcin Możejko",
        "Adam Bielecki",
        "Jurand Prądzyński",
        "Marcin Traskowski",
        "Antoni Janowski",
        "Karol Jurasz",
        "Michał Kucharczyk",
        "Hyun-Su Lee",
        "Marcelo Der Torossian Torres",
        "Cesar de la Fuente-Nunez",
        "Paulina Szymczak",
        "Michał Kmicikiewicz",
        "Ewa Szczurek"
      ],
      "published": "2025-10-02T13:07:37Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01988v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "PepCompass是一种基于黎曼几何的肽探索框架，通过构建κ-稳定黎曼流形联合体捕捉局部几何结构，提出二阶黎曼布朗高效采样和切空间突变枚举方法，结合局部枚举贝叶斯优化和势最小化测地线搜索，成功发现25种高活性抗菌肽，为抗菌肽设计提供了几何感知新范式。",
      "order": 764
    },
    {
      "arxiv_id": "2510.01987v1",
      "title": "Private Federated Multiclass Post-hoc Calibration",
      "summary": "Calibrating machine learning models so that predicted probabilities better\nreflect the true outcome frequencies is crucial for reliable decision-making\nacross many applications. In Federated Learning (FL), the goal is to train a\nglobal model on data which is distributed across multiple clients and cannot be\ncentralized due to privacy concerns. FL is applied in key areas such as\nhealthcare and finance where calibration is strongly required, yet federated\nprivate calibration has been largely overlooked. This work introduces the\nintegration of post-hoc model calibration techniques within FL. Specifically,\nwe transfer traditional centralized calibration methods such as histogram\nbinning and temperature scaling into federated environments and define new\nmethods to operate them under strong client heterogeneity. We study (1) a\nfederated setting and (2) a user-level Differential Privacy (DP) setting and\ndemonstrate how both federation and DP impacts calibration accuracy. We propose\nstrategies to mitigate degradation commonly observed under heterogeneity and\nour findings highlight that our federated temperature scaling works best for\nDP-FL whereas our weighted binning approach is best when DP is not required.",
      "authors": [
        "Samuel Maddock",
        "Graham Cormode",
        "Carsten Maple"
      ],
      "published": "2025-10-02T13:05:31Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01987v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出联邦学习中的多分类后验校准方法，将传统集中式校准技术（如直方图分箱和温度缩放）迁移到联邦环境，解决了客户端异构性下的隐私保护校准问题。研究涵盖联邦设置和用户级差分隐私设置，发现联邦温度缩放最适合DP-FL场景，而无DP时加权分箱方法表现最佳。",
      "order": 765
    },
    {
      "arxiv_id": "2510.01982v1",
      "title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models",
      "summary": "The integration of online reinforcement learning (RL) into diffusion and flow\nmodels has recently emerged as a promising approach for aligning generative\nmodels with human preferences. Stochastic sampling via Stochastic Differential\nEquations (SDE) is employed during the denoising process to generate diverse\ndenoising directions for RL exploration. While existing methods effectively\nexplore potential high-value samples, they suffer from sub-optimal preference\nalignment due to sparse and narrow reward signals. To address these challenges,\nwe propose a novel Granular-GRPO ($\\text{G}^2$RPO ) framework that achieves\nprecise and comprehensive reward assessments of sampling directions in\nreinforcement learning of flow models. Specifically, a Singular Stochastic\nSampling strategy is introduced to support step-wise stochastic exploration\nwhile enforcing a high correlation between the reward and the injected noise,\nthereby facilitating a faithful reward for each SDE perturbation. Concurrently,\nto eliminate the bias inherent in fixed-granularity denoising, we introduce a\nMulti-Granularity Advantage Integration module that aggregates advantages\ncomputed at multiple diffusion scales, producing a more comprehensive and\nrobust evaluation of the sampling directions. Experiments conducted on various\nreward models, including both in-domain and out-of-domain evaluations,\ndemonstrate that our $\\text{G}^2$RPO significantly outperforms existing\nflow-based GRPO baselines,highlighting its effectiveness and robustness.",
      "authors": [
        "Yujie Zhou",
        "Pengyang Ling",
        "Jiazi Bu",
        "Yibin Wang",
        "Yuhang Zang",
        "Jiaqi Wang",
        "Li Niu",
        "Guangtao Zhai"
      ],
      "published": "2025-10-02T12:57:12Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01982v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "G²RPO提出一种细粒度GRPO框架，通过奇异随机采样策略和多粒度优势集成模块，解决流模型强化学习中奖励信号稀疏和窄化问题，在多种奖励模型评估中显著优于现有基线方法。",
      "order": 766
    },
    {
      "arxiv_id": "2510.01970v1",
      "title": "Moon: A Modality Conversion-based Efficient Multivariate Time Series\n  Anomaly Detection",
      "summary": "Multivariate time series (MTS) anomaly detection identifies abnormal patterns\nwhere each timestamp contains multiple variables. Existing MTS anomaly\ndetection methods fall into three categories: reconstruction-based,\nprediction-based, and classifier-based methods. However, these methods face two\nkey challenges: (1) Unsupervised learning methods, such as reconstruction-based\nand prediction-based methods, rely on error thresholds, which can lead to\ninaccuracies; (2) Semi-supervised methods mainly model normal data and often\nunderuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised\nlearning methods, such as classifier-based approaches, often fail to capture\nlocal relationships, incur high computational costs, and are constrained by the\nscarcity of labeled data. To address these limitations, we propose Moon, a\nsupervised modality conversion-based multivariate time series anomaly detection\nframework. Moon enhances the efficiency and accuracy of anomaly detection while\nproviding detailed anomaly analysis reports. First, Moon introduces a novel\nmultivariate Markov Transition Field (MV-MTF) technique to convert numeric time\nseries data into image representations, capturing relationships across\nvariables and timestamps. Since numeric data retains unique patterns that\ncannot be fully captured by image conversion alone, Moon employs a\nMultimodal-CNN to integrate numeric and image data through a feature fusion\nmodel with parameter sharing, enhancing training efficiency. Finally, a\nSHAP-based anomaly explainer identifies key variables contributing to\nanomalies, improving interpretability. Extensive experiments on six real-world\nMTS datasets demonstrate that Moon outperforms six state-of-the-art methods by\nup to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation\nperformance.",
      "authors": [
        "Yuanyuan Yao",
        "Yuhan Shi",
        "Lu Chen",
        "Ziquan Fang",
        "Yunjun Gao",
        "Leong Hou U",
        "Yushuai Li",
        "Tianyi Li"
      ],
      "published": "2025-10-02T12:43:40Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01970v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "Moon是一种基于模态转换的高效多元时间序列异常检测框架，通过MV-MTF技术将数值时间序列转换为图像表示，结合多模态CNN融合数值和图像特征，利用SHAP解释器提升可解释性，在效率和准确率上显著优于现有方法。",
      "order": 767
    },
    {
      "arxiv_id": "2510.01969v1",
      "title": "Lower Bounds on Adversarial Robustness for Multiclass Classification\n  with General Loss Functions",
      "summary": "We consider adversarially robust classification in a multiclass setting under\narbitrary loss functions and derive dual and barycentric reformulations of the\ncorresponding learner-agnostic robust risk minimization problem. We provide\nexplicit characterizations for important cases such as the cross-entropy loss,\nloss functions with a power form, and the quadratic loss, extending in this way\navailable results for the 0-1 loss. These reformulations enable efficient\ncomputation of sharp lower bounds for adversarial risks and facilitate the\ndesign of robust classifiers beyond the 0-1 loss setting. Our paper uncovers\ninteresting connections between adversarial robustness, $\\alpha$-fair packing\nproblems, and generalized barycenter problems for arbitrary positive measures\nwhere Kullback-Leibler and Tsallis entropies are used as penalties. Our\ntheoretical results are accompanied with illustrative numerical experiments\nwhere we obtain tighter lower bounds for adversarial risks with the\ncross-entropy loss function.",
      "authors": [
        "Camilo Andrés García Trillos",
        "Nicolás García Trillos"
      ],
      "published": "2025-10-02T12:42:36Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01969v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文针对多分类问题中的对抗鲁棒性，提出了适用于任意损失函数的对偶和重心重构方法，扩展了传统0-1损失的结果。通过交叉熵、幂形式和二次损失等具体案例，建立了对抗鲁棒性与α-公平包装问题、广义重心问题的理论联系，并提供了更紧致的对抗风险下界计算方案。",
      "order": 768
    },
    {
      "arxiv_id": "2510.01968v1",
      "title": "Multi-bit Audio Watermarking",
      "summary": "We present Timbru, a post-hoc audio watermarking model that achieves\nstate-of-the-art robustness and imperceptibility trade-offs without training an\nembedder-detector model. Given any 44.1 kHz stereo music snippet, our method\nperforms per-audio gradient optimization to add imperceptible perturbations in\nthe latent space of a pretrained audio VAE, guided by a combined message and\nperceptual loss. The watermark can then be extracted using a pretrained CLAP\nmodel. We evaluate 16-bit watermarking on MUSDB18-HQ against AudioSeal,\nWavMark, and SilentCipher across common filtering, noise, compression,\nresampling, cropping, and regeneration attacks. Our approach attains the best\naverage bit error rates, while preserving perceptual quality, demonstrating an\nefficient, dataset-free path to imperceptible audio watermarking.",
      "authors": [
        "Luca A. Lanzendörfer",
        "Kyle Fearne",
        "Florian Grötschla",
        "Roger Wattenhofer"
      ],
      "published": "2025-10-02T12:41:01Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01968v1",
      "primary_area": "audio_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出Timbru音频水印模型，通过预训练音频VAE的潜在空间梯度优化，在不训练嵌入-检测模型的情况下实现16位水印。该方法在MUSDB18-HQ数据集上表现出最佳抗攻击能力和感知质量，为无数据集音频水印提供了高效解决方案。",
      "order": 769
    },
    {
      "arxiv_id": "2510.01963v1",
      "title": "Bias beyond Borders: Global Inequalities in AI-Generated Music",
      "summary": "While recent years have seen remarkable progress in music generation models,\nresearch on their biases across countries, languages, cultures, and musical\ngenres remains underexplored. This gap is compounded by the lack of datasets\nand benchmarks that capture the global diversity of music. To address these\nchallenges, we introduce GlobalDISCO, a large-scale dataset consisting of 73k\nmusic tracks generated by state-of-the-art commercial generative music models,\nalong with paired links to 93k reference tracks in LAION-DISCO-12M. The dataset\nspans 147 languages and includes musical style prompts extracted from\nMusicBrainz and Wikipedia. The dataset is globally balanced, representing\nmusical styles from artists across 79 countries and five continents. Our\nevaluation reveals large disparities in music quality and alignment with\nreference music between high-resource and low-resource regions. Furthermore, we\nfind marked differences in model performance between mainstream and\ngeographically niche genres, including cases where models generate music for\nregional genres that more closely align with the distribution of mainstream\nstyles.",
      "authors": [
        "Ahmet Solak",
        "Florian Grötschla",
        "Luca A. Lanzendörfer",
        "Roger Wattenhofer"
      ],
      "published": "2025-10-02T12:33:10Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01963v1",
      "primary_area": "audio_models",
      "secondary_focus": "alignment",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究引入GlobalDISCO数据集，揭示AI音乐生成模型在全球范围内存在显著偏见：高资源与低资源地区在音乐质量和参考音乐对齐度上存在巨大差异，主流与地域小众流派间模型性能差距明显，表明当前模型对全球音乐多样性覆盖不足。",
      "order": 770
    },
    {
      "arxiv_id": "2510.01944v1",
      "title": "Uniform-in-time convergence bounds for Persistent Contrastive Divergence\n  Algorithms",
      "summary": "We propose a continuous-time formulation of persistent contrastive divergence\n(PCD) for maximum likelihood estimation (MLE) of unnormalised densities. Our\napproach expresses PCD as a coupled, multiscale system of stochastic\ndifferential equations (SDEs), which perform optimisation of the parameter and\nsampling of the associated parametrised density, simultaneously.\n  From this novel formulation, we are able to derive explicit bounds for the\nerror between the PCD iterates and the MLE solution for the model parameter.\nThis is made possible by deriving uniform-in-time (UiT) bounds for the\ndifference in moments between the multiscale system and the averaged regime. An\nefficient implementation of the continuous-time scheme is introduced,\nleveraging a class of explicit, stable intregators, stochastic orthogonal\nRunge-Kutta Chebyshev (S-ROCK), for which we provide explicit error estimates\nin the long-time regime. This leads to a novel method for training energy-based\nmodels (EBMs) with explicit error guarantees.",
      "authors": [
        "Paul Felix Valsecchi Oliva",
        "O. Deniz Akyildiz",
        "Andrew Duncan"
      ],
      "published": "2025-10-02T12:12:33Z",
      "primary_category": "stat.ML",
      "arxiv_url": "https://arxiv.org/abs/2510.01944v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出持续性对比散度(PCD)的连续时间公式，通过多尺度随机微分方程系统同时进行参数优化和采样。推导了PCD迭代与最大似然估计解之间的误差界，并引入高效数值实现方法，为基于能量的模型训练提供具有明确误差保证的新方法。",
      "order": 771
    },
    {
      "arxiv_id": "2510.01943v1",
      "title": "Smooth Quasar-Convex Optimization with Constraints",
      "summary": "Quasar-convex functions form a broad nonconvex class with applications to\nlinear dynamical systems, generalized linear models, and Riemannian\noptimization, among others. Current nearly optimal algorithms work only in\naffine spaces due to the loss of one degree of freedom when working with\ngeneral convex constraints. Obtaining an accelerated algorithm that makes\nnearly optimal $\\widetilde{O}(1/(\\gamma\\sqrt{\\epsilon}))$ first-order queries\nto a $\\gamma$-quasar convex smooth function \\emph{with constraints} was\nindependently asked as an open problem in Mart\\'inez-Rubio (2022); Lezane,\nLanger, and Koolen (2024). In this work, we solve this question by designing an\ninexact accelerated proximal point algorithm that we implement using a\nfirst-order method achieving the aforementioned rate and, as a consequence, we\nimprove the complexity of the accelerated geodesically Riemannian optimization\nsolution in Mart\\'inez-Rubio (2022). We also analyze projected gradient descent\nand Frank-Wolfe algorithms in this constrained quasar-convex setting. To the\nbest of our knowledge, our work provides the first analyses of first-order\nmethods for quasar-convex smooth functions with general convex constraints.",
      "authors": [
        "David Martínez-Rubio"
      ],
      "published": "2025-10-02T12:07:05Z",
      "primary_category": "math.OC",
      "arxiv_url": "https://arxiv.org/abs/2510.01943v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文解决了带约束的准星凸平滑函数优化问题，提出了一种近似加速近端点算法，实现了近乎最优的查询复杂度，填补了该领域在凸约束条件下算法分析的空白，并改进了黎曼优化的计算复杂度。",
      "order": 772
    },
    {
      "arxiv_id": "2510.01938v1",
      "title": "StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold",
      "summary": "Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient\ntechnique for fine-tuning large-scale pre-trained models. However, it still\nlags behind full fine-tuning in performance, partly due to its insufficient\nexploitation of the geometric structure underlying low-rank manifolds. In this\npaper, we propose a geometry-aware extension of LoRA that uses a three-factor\ndecomposition $U\\!SV^\\top$. Analogous to the structure of singular value\ndecomposition (SVD), it separates the adapter's input and output subspaces, $V$\nand $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie\non the Stiefel manifold, ensuring their orthonormality throughout the training.\nTo optimize on the Stiefel manifold, we employ a flexible and modular geometric\noptimization design that converts any Euclidean optimizer to a Riemannian one.\nIt enables efficient subspace learning while remaining compatible with existing\nfine-tuning pipelines. Empirical results across a wide range of downstream\ntasks, including commonsense reasoning, math and code generation, image\nclassification, and image generation, demonstrate the superior performance of\nour approach against the recent state-of-the-art variants of LoRA. Code is\navailable at https://github.com/SonyResearch/stella.",
      "authors": [
        "Zhizhong Li",
        "Sina Sajadmanesh",
        "Jingtao Li",
        "Lingjuan Lyu"
      ],
      "published": "2025-10-02T11:59:13Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01938v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "StelLA是一种基于Stiefel流形的低秩自适应改进方法，通过三因子分解U·S·V⊤将适配器的输入输出子空间与缩放因子分离，并约束U和V在Stiefel流形上保持正交归一化。该方法采用几何优化设计，在常识推理、数学与代码生成、图像分类与生成等多项任务中优于现有LoRA变体。",
      "order": 773
    },
    {
      "arxiv_id": "2510.01934v1",
      "title": "Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors",
      "summary": "Few-shot anomaly detection streamlines and simplifies industrial safety\ninspection. However, limited samples make accurate differentiation between\nnormal and abnormal features challenging, and even more so under\ncategory-agnostic conditions. Large-scale pre-training of foundation visual\nencoders has advanced many fields, as the enormous quantity of data helps to\nlearn the general distribution of normal images. We observe that the anomaly\namount in an image directly correlates with the difference in the learnt\nembeddings and utilize this to design a few-shot anomaly detector termed\nFoundAD. This is done by learning a nonlinear projection operator onto the\nnatural image manifold. The simple operator acts as an effective tool for\nanomaly detection to characterize and identify out-of-distribution regions in\nan image. Extensive experiments show that our approach supports multi-class\ndetection and achieves competitive performance while using substantially fewer\nparameters than prior methods. Backed up by evaluations with multiple\nfoundation encoders, including fresh DINOv3, we believe this idea broadens the\nperspective on foundation features and advances the field of few-shot anomaly\ndetection.",
      "authors": [
        "Guangyao Zhai",
        "Yue Zhou",
        "Xinyan Deng",
        "Lars Heckler",
        "Nassir Navab",
        "Benjamin Busam"
      ],
      "published": "2025-10-02T11:53:20Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01934v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出FoundAD方法，利用基础视觉编码器作为少样本异常检测器。通过非线性投影算子学习自然图像流形，在少量样本下有效识别图像中的异常区域。实验表明该方法支持多类检测且参数更少，性能优于现有方法。",
      "order": 774
    },
    {
      "arxiv_id": "2510.01930v1",
      "title": "Precise Dynamics of Diagonal Linear Networks: A Unifying Analysis by\n  Dynamical Mean-Field Theory",
      "summary": "Diagonal linear networks (DLNs) are a tractable model that captures several\nnontrivial behaviors in neural network training, such as\ninitialization-dependent solutions and incremental learning. These phenomena\nare typically studied in isolation, leaving the overall dynamics insufficiently\nunderstood. In this work, we present a unified analysis of various phenomena in\nthe gradient flow dynamics of DLNs. Using Dynamical Mean-Field Theory (DMFT),\nwe derive a low-dimensional effective process that captures the asymptotic\ngradient flow dynamics in high dimensions. Analyzing this effective process\nyields new insights into DLN dynamics, including loss convergence rates and\ntheir trade-off with generalization, and systematically reproduces many of the\npreviously observed phenomena. These findings deepen our understanding of DLNs\nand demonstrate the effectiveness of the DMFT approach in analyzing\nhigh-dimensional learning dynamics of neural networks.",
      "authors": [
        "Sota Nishiyama",
        "Masaaki Imaizumi"
      ],
      "published": "2025-10-02T11:47:36Z",
      "primary_category": "stat.ML",
      "arxiv_url": "https://arxiv.org/abs/2510.01930v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究采用动力学平均场理论(DMFT)对对角线性网络(DLNs)的梯度流动力学进行统一分析，推导出高维情形下的低维有效过程，揭示了损失收敛速率与泛化性能的权衡关系，系统解释了初始化依赖解和增量学习等现象，深化了对神经网络高维学习动力学的理解。",
      "order": 775
    },
    {
      "arxiv_id": "2510.01914v1",
      "title": "Automated Defect Detection for Mass-Produced Electronic Components Based\n  on YOLO Object Detection Models",
      "summary": "Since the defect detection of conventional industry components is\ntime-consuming and labor-intensive, it leads to a significant burden on quality\ninspection personnel and makes it difficult to manage product quality. In this\npaper, we propose an automated defect detection system for the dual in-line\npackage (DIP) that is widely used in industry, using digital camera optics and\na deep learning (DL)-based model. The two most common defect categories of DIP\nare examined: (1) surface defects, and (2) pin-leg defects. However, the lack\nof defective component images leads to a challenge for detection tasks. To\nsolve this problem, the ConSinGAN is used to generate a suitable-sized dataset\nfor training and testing. Four varieties of the YOLO model are investigated\n(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.\nThe proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in\naccuracy of 95.50\\%, detection time of 285 ms, and is far superior to\nthreshold-based approaches. In addition, the supervisory control and data\nacquisition (SCADA) system is developed, and the associated sensor architecture\nis described. The proposed automated defect detection can be easily established\nwith numerous types of defects or insufficient defect data.",
      "authors": [
        "Wei-Lung Mao",
        "Chun-Chi Wang",
        "Po-Heng Chou",
        "Yen-Ting Liu"
      ],
      "published": "2025-10-02T11:33:16Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01914v1",
      "primary_area": "video_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出基于YOLO目标检测模型的自动化缺陷检测系统，用于工业双列直插封装元件。采用ConSinGAN解决缺陷样本不足问题，比较四种YOLO版本性能，其中YOLOv7结合ConSinGAN在精度(95.50%)和检测时间(285ms)上表现最优，并集成SCADA系统实现工业应用。",
      "order": 776
    },
    {
      "arxiv_id": "2510.01910v1",
      "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under\n  Deficiencies with Iterative Refinement",
      "summary": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement.",
      "authors": [
        "Zhaoyan Wang",
        "Zheng Gao",
        "Arogya Kharel",
        "In-Young Ko"
      ],
      "published": "2025-10-02T11:30:51Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01910v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本研究首次系统比较传统图神经网络与LLM增强方法在复合缺陷图数据上的表现，挑战了LLM增强必然更优的假设。提出RoGRAD框架，采用检索增强生成和迭代对比学习实现动态优化，实验显示其性能显著优于基线方法，平均提升达82.43%。",
      "order": 777
    },
    {
      "arxiv_id": "2510.01906v1",
      "title": "A Methodology for Transparent Logic-Based Classification Using a\n  Multi-Task Convolutional Tsetlin Machine",
      "summary": "The Tsetlin Machine (TM) is a novel machine learning paradigm that employs\nfinite-state automata for learning and utilizes propositional logic to\nrepresent patterns. Due to its simplistic approach, TMs are inherently more\ninterpretable than learning algorithms based on Neural Networks. The\nConvolutional TM has shown comparable performance on various datasets such as\nMNIST, K-MNIST, F-MNIST and CIFAR-2. In this paper, we explore the\napplicability of the TM architecture for large-scale multi-channel (RGB) image\nclassification. We propose a methodology to generate both local interpretations\nand global class representations. The local interpretations can be used to\nexplain the model predictions while the global class representations aggregate\nimportant patterns for each class. These interpretations summarize the\nknowledge captured by the convolutional clauses, which can be visualized as\nimages. We evaluate our methods on MNIST and CelebA datasets, using models that\nachieve 98.5\\% accuracy on MNIST and 86.56\\% F1-score on CelebA (compared to\n88.07\\% for ResNet50) respectively. We show that the TM performs competitively\nto this deep learning model while maintaining its interpretability, even in\nlarge-scale complex training environments. This contributes to a better\nunderstanding of TM clauses and provides insights into how these models can be\napplied to more complex and diverse datasets.",
      "authors": [
        "Mayur Kishor Shende",
        "Ole-Christoffer Granmo",
        "Runar Helin",
        "Vladimir I. Zadorozhny",
        "Rishad Shafik"
      ],
      "published": "2025-10-02T11:25:08Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01906v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种基于Tsetlin机的透明逻辑分类方法，通过多任务卷积架构实现大规模RGB图像分类。该方法能生成局部解释和全局类别表示，在MNIST和CelebA数据集上分别达到98.5%准确率和86.56% F1分数，性能接近ResNet50但保持可解释性，为复杂数据集上的可解释AI提供新思路。",
      "order": 778
    },
    {
      "arxiv_id": "2510.01902v1",
      "title": "Constrained Adaptive Rejection Sampling",
      "summary": "Language Models (LMs) are increasingly used in applications where generated\noutputs must satisfy strict semantic or syntactic constraints. Existing\napproaches to constrained generation fall along a spectrum: greedy constrained\ndecoding methods enforce validity during decoding but distort the LM's\ndistribution, while rejection sampling (RS) preserves fidelity but wastes\ncomputation by discarding invalid outputs. Both extremes are problematic in\ndomains such as program fuzzing, where both validity and diversity of samples\nare essential. We present Constrained Adaptive Rejection Sampling (CARS), an\napproach that strictly improves the sample-efficiency of RS without\ndistributional distortion. CARS begins with unconstrained LM sampling and\nadaptively rules out constraint-violating continuations by recording them in a\ntrie and subtracting their probability mass from future draws. This adaptive\npruning ensures that prefixes proven invalid are never revisited, acceptance\nrates improve monotonically, and the resulting samples exactly follow the\nconstrained distribution. In experiments on a variety of domains -- e.g.,\nprogram fuzzing and molecular generation -- CARS consistently achieves higher\nefficiency -- measured in the number of LM forward passes per valid sample --\nwhile also producing stronger sample diversity than both GCD and methods that\napproximate the LM's distribution.",
      "authors": [
        "Paweł Parys",
        "Sairam Vaidya",
        "Taylor Berg-Kirkpatrick",
        "Loris D'Antoni"
      ],
      "published": "2025-10-02T11:17:26Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01902v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "code_generation",
      "tldr_zh": "本文提出约束自适应拒绝采样(CARS)方法，在保持语言模型分布不变的前提下，通过自适应剪枝技术排除违反约束的文本延续，显著提升采样效率。相比传统约束解码和拒绝采样，CARS在程序模糊测试和分子生成等任务中实现了更高的采样效率和样本多样性。",
      "order": 779
    },
    {
      "arxiv_id": "2510.01899v1",
      "title": "Multimodal Foundation Models for Early Disease Detection",
      "summary": "Healthcare generates diverse streams of data, including electronic health\nrecords (EHR), medical imaging, genetics, and ongoing monitoring from wearable\ndevices. Traditional diagnostic models frequently analyze these sources in\nisolation, which constrains their capacity to identify cross-modal correlations\nessential for early disease diagnosis. Our research presents a multimodal\nfoundation model that consolidates diverse patient data through an\nattention-based transformer framework. At first, dedicated encoders put each\nmodality into a shared latent space. Then, they combine them using multi-head\nattention and residual normalization. The architecture is made for pretraining\non many tasks, which makes it easy to adapt to new diseases and datasets with\nlittle extra work. We provide an experimental strategy that uses benchmark\ndatasets in oncology, cardiology, and neurology, with the goal of testing early\ndetection tasks. The framework includes data governance and model management\ntools in addition to technological performance to improve transparency,\nreliability, and clinical interpretability. The suggested method works toward a\nsingle foundation model for precision diagnostics, which could improve the\naccuracy of predictions and help doctors make decisions.",
      "authors": [
        "Md Talha Mohsin",
        "Ismail Abdulrashid"
      ],
      "published": "2025-10-02T11:12:57Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01899v1",
      "primary_area": "multimodal_models",
      "secondary_focus": "model_architecture",
      "application_domain": "medical_ai",
      "tldr_zh": "本研究提出一种基于注意力Transformer的多模态基础模型，整合电子病历、医学影像、基因数据和可穿戴设备监测等多种医疗数据源，通过专用编码器和多头注意力机制实现跨模态关联，支持多任务预训练并易于适应新疾病，在肿瘤学、心脏病学和神经学早期检测任务中验证了其性能，同时包含数据治理和模型管理工具以提高临床可解释性。",
      "order": 780
    },
    {
      "arxiv_id": "2510.01894v1",
      "title": "Multi-marginal temporal Schrödinger Bridge Matching for video\n  generation from unpaired data",
      "summary": "Many natural dynamic processes -- such as in vivo cellular differentiation or\ndisease progression -- can only be observed through the lens of static sample\nsnapshots. While challenging, reconstructing their temporal evolution to\ndecipher underlying dynamic properties is of major interest to scientific\nresearch. Existing approaches enable data transport along a temporal axis but\nare poorly scalable in high dimension and require restrictive assumptions to be\nmet. To address these issues, we propose \\textit{\\textbf{Multi-Marginal\ntemporal Schr\\\"odinger Bridge Matching}} (\\textbf{MMtSBM}) \\textit{for video\ngeneration from unpaired data}, extending the theoretical guarantees and\nempirical efficiency of Diffusion Schr\\\"odinger Bridge Matching\n(arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting\nalgorithm to multiple marginals in a novel factorized fashion. Experiments show\nthat MMtSBM retains theoretical properties on toy examples, achieves\nstate-of-the-art performance on real world datasets such as transcriptomic\ntrajectory inference in 100 dimensions, and for the first time recovers\ncouplings and dynamics in very high dimensional image settings. Our work\nestablishes multi-marginal Schr\\\"odinger bridges as a practical and principled\napproach for recovering hidden dynamics from static data.",
      "authors": [
        "Thomas Gravier",
        "Thomas Boyer",
        "Auguste Genovesio"
      ],
      "published": "2025-10-02T11:00:58Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01894v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "提出多边际时间薛定谔桥匹配方法，用于从非配对数据生成视频，通过因子化多边际推导迭代马尔可夫拟合算法，在转录组轨迹推断和高维图像动态恢复中实现最先进性能。",
      "order": 781
    },
    {
      "arxiv_id": "2510.01878v1",
      "title": "Randomized Gradient Subspaces for Efficient Large Language Model\n  Training",
      "summary": "Training large language models (LLMs) is often bottlenecked by extreme memory\ndemands, with optimizer states dominating the footprint. Recent works mitigates\nthis cost by projecting gradients into low-dimensional subspaces using\nsophisticated update strategies. In this paper, we analyze the dynamics of\ngradient space and its underlying subspaces. We find that while a small\nsubspace captures most gradient energy, a significant portion still resides in\nthe residual bulk; moreover, the influence of the core subspace diminishes over\ntime and in deeper layers. We also observe that the gradient space exhibits\nnear-flat curvature, calling for algorithms that explicitly account for this\ngeometry. Motivated by these insights, we introduce a suite of randomized\nalgorithms, GrassWalk and GrassJump, which exploit subspace and achieve\nstate-of-the-art memory savings while improving performance on LLaMA-1B and\nLLaMA-7B pretraining.",
      "authors": [
        "Sahar Rajabi",
        "Nayeema Nonta",
        "Samanvay Vajpayee",
        "Sirisha Rambhatla"
      ],
      "published": "2025-10-02T10:35:38Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01878v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出GrassWalk和GrassJump两种随机算法，通过分析梯度空间动态特性，在低维子空间投影梯度以大幅降低大语言模型训练内存占用，在LLaMA-1B/7B预训练中实现最优内存节省与性能提升。",
      "order": 782
    },
    {
      "arxiv_id": "2510.01874v1",
      "title": "Deep Hedging Under Non-Convexity: Limitations and a Case for AlphaZero",
      "summary": "This paper examines replication portfolio construction in incomplete markets\n- a key problem in financial engineering with applications in pricing, hedging,\nbalance sheet management, and energy storage planning. We model this as a\ntwo-player game between an investor and the market, where the investor makes\nstrategic bets on future states while the market reveals outcomes. Inspired by\nthe success of Monte Carlo Tree Search in stochastic games, we introduce an\nAlphaZero-based system and compare its performance to deep hedging - a widely\nused industry method based on gradient descent. Through theoretical analysis\nand experiments, we show that deep hedging struggles in environments where the\n$Q$-function is not subject to convexity constraints - such as those involving\nnon-convex transaction costs, capital constraints, or regulatory limitations -\nconverging to local optima. We construct specific market environments to\nhighlight these limitations and demonstrate that AlphaZero consistently finds\nnear-optimal replication strategies. On the theoretical side, we establish a\nconnection between deep hedging and convex optimization, suggesting that its\neffectiveness is contingent on convexity assumptions. Our experiments further\nsuggest that AlphaZero is more sample-efficient - an important advantage in\ndata-scarce, overfitting-prone derivative markets.",
      "authors": [
        "Matteo Maggiolo",
        "Giuseppe Nuti",
        "Miroslav Štrupl",
        "Oleg Szehr"
      ],
      "published": "2025-10-02T10:28:59Z",
      "primary_category": "stat.ML",
      "arxiv_url": "https://arxiv.org/abs/2510.01874v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "financial_ai",
      "tldr_zh": "本文研究不完全市场中复制投资组合构建问题，提出基于AlphaZero的系统并与深度对冲方法对比。理论分析和实验表明，在非凸交易成本、资本约束等环境下，深度对冲易陷入局部最优，而AlphaZero能持续找到近似最优策略且样本效率更高。",
      "order": 783
    },
    {
      "arxiv_id": "2510.01871v1",
      "title": "Ranking Items from Discrete Ratings: The Cost of Unknown User Thresholds",
      "summary": "Ranking items is a central task in many information retrieval and recommender\nsystems. User input for the ranking task often comes in the form of ratings on\na coarse discrete scale. We ask whether it is possible to recover a\nfine-grained item ranking from such coarse-grained ratings. We model items as\nhaving scores and users as having thresholds; a user rates an item positively\nif the item's score exceeds the user's threshold. Although all users agree on\nthe total item order, estimating that order is challenging when both the scores\nand the thresholds are latent. Under our model, any ranking method naturally\npartitions the $n$ items into bins; the bins are ordered, but the items inside\neach bin are still unordered. Users arrive sequentially, and every new user can\nbe queried to refine the current ranking. We prove that achieving a\nnear-perfect ranking, measured by Spearman distance, requires $\\Theta(n^2)$\nusers (and therefore $\\Omega(n^2)$ queries). This is significantly worse than\nthe $O(n\\log n)$ queries needed to rank from comparisons; the gap reflects the\nadditional queries needed to identify the users who have the appropriate\nthresholds. Our bound also quantifies the impact of a mismatch between score\nand threshold distributions via a quadratic divergence factor. To show the\ntightness of our results, we provide a ranking algorithm whose query complexity\nmatches our bound up to a logarithmic factor. Our work reveals a tension in\nonline ranking: diversity in thresholds is necessary to merge coarse ratings\nfrom many users into a fine-grained ranking, but this diversity has a cost if\nthe thresholds are a priori unknown.",
      "authors": [
        "Oscar Villemaud",
        "Suryanarayana Sankagiri",
        "Matthias Grossglauser"
      ],
      "published": "2025-10-02T10:23:52Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01871v1",
      "primary_area": "text_models",
      "secondary_focus": "reasoning",
      "application_domain": "general_purpose",
      "tldr_zh": "本文研究从离散评分中恢复细粒度物品排序的问题，提出用户阈值模型并证明实现近似完美排序需要Θ(n²)用户查询，远高于基于比较排序的O(n log n)，揭示了阈值多样性在在线排序中的成本与必要性之间的张力。",
      "order": 784
    },
    {
      "arxiv_id": "2510.01867v1",
      "title": "Universal Dynamic Regret and Constraint Violation Bounds for Constrained\n  Online Convex Optimization",
      "summary": "We consider a generalization of the celebrated Online Convex Optimization\n(OCO) framework with online adversarial constraints. We present two algorithms\nhaving simple modular structures that yield universal dynamic regret and\ncumulative constraint violation bounds, improving upon the state-of-the-art\nresults. Our results hold in the most general case when both the cost and\nconstraint functions are chosen arbitrarily by an adversary, and the constraint\nfunctions need not contain any common feasible point. The results are\nestablished by reducing the constrained learning problem to an instance of the\nstandard OCO problem with specially constructed surrogate cost functions.",
      "authors": [
        "Subhamon Supantha",
        "Abhishek Sinha"
      ],
      "published": "2025-10-02T10:19:16Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01867v1",
      "primary_area": "text_models",
      "secondary_focus": "training_optimization",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出两种模块化算法，用于带对抗性约束的在线凸优化问题，在动态遗憾和累积约束违反方面改进了现有最优结果。通过将约束学习问题转化为标准OCO问题，算法在最一般情况下（代价函数和约束函数均由对手任意选择）仍能保证性能。",
      "order": 785
    },
    {
      "arxiv_id": "2510.01863v1",
      "title": "Microscaling Floating Point Formats for Large Language Models",
      "summary": "The increasing computational and memory demands of large language models\n(LLMs) necessitate innovative approaches to optimize resource usage without\ncompromising performance. This paper leverages microscaling floating-point\nformats, a novel technique designed to address these challenges by reducing the\nstorage and computational overhead associated with numerical representations in\nLLMs. Unlike traditional floating-point representations that allocate a\ndedicated scale for each value, microscaling employs a shared scale across a\nblock of values, enabling compact one-byte floating-point representations while\nmaintaining an extended dynamic range. We explore the application of\nmicroscaling in the context of 8-bit floating-point formats to significantly\nreduce memory footprint and computational costs. We tested several\nconfigurations of microscaling floats within the GPT-2 LLM architecture,\ndemonstrating that microscaling data formats can achieve competitive accuracy\nduring training and inference, proving its efficacy as a resource-efficient\nalternative for deploying LLMs at scale. The source code is publicly available\nat: https://github.com/unipi-dii-compressedarith/llm.c-sve",
      "authors": [
        "Marco Cococcioni",
        "Dario Pagani",
        "Federico Rossi"
      ],
      "published": "2025-10-02T10:08:59Z",
      "primary_category": "cs.NE",
      "arxiv_url": "https://arxiv.org/abs/2510.01863v1",
      "primary_area": "text_models",
      "secondary_focus": "model_compression",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出微缩放浮点格式，通过共享数值块尺度实现8位浮点表示，在GPT-2模型中验证了该技术能显著降低大语言模型的内存占用和计算成本，同时保持训练与推理的竞争性精度。",
      "order": 786
    },
    {
      "arxiv_id": "2510.01858v1",
      "title": "Compositional meta-learning through probabilistic task inference",
      "summary": "To solve a new task from minimal experience, it is essential to effectively\nreuse knowledge from previous tasks, a problem known as meta-learning.\nCompositional solutions, where common elements of computation are flexibly\nrecombined into new configurations, are particularly well-suited for\nmeta-learning. Here, we propose a compositional meta-learning model that\nexplicitly represents tasks as structured combinations of reusable\ncomputations. We achieve this by learning a generative model that captures the\nunderlying components and their statistics shared across a family of tasks.\nThis approach transforms learning a new task into a probabilistic inference\nproblem, which allows for finding solutions without parameter updates through\nhighly constrained hypothesis testing. Our model successfully recovers ground\ntruth components and statistics in rule learning and motor learning tasks. We\nthen demonstrate its ability to quickly infer new solutions from just single\nexamples. Together, our framework joins the expressivity of neural networks\nwith the data-efficiency of probabilistic inference to achieve rapid\ncompositional meta-learning.",
      "authors": [
        "Jacob J. W. Bakermans",
        "Pablo Tano",
        "Reidar Riveland",
        "Charles Findling",
        "Alexandre Pouget"
      ],
      "published": "2025-10-02T09:58:48Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01858v1",
      "primary_area": "text_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "本文提出一种组合式元学习模型，通过概率任务推理将任务表示为可重用计算单元的结构化组合。该模型学习捕捉任务族共享的底层组件及其统计特性，将新任务学习转化为概率推断问题，无需参数更新即可通过约束假设检验找到解决方案。在规则学习和运动学习任务中成功恢复真实组件，并能从单样本快速推断新解决方案。",
      "order": 787
    },
    {
      "arxiv_id": "2510.01855v1",
      "title": "Explicit Discovery of Nonlinear Symmetries from Dynamic Data",
      "summary": "Symmetry is widely applied in problems such as the design of equivariant\nnetworks and the discovery of governing equations, but in complex scenarios, it\nis not known in advance. Most previous symmetry discovery methods are limited\nto linear symmetries, and recent attempts to discover nonlinear symmetries fail\nto explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD,\nwhich is, to our knowledge, the first method capable of determining the number\nof infinitesimal generators with nonlinear terms and their explicit\nexpressions. We specify a function library for the infinitesimal group action\nand aim to solve for its coefficient matrix, proving that its prolongation\nformula for differential equations, which governs dynamic data, is also linear\nwith respect to the coefficient matrix. By substituting the central differences\nof the data and the Jacobian matrix of the trained neural network into the\ninfinitesimal criterion, we get a system of linear equations for the\ncoefficient matrix, which can then be solved using SVD. On top quark tagging\nand a series of dynamic systems, LieNLSD shows qualitative advantages over\nexisting methods and improves the long rollout accuracy of neural PDE solvers\nby over 20% while applying to guide data augmentation. Code and data are\navailable at https://github.com/hulx2002/LieNLSD.",
      "authors": [
        "Lexiang Hu",
        "Yikang Li",
        "Zhouchen Lin"
      ],
      "published": "2025-10-02T09:54:08Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01855v1",
      "primary_area": "diffusion_models",
      "secondary_focus": "model_architecture",
      "application_domain": "general_purpose",
      "tldr_zh": "LieNLSD是首个能从动态数据中显式发现非线性对称性并确定无穷小生成元数量及表达式的方法，通过函数库和SVD求解系数矩阵，在物理系统和神经网络PDE求解器中展现优越性能，提升长期预测精度20%以上。",
      "order": 788
    }
  ]
}