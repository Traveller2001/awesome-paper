{
  "generated_at": "2025-10-05T06:20:06.782657Z",
  "categories": [
    "cs.AI"
  ],
  "paper_count": 328,
  "papers": [
    {
      "arxiv_id": "2510.02307v1",
      "title": "NoiseShift: Resolution-Aware Noise Recalibration for Better\n  Low-Resolution Image Generation",
      "summary": "Text-to-image diffusion models trained on a fixed set of resolutions often\nfail to generalize, even when asked to generate images at lower resolutions\nthan those seen during training. High-resolution text-to-image generators are\ncurrently unable to easily offer an out-of-the-box budget-efficient alternative\nto their users who might not need high-resolution images. We identify a key\ntechnical insight in diffusion models that when addressed can help tackle this\nlimitation: Noise schedulers have unequal perceptual effects across\nresolutions. The same level of noise removes disproportionately more signal\nfrom lower-resolution images than from high-resolution images, leading to a\ntrain-test mismatch. We propose NoiseShift, a training-free method that\nrecalibrates the noise level of the denoiser conditioned on resolution size.\nNoiseShift requires no changes to model architecture or sampling schedule and\nis compatible with existing models. When applied to Stable Diffusion 3, Stable\nDiffusion 3.5, and Flux-Dev, quality at low resolutions is significantly\nimproved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and\nFlux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by\n10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results\ndemonstrate the effectiveness of NoiseShift in mitigating resolution-dependent\nartifacts and enhancing the quality of low-resolution image generation.",
      "authors": [
        "Ruozhen He",
        "Moayed Haji-Ali",
        "Ziyan Yang",
        "Vicente Ordonez"
      ],
      "published": "2025-10-02T17:59:43Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02307v1"
    },
    {
      "arxiv_id": "2510.02305v1",
      "title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is\n  Geometry Adaptive",
      "summary": "Diffusion models have achieved state-of-the-art performance, demonstrating\nremarkable generalisation capabilities across diverse domains. However, the\nmechanisms underpinning these strong capabilities remain only partially\nunderstood. A leading conjecture, based on the manifold hypothesis, attributes\nthis success to their ability to adapt to low-dimensional geometric structure\nwithin the data. This work provides evidence for this conjecture, focusing on\nhow such phenomena could result from the formulation of the learning problem\nthrough score matching. We inspect the role of implicit regularisation by\ninvestigating the effect of smoothing minimisers of the empirical score\nmatching objective. Our theoretical and empirical results confirm that\nsmoothing the score function -- or equivalently, smoothing in the log-density\ndomain -- produces smoothing tangential to the data manifold. In addition, we\nshow that the manifold along which the diffusion model generalises can be\ncontrolled by choosing an appropriate smoothing.",
      "authors": [
        "Tyler Farghly",
        "Peter Potaptchik",
        "Samuel Howard",
        "George Deligiannidis",
        "Jakiw Pidstrigach"
      ],
      "published": "2025-10-02T17:59:39Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02305v1"
    },
    {
      "arxiv_id": "2510.02300v1",
      "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
      "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.",
      "authors": [
        "Runqian Wang",
        "Yilun Du"
      ],
      "published": "2025-10-02T17:59:06Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02300v1"
    },
    {
      "arxiv_id": "2510.02297v1",
      "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
      "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
      "authors": [
        "Wentao Zhang",
        "Yang Young Lu",
        "Yuntian Deng"
      ],
      "published": "2025-10-02T17:59:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02297v1"
    },
    {
      "arxiv_id": "2510.02295v1",
      "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
      "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.",
      "authors": [
        "Enxin Song",
        "Wenhao Chai",
        "Shusheng Yang",
        "Ethan Armand",
        "Xiaojun Shan",
        "Haiyang Xu",
        "Jianwen Xie",
        "Zhuowen Tu"
      ],
      "published": "2025-10-02T17:58:54Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02295v1"
    },
    {
      "arxiv_id": "2510.02294v1",
      "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
      "summary": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of\nstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike\nprevious top-ranking embedding models that require massive contrastive\npretraining, sophisticated training pipelines, and costly synthetic training\ndata, F2LLM is directly finetuned from foundation models on 6 million\nquery-document-negative tuples curated from open-source, non-synthetic\ndatasets, striking a strong balance between training cost, model size, and\nembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd\namong models with approximately 4B parameters and 7th overall, while F2LLM-1.7B\nranks 1st among models in the 1B-2B size range. To facilitate future research\nin the field, we release the models, training dataset, and code, positioning\nF2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
      "authors": [
        "Ziyin Zhang",
        "Zihan Liao",
        "Hang Yu",
        "Peng Di",
        "Rui Wang"
      ],
      "published": "2025-10-02T17:58:49Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02294v1"
    },
    {
      "arxiv_id": "2510.02286v1",
      "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
      "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
      "authors": [
        "Ruohao Guo",
        "Afshin Oroojlooy",
        "Roshan Sridhar",
        "Miguel Ballesteros",
        "Alan Ritter",
        "Dan Roth"
      ],
      "published": "2025-10-02T17:57:05Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02286v1"
    },
    {
      "arxiv_id": "2510.02284v1",
      "title": "Learning to Generate Object Interactions with Physics-Guided Video\n  Diffusion",
      "summary": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.",
      "authors": [
        "David Romero",
        "Ariana Bermudez",
        "Hao Li",
        "Fabio Pizzati",
        "Ivan Laptev"
      ],
      "published": "2025-10-02T17:56:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02284v1"
    },
    {
      "arxiv_id": "2510.02283v1",
      "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
      "summary": "Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/",
      "authors": [
        "Justin Cui",
        "Jie Wu",
        "Ming Li",
        "Tao Yang",
        "Xiaojie Li",
        "Rui Wang",
        "Andrew Bai",
        "Yuanhao Ban",
        "Cho-Jui Hsieh"
      ],
      "published": "2025-10-02T17:55:42Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02283v1"
    },
    {
      "arxiv_id": "2510.02279v1",
      "title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods\n  for Natural Language Generation",
      "summary": "Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings.",
      "authors": [
        "Mykyta Ielanskyi",
        "Kajetan Schweighofer",
        "Lukas Aichberger",
        "Sepp Hochreiter"
      ],
      "published": "2025-10-02T17:54:09Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02279v1"
    },
    {
      "arxiv_id": "2510.02276v1",
      "title": "BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge\n  Transfer across Biosignals",
      "summary": "Biosignals offer valuable insights into the physiological states of the human\nbody. Although biosignal modalities differ in functionality, signal fidelity,\nsensor comfort, and cost, they are often intercorrelated, reflecting the\nholistic and interconnected nature of human physiology. This opens up the\npossibility of performing the same tasks using alternative biosignal\nmodalities, thereby improving the accessibility, usability, and adaptability of\nhealth monitoring systems. However, the limited availability of large labeled\ndatasets presents challenges for training models tailored to specific tasks and\nmodalities of interest. Unsupervised cross-modal knowledge transfer offers a\npromising solution by leveraging knowledge from an existing modality to support\nmodel training for a new modality. Existing methods are typically based on\nknowledge distillation, which requires running a teacher model alongside\nstudent model training, resulting in high computational and memory overhead.\nThis challenge is further exacerbated by the recent development of foundation\nmodels that demonstrate superior performance and generalization across tasks at\nthe cost of large model sizes. To this end, we explore a new framework for\nunsupervised cross-modal knowledge transfer of biosignals by training a\nlightweight bridge network to align the intermediate representations and enable\ninformation flow between foundation models and across modalities. Specifically,\nwe introduce an efficient strategy for selecting alignment positions where the\nbridge should be constructed, along with a flexible prototype network as the\nbridge architecture. Extensive experiments across multiple biosignal\nmodalities, tasks, and datasets show that BioX-Bridge reduces the number of\ntrainable parameters by 88--99\\% while maintaining or even improving transfer\nperformance compared to state-of-the-art methods.",
      "authors": [
        "Chenqi Li",
        "Yu Liu",
        "Timothy Denison",
        "Tingting Zhu"
      ],
      "published": "2025-10-02T17:51:19Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02276v1"
    },
    {
      "arxiv_id": "2510.02272v1",
      "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
      "summary": "Recent advancements in Reinforcement Post-Training (RPT) have significantly\nenhanced the capabilities of Large Reasoning Models (LRMs), sparking increased\ninterest in the generalization of RL-based reasoning. While existing work has\nprimarily focused on investigating its generalization across tasks or\nmodalities, this study proposes a novel cross-linguistic perspective to\ninvestigate reasoning generalization. This raises a crucial question:\n$\\textit{Does the reasoning capability achieved from English RPT effectively\ntransfer to other languages?}$ We address this by systematically evaluating\nEnglish-centric LRMs on multilingual reasoning benchmarks and introducing a\nmetric to quantify cross-lingual transferability. Our findings reveal that\ncross-lingual transferability varies significantly across initial model, target\nlanguage, and training paradigm. Through interventional studies, we find that\nmodels with stronger initial English capabilities tend to over-rely on\nEnglish-specific patterns, leading to diminished cross-lingual generalization.\nTo address this, we conduct a thorough parallel training study. Experimental\nresults yield three key findings: $\\textbf{First-Parallel Leap}$, a substantial\nleap in performance when transitioning from monolingual to just a single\nparallel language, and a predictable $\\textbf{Parallel Scaling Law}$, revealing\nthat cross-lingual reasoning transfer follows a power-law with the number of\ntraining parallel languages. Moreover, we identify the discrepancy between\nactual monolingual performance and the power-law prediction as\n$\\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs\nfail to fully generalize across languages. Our study challenges the assumption\nthat LRM reasoning mirrors human cognition, providing critical insights for the\ndevelopment of more language-agnostic LRMs.",
      "authors": [
        "Wen Yang",
        "Junhong Wu",
        "Chong Li",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "published": "2025-10-02T17:49:49Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02272v1"
    },
    {
      "arxiv_id": "2510.02271v1",
      "title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in\n  Tool-Augmented Agents",
      "summary": "Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling.",
      "authors": [
        "Yaxin Du",
        "Yuanshuo Zhang",
        "Xiyuan Yang",
        "Yifan Zhou",
        "Cheng Wang",
        "Gongyi Zou",
        "Xianghe Pang",
        "Wenhao Wang",
        "Menglan Chen",
        "Shuo Tang",
        "Zhiyu Li",
        "Siheng Chen"
      ],
      "published": "2025-10-02T17:48:03Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02271v1"
    },
    {
      "arxiv_id": "2510.02270v1",
      "title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for\n  Fine-Grained Image Classification",
      "summary": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP.",
      "authors": [
        "Sathira Silva",
        "Eman Ali",
        "Chetan Arora",
        "Muhammad Haris Khan"
      ],
      "published": "2025-10-02T17:47:39Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02270v1"
    },
    {
      "arxiv_id": "2510.02265v1",
      "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement\n  Learning",
      "summary": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime.",
      "authors": [
        "Yalin E. Sagduyu",
        "Tugba Erpek",
        "Kemal Davaslioglu",
        "Sastry Kompella"
      ],
      "published": "2025-10-02T17:44:38Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02265v1"
    },
    {
      "arxiv_id": "2510.02264v1",
      "title": "Paving the Way Towards Kinematic Assessment Using Monocular Video: A\n  Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose\n  Estimators Against Inertial Sensors in Daily Living Activities",
      "summary": "Advances in machine learning and wearable sensors offer new opportunities for\ncapturing and analyzing human movement outside specialized laboratories.\nAccurate assessment of human movement under real-world conditions is essential\nfor telemedicine, sports science, and rehabilitation. This preclinical\nbenchmark compares monocular video-based 3D human pose estimation models with\ninertial measurement units (IMUs), leveraging the VIDIMU dataset containing a\ntotal of 13 clinically relevant daily activities which were captured using both\ncommodity video cameras and five IMUs. During this initial study only healthy\nsubjects were recorded, so results cannot be generalized to pathological\ncohorts. Joint angles derived from state-of-the-art deep learning frameworks\n(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA\nBodyTrack) were evaluated against joint angles computed from IMU data using\nOpenSim inverse kinematics following the Human3.6M dataset format with 17\nkeypoints. Among them, MotionAGFormer demonstrated superior performance,\nachieving the lowest overall RMSE ($9.27\\deg \\pm 4.80\\deg$) and MAE ($7.86\\deg\n\\pm 4.18\\deg$), as well as the highest Pearson correlation ($0.86 \\pm 0.15$)\nand the highest coefficient of determination $R^{2}$ ($0.67 \\pm 0.28$). The\nresults reveal that both technologies are viable for out-of-the-lab kinematic\nassessment. However, they also highlight key trade-offs between video- and\nsensor-based approaches including costs, accessibility, and precision. This\nstudy clarifies where off-the-shelf video models already provide clinically\npromising kinematics in healthy adults and where they lag behind IMU-based\nestimates while establishing valuable guidelines for researchers and clinicians\nseeking to develop robust, cost-effective, and user-friendly solutions for\ntelehealth and remote patient monitoring.",
      "authors": [
        "Mario Medrano-Paredes",
        "Carmen Fernández-González",
        "Francisco-Javier Díaz-Pernas",
        "Hichem Saoudi",
        "Javier González-Alonso",
        "Mario Martínez-Zarzuela"
      ],
      "published": "2025-10-02T17:44:31Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02264v1"
    },
    {
      "arxiv_id": "2510.02263v1",
      "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
      "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.",
      "authors": [
        "Yuxiao Qu",
        "Anikait Singh",
        "Yoonho Lee",
        "Amrith Setlur",
        "Ruslan Salakhutdinov",
        "Chelsea Finn",
        "Aviral Kumar"
      ],
      "published": "2025-10-02T17:44:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02263v1"
    },
    {
      "arxiv_id": "2510.02253v1",
      "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
      "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
      "authors": [
        "Zihan Zhou",
        "Shilin Lu",
        "Shuli Leng",
        "Shaocong Zhang",
        "Zhuming Lian",
        "Xinlei Yu",
        "Adams Wai-Kin Kong"
      ],
      "published": "2025-10-02T17:39:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02253v1"
    },
    {
      "arxiv_id": "2510.02250v1",
      "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
      "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
      "authors": [
        "Gonzalo Gonzalez-Pumariega",
        "Vincent Tu",
        "Chih-Lun Lee",
        "Jiachen Yang",
        "Ang Li",
        "Xin Eric Wang"
      ],
      "published": "2025-10-02T17:37:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02250v1"
    },
    {
      "arxiv_id": "2510.02249v1",
      "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process.",
      "authors": [
        "Tianyi Jiang",
        "Yi Bin",
        "Yujuan Ding",
        "Kainian Zhu",
        "Fei Ma",
        "Jingkuan Song",
        "Heng Tao Shen"
      ],
      "published": "2025-10-02T17:36:50Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02249v1"
    },
    {
      "arxiv_id": "2510.02245v1",
      "title": "ExGRPO: Learning to Reason from Experience",
      "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.",
      "authors": [
        "Runzhe Zhan",
        "Yafu Li",
        "Zhi Wang",
        "Xiaoye Qu",
        "Dongrui Liu",
        "Jing Shao",
        "Derek F. Wong",
        "Yu Cheng"
      ],
      "published": "2025-10-02T17:31:30Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02245v1"
    },
    {
      "arxiv_id": "2510.02240v1",
      "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
      "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.",
      "authors": [
        "Sicheng Feng",
        "Kaiwen Tuo",
        "Song Wang",
        "Lingdong Kong",
        "Jianke Zhu",
        "Huan Wang"
      ],
      "published": "2025-10-02T17:29:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02240v1"
    },
    {
      "arxiv_id": "2510.02230v1",
      "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.",
      "authors": [
        "Phuc Minh Nguyen",
        "Chinh D. La",
        "Duy M. H. Nguyen",
        "Nitesh V. Chawla",
        "Binh T. Nguyen",
        "Khoa D. Doan"
      ],
      "published": "2025-10-02T17:17:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02230v1"
    },
    {
      "arxiv_id": "2510.02227v1",
      "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.",
      "authors": [
        "Xiaoyang Yuan",
        "Yujuan Ding",
        "Yi Bin",
        "Wenqi Shao",
        "Jinyu Cai",
        "Jingkuan Song",
        "Yang Yang",
        "Hengtao Shen"
      ],
      "published": "2025-10-02T17:14:00Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02227v1"
    },
    {
      "arxiv_id": "2510.02226v1",
      "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models",
      "summary": "Recent advances in generative video models have enabled the creation of\nhigh-quality videos based on natural language prompts. However, these models\nfrequently lack fine-grained temporal control, meaning they do not allow users\nto specify when particular visual elements should appear within a generated\nsequence. In this work, we introduce TempoControl, a method that allows for\ntemporal alignment of visual concepts during inference, without requiring\nretraining or additional supervision. TempoControl utilizes cross-attention\nmaps, a key component of text-to-video diffusion models, to guide the timing of\nconcepts through a novel optimization approach. Our method steers attention\nusing three complementary principles: aligning its temporal shape with a\ncontrol signal (via correlation), amplifying it where visibility is needed (via\nenergy), and maintaining spatial focus (via entropy). TempoControl allows\nprecise control over timing while ensuring high video quality and diversity. We\ndemonstrate its effectiveness across various video generation applications,\nincluding temporal reordering for single and multiple objects, as well as\naction and audio-aligned generation.",
      "authors": [
        "Shira Schiber",
        "Ofir Lindenbaum",
        "Idan Schwartz"
      ],
      "published": "2025-10-02T17:13:35Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02226v1"
    },
    {
      "arxiv_id": "2510.02212v1",
      "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via\n  Reinforcement Learning",
      "summary": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks.",
      "authors": [
        "Hanyang Zhao",
        "Dawen Liang",
        "Wenpin Tang",
        "David Yao",
        "Nathan Kallus"
      ],
      "published": "2025-10-02T16:57:24Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02212v1"
    },
    {
      "arxiv_id": "2510.02202v1",
      "title": "Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet\n  Challenge 2025",
      "summary": "Objective: Chagas disease is a parasitic infection that is endemic to South\nAmerica, Central America, and, more recently, the U.S., primarily transmitted\nby insects. Chronic Chagas disease can cause cardiovascular diseases and\ndigestive problems. Serological testing capacities for Chagas disease are\nlimited, but Chagas cardiomyopathy often manifests in ECGs, providing an\nopportunity to prioritize patients for testing and treatment. Approach: The\nGeorge B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic\napproaches for identifying Chagas disease from electrocardiograms (ECGs). Main\nresults: This Challenge provides multiple innovations. First, we leveraged\nseveral datasets with labels from patient reports and serological testing,\nprovided a large dataset with weak labels and smaller datasets with strong\nlabels. Second, we augmented the data to support model robustness and\ngeneralizability to unseen data sources. Third, we applied an evaluation metric\nthat captured the local serological testing capacity for Chagas disease to\nframe the machine learning problem as a triage task. Significance: Over 630\nparticipants from 111 teams submitted over 1300 entries during the Challenge,\nrepresenting diverse approaches from academia and industry worldwide.",
      "authors": [
        "Matthew A. Reyna",
        "Zuzana Koscova",
        "Jan Pavlus",
        "Soheil Saghafi",
        "James Weigle",
        "Andoni Elola",
        "Salman Seyedi",
        "Kiersten Campbell",
        "Qiao Li",
        "Ali Bahrami Rad",
        "Antônio H. Ribeiro",
        "Antonio Luiz P. Ribeiro",
        "Reza Sameni",
        "Gari D. Clifford"
      ],
      "published": "2025-10-02T16:50:36Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02202v1"
    },
    {
      "arxiv_id": "2510.02200v1",
      "title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge\n  Graph Exploration Utilities",
      "summary": "Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain.",
      "authors": [
        "Felix Brei",
        "Lorenz Bühmann",
        "Johannes Frey",
        "Daniel Gerber",
        "Lars-Peter Meyer",
        "Claus Stadler",
        "Kirill Bulert"
      ],
      "published": "2025-10-02T16:49:27Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02200v1"
    },
    {
      "arxiv_id": "2510.02194v1",
      "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language\n  Models",
      "summary": "Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control.",
      "authors": [
        "Yuhao Sun",
        "Zhuoer Xu",
        "Shiwen Cui",
        "Kun Yang",
        "Lingyun Yu",
        "Yongdong Zhang",
        "Hongtao Xie"
      ],
      "published": "2025-10-02T16:43:33Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02194v1"
    },
    {
      "arxiv_id": "2510.02190v1",
      "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
      "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.",
      "authors": [
        "Yang Yao",
        "Yixu Wang",
        "Yuxuan Zhang",
        "Yi Lu",
        "Tianle Gu",
        "Lingyu Li",
        "Dingyi Zhao",
        "Keming Wu",
        "Haozhe Wang",
        "Ping Nie",
        "Yan Teng",
        "Yingchun Wang"
      ],
      "published": "2025-10-02T16:40:02Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02190v1"
    },
    {
      "arxiv_id": "2510.02181v1",
      "title": "EvolveCaptions: Empowering DHH Users Through Real-Time Collaborative\n  Captioning",
      "summary": "Automatic Speech Recognition (ASR) systems often fail to accurately\ntranscribe speech from Deaf and Hard of Hearing (DHH) individuals, especially\nduring real-time conversations. Existing personalization approaches typically\nrequire extensive pre-recorded data and place the burden of adaptation on the\nDHH speaker. We present EvolveCaptions, a real-time, collaborative ASR\nadaptation system that supports in-situ personalization with minimal effort.\nHearing participants correct ASR errors during live conversations. Based on\nthese corrections, the system generates short, phonetically targeted prompts\nfor the DHH speaker to record, which are then used to fine-tune the ASR model.\nIn a study with 12 DHH and six hearing participants, EvolveCaptions reduced\nWord Error Rate (WER) across all DHH users within one hour of use, using only\nfive minutes of recording time on average. Participants described the system as\nintuitive, low-effort, and well-integrated into communication. These findings\ndemonstrate the promise of collaborative, real-time ASR adaptation for more\nequitable communication.",
      "authors": [
        "Liang-Yuan Wu",
        "Dhruv Jain"
      ],
      "published": "2025-10-02T16:32:29Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.02181v1"
    },
    {
      "arxiv_id": "2510.02180v1",
      "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement\n  Learning",
      "summary": "Inverse Reinforcement Learning aims to recover reward models from expert\ndemonstrations, but traditional methods yield \"black-box\" models that are\ndifficult to interpret and debug. In this work, we introduce GRACE (Generating\nRewards As CodE), a method for using Large Language Models within an\nevolutionary search to reverse-engineer an interpretable, code-based reward\nfunction directly from expert trajectories. The resulting reward function is\nexecutable code that can be inspected and verified. We empirically validate\nGRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns\nhighly accurate rewards, even in complex, multi-task settings. Further, we\ndemonstrate that the resulting reward leads to strong policies, compared to\nboth competitive Imitation Learning and online RL approaches with ground-truth\nrewards. Finally, we show that GRACE is able to build complex reward APIs in\nmulti-task setups.",
      "authors": [
        "Silvia Sapora",
        "Devon Hjelm",
        "Alexander Toshev",
        "Omar Attia",
        "Bogdan Mazoure"
      ],
      "published": "2025-10-02T16:31:39Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02180v1"
    },
    {
      "arxiv_id": "2510.02173v1",
      "title": "Learning to Reason for Hallucination Span Detection",
      "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
      "authors": [
        "Hsuan Su",
        "Ting-Yao Hu",
        "Hema Swetha Koppula",
        "Kundan Krishna",
        "Hadi Pouransari",
        "Cheng-Yu Hsieh",
        "Cem Koc",
        "Joseph Yitan Cheng",
        "Oncel Tuzel",
        "Raviteja Vemulapalli"
      ],
      "published": "2025-10-02T16:24:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02173v1"
    },
    {
      "arxiv_id": "2510.02171v1",
      "title": "Go witheFlow: Real-time Emotion Driven Audio Effects Modulation",
      "summary": "Music performance is a distinctly human activity, intrinsically linked to the\nperformer's ability to convey, evoke, or express emotion. Machines cannot\nperform music in the human sense; they can produce, reproduce, execute, or\nsynthesize music, but they lack the capacity for affective or emotional\nexperience. As such, music performance is an ideal candidate through which to\nexplore aspects of collaboration between humans and machines. In this paper, we\nintroduce the witheFlow system, designed to enhance real-time music performance\nby automatically modulating audio effects based on features extracted from both\nbiosignals and the audio itself. The system, currently in a proof-of-concept\nphase, is designed to be lightweight, able to run locally on a laptop, and is\nopen-source given the availability of a compatible Digital Audio Workstation\nand sensors.",
      "authors": [
        "Edmund Dervakos",
        "Spyridon Kantarelis",
        "Vassilis Lyberatos",
        "Jason Liartis",
        "Giorgos Stamou"
      ],
      "published": "2025-10-02T16:23:47Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.02171v1"
    },
    {
      "arxiv_id": "2510.02166v1",
      "title": "SIEVE: Towards Verifiable Certification for Code-datasets",
      "summary": "Code agents and empirical software engineering rely on public code datasets,\nyet these datasets lack verifiable quality guarantees. Static 'dataset cards'\ninform, but they are neither auditable nor do they offer statistical\nguarantees, making it difficult to attest to dataset quality. Teams build\nisolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We\npresent SIEVE, a community-driven framework. It turns per-property checks into\nConfidence Cards-machine-readable, verifiable certificates with anytime-valid\nstatistical bounds. We outline a research plan to bring SIEVE to maturity,\nreplacing narrative cards with anytime-verifiable certification. This shift is\nexpected to lower quality-assurance costs and increase trust in code-datasets.",
      "authors": [
        "Fatou Ndiaye Mbodji",
        "El-hacen Diallo",
        "Jordan Samhi",
        "Kui Liu",
        "Jacques Klein",
        "Tegawendé F. Bissyande"
      ],
      "published": "2025-10-02T16:14:23Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.02166v1"
    },
    {
      "arxiv_id": "2510.02161v1",
      "title": "Comparing Contrastive and Triplet Loss in Audio-Visual Embedding:\n  Intra-Class Variance and Greediness Analysis",
      "summary": "Contrastive loss and triplet loss are widely used objectives in deep metric\nlearning, yet their effects on representation quality remain insufficiently\nunderstood. We present a theoretical and empirical comparison of these losses,\nfocusing on intra- and inter-class variance and optimization behavior (e.g.,\ngreedy updates). Through task-specific experiments with consistent settings on\nsynthetic data and real datasets-MNIST, CIFAR-10-it is shown that triplet loss\npreserves greater variance within and across classes, supporting finer-grained\ndistinctions in the learned representations. In contrast, contrastive loss\ntends to compact intra-class embeddings, which may obscure subtle semantic\ndifferences. To better understand their optimization dynamics, By examining\nloss-decay rate, active ratio, and gradient norm, we find that contrastive loss\ndrives many small updates early on, while triplet loss produces fewer but\nstronger updates that sustain learning on hard examples. Finally, across both\nclassification and retrieval tasks on MNIST, CIFAR-10, CUB-200, and CARS196\ndatasets, our results consistently show that triplet loss yields superior\nperformance, which suggests using triplet loss for detail retention and\nhard-sample focus, and contrastive loss for smoother, broad-based embedding\nrefinement.",
      "authors": [
        "Donghuo Zeng"
      ],
      "published": "2025-10-02T16:11:46Z",
      "primary_category": "cs.MM",
      "arxiv_url": "https://arxiv.org/abs/2510.02161v1"
    },
    {
      "arxiv_id": "2510.02155v1",
      "title": "Unlocking Vision-Language Models for Video Anomaly Detection via\n  Fine-Grained Prompting",
      "summary": "Prompting has emerged as a practical way to adapt frozen vision-language\nmodels (VLMs) for video anomaly detection (VAD). Yet, existing prompts are\noften overly abstract, overlooking the fine-grained human-object interactions\nor action semantics that define complex anomalies in surveillance videos. We\npropose ASK-Hint, a structured prompting framework that leverages\naction-centric knowledge to elicit more accurate and interpretable reasoning\nfrom frozen VLMs. Our approach organizes prompts into semantically coherent\ngroups (e.g. violence, property crimes, public safety) and formulates\nfine-grained guiding questions that align model predictions with discriminative\nvisual cues. Extensive experiments on UCF-Crime and XD-Violence show that\nASK-Hint consistently improves AUC over prior baselines, achieving\nstate-of-the-art performance compared to both fine-tuned and training-free\nmethods. Beyond accuracy, our framework provides interpretable reasoning traces\ntowards anomaly and demonstrates strong generalization across datasets and VLM\nbackbones. These results highlight the critical role of prompt granularity and\nestablish ASK-Hint as a new training-free and generalizable solution for\nexplainable video anomaly detection.",
      "authors": [
        "Shu Zou",
        "Xinyu Tian",
        "Lukas Wesemann",
        "Fabian Waschkowski",
        "Zhaoyuan Yang",
        "Jing Zhang"
      ],
      "published": "2025-10-02T16:06:31Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02155v1"
    },
    {
      "arxiv_id": "2510.02153v1",
      "title": "Human-Robo-advisor collaboration in decision-making: Evidence from a\n  multiphase mixed methods experimental study",
      "summary": "Robo-advisors (RAs) are cost-effective, bias-resistant alternatives to human\nfinancial advisors, yet adoption remains limited. While prior research has\nexamined user interactions with RAs, less is known about how individuals\ninterpret RA roles and integrate their advice into decision-making. To address\nthis gap, this study employs a multiphase mixed methods design integrating a\nbehavioral experiment (N = 334), thematic analysis, and follow-up quantitative\ntesting. Findings suggest that people tend to rely on RAs, with reliance shaped\nby information about RA performance and the framing of advice as gains or\nlosses. Thematic analysis reveals three RA roles in decision-making and four\nuser types, each reflecting distinct patterns of advice integration. In\naddition, a 2 x 2 typology categorizes antecedents of acceptance into enablers\nand inhibitors at both the individual and algorithmic levels. By combining\nbehavioral, interpretive, and confirmatory evidence, this study advances\nunderstanding of human-RA collaboration and provides actionable insights for\ndesigning more trustworthy and adaptive RA systems.",
      "authors": [
        "Hasan Mahmud",
        "Najmul Islam",
        "Satish Krishnan"
      ],
      "published": "2025-10-02T16:04:31Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.02153v1"
    },
    {
      "arxiv_id": "2510.02143v1",
      "title": "How to Find Fantastic Papers: Self-Rankings as a Powerful Predictor of\n  Scientific Impact Beyond Peer Review",
      "summary": "Peer review in academic research aims not only to ensure factual correctness\nbut also to identify work of high scientific potential that can shape future\nresearch directions. This task is especially critical in fast-moving fields\nsuch as artificial intelligence (AI), yet it has become increasingly difficult\ngiven the rapid growth of submissions. In this paper, we investigate an\nunderexplored measure for identifying high-impact research: authors' own\nrankings of their multiple submissions to the same AI conference. Grounded in\ngame-theoretic reasoning, we hypothesize that self-rankings are informative\nbecause authors possess unique understanding of their work's conceptual depth\nand long-term promise. To test this hypothesis, we conducted a large-scale\nexperiment at a leading AI conference, where 1,342 researchers self-ranked\ntheir 2,592 submissions by perceived quality. Tracking outcomes over more than\na year, we found that papers ranked highest by their authors received twice as\nmany citations as their lowest-ranked counterparts; self-rankings were\nespecially effective at identifying highly cited papers (those with over 150\ncitations). Moreover, we showed that self-rankings outperformed peer review\nscores in predicting future citation counts. Our results remained robust after\naccounting for confounders such as preprint posting time and self-citations.\nTogether, these findings demonstrate that authors' self-rankings provide a\nreliable and valuable complement to peer review for identifying and elevating\nhigh-impact research in AI.",
      "authors": [
        "Buxin Su",
        "Natalie Collina",
        "Garrett Wen",
        "Didong Li",
        "Kyunghyun Cho",
        "Jianqing Fan",
        "Bingxin Zhao",
        "Weijie Su"
      ],
      "published": "2025-10-02T15:50:21Z",
      "primary_category": "stat.AP",
      "arxiv_url": "https://arxiv.org/abs/2510.02143v1"
    },
    {
      "arxiv_id": "2510.02139v1",
      "title": "BioinfoMCP: A Unified Platform Enabling MCP Interfaces in Agentic\n  Bioinformatics",
      "summary": "Bioinformatics tools are essential for complex computational biology tasks,\nyet their integration with emerging AI-agent frameworks is hindered by\nincompatible interfaces, heterogeneous input-output formats, and inconsistent\nparameter conventions. The Model Context Protocol (MCP) provides a standardized\nframework for tool-AI communication, but manually converting hundreds of\nexisting and rapidly growing specialized bioinformatics tools into\nMCP-compliant servers is labor-intensive and unsustainable. Here, we present\nBioinfoMCP, a unified platform comprising two components: BioinfoMCP Converter,\nwhich automatically generates robust MCP servers from tool documentation using\nlarge language models, and BioinfoMCP Benchmark, which systematically validates\nthe reliability and versatility of converted tools across diverse computational\ntasks. We present a platform of 38 MCP-converted bioinformatics tools,\nextensively validated to show that 94.7% successfully executed complex\nworkflows across three widely used AI-agent platforms. By removing technical\nbarriers to AI automation, BioinfoMCP enables natural-language interaction with\nsophisticated bioinformatics analyses without requiring extensive programming\nexpertise, offering a scalable path to intelligent, interoperable computational\nbiology.",
      "authors": [
        "Florensia Widjaja",
        "Zhangtianyi Chen",
        "Juexiao Zhou"
      ],
      "published": "2025-10-02T15:47:59Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.02139v1"
    },
    {
      "arxiv_id": "2510.02133v1",
      "title": "FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic\n  Documents for Training Document Understanding Models",
      "summary": "Developing document understanding models at enterprise scale requires large,\ndiverse, and well-annotated datasets spanning a wide range of document types.\nHowever, collecting such data is prohibitively expensive due to privacy\nconstraints, legal restrictions, and the sheer volume of manual annotation\nneeded - costs that can scale into millions of dollars. We introduce FlexDoc, a\nscalable synthetic data generation framework that combines Stochastic Schemas\nand Parameterized Sampling to produce realistic, multilingual semi-structured\ndocuments with rich annotations. By probabilistically modeling layout patterns,\nvisual structure, and content variability, FlexDoc enables the controlled\ngeneration of diverse document variants at scale. Experiments on Key\nInformation Extraction (KIE) tasks demonstrate that FlexDoc-generated data\nimproves the absolute F1 Score by up to 11% when used to augment real datasets,\nwhile reducing annotation effort by over 90% compared to traditional\nhard-template methods. The solution is in active deployment, where it has\naccelerated the development of enterprise-grade document understanding models\nwhile significantly reducing data acquisition and annotation costs.",
      "authors": [
        "Karan Dua",
        "Hitesh Laxmichand Patel",
        "Puneet Mittal",
        "Ranjeet Gupta",
        "Amit Agarwal",
        "Praneet Pabolu",
        "Srikant Panda",
        "Hansa Meghwani",
        "Graham Horwood",
        "Fahad Shah"
      ],
      "published": "2025-10-02T15:42:35Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02133v1"
    },
    {
      "arxiv_id": "2510.02128v1",
      "title": "The Disparate Impacts of Speculative Decoding",
      "summary": "The practice of speculative decoding, whereby inference is probabilistically\nsupported by a smaller, cheaper, ``drafter'' model, has become a standard\ntechnique for systematically reducing the decoding time of large language\nmodels. This paper conducts an analysis of speculative decoding through the\nlens of its potential disparate speed-up rates across tasks. Crucially, the\npaper shows that speed-up gained from speculative decoding is not uniformly\ndistributed across tasks, consistently diminishing for under-fit, and often\nunderrepresented tasks. To better understand this phenomenon, we derive an\nanalysis to quantify this observed ``unfairness'' and draw attention to the\nfactors that motivate such disparate speed-ups to emerge. Further, guided by\nthese insights, the paper proposes a mitigation strategy designed to reduce\nspeed-up disparities and validates the approach across several model pairs,\nrevealing on average a 12% improvement in our fairness metric.",
      "authors": [
        "Jameson Sandler",
        "Ahmet Üstün",
        "Marco Romanelli",
        "Sara Hooker",
        "Ferdinando Fioretto"
      ],
      "published": "2025-10-02T15:38:57Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02128v1"
    },
    {
      "arxiv_id": "2510.02125v1",
      "title": "Do AI Models Perform Human-like Abstract Reasoning Across Modalities?",
      "summary": "OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI\nbenchmark, but does that mean state-of-the-art models recognize and reason with\nthe abstractions that the task creators intended? We investigate models'\nabstraction abilities on ConceptARC. We evaluate models under settings that\nvary the input modality (textual vs. visual), whether the model is permitted to\nuse external Python tools, and, for reasoning models, the amount of reasoning\neffort. In addition to measuring output accuracy, we perform fine-grained\nevaluation of the natural-language rules that models generate to explain their\nsolutions. This dual evaluation lets us assess whether models solve tasks using\nthe abstractions ConceptARC was designed to elicit, rather than relying on\nsurface-level patterns. Our results show that, while some models using\ntext-based representations match human output accuracy, the best models' rules\nare often based on surface-level ``shortcuts'' and capture intended\nabstractions far less often than humans. Thus their capabilities for general\nabstract reasoning may be overestimated by evaluations based on accuracy alone.\nIn the visual modality, AI models' output accuracy drops sharply, yet our\nrule-level analysis reveals that models might be underestimated, as they still\nexhibit a substantial share of rules that capture intended abstractions, but\nare often unable to correctly apply these rules. In short, our results show\nthat models still lag humans in abstract reasoning, and that using accuracy\nalone to evaluate abstract reasoning on ARC-like tasks may overestimate\nabstract-reasoning capabilities in textual modalities and underestimate it in\nvisual modalities. We believe that our evaluation framework offers a more\nfaithful picture of multimodal models' abstract reasoning abilities and a more\nprincipled way to track progress toward human-like, abstraction-centered\nintelligence.",
      "authors": [
        "Claas Beger",
        "Ryan Yi",
        "Shuhao Fu",
        "Arseny Moskvichev",
        "Sarah W. Tsai",
        "Sivasankaran Rajamanickam",
        "Melanie Mitchell"
      ],
      "published": "2025-10-02T15:35:10Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02125v1"
    },
    {
      "arxiv_id": "2510.02120v1",
      "title": "VarCoNet: A variability-aware self-supervised framework for functional\n  connectome extraction from resting-state fMRI",
      "summary": "Accounting for inter-individual variability in brain function is key to\nprecision medicine. Here, by considering functional inter-individual\nvariability as meaningful data rather than noise, we introduce VarCoNet, an\nenhanced self-supervised framework for robust functional connectome (FC)\nextraction from resting-state fMRI (rs-fMRI) data. VarCoNet employs\nself-supervised contrastive learning to exploit inherent functional\ninter-individual variability, serving as a brain function encoder that\ngenerates FC embeddings readily applicable to downstream tasks even in the\nabsence of labeled data. Contrastive learning is facilitated by a novel\naugmentation strategy based on segmenting rs-fMRI signals. At its core,\nVarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-series\nprocessing, enhanced with a robust Bayesian hyperparameter optimization. Our\nVarCoNet framework is evaluated on two downstream tasks: (i) subject\nfingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii)\nautism spectrum disorder (ASD) classification, using rs-fMRI data from the\nABIDE I and ABIDE II datasets. Using different brain parcellations, our\nextensive testing against state-of-the-art methods, including 13 deep learning\nmethods, demonstrates VarCoNet's superiority, robustness, interpretability, and\ngeneralizability. Overall, VarCoNet provides a versatile and robust framework\nfor FC analysis in rs-fMRI.",
      "authors": [
        "Charalampos Lamprou",
        "Aamna Alshehhi",
        "Leontios J. Hadjileontiadis",
        "Mohamed L. Seghier"
      ],
      "published": "2025-10-02T15:29:17Z",
      "primary_category": "cs.NE",
      "arxiv_url": "https://arxiv.org/abs/2510.02120v1"
    },
    {
      "arxiv_id": "2510.02109v1",
      "title": "SpurBreast: A Curated Dataset for Investigating Spurious Correlations in\n  Real-world Breast MRI Classification",
      "summary": "Deep neural networks (DNNs) have demonstrated remarkable success in medical\nimaging, yet their real-world deployment remains challenging due to spurious\ncorrelations, where models can learn non-clinical features instead of\nmeaningful medical patterns. Existing medical imaging datasets are not designed\nto systematically study this issue, largely due to restrictive licensing and\nlimited supplementary patient data. To address this gap, we introduce\nSpurBreast, a curated breast MRI dataset that intentionally incorporates\nspurious correlations to evaluate their impact on model performance. Analyzing\nover 100 features involving patient, device, and imaging protocol, we identify\ntwo dominant spurious signals: magnetic field strength (a global feature\ninfluencing the entire image) and image orientation (a local feature affecting\nspatial alignment). Through controlled dataset splits, we demonstrate that DNNs\ncan exploit these non-clinical signals, achieving high validation accuracy\nwhile failing to generalize to unbiased test data. Alongside these two datasets\ncontaining spurious correlations, we also provide benchmark datasets without\nspurious correlations, allowing researchers to systematically investigate\nclinically relevant and irrelevant features, uncertainty estimation,\nadversarial robustness, and generalization strategies. Models and datasets are\navailable at https://github.com/utkuozbulak/spurbreast.",
      "authors": [
        "Jong Bum Won",
        "Wesley De Neve",
        "Joris Vankerschaver",
        "Utku Ozbulak"
      ],
      "published": "2025-10-02T15:16:20Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.02109v1"
    },
    {
      "arxiv_id": "2510.02108v1",
      "title": "Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant\n  Neural Network",
      "summary": "Although symbol-level precoding (SLP) based on constructive interference (CI)\nexploitation offers performance gains, its high complexity remains a\nbottleneck. This paper addresses this challenge with an end-to-end deep\nlearning (DL) framework with low inference complexity that leverages the\nstructure of the optimal SLP solution in the closed-form and its inherent\ntensor equivariance (TE), where TE denotes that a permutation of the input\ninduces the corresponding permutation of the output. Building upon the\ncomputationally efficient model-based formulations, as well as their known\nclosed-form solutions, we analyze their relationship with linear precoding (LP)\nand investigate the corresponding optimality condition. We then construct a\nmapping from the problem formulation to the solution and prove its TE, based on\nwhich the designed networks reveal a specific parameter-sharing pattern that\ndelivers low computational complexity and strong generalization. Leveraging\nthese, we propose the backbone of the framework with an attention-based TE\nmodule, achieving linear computational complexity. Furthermore, we demonstrate\nthat such a framework is also applicable to imperfect CSI scenarios, where we\ndesign a TE-based network to map the CSI, statistics, and symbols to auxiliary\nvariables. Simulation results show that the proposed framework captures\nsubstantial performance gains of optimal SLP, while achieving an approximately\n80-times speedup over conventional methods and maintaining strong\ngeneralization across user numbers and symbol block lengths.",
      "authors": [
        "Jinshuo Zhang",
        "Yafei Wang",
        "Xinping Yi",
        "Wenjin Wang",
        "Shi Jin",
        "Symeon Chatzinotas",
        "Björn Ottersten"
      ],
      "published": "2025-10-02T15:15:50Z",
      "primary_category": "eess.SP",
      "arxiv_url": "https://arxiv.org/abs/2510.02108v1"
    },
    {
      "arxiv_id": "2510.02100v1",
      "title": "When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based\n  Tracking in Surgical Videos",
      "summary": "Video object segmentation (VOS) models such as SAM2 offer promising zero-shot\ntracking capabilities for surgical videos using minimal user input. Among the\navailable input types, point-based tracking offers an efficient and low-cost\nalternative, yet its reliability and failure cases in complex surgical\nenvironments are not well understood. In this work, we systematically analyze\nthe failure modes of point-based tracking in laparoscopic cholecystectomy\nvideos. Focusing on three surgical targets, the gallbladder, grasper, and\nL-hook electrocautery, we compare the performance of point-based tracking with\nsegmentation mask initialization. Our results show that point-based tracking is\ncompetitive for surgical tools but consistently underperforms for anatomical\ntargets, where tissue similarity and ambiguous boundaries lead to failure.\nThrough qualitative analysis, we reveal key factors influencing tracking\noutcomes and provide several actionable recommendations for selecting and\nplacing tracking points to improve performance in surgical video analysis.",
      "authors": [
        "Woowon Jang",
        "Jiwon Im",
        "Juseung Choi",
        "Niki Rashidian",
        "Wesley De Neve",
        "Utku Ozbulak"
      ],
      "published": "2025-10-02T15:06:49Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02100v1"
    },
    {
      "arxiv_id": "2510.02091v1",
      "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning",
      "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models.",
      "authors": [
        "Xinyuan Song",
        "Keyu Wang",
        "PengXiang Li",
        "Lu Yin",
        "Shiwei Liu"
      ],
      "published": "2025-10-02T14:57:13Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02091v1"
    },
    {
      "arxiv_id": "2510.02084v1",
      "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
      "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
      "authors": [
        "Kuiye Ding",
        "Fanda Fan",
        "Zheya Wang",
        "Hongxiao Li",
        "Yifan Wang",
        "Lei Wang",
        "Chunjie Luo",
        "Jianfeng Zhan"
      ],
      "published": "2025-10-02T14:50:50Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02084v1"
    },
    {
      "arxiv_id": "2510.02060v1",
      "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly\n  Detection",
      "summary": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD.",
      "authors": [
        "Sanghyu Yoon",
        "Dongmin Kim",
        "Suhee Yoon",
        "Ye Seul Sim",
        "Seungdong Yoa",
        "Hye-Seung Cho",
        "Soonyoung Lee",
        "Hankook Lee",
        "Woohyung Lim"
      ],
      "published": "2025-10-02T14:28:45Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02060v1"
    },
    {
      "arxiv_id": "2510.02036v1",
      "title": "The Current State of AI Bias Bounties: An Overview of Existing\n  Programmes and Research",
      "summary": "Current bias evaluation methods rarely engage with communities impacted by AI\nsystems. Inspired by bug bounties, bias bounties have been proposed as a\nreward-based method that involves communities in AI bias detection by asking\nusers of AI systems to report biases they encounter when interacting with such\nsystems. In the absence of a state-of-the-art review, this survey aimed to\nidentify and analyse existing AI bias bounty programmes and to present academic\nliterature on bias bounties. Google, Google Scholar, PhilPapers, and IEEE\nXplore were searched, and five bias bounty programmes, as well as five research\npublications, were identified. All bias bounties were organised by U.S.-based\norganisations as time-limited contests, with public participation in four\nprogrammes and prize pools ranging from 7,000 to 24,000 USD. The five research\npublications included a report on the application of bug bounties to\nalgorithmic harms, an article addressing Twitter's bias bounty, a proposal for\nbias bounties as an institutional mechanism to increase AI scrutiny, a workshop\ndiscussing bias bounties from queer perspectives, and an algorithmic framework\nfor bias bounties. We argue that reducing the technical requirements to enter\nbounty programmes is important to include those without coding experience.\nGiven the limited adoption of bias bounties, future efforts should explore the\ntransferability of the best practices from bug bounties and examine how such\nprogrammes can be designed to be sensitive to underrepresented groups while\nlowering adoption barriers for organisations.",
      "authors": [
        "Sergej Kucenko",
        "Nathaniel Dennler",
        "Fengxiang He"
      ],
      "published": "2025-10-02T14:09:11Z",
      "primary_category": "cs.CY",
      "arxiv_url": "https://arxiv.org/abs/2510.02036v1"
    },
    {
      "arxiv_id": "2510.02028v1",
      "title": "LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud\n  Reconstruction",
      "summary": "This work proposed a 3D autoencoder architecture, named LiLa-Net, which\nencodes efficient features from real traffic environments, employing only the\nLiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,\nequipped with Velodyne LiDAR. The system leverage skip connections concept to\nimprove the performance without using extensive resources as the\nstate-of-the-art architectures. Key changes include reducing the number of\nencoder layers and simplifying the skip connections, while still producing an\nefficient and representative latent space which allows to accurately\nreconstruct the original point cloud. Furthermore, an effective balance has\nbeen achieved between the information carried by the skip connections and the\nlatent encoding, leading to improved reconstruction quality without\ncompromising performance. Finally, the model demonstrates strong generalization\ncapabilities, successfully reconstructing objects unrelated to the original\ntraffic environment.",
      "authors": [
        "Mario Resino",
        "Borja Pérez",
        "Jaime Godoy",
        "Abdulla Al-Kaff",
        "Fernando García"
      ],
      "published": "2025-10-02T14:00:20Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02028v1"
    },
    {
      "arxiv_id": "2510.02027v1",
      "title": "Zero-shot reasoning for simulating scholarly peer-review",
      "summary": "The scholarly publishing ecosystem faces a dual crisis of unmanageable\nsubmission volumes and unregulated AI, creating an urgent need for new\ngovernance models to safeguard scientific integrity. The traditional human-only\npeer review regime lacks a scalable, objective benchmark, making editorial\nprocesses opaque and difficult to audit. Here we investigate a deterministic\nsimulation framework that provides the first stable, evidence-based standard\nfor evaluating AI-generated peer review reports. Analyzing 352 peer-review\nsimulation reports, we identify consistent system state indicators that\ndemonstrate its reliability. First, the system is able to simulate calibrated\neditorial judgment, with 'Revise' decisions consistently forming the majority\noutcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt\nto field-specific norms, rising to 45% in Health Sciences. Second, it maintains\nunwavering procedural integrity, enforcing a stable 29% evidence-anchoring\ncompliance rate that remains invariant across diverse review tasks and\nscientific domains. These findings demonstrate a system that is predictably\nrule-bound, mitigating the stochasticity of generative AI. For the scientific\ncommunity, this provides a transparent tool to ensure fairness; for publishing\nstrategists, it offers a scalable instrument for auditing workflows, managing\nintegrity risks, and implementing evidence-based governance. The framework\nrepositions AI as an essential component of institutional accountability,\nproviding the critical infrastructure to maintain trust in scholarly\ncommunication.",
      "authors": [
        "Khalid M. Saqr"
      ],
      "published": "2025-10-02T13:59:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02027v1"
    },
    {
      "arxiv_id": "2510.02001v1",
      "title": "Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using\n  GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output\n  (SLSO) Framework",
      "summary": "In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to\nautomatically generate jaw cyst findings on dental panoramic radiographs. To\nimprove accuracy, we constructed a Self-correction Loop with Structured Output\n(SLSO) framework and verified its effectiveness. A 10-step process was\nimplemented for 22 cases of jaw cysts, including image input and analysis,\nstructured data generation, tooth number extraction and consistency checking,\niterative regeneration when inconsistencies were detected, and finding\ngeneration with subsequent restructuring and consistency verification. A\ncomparative experiment was conducted using the conventional Chain-of-Thought\n(CoT) method across seven evaluation items: transparency, internal structure,\nborders, root resorption, tooth movement, relationships with other structures,\nand tooth number. The results showed that the proposed SLSO framework improved\noutput accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates\nfor tooth number, tooth movement, and root resorption, respectively. In the\nsuccessful cases, a consistently structured output was achieved after up to\nfive regenerations. Although statistical significance was not reached because\nof the small size of the dataset, the overall SLSO framework enforced negative\nfinding descriptions, suppressed hallucinations, and improved tooth number\nidentification accuracy. However, the accurate identification of extensive\nlesions spanning multiple teeth is limited. Nevertheless, further refinement is\nrequired to enhance overall performance and move toward a practical finding\ngeneration system.",
      "authors": [
        "Nanaka Hosokawa",
        "Ryo Takahashi",
        "Tomoya Kitano",
        "Yukihiro Iida",
        "Chisako Muramatsu",
        "Tatsuro Hayashi",
        "Yuta Seino",
        "Xiangrong Zhou",
        "Takeshi Hara",
        "Akitoshi Katsumata",
        "Hiroshi Fujita"
      ],
      "published": "2025-10-02T13:22:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.02001v1"
    },
    {
      "arxiv_id": "2510.01994v1",
      "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation",
      "summary": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research.",
      "authors": [
        "Chen Yang",
        "Lin Yang",
        "Ziqi Wang",
        "Dong Wang",
        "Jianyi Zhou",
        "Junjie Chen"
      ],
      "published": "2025-10-02T13:15:40Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.01994v1"
    },
    {
      "arxiv_id": "2510.01967v1",
      "title": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using\n  ZK-SNARKs",
      "summary": "As image generation models grow increasingly powerful and accessible,\nconcerns around authenticity, ownership, and misuse of synthetic media have\nbecome critical. The ability to generate lifelike images indistinguishable from\nreal ones introduces risks such as misinformation, deepfakes, and intellectual\nproperty violations. Traditional watermarking methods either degrade image\nquality, are easily removed, or require access to confidential model internals\n- making them unsuitable for secure and scalable deployment. We are the first\nto introduce ZK-WAGON, a novel system for watermarking image generation models\nusing the Zero-Knowledge Succinct Non Interactive Argument of Knowledge\n(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing\nmodel weights, generation prompts, or any sensitive internal information. We\npropose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively\nconvert key layers of an image generation model into a circuit, reducing proof\ngeneration time significantly. Generated ZK-SNARK proofs are imperceptibly\nembedded into a generated image via Least Significant Bit (LSB) steganography.\nWe demonstrate this system on both GAN and Diffusion models, providing a\nsecure, model-agnostic pipeline for trustworthy AI image generation.",
      "authors": [
        "Aadarsh Anantha Ramakrishnan",
        "Shubham Agarwal",
        "Selvanayagam S",
        "Kunwar Singh"
      ],
      "published": "2025-10-02T12:39:57Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01967v1"
    },
    {
      "arxiv_id": "2510.01958v1",
      "title": "Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for\n  Improved Cross-Corpus Speech Enhancement",
      "summary": "Recent advances in speech enhancement have shown that models combining Mamba\nand attention mechanisms yield superior cross-corpus generalization\nperformance. At the same time, integrating Mamba in a U-Net structure has\nyielded state-of-the-art enhancement performance, while reducing both model\nsize and computational complexity. Inspired by these insights, we propose\nRWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and\nmulti-head attention in a U-Net structure for improved cross-corpus\nperformance. Resolution-wise shared attention (RWSA) refers to layerwise\nattention-sharing across corresponding time- and frequency resolutions. Our\nbest-performing RWSA-MambaUNet model achieves state-of-the-art generalization\nperformance on two out-of-domain test sets. Notably, our smallest model\nsurpasses all baselines on the out-of-domain DNS 2020 test set in terms of\nPESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM_v2 test set in terms\nof SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and\na fraction of the FLOPs.",
      "authors": [
        "Nikolai Lund Kühne",
        "Jesper Jensen",
        "Jan Østergaard",
        "Zheng-Hua Tan"
      ],
      "published": "2025-10-02T12:27:29Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01958v1"
    },
    {
      "arxiv_id": "2510.01934v1",
      "title": "Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors",
      "summary": "Few-shot anomaly detection streamlines and simplifies industrial safety\ninspection. However, limited samples make accurate differentiation between\nnormal and abnormal features challenging, and even more so under\ncategory-agnostic conditions. Large-scale pre-training of foundation visual\nencoders has advanced many fields, as the enormous quantity of data helps to\nlearn the general distribution of normal images. We observe that the anomaly\namount in an image directly correlates with the difference in the learnt\nembeddings and utilize this to design a few-shot anomaly detector termed\nFoundAD. This is done by learning a nonlinear projection operator onto the\nnatural image manifold. The simple operator acts as an effective tool for\nanomaly detection to characterize and identify out-of-distribution regions in\nan image. Extensive experiments show that our approach supports multi-class\ndetection and achieves competitive performance while using substantially fewer\nparameters than prior methods. Backed up by evaluations with multiple\nfoundation encoders, including fresh DINOv3, we believe this idea broadens the\nperspective on foundation features and advances the field of few-shot anomaly\ndetection.",
      "authors": [
        "Guangyao Zhai",
        "Yue Zhou",
        "Xinyan Deng",
        "Lars Heckler",
        "Nassir Navab",
        "Benjamin Busam"
      ],
      "published": "2025-10-02T11:53:20Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01934v1"
    },
    {
      "arxiv_id": "2510.01924v1",
      "title": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning",
      "summary": "As large language models (LLMs) are increasingly used to model and augment\ncollective decision-making, it is critical to examine their alignment with\nhuman social reasoning. We present an empirical framework for assessing\ncollective alignment, in contrast to prior work on the individual level. Using\nthe Lost at Sea social psychology task, we conduct a large-scale online\nexperiment (N=748), randomly assigning groups to leader elections with either\nvisible demographic attributes (e.g. name, gender) or pseudonymous aliases. We\nthen simulate matched LLM groups conditioned on the human data, benchmarking\nGemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some\nmirror human biases; others mask these biases and attempt to compensate for\nthem. We empirically demonstrate that human-AI alignment in collective\nreasoning depends on context, cues, and model-specific inductive biases.\nUnderstanding how LLMs align with collective human behavior is critical to\nadvancing socially-aligned AI, and demands dynamic benchmarks that capture the\ncomplexities of collective reasoning.",
      "authors": [
        "Crystal Qian",
        "Aaron Parisi",
        "Clémentine Bouleau",
        "Vivian Tsai",
        "Maël Lebreton",
        "Lucas Dixon"
      ],
      "published": "2025-10-02T11:41:30Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01924v1"
    },
    {
      "arxiv_id": "2510.01914v1",
      "title": "Automated Defect Detection for Mass-Produced Electronic Components Based\n  on YOLO Object Detection Models",
      "summary": "Since the defect detection of conventional industry components is\ntime-consuming and labor-intensive, it leads to a significant burden on quality\ninspection personnel and makes it difficult to manage product quality. In this\npaper, we propose an automated defect detection system for the dual in-line\npackage (DIP) that is widely used in industry, using digital camera optics and\na deep learning (DL)-based model. The two most common defect categories of DIP\nare examined: (1) surface defects, and (2) pin-leg defects. However, the lack\nof defective component images leads to a challenge for detection tasks. To\nsolve this problem, the ConSinGAN is used to generate a suitable-sized dataset\nfor training and testing. Four varieties of the YOLO model are investigated\n(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.\nThe proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in\naccuracy of 95.50\\%, detection time of 285 ms, and is far superior to\nthreshold-based approaches. In addition, the supervisory control and data\nacquisition (SCADA) system is developed, and the associated sensor architecture\nis described. The proposed automated defect detection can be easily established\nwith numerous types of defects or insufficient defect data.",
      "authors": [
        "Wei-Lung Mao",
        "Chun-Chi Wang",
        "Po-Heng Chou",
        "Yen-Ting Liu"
      ],
      "published": "2025-10-02T11:33:16Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01914v1"
    },
    {
      "arxiv_id": "2510.01910v1",
      "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under\n  Deficiencies with Iterative Refinement",
      "summary": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement.",
      "authors": [
        "Zhaoyan Wang",
        "Zheng Gao",
        "Arogya Kharel",
        "In-Young Ko"
      ],
      "published": "2025-10-02T11:30:51Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01910v1"
    },
    {
      "arxiv_id": "2510.01902v1",
      "title": "Constrained Adaptive Rejection Sampling",
      "summary": "Language Models (LMs) are increasingly used in applications where generated\noutputs must satisfy strict semantic or syntactic constraints. Existing\napproaches to constrained generation fall along a spectrum: greedy constrained\ndecoding methods enforce validity during decoding but distort the LM's\ndistribution, while rejection sampling (RS) preserves fidelity but wastes\ncomputation by discarding invalid outputs. Both extremes are problematic in\ndomains such as program fuzzing, where both validity and diversity of samples\nare essential. We present Constrained Adaptive Rejection Sampling (CARS), an\napproach that strictly improves the sample-efficiency of RS without\ndistributional distortion. CARS begins with unconstrained LM sampling and\nadaptively rules out constraint-violating continuations by recording them in a\ntrie and subtracting their probability mass from future draws. This adaptive\npruning ensures that prefixes proven invalid are never revisited, acceptance\nrates improve monotonically, and the resulting samples exactly follow the\nconstrained distribution. In experiments on a variety of domains -- e.g.,\nprogram fuzzing and molecular generation -- CARS consistently achieves higher\nefficiency -- measured in the number of LM forward passes per valid sample --\nwhile also producing stronger sample diversity than both GCD and methods that\napproximate the LM's distribution.",
      "authors": [
        "Paweł Parys",
        "Sairam Vaidya",
        "Taylor Berg-Kirkpatrick",
        "Loris D'Antoni"
      ],
      "published": "2025-10-02T11:17:26Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01902v1"
    },
    {
      "arxiv_id": "2510.01899v1",
      "title": "Multimodal Foundation Models for Early Disease Detection",
      "summary": "Healthcare generates diverse streams of data, including electronic health\nrecords (EHR), medical imaging, genetics, and ongoing monitoring from wearable\ndevices. Traditional diagnostic models frequently analyze these sources in\nisolation, which constrains their capacity to identify cross-modal correlations\nessential for early disease diagnosis. Our research presents a multimodal\nfoundation model that consolidates diverse patient data through an\nattention-based transformer framework. At first, dedicated encoders put each\nmodality into a shared latent space. Then, they combine them using multi-head\nattention and residual normalization. The architecture is made for pretraining\non many tasks, which makes it easy to adapt to new diseases and datasets with\nlittle extra work. We provide an experimental strategy that uses benchmark\ndatasets in oncology, cardiology, and neurology, with the goal of testing early\ndetection tasks. The framework includes data governance and model management\ntools in addition to technological performance to improve transparency,\nreliability, and clinical interpretability. The suggested method works toward a\nsingle foundation model for precision diagnostics, which could improve the\naccuracy of predictions and help doctors make decisions.",
      "authors": [
        "Md Talha Mohsin",
        "Ismail Abdulrashid"
      ],
      "published": "2025-10-02T11:12:57Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01899v1"
    },
    {
      "arxiv_id": "2510.01891v1",
      "title": "HRTFformer: A Spatially-Aware Transformer for Personalized HRTF\n  Upsampling in Immersive Audio Rendering",
      "summary": "Personalized Head-Related Transfer Functions (HRTFs) are starting to be\nintroduced in many commercial immersive audio applications and are crucial for\nrealistic spatial audio rendering. However, one of the main hesitations\nregarding their introduction is that creating personalized HRTFs is impractical\nat scale due to the complexities of the HRTF measurement process. To mitigate\nthis drawback, HRTF spatial upsampling has been proposed with the aim of\nreducing measurements required. While prior work has seen success with\ndifferent machine learning (ML) approaches, these models often struggle with\nlong-range spatial consistency and generalization at high upsampling factors.\nIn this paper, we propose a novel transformer-based architecture for HRTF\nupsampling, leveraging the attention mechanism to better capture spatial\ncorrelations across the HRTF sphere. Working in the spherical harmonic (SH)\ndomain, our model learns to reconstruct high-resolution HRTFs from sparse input\nmeasurements with significantly improved accuracy. To enhance spatial\ncoherence, we introduce a neighbor dissimilarity loss that promotes magnitude\nsmoothness, yielding more realistic upsampling. We evaluate our method using\nboth perceptual localization models and objective spectral distortion metrics.\nExperiments show that our model surpasses leading methods by a substantial\nmargin in generating realistic, high-fidelity HRTFs.",
      "authors": [
        "Xuyi Hu",
        "Jian Li",
        "Shaojie Zhang",
        "Stefan Goetz",
        "Lorenzo Picinali",
        "Ozgur B. Akan",
        "Aidan O. T. Hogg"
      ],
      "published": "2025-10-02T10:59:21Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01891v1"
    },
    {
      "arxiv_id": "2510.01889v1",
      "title": "Small is Sufficient: Reducing the World AI Energy Consumption Through\n  Model Selection",
      "summary": "The energy consumption and carbon footprint of Artificial Intelligence (AI)\nhave become critical concerns due to rising costs and environmental impacts. In\nresponse, a new trend in green AI is emerging, shifting from the \"bigger is\nbetter\" paradigm, which prioritizes large models, to \"small is sufficient\",\nemphasizing energy sobriety through smaller, more efficient models.\n  We explore how the AI community can adopt energy sobriety today by focusing\non model selection during inference. Model selection consists of choosing the\nmost appropriate model for a given task, a simple and readily applicable\nmethod, unlike approaches requiring new hardware or architectures. Our\nhypothesis is that, as in many industrial activities, marginal utility gains\ndecrease with increasing model size. Thus, applying model selection can\nsignificantly reduce energy consumption while maintaining good utility for AI\ninference.\n  We conduct a systematic study of AI tasks, analyzing their popularity, model\nsize, and efficiency. We examine how the maturity of different tasks and model\nadoption patterns impact the achievable energy savings, ranging from 1% to 98%\nfor different tasks. Our estimates indicate that applying model selection could\nreduce AI energy consumption by 27.8%, saving 31.9 TWh worldwide in 2025 -\nequivalent to the annual output of five nuclear power reactors.",
      "authors": [
        "Tiago da Silva Barros",
        "Frédéric Giroire",
        "Ramon Aparicio-Pardo",
        "Joanna Moulierac"
      ],
      "published": "2025-10-02T10:58:13Z",
      "primary_category": "cs.CY",
      "arxiv_url": "https://arxiv.org/abs/2510.01889v1"
    },
    {
      "arxiv_id": "2510.01887v1",
      "title": "FINCH: Financial Intelligence using Natural language for Contextualized\n  SQL Handling",
      "summary": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has long been a central challenge in NLP. While progress has been\nsignificant, applying it to the financial domain remains especially difficult\ndue to complex schema, domain-specific terminology, and high stakes of error.\nDespite this, there is no dedicated large-scale financial dataset to advance\nresearch, creating a critical gap. To address this, we introduce a curated\nfinancial dataset (FINCH) comprising 292 tables and 75,725 natural language-SQL\npairs, enabling both fine-tuning and rigorous evaluation. Building on this\nresource, we benchmark reasoning models and language models of varying scales,\nproviding a systematic analysis of their strengths and limitations in financial\nText-to-SQL tasks. Finally, we propose a finance-oriented evaluation metric\n(FINCH Score) that captures nuances overlooked by existing measures, offering a\nmore faithful assessment of model performance.",
      "authors": [
        "Avinash Kumar Singh",
        "Bhaskarjit Sarmah",
        "Stefano Pasquali"
      ],
      "published": "2025-10-02T10:55:11Z",
      "primary_category": "q-fin.CP",
      "arxiv_url": "https://arxiv.org/abs/2510.01887v1"
    },
    {
      "arxiv_id": "2510.01879v1",
      "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration",
      "summary": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs.",
      "authors": [
        "Yisu Wang",
        "Ming Wang",
        "Haoyuan Song",
        "Wenjie Huang",
        "Chaozheng Wang",
        "Yi Xie",
        "Xuming Ran"
      ],
      "published": "2025-10-02T10:35:39Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01879v1"
    },
    {
      "arxiv_id": "2510.01869v1",
      "title": "TACOS: Task Agnostic COordinator of a multi-drone System",
      "summary": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module.",
      "authors": [
        "Alessandro Nazzari",
        "Roberto Rubinacci",
        "Marco Lovera"
      ],
      "published": "2025-10-02T10:21:35Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01869v1"
    },
    {
      "arxiv_id": "2510.01864v1",
      "title": "A Modular Theory of Subjective Consciousness for Natural and Artificial\n  Minds",
      "summary": "Understanding how subjective experience arises from information processing\nremains a central challenge in neuroscience, cognitive science, and AI\nresearch. The Modular Consciousness Theory (MCT) proposes a biologically\ngrounded and computationally explicit framework in which consciousness is a\ndiscrete sequence of Integrated Informational States (IISs). Each IIS is a\npacket of integrated information tagged with a multidimensional density vector\nthat quantifies informational richness. Its magnitude correlates with\nsubjective intensity, shaping memory, behavior, and continuity of experience.\nInputs from body and environment are adaptively filtered, processed by modules\n(abstraction, narration, evaluation, self-evaluation), and integrated into an\nIIS. The resulting packet, tagged with its density vector, is transmitted to\nbehavioral readiness, memory, and decision-making modules, closing the loop.\nThis explains why strongly tagged states exert greater influence on long-term\nmemory and action. Unlike Global Workspace Theory, Integrated Information\nTheory, or Higher-Order Thought, MCT specifies a full computational pipeline\nproducing discrete informational units with quantifiable internal structure.\nSubjectivity is reframed as a correlate of the density-tagging signal with\nfunctional consequences. MCT generates testable predictions, such as stress\nenhancing memory encoding, and provides a naturalistic blueprint for both\nbiological and artificial architectures. Consciousness, in this view, is not an\nirreducible essence but an evolvable, quantifiable, and constructible feature\nof complex information processing.",
      "authors": [
        "Michaël Gillon"
      ],
      "published": "2025-10-02T10:11:56Z",
      "primary_category": "q-bio.NC",
      "arxiv_url": "https://arxiv.org/abs/2510.01864v1"
    },
    {
      "arxiv_id": "2510.01857v1",
      "title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via\n  Inverse Reinforcement Learning",
      "summary": "We reframe and operationalise adversarial inverse reinforcement learning\n(IRL) to large language model reasoning, learning a dense, token-level reward\nmodel for process supervision directly from expert demonstrations rather than\nimitating style via supervised fine-tuning. The learned reasoning reward serves\ntwo complementary roles: (i) it provides step-level feedback to optimise a\nreasoning policy during training; and (ii) it functions at inference as a\ncritic to rerank sampled traces under fixed compute budgets. We demonstrate\nthat our approach prioritises correctness over surface form, yielding scores\nthat correlate with eventual answer validity and enabling interpretable\nlocalisation of errors within a trace. Empirically, on GSM8K with Llama3 and\nQwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a\nlearning signal to elicit reasoning, and (ii) predictive performance is\nimproved from reward-guided reranking (notably for Llama-based policies). By\nunifying training signals, inference-time selection, and token-level\ndiagnostics into a single reasoning reward, this work suggests reusable\nprocess-level rewards with broad potential to enhance multi-step reasoning in\nlanguage models.",
      "authors": [
        "Claudio Fanconi",
        "Nicolás Astorga",
        "Mihaela van der Schaar"
      ],
      "published": "2025-10-02T09:55:26Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01857v1"
    },
    {
      "arxiv_id": "2510.01850v1",
      "title": "NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset\n  for Narrowband Powerline Communications",
      "summary": "Capturing comprehensive statistics of nonperiodic asynchronous impulsive\nnoise is a critical issue in enhancing impulse noise processing for narrowband\npowerline communication (NB-PLC) transceivers. However, existing mathematical\nnoise generative models capture only some of the characteristics of additive\nnoise. Therefore, we propose a generative adversarial network (GAN), called the\nnoise-generation GAN (NGGAN), that learns the complicated characteristics of\npractically measured noise samples for data augmentation. To closely match the\nstatistics of complicated noise in NB-PLC systems, we measured the NB-PLC noise\nvia the analog coupling and bandpass filtering circuits of a commercial NB-PLC\nmodem to build a realistic dataset. Specifically, the NGGAN design approaches\nbased on the practically measured dataset are as follows: (i) we design the\nlength of input signals that the NGGAN model can fit to facilitate\ncyclo-stationary noise generation. (ii) Wasserstein distance is used as a loss\nfunction to enhance the similarity between the generated noise and the training\ndataset and ensure that the sample diversity is sufficient for various\napplications. (iii) To measure the similarity performance of the GAN-based\nmodels based on mathematical and practically measured datasets, we perform\nquantitative and qualitative analyses. The training datasets include (1) a\npiecewise spectral cyclo-stationary Gaussian model (PSCGM), (2) a\nfrequency-shift (FRESH) filter, and (3) practical measurements from NB-PLC\nsystems. Simulation results demonstrate that the proposed NGGAN trained using\nwaveform characteristics is closer to the practically measured dataset in terms\nof the quality of the generated noise.",
      "authors": [
        "Ying-Ren Chien",
        "Po-Heng Chou",
        "You-Jie Peng",
        "Chun-Yuan Huang",
        "Hen-Wai Tsao",
        "Yu Tsao"
      ],
      "published": "2025-10-02T09:47:56Z",
      "primary_category": "eess.SP",
      "arxiv_url": "https://arxiv.org/abs/2510.01850v1"
    },
    {
      "arxiv_id": "2510.01842v1",
      "title": "Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model\n  Selection and Benchmarking for Tabular datasets",
      "summary": "The field of AutoML has made remarkable progress in post-hoc model selection,\nwith libraries capable of automatically identifying the most performing models\nfor a given dataset. Nevertheless, these methods often rely on exhaustive\nhyperparameter searches, where methods automatically train and test different\ntypes of models on the target dataset. Contrastingly, pre-hoc prediction\nemerges as a promising alternative, capable of bypassing exhaustive search\nthrough intelligent pre-selection of models. Despite its potential, pre-hoc\nprediction remains under-explored in the literature. This paper explores the\nintersection of AutoML and pre-hoc model selection by leveraging traditional\nmodels and Large Language Model (LLM) agents to reduce the search space of\nAutoML libraries. By relying on dataset descriptions and statistical\ninformation, we reduce the AutoML search space. Our methodology is applied to\nthe AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark\ncontaining 175 tabular classification datasets available on OpenML. The\nproposed approach offers a shift in AutoML workflows, significantly reducing\ncomputational overhead, while still selecting the best model for the given\ndataset.",
      "authors": [
        "Yannis Belkhiter",
        "Seshu Tirupathi",
        "Giulio Zizzo",
        "Sachin Sharma",
        "John D. Kelleher"
      ],
      "published": "2025-10-02T09:37:12Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01842v1"
    },
    {
      "arxiv_id": "2510.01833v1",
      "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for\n  LLM Reasoning",
      "summary": "Large language models (LLMs) have demonstrated remarkable reasoning abilities\nin complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,\ndue to their autoregressive token-level generation, the reasoning process is\nlargely constrained to local decision-making and lacks global planning. This\nlimitation frequently results in redundant, incoherent, or inaccurate\nreasoning, which significantly degrades overall performance. Existing\napproaches, such as tree-based algorithms and reinforcement learning (RL),\nattempt to address this issue but suffer from high computational costs and\noften fail to produce optimal reasoning trajectories. To tackle this challenge,\nwe propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy\nOptimization PTA-GRPO, a two-stage framework designed to improve both\nhigh-level planning and fine-grained CoT reasoning. In the first stage, we\nleverage advanced LLMs to distill CoT into compact high-level guidance, which\nis then used for supervised fine-tuning (SFT). In the second stage, we\nintroduce a guidance-aware RL method that jointly optimizes the final output\nand the quality of high-level guidance, thereby enhancing reasoning\neffectiveness. We conduct extensive experiments on multiple mathematical\nreasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across\ndiverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and\nLLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently\nachieves stable and significant improvements across different models and tasks,\nvalidating its effectiveness and generalization.",
      "authors": [
        "Zhihao Dou",
        "Qinjian Zhao",
        "Zhongwei Wan",
        "Dinggen Zhang",
        "Weida Wang",
        "Towsif Raiyan",
        "Benteng Chen",
        "Qingtao Pan",
        "Yang Ouyang",
        "Zhiqiang Gao",
        "Shufei Zhang",
        "Sumon Biswas"
      ],
      "published": "2025-10-02T09:28:13Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01833v1"
    },
    {
      "arxiv_id": "2510.01815v1",
      "title": "Human-AI Teaming Co-Learning in Military Operations",
      "summary": "In a time of rapidly evolving military threats and increasingly complex\noperational environments, the integration of AI into military operations proves\nsignificant advantages. At the same time, this implies various challenges and\nrisks regarding building and deploying human-AI teaming systems in an effective\nand ethical manner. Currently, understanding and coping with them are often\ntackled from an external perspective considering the human-AI teaming system as\na collective agent. Nevertheless, zooming into the dynamics involved inside the\nsystem assures dealing with a broader palette of relevant multidimensional\nresponsibility, safety, and robustness aspects. To this end, this research\nproposes the design of a trustworthy co-learning model for human-AI teaming in\nmilitary operations that encompasses a continuous and bidirectional exchange of\ninsights between the human and AI agents as they jointly adapt to evolving\nbattlefield conditions. It does that by integrating four dimensions. First,\nadjustable autonomy for dynamically calibrating the autonomy levels of agents\ndepending on aspects like mission state, system confidence, and environmental\nuncertainty. Second, multi-layered control which accounts continuous oversight,\nmonitoring of activities, and accountability. Third, bidirectional feedback\nwith explicit and implicit feedback loops between the agents to assure a proper\ncommunication of reasoning, uncertainties, and learned adaptations that each of\nthe agents has. And fourth, collaborative decision-making which implies the\ngeneration, evaluation, and proposal of decisions associated with confidence\nlevels and rationale behind them. The model proposed is accompanied by concrete\nexemplifications and recommendations that contribute to further developing\nresponsible and trustworthy human-AI teaming systems in military operations.",
      "authors": [
        "Clara Maathuis",
        "Kasper Cools"
      ],
      "published": "2025-10-02T09:01:01Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01815v1"
    },
    {
      "arxiv_id": "2510.01812v1",
      "title": "SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment",
      "summary": "Singing voice generation progresses rapidly, yet evaluating singing quality\nremains a critical challenge. Human subjective assessment, typically in the\nform of listening tests, is costly and time consuming, while existing objective\nmetrics capture only limited perceptual aspects. In this work, we introduce\nSingMOS-Pro, a dataset for automatic singing quality assessment. Building on\nour preview version SingMOS, which provides only overall ratings, SingMOS-Pro\nexpands annotations of the additional part to include lyrics, melody, and\noverall quality, offering broader coverage and greater diversity. The dataset\ncontains 7,981 singing clips generated by 41 models across 12 datasets,\nspanning from early systems to recent advances. Each clip receives at least\nfive ratings from professional annotators, ensuring reliability and\nconsistency. Furthermore, we explore how to effectively utilize MOS data\nannotated under different standards and benchmark several widely used\nevaluation methods from related tasks on SingMOS-Pro, establishing strong\nbaselines and practical references for future research. The dataset can be\naccessed at https://huggingface.co/datasets/TangRain/SingMOS-Pro.",
      "authors": [
        "Yuxun Tang",
        "Lan Liu",
        "Wenhao Feng",
        "Yiwen Zhao",
        "Jionghao Han",
        "Yifeng Yu",
        "Jiatong Shi",
        "Qin Jin"
      ],
      "published": "2025-10-02T08:53:49Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01812v1"
    },
    {
      "arxiv_id": "2510.01800v1",
      "title": "REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing",
      "summary": "Academic regulation advising is essential for helping students interpret and\ncomply with institutional policies, yet building effective systems requires\ndomain specific regulatory resources. To address this challenge, we propose\nREBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval\nreasoning framework that integrates retrieval augmented generation with graph\nbased reasoning. CatRAG unifies dense retrieval and graph reasoning, supported\nby a hierarchical, category labeled knowledge graph enriched with semantic\nfeatures for domain alignment. A lightweight intent classifier routes queries\nto the appropriate retrieval modules, ensuring both factual accuracy and\ncontextual depth. We construct a regulation specific dataset and evaluate REBot\non classification and question answering tasks, achieving state of the art\nperformance with an F1 score of 98.89%. Finally, we implement a web application\nthat demonstrates the practical value of REBot in real world academic advising\nscenarios.",
      "authors": [
        "Thanh Ma",
        "Tri-Tam La",
        "Lam-Thu Le Huu",
        "Minh-Nghi Nguyen",
        "Khanh-Van Pham Luu",
        "Huu-Hoa Nguyen"
      ],
      "published": "2025-10-02T08:40:55Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01800v1"
    },
    {
      "arxiv_id": "2510.01796v1",
      "title": "Rethinking the shape convention of an MLP",
      "summary": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow\ndesign where skip connections operate at the input/output dimensions while\nprocessing occurs in expanded hidden spaces. We challenge this convention by\nproposing wide-narrow-wide (Hourglass) MLP blocks where skip connections\noperate at expanded dimensions while residual computation flows through narrow\nbottlenecks. This inversion leverages higher-dimensional spaces for incremental\nrefinement while maintaining computational efficiency through parameter-matched\ndesigns. Implementing Hourglass MLPs requires an initial projection to lift\ninput signals to expanded dimensions. We propose that this projection can\nremain fixed at random initialization throughout training, enabling efficient\ntraining and inference implementations. We evaluate both architectures on\ngenerative tasks over popular image datasets, characterizing\nperformance-parameter Pareto frontiers through systematic architectural search.\nResults show that Hourglass architectures consistently achieve superior Pareto\nfrontiers compared to conventional designs. As parameter budgets increase,\noptimal Hourglass configurations favor deeper networks with wider skip\nconnections and narrower bottlenecks-a scaling pattern distinct from\nconventional MLPs. Our findings suggest reconsidering skip connection placement\nin modern architectures, with potential applications extending to Transformers\nand other residual networks.",
      "authors": [
        "Meng-Hsi Chen",
        "Yu-Ang Lee",
        "Feng-Ting Liao",
        "Da-shan Shiu"
      ],
      "published": "2025-10-02T08:38:15Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01796v1"
    },
    {
      "arxiv_id": "2510.01795v1",
      "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language\n  Models in Autonomous Driving",
      "summary": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving\nfor unified perception and reasoning, but high inference latency hinders\nreal-time deployment. Early-exit reduces latency by terminating inference at\nintermediate layers, yet its task-dependent nature limits generalization across\ndiverse scenarios. We observe that this limitation aligns with autonomous\ndriving: navigation systems can anticipate upcoming contexts (e.g.,\nintersections, traffic lights), indicating which tasks will be required. We\npropose Nav-EE, a navigation-guided early-exit framework that precomputes\ntask-specific exit layers offline and dynamically applies them online based on\nnavigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE\nachieves accuracy comparable to full inference while reducing latency by up to\n63.9%. Real-vehicle integration with Autoware Universe further demonstrates\nreduced inference latency (600ms to 300ms), supporting faster decision-making\nin complex scenarios. These results suggest that coupling navigation foresight\nwith early-exit offers a viable path toward efficient deployment of large\nmodels in autonomous systems. Code and data are available at our anonymous\nrepository: https://anonymous.4open.science/r/Nav-EE-BBC4",
      "authors": [
        "Haibo Hu",
        "Lianming Huang",
        "Xinyu Wang",
        "Yufei Cui",
        "Nan Guan",
        "Chun Jason Xue"
      ],
      "published": "2025-10-02T08:37:58Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01795v1"
    },
    {
      "arxiv_id": "2510.01792v1",
      "title": "Comparison of Unsupervised Metrics for Evaluating Judicial Decision\n  Extraction",
      "summary": "The rapid advancement of artificial intelligence in legal natural language\nprocessing demands scalable methods for evaluating text extraction from\njudicial decisions. This study evaluates 16 unsupervised metrics, including\nnovel formulations, to assess the quality of extracting seven semantic blocks\nfrom 1,000 anonymized Russian judicial decisions, validated against 7,168\nexpert reviews on a 1--5 Likert scale. These metrics, spanning document-based,\nsemantic, structural, pseudo-ground truth, and legal-specific categories,\noperate without pre-annotated ground truth. Bootstrapped correlations, Lin's\nconcordance correlation coefficient (CCC), and mean absolute error (MAE) reveal\nthat Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =\n0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =\n0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density\n(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative\ncorrelations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin\nCCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using\ngpt-4.1-mini via g4f, suggests limited specialization for legal textse. These\nfindings highlight that unsupervised metrics, including LLM-based approaches,\nenable scalable screening but, with moderate correlations and low CCC values,\ncannot fully replace human judgment in high-stakes legal contexts. This work\nadvances legal NLP by providing annotation-free evaluation tools, with\nimplications for judicial analytics and ethical AI deployment.",
      "authors": [
        "Ivan Leonidovich Litvak",
        "Anton Kostin",
        "Fedor Lashkin",
        "Tatiana Maksiyan",
        "Sergey Lagutin"
      ],
      "published": "2025-10-02T08:32:16Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01792v1"
    },
    {
      "arxiv_id": "2510.01784v1",
      "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation",
      "summary": "Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models.",
      "authors": [
        "Xiaofei Wu",
        "Guozhen Zhang",
        "Zhiyong Xu",
        "Yuan Zhou",
        "Qinglin Lu",
        "Xuming He"
      ],
      "published": "2025-10-02T08:22:46Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01784v1"
    },
    {
      "arxiv_id": "2510.01782v1",
      "title": "Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware\n  Refusal in Factual Tasks",
      "summary": "Large Language Models (LLMs) should refuse to answer questions beyond their\nknowledge. This capability, which we term knowledge-aware refusal, is crucial\nfor factual reliability. However, existing metrics fail to faithfully measure\nthis ability. On the one hand, simple refusal-based metrics are biased by\nrefusal rates and yield inconsistent scores when models exhibit different\nrefusal tendencies. On the other hand, existing calibration metrics are\nproxy-based, capturing the performance of auxiliary calibration processes\nrather than the model's actual refusal behavior. In this work, we propose the\nRefusal Index (RI), a principled metric that measures how accurately LLMs\nrefuse questions they do not know. We define RI as Spearman's rank correlation\nbetween refusal probability and error probability. To make RI practically\nmeasurable, we design a lightweight two-pass evaluation method that efficiently\nestimates RI from observed refusal rates across two standard evaluation runs.\nExtensive experiments across 16 models and 5 datasets demonstrate that RI\naccurately quantifies a model's intrinsic knowledge-aware refusal capability in\nfactual tasks. Notably, RI remains stable across different refusal rates and\nprovides consistent model rankings independent of a model's overall accuracy\nand refusal rates. More importantly, RI provides insight into an important but\npreviously overlooked aspect of LLM factuality: while LLMs achieve high\naccuracy on factual tasks, their refusal behavior can be unreliable and\nfragile. This finding highlights the need to complement traditional accuracy\nmetrics with the Refusal Index for comprehensive factuality evaluation.",
      "authors": [
        "Wenbo Pan",
        "Jie Xu",
        "Qiguang Chen",
        "Junhao Dong",
        "Libo Qin",
        "Xinfeng Li",
        "Haining Yu",
        "Xiaohua Jia"
      ],
      "published": "2025-10-02T08:20:36Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01782v1"
    },
    {
      "arxiv_id": "2510.01780v1",
      "title": "Secure Multi-Modal Data Fusion in Federated Digital Health Systems via\n  MCP",
      "summary": "Secure and interoperable integration of heterogeneous medical data remains a\ngrand challenge in digital health. Current federated learning (FL) frameworks\noffer privacy-preserving model training but lack standardized mechanisms to\norchestrate multi-modal data fusion across distributed and resource-constrained\nenvironments. This study introduces a novel framework that leverages the Model\nContext Protocol (MCP) as an interoperability layer for secure, cross-agent\ncommunication in multi-modal federated healthcare systems. The proposed\narchitecture unifies three pillars: (i) multi-modal feature alignment for\nclinical imaging, electronic medical records, and wearable IoT data; (ii)\nsecure aggregation with differential privacy to protect patient-sensitive\nupdates; and (iii) energy-aware scheduling to mitigate dropouts in mobile\nclients. By employing MCP as a schema-driven interface, the framework enables\nadaptive orchestration of AI agents and toolchains while ensuring compliance\nwith privacy regulations. Experimental evaluation on benchmark datasets and\npilot clinical cohorts demonstrates up to 9.8\\% improvement in diagnostic\naccuracy compared with baseline FL, a 54\\% reduction in client dropout rates,\nand clinically acceptable privacy--utility trade-offs. These results highlight\nMCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward\nequitable, next-generation federated health infrastructures.",
      "authors": [
        "Aueaphum Aueawatthanaphisut"
      ],
      "published": "2025-10-02T08:19:56Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01780v1"
    },
    {
      "arxiv_id": "2510.01758v1",
      "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in\n  Vision Tasks",
      "summary": "Latent representations are critical for the performance and robustness of\nmachine learning models, as they encode the essential features of data in a\ncompact and informative manner. However, in vision tasks, these representations\nare often affected by noisy or irrelevant features, which can degrade the\nmodel's performance and generalization capabilities. This paper presents a\nnovel approach for enhancing latent representations using unsupervised Dynamic\nFeature Selection (DFS). For each instance, the proposed method identifies and\nremoves misleading or redundant information in images, ensuring that only the\nmost relevant features contribute to the latent space. By leveraging an\nunsupervised framework, our approach avoids reliance on labeled data, making it\nbroadly applicable across various domains and datasets. Experiments conducted\non image datasets demonstrate that models equipped with unsupervised DFS\nachieve significant improvements in generalization performance across various\ntasks, including clustering and image generation, while incurring a minimal\nincrease in the computational cost.",
      "authors": [
        "Bruno Corcuera",
        "Carlos Eiras-Franco",
        "Brais Cancela"
      ],
      "published": "2025-10-02T07:46:59Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01758v1"
    },
    {
      "arxiv_id": "2510.01751v1",
      "title": "A cybersecurity AI agent selection and decision support framework",
      "summary": "This paper presents a novel, structured decision support framework that\nsystematically aligns diverse artificial intelligence (AI) agent architectures,\nreactive, cognitive, hybrid, and learning, with the comprehensive National\nInstitute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.\nBy integrating agent theory with industry guidelines, this framework provides a\ntransparent and stepwise methodology for selecting and deploying AI solutions\nto address contemporary cyber threats. Employing a granular decomposition of\nNIST CSF 2.0 functions into specific tasks, the study links essential AI agent\nproperties such as autonomy, adaptive learning, and real-time responsiveness to\neach subcategory's security requirements. In addition, it outlines graduated\nlevels of autonomy (assisted, augmented, and fully autonomous) to accommodate\norganisations at varying stages of cybersecurity maturity. This holistic\napproach transcends isolated AI applications, providing a unified detection,\nincident response, and governance strategy. Through conceptual validation, the\nframework demonstrates how tailored AI agent deployments can align with\nreal-world constraints and risk profiles, enhancing situational awareness,\naccelerating response times, and fortifying long-term resilience via adaptive\nrisk management. Ultimately, this research bridges the gap between theoretical\nAI constructs and operational cybersecurity demands, establishing a foundation\nfor robust, empirically validated multi-agent systems that adhere to industry\nstandards.",
      "authors": [
        "Masike Malatji"
      ],
      "published": "2025-10-02T07:38:21Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01751v1"
    },
    {
      "arxiv_id": "2510.01736v1",
      "title": "Machine-interpretable Engineering Design Standards for Valve\n  Specification",
      "summary": "Engineering design processes use technical specifications and must comply\nwith standards. Product specifications, product type data sheets, and design\nstandards are still mainly document-centric despite the ambition to digitalize\nindustrial work. In this paper, we demonstrate how to transform information\nheld in engineering design standards into modular, reusable,\nmachine-interpretable ontologies and use the ontologies in quality assurance of\nthe plant design and equipment selection process. We use modelling patterns to\ncreate modular ontologies for knowledge captured in the text and in frequently\nreferenced tables in International Standards for piping, material and valve\ndesign. These modules are exchangeable, as stored in a W3C compliant format,\nand interoperable as they are aligned with the top-level ontology ISO DIS\n23726-3: Industrial Data Ontology (IDO).\n  We test these ontologies, created based on international material and piping\nstandards and industry norms, on a valve selection process. Valves are\ninstantiated in semantic asset models as individuals along with a semantic\nrepresentation of the environmental condition at their location on the asset.\nWe create \"functional location tags\" as OWL individuals that become instances\nof OWL class Valve Data Sheet (VDS) specified valves. Similarly we create\ninstances of manufacturer product type. Our approach enables automated\nvalidation that a specific VDS is compliant with relevant industry standards.\nUsing semantic reasoning and executable design rules, we also determine whether\nthe product type meets the valve specification. Creation of shared, reusable\nIDO-based modular ontologies for design standards enables semantic reasoning to\nbe applied to equipment selection processes and demonstrates the potential of\nthis approach for Standards Bodies wanting to transition to digitized Smart\nStandards.",
      "authors": [
        "Anders Gjerver",
        "Rune Frostad",
        "Vedrana Barisic",
        "Melinda Hodkiewicz",
        "Caitlin Woods",
        "Mihaly Fekete",
        "Arild Braathen Torjusen",
        "Johan Wilhelm Kluwer"
      ],
      "published": "2025-10-02T07:20:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01736v1"
    },
    {
      "arxiv_id": "2510.01724v1",
      "title": "MetaboT: AI-based agent for natural language-based interaction with\n  metabolomics knowledge graphs",
      "summary": "Mass spectrometry metabolomics generates vast amounts of data requiring\nadvanced methods for interpretation. Knowledge graphs address these challenges\nby structuring mass spectrometry data, metabolite information, and their\nrelationships into a connected network (Gaudry et al. 2024). However, effective\nuse of a knowledge graph demands an in-depth understanding of its ontology and\nits query language syntax. To overcome this, we designed MetaboT, an AI system\nutilizing large language models (LLMs) to translate user questions into SPARQL\nsemantic query language for operating on knowledge graphs (Steve Harris 2013).\nWe demonstrate its effectiveness using the Experimental Natural Products\nKnowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural\nproducts (Gaudry et al. 2024).MetaboT employs specialized AI agents for\nhandling user queries and interacting with the knowledge graph by breaking down\ncomplex tasks into discrete components, each managed by a specialised agent\n(Fig. 1a). The multi-agent system is constructed using the LangChain and\nLangGraph libraries, which facilitate the integration of LLMs with external\ntools and information sources (LangChain, n.d.). The query generation process\nfollows a structured workflow. First, the Entry Agent determines if the\nquestion is new or a follow-up to previous interactions. New questions are\nforwarded to the Validator Agent, which verifies if the question is related to\nthe knowledge graph. Then, the valid question is sent to the Supervisor Agent,\nwhich identifies if the question requires chemical conversions or standardized\nidentifiers. In this case it delegates the question to the Knowledge Graph\nAgent, which can use tools to extract necessary details, such as URIs or\ntaxonomies of chemical names, from the user query. Finally, an agent\nresponsible for crafting the SPARQL queries equipped with the ontology of the\nknowledge graph uses the provided identifiers to generate the query. Then, the\nsystem executes the generated query against the metabolomics knowledge graph\nand returns structured results to the user (Fig. 1b). To assess the performance\nof MetaboT we have curated 50 metabolomics-related questions and their expected\nanswers. In addition to submitting these questions to MetaboT, we evaluated a\nbaseline by submitting them to a standard LLM (GPT-4o) with a prompt that\nincorporated the knowledge graph ontology but did not provide specific entity\nIDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,\nunderscoring the necessity of our multi-agent system for accurately retrieving\nentities and generating correct SPARQL queries. MetaboT demonstrates promising\nperformance as a conversational question-answering assistant, enabling\nresearchers to retrieve structured metabolomics data through natural language\nqueries. By automating the generation and execution of SPARQL queries, it\nremoves technical barriers that have traditionally hindered access to knowledge\ngraphs. Importantly, MetaboT leverages the capabilities of LLMs while\nmaintaining experimentally grounded query generation, ensuring that outputs\nremain aligned with domain-specific standards and data structures. This\napproach facilitates data-driven discoveries by bridging the gap between\ncomplex semantic technologies and user-friendly interaction. MetaboT is\naccessible at [https://metabot.holobiomicslab.eu/], and its source code is\navailable at [https://github.com/HolobiomicsLab/MetaboT].",
      "authors": [
        "Madina Bekbergenova",
        "Lucas Pradi",
        "Benjamin Navet",
        "Emma Tysinger",
        "Franck Michel",
        "Matthieu Feraud",
        "Yousouf Taghzouti",
        "Yan Zhou Chen",
        "Olivier Kirchhoffer",
        "Florence Mehl",
        "Martin Legrand",
        "Tao Jiang",
        "Marco Pagni",
        "Soha Hassoun",
        "Jean-Luc Wolfender",
        "Wout Bittremieux",
        "Fabien Gandon",
        "Louis-Félix Nothias"
      ],
      "published": "2025-10-02T07:05:29Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01724v1"
    },
    {
      "arxiv_id": "2510.01722v1",
      "title": "Emotional Text-To-Speech Based on Mutual-Information-Guided\n  Emotion-Timbre Disentanglement",
      "summary": "Current emotional Text-To-Speech (TTS) and style transfer methods rely on\nreference encoders to control global style or emotion vectors, but do not\ncapture nuanced acoustic details of the reference speech. To this end, we\npropose a novel emotional TTS method that enables fine-grained phoneme-level\nemotion embedding prediction while disentangling intrinsic attributes of the\nreference speech. The proposed method employs a style disentanglement method to\nguide two feature extractors, reducing mutual information between timbre and\nemotion features, and effectively separating distinct style components from the\nreference speech. Experimental results demonstrate that our method outperforms\nbaseline TTS systems in generating natural and emotionally rich speech. This\nwork highlights the potential of disentangled and fine-grained representations\nin advancing the quality and flexibility of emotional TTS systems.",
      "authors": [
        "Jianing Yang",
        "Sheng Li",
        "Takahiro Shinozaki",
        "Yuki Saito",
        "Hiroshi Saruwatari"
      ],
      "published": "2025-10-02T07:03:50Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01722v1"
    },
    {
      "arxiv_id": "2510.01717v1",
      "title": "Latency-aware Multimodal Federated Learning over UAV Networks",
      "summary": "This paper investigates federated multimodal learning (FML) assisted by\nunmanned aerial vehicles (UAVs) with a focus on minimizing system latency and\nproviding convergence analysis. In this framework, UAVs are distributed\nthroughout the network to collect data, participate in model training, and\ncollaborate with a base station (BS) to build a global model. By utilizing\nmultimodal sensing, the UAVs overcome the limitations of unimodal systems,\nenhancing model accuracy, generalization, and offering a more comprehensive\nunderstanding of the environment. The primary objective is to optimize FML\nsystem latency in UAV networks by jointly addressing UAV sensing scheduling,\npower control, trajectory planning, resource allocation, and BS resource\nmanagement. To address the computational complexity of our latency minimization\nproblem, we propose an efficient iterative optimization algorithm combining\nblock coordinate descent and successive convex approximation techniques, which\nprovides high-quality approximate solutions. We also present a theoretical\nconvergence analysis for the UAV-assisted FML framework under a non-convex loss\nfunction. Numerical experiments demonstrate that our FML framework outperforms\nexisting approaches in terms of system latency and model training performance\nunder different data settings.",
      "authors": [
        "Shaba Shaon",
        "Dinh C. Nguyen"
      ],
      "published": "2025-10-02T06:57:44Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01717v1"
    },
    {
      "arxiv_id": "2510.01715v1",
      "title": "PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal\n  Positional Encoding and Reinforcement Learning",
      "summary": "Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based\nalgorithm, enabling AI-driven artistic image synthesis. However, existing CNN\nand transformer-based models struggle to scale efficiently to complex styles\nand high-resolution inputs. We introduce PyramidStyler, a transformer framework\nwith Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding\nthat captures both local details and global context while reducing\ncomputational load. We further incorporate reinforcement learning to\ndynamically optimize stylization, accelerating convergence. Trained on\nMicrosoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to\n2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s\ninference--and yields further improvements (content 2.03; style 0.75) with\nminimal speed penalty (1.40 s) when using RL. These results demonstrate\nreal-time, high-quality artistic rendering, with broad applications in media\nand design.",
      "authors": [
        "Raahul Krishna Durairaju",
        "K. Saruladha"
      ],
      "published": "2025-10-02T06:54:52Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01715v1"
    },
    {
      "arxiv_id": "2510.01708v1",
      "title": "PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via\n  Multi-Simulator Dynamics Randomization",
      "summary": "Humanoid whole-body control (WBC) policies trained in simulation often suffer\nfrom the sim-to-real gap, which fundamentally arises from simulator inductive\nbias, the inherent assumptions and limitations of any single simulator. These\nbiases lead to nontrivial discrepancies both across simulators and between\nsimulation and the real world. To mitigate the effect of simulator inductive\nbias, the key idea is to train policies jointly across multiple simulators,\nencouraging the learned controller to capture dynamics that generalize beyond\nany single simulator's assumptions. We thus introduce PolySim, a WBC training\nplatform that integrates multiple heterogeneous simulators. PolySim can launch\nparallel environments from different engines simultaneously within a single\ntraining run, thereby realizing dynamics-level domain randomization.\nTheoretically, we show that PolySim yields a tighter upper bound on simulator\ninductive bias than single-simulator training. In experiments, PolySim\nsubstantially reduces motion-tracking error in sim-to-sim evaluations; for\nexample, on MuJoCo, it improves execution success by 52.8 over an IsaacSim\nbaseline. PolySim further enables zero-shot deployment on a real Unitree G1\nwithout additional fine-tuning, showing effective transfer from simulation to\nthe real world. We will release the PolySim code upon acceptance of this work.",
      "authors": [
        "Zixing Lei",
        "Zibo Zhou",
        "Sheng Yin",
        "Yueru Chen",
        "Qingyao Xu",
        "Weixin Li",
        "Yunhong Wang",
        "Bowei Tang",
        "Wei Jing",
        "Siheng Chen"
      ],
      "published": "2025-10-02T06:31:42Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01708v1"
    },
    {
      "arxiv_id": "2510.01706v1",
      "title": "Representational Alignment Across Model Layers and Brain Regions with\n  Hierarchical Optimal Transport",
      "summary": "Standard representational similarity methods align each layer of a network to\nits best match in another independently, producing asymmetric results, lacking\na global alignment score, and struggling with networks of different depths.\nThese limitations arise from ignoring global activation structure and\nrestricting mappings to rigid one-to-one layer correspondences. We propose\nHierarchical Optimal Transport (HOT), a unified framework that jointly infers\nsoft, globally consistent layer-to-layer couplings and neuron-level transport\nplans. HOT allows source neurons to distribute mass across multiple target\nlayers while minimizing total transport cost under marginal constraints. This\nyields both a single alignment score for the entire network comparison and a\nsoft transport plan that naturally handles depth mismatches through mass\ndistribution. We evaluate HOT on vision models, large language models, and\nhuman visual cortex recordings. Across all domains, HOT matches or surpasses\nstandard pairwise matching in alignment quality. Moreover, it reveals smooth,\nfine-grained hierarchical correspondences: early layers map to early layers,\ndeeper layers maintain relative positions, and depth mismatches are resolved by\ndistributing representations across multiple layers. These structured patterns\nemerge naturally from global optimization without being imposed, yet are absent\nin greedy layer-wise methods. HOT thus enables richer, more interpretable\ncomparisons between representations, particularly when networks differ in\narchitecture or depth.",
      "authors": [
        "Shaan Shah",
        "Meenakshi Khosla"
      ],
      "published": "2025-10-02T06:25:06Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01706v1"
    },
    {
      "arxiv_id": "2510.01704v1",
      "title": "Holistic Order Prediction in Natural Scenes",
      "summary": "Even in controlled settings, understanding instance-wise geometries is a\nchallenging task for a wide range of visual models. Although specialized\nsystems exist, modern arts rely on expensive input formats (category labels,\nbinary segmentation masks) and inference costs (a quadratic amount of forward\npasses). We mitigate these limitations by proposing InstaFormer, a network\ncapable of holistic order prediction. That is, solely given an input RGB image,\nInstaFormer returns the full occlusion and depth orderings for all the\ninstances in the scene in a single forward pass. At its core, InstaFormer\nrelies on interactions between object queries and latent mask descriptors that\nsemantically represent the same objects while carrying complementary\ninformation. We comprehensively benchmark and ablate our approach to highlight\nits effectiveness. Our code and models are open-source and available at this\nURL: https://github.com/SNU-VGILab/InstaOrder.",
      "authors": [
        "Pierre Musacchio",
        "Hyunmin Lee",
        "Jaesik Park"
      ],
      "published": "2025-10-02T06:24:12Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01704v1"
    },
    {
      "arxiv_id": "2510.01700v1",
      "title": "VaPR -- Vision-language Preference alignment for Reasoning",
      "summary": "Preference finetuning methods like Direct Preference Optimization (DPO) with\nAI-generated feedback have shown promise in aligning Large Vision-Language\nModels (LVLMs) with human preferences. However, existing techniques overlook\nthe prevalence of noise in synthetic preference annotations in the form of\nstylistic and length biases. To this end, we introduce a hard-negative response\ngeneration framework based on LLM-guided response editing, that produces\nrejected responses with targeted errors, maintaining stylistic and length\nsimilarity to the accepted ones. Using this framework, we develop the VaPR\ndataset, comprising 30K high-quality samples, to finetune three LVLM families:\nLLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver\nsignificant performance improvements across ten benchmarks, achieving average\ngains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable\nimprovements on reasoning tasks. A scaling analysis shows that performance\nconsistently improves with data size, with LLaVA models benefiting even at\nsmaller scales. Moreover, VaPR reduces the tendency to answer \"Yes\" in binary\nquestions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we\nshow that the framework generalizes to open-source LLMs as editors, with models\ntrained on VaPR-OS achieving ~99% of the performance of models trained on\n\\name, which is synthesized using GPT-4o. Our data, models, and code can be\nfound on the project page https://vap-r.github.io",
      "authors": [
        "Rohan Wadhawan",
        "Fabrice Y Harel-Canada",
        "Zi-Yi Dou",
        "Suhaila Shakiah",
        "Robinson Piramuthu",
        "Nanyun Peng"
      ],
      "published": "2025-10-02T06:10:43Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01700v1"
    },
    {
      "arxiv_id": "2510.01688v1",
      "title": "Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation",
      "summary": "Recent advances in Large Language Models (LLMs) have brought significant\nimprovements to various service domains, including chatbots and medical\npre-consultation applications. In the healthcare domain, the most common\napproach for adapting LLMs to multi-turn dialogue generation is Supervised\nFine-Tuning (SFT). However, datasets for SFT in tasks like medical\npre-consultation typically exhibit a skewed turn-count distribution. Training\non such data induces a novel failure mechanism we term **Format Inertia**,\nwhere models tend to generate repetitive, format-correct, but diagnostically\nuninformative questions in long medical dialogues. To mitigate this observed\nfailure mechanism, we adopt a simple, data-centric method that rebalances the\nturn-count distribution of the training dataset. Experimental results show that\nour approach substantially alleviates Format Inertia in medical\npre-consultation.",
      "authors": [
        "Seungseop Lim",
        "Gibaeg Kim",
        "Wooseok Han",
        "Jean Seo",
        "Hyunkyung Lee",
        "Jaehyo Yoo",
        "Eunho Yang"
      ],
      "published": "2025-10-02T05:29:38Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01688v1"
    },
    {
      "arxiv_id": "2510.01687v1",
      "title": "Improving AGI Evaluation: A Data Science Perspective",
      "summary": "Evaluation of potential AGI systems and methods is difficult due to the\nbreadth of the engineering goal. We have no methods for perfect evaluation of\nthe end state, and instead measure performance on small tests designed to\nprovide directional indication that we are approaching AGI. In this work we\nargue that AGI evaluation methods have been dominated by a design philosophy\nthat uses our intuitions of what intelligence is to create synthetic tasks,\nthat have performed poorly in the history of AI. Instead we argue for an\nalternative design philosophy focused on evaluating robust task execution that\nseeks to demonstrate AGI through competence. This perspective is developed from\ncommon practices in data science that are used to show that a system can be\nreliably deployed. We provide practical examples of what this would mean for\nAGI evaluation.",
      "authors": [
        "John Hawkins"
      ],
      "published": "2025-10-02T05:27:29Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01687v1"
    },
    {
      "arxiv_id": "2510.01685v1",
      "title": "How Do Language Models Compose Functions?",
      "summary": "While large language models (LLMs) appear to be increasingly capable of\nsolving compositional tasks, it is an open question whether they do so using\ncompositional mechanisms. In this work, we investigate how feedforward LLMs\nsolve two-hop factual recall tasks, which can be expressed compositionally as\n$g(f(x))$. We first confirm that modern LLMs continue to suffer from the\n\"compositionality gap\": i.e. their ability to compute both $z = f(x)$ and $y =\ng(z)$ does not entail their ability to compute the composition $y = g(f(x))$.\nThen, using logit lens on their residual stream activations, we identify two\nprocessing mechanisms, one which solves tasks $\\textit{compositionally}$,\ncomputing $f(x)$ along the way to computing $g(f(x))$, and one which solves\nthem $\\textit{directly}$, without any detectable signature of the intermediate\nvariable $f(x)$. Finally, we find that which mechanism is employed appears to\nbe related to the embedding space geometry, with the idiomatic mechanism being\ndominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in\nthe embedding spaces. We fully release our data and code at:\nhttps://github.com/apoorvkh/composing-functions .",
      "authors": [
        "Apoorv Khandelwal",
        "Ellie Pavlick"
      ],
      "published": "2025-10-02T05:21:34Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01685v1"
    },
    {
      "arxiv_id": "2510.01681v1",
      "title": "Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning",
      "summary": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet they\nfrequently struggle with tasks requiring precise understanding and handling of\nfine-grained visual elements. This is mainly due to information loss during\nimage encoding or insufficient attention to critical regions. Recent work has\nshown promise by incorporating pixel-level visual information into the\nreasoning process, enabling VLMs to access high-resolution visual details\nduring their thought process. However, this pixel-level information is often\noverused, leading to inefficiency and distraction from irrelevant visual\ndetails. To address these challenges, we propose the first framework for\nadaptive pixel reasoning that dynamically determines necessary pixel-level\noperations based on the input query. Specifically, we first apply\noperation-aware supervised fine-tuning to establish baseline competence in\ntextual reasoning and visual operations, then design a novel rollout-guided\nreinforcement learning framework relying on feedback of the model's own\nresponses, which enables the VLM to determine when pixel operations should be\ninvoked based on query difficulty. Experiments on extensive multimodal\nreasoning benchmarks show that our model achieves superior performance while\nsignificantly reducing unnecessary visual operations. Impressively, our model\nachieves 73.4\\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of\nonly 20.1\\%, improving accuracy and simultaneously reducing tool usage by\n66.5\\% compared to the previous methods.",
      "authors": [
        "Xuchen Li",
        "Xuzhao Li",
        "Jiahui Gao",
        "Renjie Pi",
        "Shiyu Hu",
        "Wentao Zhang"
      ],
      "published": "2025-10-02T05:14:52Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01681v1"
    },
    {
      "arxiv_id": "2510.01674v1",
      "title": "FOR-Prompting: From Objection to Revision via an Asymmetric Prompting\n  Protocol",
      "summary": "Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)\norganize internal deliberation but lack an explicit mechanism for external\nquestioning that elicits self-revision. We present FOR-Prompting (From\nObjection to Revision Prompting), an asymmetric protocol where a Defender\nproposes an answer, an Objectioner raises question-style objections with no\ndirect fixes, and a Host enforces consistency and closure. On GSM8K we observe\nabout a 22% point gain over single-prompt and accuracy on par with CoT, with\nmore than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1\njudge. FOR-Prompting also corrects mistakes without tools or human supervision\non tricky queries, and improves performance for small-scale model (approx. 19%\naccuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for\nsmall models and on personal device use. Beyond factual QA, qualitative\nanalyses on open-ended tasks show enhanced exploration and refinement, with\ndialogue traces that make assumptions and trade-offs explicit. The protocol is\nmodel agnostic and operates purely at the prompt level through role-structured\nturns, so it works with hosted and local models of different sizes without\nretraining, and it supports large-scale study of objection-guided reasoning.",
      "authors": [
        "He Zhang",
        "Anzhou Zhang",
        "Jian Dai"
      ],
      "published": "2025-10-02T04:57:58Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01674v1"
    },
    {
      "arxiv_id": "2510.01671v1",
      "title": "A Locally Executable AI System for Improving Preoperative Patient\n  Communication: A Multi-Domain Clinical Evaluation",
      "summary": "Patients awaiting invasive procedures often have unanswered pre-procedural\nquestions; however, time-pressured workflows and privacy constraints limit\npersonalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave\nNo One Behind Architecture), a safety-first, local-first system that routes\ninputs with a high-precision sentence-transformer classifier and returns\nverbatim answers from a clinician-curated FAQ for clinical queries, eliminating\nfree-text generation in the clinical path. We evaluated two domains (tooth\nextraction and gastroscopy) using expert-reviewed validation sets\n(n=400/domain) for thresholding and independent test sets (n=200/domain). Among\nthe four encoders, E5-large-instruct (560M) achieved an overall accuracy of\n0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were\nstatistically indistinguishable from GPT-4o on this task; Gemini made no errors\non this test set. Energy logging shows that the non-generative clinical path\nconsumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local\n8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single\non-prem GPU. These results indicate that near-frontier discrimination and\ngeneration-induced errors are structurally avoided in the clinical path by\nreturning vetted FAQ answers verbatim, supporting privacy, sustainability, and\nequitable deployment in bandwidth-limited environments.",
      "authors": [
        "Motoki Sato",
        "Yuki Matsushita",
        "Hidekazu Takahashi",
        "Tomoaki Kakazu",
        "Sou Nagata",
        "Mizuho Ohnuma",
        "Atsushi Yoshikawa",
        "Masayuki Yamamura"
      ],
      "published": "2025-10-02T04:53:11Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01671v1"
    },
    {
      "arxiv_id": "2510.01670v1",
      "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
      "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.",
      "authors": [
        "Erfan Shayegani",
        "Keegan Hines",
        "Yue Dong",
        "Nael Abu-Ghazaleh",
        "Roman Lutz",
        "Spencer Whitehead",
        "Vidhisha Balachandran",
        "Besmira Nushi",
        "Vibhav Vineet"
      ],
      "published": "2025-10-02T04:52:15Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01670v1"
    },
    {
      "arxiv_id": "2510.01664v1",
      "title": "GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents",
      "summary": "This study demonstrates that GuruAgents, prompt-guided AI agents, can\nsystematically operationalize the strategies of legendary investment gurus. We\ndevelop five distinct GuruAgents, each designed to emulate an iconic investor,\nby encoding their distinct philosophies into LLM prompts that integrate\nfinancial tools and a deterministic reasoning pipeline. In a backtest on\nNASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique\nbehaviors driven by their prompted personas. The Buffett GuruAgent achieves the\nhighest performance, delivering a 42.2\\% CAGR that significantly outperforms\nbenchmarks, while other agents show varied results. These findings confirm that\nprompt engineering can successfully translate the qualitative philosophies of\ninvestment gurus into reproducible, quantitative strategies, highlighting a\nnovel direction for automated systematic investing. The source code and data\nare available at https://github.com/yejining99/GuruAgents.",
      "authors": [
        "Yejin Kim",
        "Youngbin Lee",
        "Juhyeong Kim",
        "Yongjae Lee"
      ],
      "published": "2025-10-02T04:45:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01664v1"
    },
    {
      "arxiv_id": "2510.01663v1",
      "title": "Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via\n  Shapley Value",
      "summary": "For many real-world applications, understanding feature-outcome relationships\nis as crucial as achieving high predictive accuracy. While traditional neural\nnetworks excel at prediction, their black-box nature obscures underlying\nfunctional relationships. Kolmogorov--Arnold Networks (KANs) address this by\nemploying learnable spline-based activation functions on edges, enabling\nrecovery of symbolic representations while maintaining competitive performance.\nHowever, KAN's architecture presents unique challenges for network pruning.\nConventional magnitude-based methods become unreliable due to sensitivity to\ninput coordinate shifts. We propose \\textbf{ShapKAN}, a pruning framework using\nShapley value attribution to assess node importance in a shift-invariant\nmanner. Unlike magnitude-based approaches, ShapKAN quantifies each node's\nactual contribution, ensuring consistent importance rankings regardless of\ninput parameterization. Extensive experiments on synthetic and real-world\ndatasets demonstrate that ShapKAN preserves true node importance while enabling\neffective network compression. Our approach improves KAN's interpretability\nadvantages, facilitating deployment in resource-constrained environments.",
      "authors": [
        "Wangxuan Fan",
        "Ching Wang",
        "Siqi Li",
        "Nan Liu"
      ],
      "published": "2025-10-02T04:45:02Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01663v1"
    },
    {
      "arxiv_id": "2510.01659v1",
      "title": "MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue\n  Summarization",
      "summary": "Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging\napplications. To support the development of effective MDS models, robust\nautomatic evaluation methods are essential for reducing both cost and human\neffort. However, such methods require a strong meta-evaluation benchmark\ngrounded in human annotations. In this work, we introduce MDSEval, the first\nmeta-evaluation benchmark for MDS, consisting image-sharing dialogues,\ncorresponding summaries, and human judgments across eight well-defined quality\naspects. To ensure data quality and richfulness, we propose a novel filtering\nframework leveraging Mutually Exclusive Key Information (MEKI) across\nmodalities. Our work is the first to identify and formalize key evaluation\ndimensions specific to MDS. We benchmark state-of-the-art modal evaluation\nmethods, revealing their limitations in distinguishing summaries from advanced\nMLLMs and their susceptibility to various bias.",
      "authors": [
        "Yinhong Liu",
        "Jianfeng He",
        "Hang Su",
        "Ruixue Lian",
        "Yi Nian",
        "Jake Vincent",
        "Srikanth Vishnubhotla",
        "Robinson Piramuthu",
        "Saab Mansour"
      ],
      "published": "2025-10-02T04:38:27Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01659v1"
    },
    {
      "arxiv_id": "2510.01658v1",
      "title": "Learning Time-Series Representations by Hierarchical\n  Uniformity-Tolerance Latent Balancing",
      "summary": "We propose TimeHUT, a novel method for learning time-series representations\nby hierarchical uniformity-tolerance balancing of contrastive representations.\nOur method uses two distinct losses to learn strong representations with the\naim of striking an effective balance between uniformity and tolerance in the\nembedding space. First, TimeHUT uses a hierarchical setup to learn both\ninstance-wise and temporal information from input time-series. Next, we\nintegrate a temperature scheduler within the vanilla contrastive loss to\nbalance the uniformity and tolerance characteristics of the embeddings.\nAdditionally, a hierarchical angular margin loss enforces instance-wise and\ntemporal contrast losses, creating geometric margins between positive and\nnegative pairs of temporal sequences. This approach improves the coherence of\npositive pairs and their separation from the negatives, enhancing the capture\nof temporal dependencies within a time-series sample. We evaluate our approach\non a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and\nmultivariate classification, as well as Yahoo and KPI datasets for anomaly\ndetection. The results demonstrate that TimeHUT outperforms prior methods by\nconsiderable margins on classification, while obtaining competitive results for\nanomaly detection. Finally, detailed sensitivity and ablation studies are\nperformed to evaluate different components and hyperparameters of our method.",
      "authors": [
        "Amin Jalali",
        "Milad Soltany",
        "Michael Greenspan",
        "Ali Etemad"
      ],
      "published": "2025-10-02T04:30:13Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01658v1"
    },
    {
      "arxiv_id": "2510.01656v1",
      "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM\n  reasoning",
      "summary": "Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing\nthem with average advantage baselines. This shift is largely pragmatic:\nconventional value functions are computationally expensive to train at LLM\nscale and often fail under sparse rewards and long reasoning horizons. We\nrevisit this bottleneck from an architectural perspective and introduce\nAsymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable\nframework that restores the critics role while remaining efficient in\nlarge-model settings. AsyPPO employs a set of lightweight mini-critics, each\ntrained on disjoint prompt shards. This design encourages diversity while\npreserving calibration, reducing value-estimation bias. Beyond robust\nestimation, AsyPPO leverages inter-critic uncertainty to refine the policy\nupdate: (i) masking advantages in states where critics agree and gradients add\nlittle learning signal, and (ii) filtering high-divergence states from entropy\nregularization, suppressing spurious exploration. After training on open-source\ndata with only 5,000 samples, AsyPPO consistently improves learning stability\nand performance across multiple benchmarks over strong baselines, such as GRPO,\nachieving performance gains of more than six percent on Qwen3-4b-Base and about\nthree percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without\nadditional tricks. These results highlight the importance of architectural\ninnovations for scalable, efficient algorithms.",
      "authors": [
        "Jiashun Liu",
        "Johan Obando-Ceron",
        "Han Lu",
        "Yancheng He",
        "Weixun Wang",
        "Wenbo Su",
        "Bo Zheng",
        "Pablo Samuel Castro",
        "Aaron Courville",
        "Ling Pan"
      ],
      "published": "2025-10-02T04:24:27Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01656v1"
    },
    {
      "arxiv_id": "2510.01654v1",
      "title": "SoK: Measuring What Matters for Closed-Loop Security Agents",
      "summary": "Cybersecurity is a relentless arms race, with AI driven offensive systems\nevolving faster than traditional defenses can adapt. Research and tooling\nremain fragmented across isolated defensive functions, creating blind spots\nthat adversaries exploit. Autonomous agents capable of integrating, exploit\nconfirmation, remediation, and validation into a single closed loop offer\npromise, but the field lacks three essentials: a framework defining the agentic\ncapabilities of security systems across security life cycle, a principled\nmethod for evaluating closed loop agents, and a benchmark for measuring their\nperformance in practice. We introduce CLASP: the Closed-Loop Autonomous\nSecurity Performance framework which aligns the security lifecycle\n(reconnaissance, exploitation, root cause analysis, patch synthesis,\nvalidation) with core agentic capabilities (planning, tool use, memory,\nreasoning, reflection & perception) providing a common vocabulary and rubric\nfor assessing agentic capabilities in security tasks. By applying CLASP to 21\nrepresentative works, we map where systems demonstrate strengths, and where\ncapability gaps persist. We then define the Closed-Loop Capability (CLC) Score,\na composite metric quantifying both degree of loop closure and operational\neffectiveness, and outline the requirements for a closed loop benchmark.\nTogether, CLASP and the CLC Score, provide the vocabulary, diagnostics, and\nmeasurements needed to advance both function level performance and measure\nclosed loop security agents.",
      "authors": [
        "Mudita Khurana",
        "Raunak Jain"
      ],
      "published": "2025-10-02T04:20:35Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01654v1"
    },
    {
      "arxiv_id": "2510.01650v1",
      "title": "The Unseen Frontier: Pushing the Limits of LLM Sparsity with\n  Surrogate-Free ADMM",
      "summary": "Neural network pruning is a promising technique to mitigate the excessive\ncomputational and memory requirements of large language models (LLMs). Despite\nits promise, however, progress in this area has diminished, as conventional\nmethods are seemingly unable to surpass moderate sparsity levels (50-60%)\nwithout severely degrading model accuracy. This work breaks through the current\nimpasse, presenting a principled and effective method called $\\texttt{Elsa}$,\nwhich achieves extreme sparsity levels of up to 90% while retaining high model\nfidelity. This is done by identifying several limitations in current practice,\nall of which can be traced back to their reliance on a surrogate objective\nformulation. $\\texttt{Elsa}$ tackles this issue directly and effectively via\nstandard and well-established constrained optimization techniques based on\nADMM. Our extensive experiments across a wide range of models and scales show\nthat $\\texttt{Elsa}$ achieves substantial improvements over existing methods;\ne.g., it achieves 7.8$\\times$ less perplexity than the best existing method on\nLLaMA-2-7B at 90% sparsity. Furthermore, we present\n$\\texttt{Elsa}_{\\text{-L}}$, a quantized variant that scales to extremely large\nmodels (27B), and establish its theoretical convergence guarantees. These\nresults highlight meaningful progress in advancing the frontier of LLM\nsparsity, while promising that significant opportunities for further\nadvancement may remain in directions that have so far attracted limited\nexploration.",
      "authors": [
        "Kwanhee Lee",
        "Hyeondo Jang",
        "Dongyeop Lee",
        "Dan Alistarh",
        "Namhoon Lee"
      ],
      "published": "2025-10-02T04:10:17Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01650v1"
    },
    {
      "arxiv_id": "2510.01649v1",
      "title": "Source-Free Cross-Domain Continual Learning",
      "summary": "Although existing cross-domain continual learning approaches successfully\naddress many streaming tasks having domain shifts, they call for a fully\nlabeled source domain hindering their feasibility in the privacy constrained\nenvironments. This paper goes one step ahead with the problem of source-free\ncross-domain continual learning where the use of source-domain samples are\ncompletely prohibited. We propose the idea of rehearsal-free frequency-aware\ndynamic prompt collaborations (REFEREE) to cope with the absence of labeled\nsource-domain samples in realm of cross-domain continual learning. REFEREE is\nbuilt upon a synergy between a source-pre-trained model and a large-scale\nvision-language model, thus overcoming the problem of sub-optimal\ngeneralizations when relying only on a source pre-trained model. The domain\nshift problem between the source domain and the target domain is handled by a\nfrequency-aware prompting technique encouraging low-frequency components while\nsuppressing high-frequency components. This strategy generates frequency-aware\naugmented samples, robust against noisy pseudo labels. The noisy pseudo-label\nproblem is further addressed with the uncertainty-aware weighting strategy\nwhere the mean and covariance matrix are weighted by prediction uncertainties,\nthus mitigating the adverse effects of the noisy pseudo label. Besides, the\nissue of catastrophic forgetting (CF) is overcome by kernel linear discriminant\nanalysis (KLDA) where the backbone network is frozen while the classification\nis performed using the linear discriminant analysis approach guided by the\nrandom kernel method. Our rigorous numerical studies confirm the advantage of\nour approach where it beats prior arts having access to source domain samples\nwith significant margins.",
      "authors": [
        "Muhammad Tanzil Furqon",
        "Mahardhika Pratama",
        "Igor Škrjanc",
        "Lin Liu",
        "Habibullah Habibullah",
        "Kutluyil Dogancay"
      ],
      "published": "2025-10-02T04:09:25Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01649v1"
    },
    {
      "arxiv_id": "2510.01645v1",
      "title": "Position: Privacy Is Not Just Memorization!",
      "summary": "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats.",
      "authors": [
        "Niloofar Mireshghallah",
        "Tianshi Li"
      ],
      "published": "2025-10-02T04:02:06Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01645v1"
    },
    {
      "arxiv_id": "2510.01644v1",
      "title": "NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with\n  BERT",
      "summary": "Large Language Models (LLMs) suffer from a range of vulnerabilities that\nallow malicious users to solicit undesirable responses through manipulation of\nthe input text. These so-called jailbreak prompts are designed to trick the LLM\ninto circumventing the safety guardrails put in place to keep responses\nacceptable to the developer's policies. In this study, we analyse the ability\nof different machine learning models to distinguish jailbreak prompts from\ngenuine uses, including looking at our ability to identify jailbreaks that use\npreviously unseen strategies. Our results indicate that using current datasets\nthe best performance is achieved by fine tuning a Bidirectional Encoder\nRepresentations from Transformers (BERT) model end-to-end for identifying\njailbreaks. We visualise the keywords that distinguish jailbreak from genuine\nprompts and conclude that explicit reflexivity in prompt structure could be a\nsignal of jailbreak intention.",
      "authors": [
        "John Hawkins",
        "Aditya Pramar",
        "Rodney Beard",
        "Rohitash Chandra"
      ],
      "published": "2025-10-02T03:55:29Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01644v1"
    },
    {
      "arxiv_id": "2510.01639v1",
      "title": "Understanding the Geospatial Reasoning Capabilities of LLMs: A\n  Trajectory Recovery Perspective",
      "summary": "We explore the geospatial reasoning capabilities of Large Language Models\n(LLMs), specifically, whether LLMs can read road network maps and perform\nnavigation. We frame trajectory recovery as a proxy task, which requires models\nto reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with\nover 4,000 real-world trajectories across diverse regions and transportation\nmodes. Using road network as context, our prompting framework enables LLMs to\ngenerate valid paths without accessing any external navigation tools.\nExperiments show that LLMs outperform off-the-shelf baselines and specialized\ntrajectory recovery models, with strong zero-shot generalization. Fine-grained\nanalysis shows that LLMs have strong comprehension of the road network and\ncoordinate systems, but also pose systematic biases with respect to regions and\ntransportation modes. Finally, we demonstrate how LLMs can enhance navigation\nexperiences by reasoning over maps in flexible ways to incorporate user\npreferences.",
      "authors": [
        "Thinh Hung Truong",
        "Jey Han Lau",
        "Jianzhong Qi"
      ],
      "published": "2025-10-02T03:37:41Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01639v1"
    },
    {
      "arxiv_id": "2510.01638v1",
      "title": "Towards Human-Centered RegTech: Unpacking Professionals' Strategies and\n  Needs for Using LLMs Safely",
      "summary": "Large Language Models are profoundly changing work patterns in high-risk\nprofessional domains, yet their application also introduces severe and\nunderexplored compliance risks. To investigate this issue, we conducted\nsemi-structured interviews with 24 highly-skilled knowledge workers from\nindustries such as law, healthcare, and finance. The study found that these\nexperts are commonly concerned about sensitive information leakage,\nintellectual property infringement, and uncertainty regarding the quality of\nmodel outputs. In response, they spontaneously adopt various mitigation\nstrategies, such as actively distorting input data and limiting the details in\ntheir prompts. However, the effectiveness of these spontaneous efforts is\nlimited due to a lack of specific compliance guidance and training for Large\nLanguage Models. Our research reveals a significant gap between current NLP\ntools and the actual compliance needs of experts. This paper positions these\nvaluable empirical findings as foundational work for building the next\ngeneration of Human-Centered, Compliance-Driven Natural Language Processing for\nRegulatory Technology (RegTech), providing a critical human-centered\nperspective and design requirements for engineering NLP systems that can\nproactively support expert compliance workflows.",
      "authors": [
        "Siying Hu",
        "Yaxing Yao",
        "Zhicong Lu"
      ],
      "published": "2025-10-02T03:35:46Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.01638v1"
    },
    {
      "arxiv_id": "2510.01632v1",
      "title": "BioBlobs: Differentiable Graph Partitioning for Protein Representation\n  Learning",
      "summary": "Protein function is driven by coherent substructures which vary in size and\ntopology, yet current protein representation learning models (PRL) distort\nthese signals by relying on rigid substructures such as k-hop and fixed radius\nneighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable\nmodule that represents proteins by dynamically partitioning structures into\nflexibly-sized, non-overlapping substructures (\"blobs\"). The resulting blobs\nare quantized into a shared and interpretable codebook, yielding a discrete\nvocabulary of function-relevant protein substructures used to compute protein\nembeddings. We show that BioBlobs representations improve the performance of\nwidely used protein encoders such as GVP-GNN across various PRL tasks. Our\napproach highlights the value of architectures that directly capture\nfunction-relevant protein substructures, enabling both improved predictive\nperformance and mechanistic insight into protein function.",
      "authors": [
        "Xin Wang",
        "Carlos Oliver"
      ],
      "published": "2025-10-02T03:25:02Z",
      "primary_category": "q-bio.BM",
      "arxiv_url": "https://arxiv.org/abs/2510.01632v1"
    },
    {
      "arxiv_id": "2510.01631v1",
      "title": "Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of\n  Scaling Laws, Benefits, and Pitfalls",
      "summary": "Training data plays a crucial role in Large Language Models (LLM) scaling,\nyet high quality data is of limited supply. Synthetic data techniques offer a\npotential path toward sidestepping these limitations. We conduct a large-scale\nempirical investigation (>1000 LLMs with >100k GPU hours) using a unified\nprotocol and scaling laws, comparing natural web data, diverse synthetic types\n(rephrased text, generated textbooks), and mixtures of natural and synthetic\ndata. Specifically, we found pre-training on rephrased synthetic data\n\\textit{alone} is not faster than pre-training on natural web texts; while\npre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts\ncan speed up 5-10x (to reach the same validation loss) at larger data budgets.\nPre-training on textbook-style synthetic data \\textit{alone} results in notably\nhigher loss on many downstream domains especially at small data budgets. \"Good\"\nratios of synthetic data in training data mixtures depend on the model size and\ndata budget, empirically converging to ~30% for rephrased synthetic data.\nLarger generator models do not necessarily yield better pre-training data than\n~8B-param models. These results contribute mixed evidence on \"model collapse\"\nduring large-scale single-round (n=1) model training on synthetic\ndata--training on rephrased synthetic data shows no degradation in performance\nin foreseeable scales whereas training on mixtures of textbook-style\npure-generated synthetic data shows patterns predicted by \"model collapse\". Our\nwork demystifies synthetic data in pre-training, validates its conditional\nbenefits, and offers practical guidance.",
      "authors": [
        "Feiyang Kang",
        "Newsha Ardalani",
        "Michael Kuchnik",
        "Youssef Emad",
        "Mostafa Elhoushi",
        "Shubhabrata Sengupta",
        "Shang-Wen Li",
        "Ramya Raghavendra",
        "Ruoxi Jia",
        "Carole-Jean Wu"
      ],
      "published": "2025-10-02T03:24:42Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01631v1"
    },
    {
      "arxiv_id": "2510.01624v1",
      "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What\n  to Use Instead",
      "summary": "In post-training for reasoning Large Language Models (LLMs), the current\nstate of practice trains LLMs in two independent stages: Supervised Fine-Tuning\n(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as\n``RL'' below). In this work, we challenge whether high SFT scores translate to\nimproved performance after RL. We provide extensive counter-examples where this\nis not true. We find high SFT scores can be biased toward simpler or more\nhomogeneous data and are not reliably predictive of subsequent RL gains or\nscaled-up post-training effectiveness. In some cases, RL training on models\nwith improved SFT performance could lead to substantially worse outcome\ncompared to RL on the base model without SFT. We study alternative metrics and\nidentify generalization loss on held-out reasoning examples and Pass@large k\nperformance to provide strong proxies for the RL outcome. We trained hundreds\nof models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive\nevaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU\nhours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple\nstate-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL\nperformance, prediction based on generalization loss and Pass@large k achieves\nsubstantial higher precision, improving $R^2$ coefficient and Spearman's rank\ncorrelation coefficient by up to 0.5 (2x). This provides strong utility for\nbroad use cases. For example, in most experiments, we find SFT training on\nunique examples for a one epoch underperforms training on half examples for two\nepochs, either after SFT or SFT-then-RL; With the same SFT budget, training\nonly on short examples may lead to better SFT performance, though, it often\nleads to worse outcome after RL compared to training on examples with varying\nlengths. Evaluation tool will be open-sourced.",
      "authors": [
        "Feiyang Kang",
        "Michael Kuchnik",
        "Karthik Padthe",
        "Marin Vlastelica",
        "Ruoxi Jia",
        "Carole-Jean Wu",
        "Newsha Ardalani"
      ],
      "published": "2025-10-02T02:57:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01624v1"
    },
    {
      "arxiv_id": "2510.01622v1",
      "title": "LLM4Rec: Large Language Models for Multimodal Generative Recommendation\n  with Causal Debiasing",
      "summary": "Contemporary generative recommendation systems face significant challenges in\nhandling multimodal data, eliminating algorithmic biases, and providing\ntransparent decision-making processes. This paper introduces an enhanced\ngenerative recommendation framework that addresses these limitations through\nfive key innovations: multimodal fusion architecture, retrieval-augmented\ngeneration mechanisms, causal inference-based debiasing, explainable\nrecommendation generation, and real-time adaptive learning capabilities. Our\nframework leverages advanced large language models as the backbone while\nincorporating specialized modules for cross-modal understanding, contextual\nknowledge integration, bias mitigation, explanation synthesis, and continuous\nmodel adaptation. Extensive experiments on three benchmark datasets\n(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistent\nimprovements in recommendation accuracy, fairness, and diversity compared to\nexisting approaches. The proposed framework achieves up to 2.3% improvement in\nNDCG@10 and 1.4% enhancement in diversity metrics while maintaining\ncomputational efficiency through optimized inference strategies.",
      "authors": [
        "Bo Ma",
        "Hang Li",
        "ZeHua Hu",
        "XiaoFan Gui",
        "LuYao Liu",
        "Simon Lau"
      ],
      "published": "2025-10-02T02:53:05Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01622v1"
    },
    {
      "arxiv_id": "2510.01620v1",
      "title": "Learning to Decide with Just Enough: Information-Theoretic Context\n  Summarization for CDMPs",
      "summary": "Contextual Markov Decision Processes (CMDPs) offer a framework for sequential\ndecision-making under external signals, but existing methods often fail to\ngeneralize in high-dimensional or unstructured contexts, resulting in excessive\ncomputation and unstable performance. We propose an information-theoretic\nsummarization approach that uses large language models (LLMs) to compress\ncontextual inputs into low-dimensional, semantically rich summaries. These\nsummaries augment states by preserving decision-critical cues while reducing\nredundancy. Building on the notion of approximate context sufficiency, we\nprovide, to our knowledge, the first regret bounds and a latency-entropy\ntrade-off characterization for CMDPs. Our analysis clarifies how\ninformativeness impacts computational cost. Experiments across discrete,\ncontinuous, visual, and recommendation benchmarks show that our method\noutperforms raw-context and non-context baselines, improving reward, success\nrate, and sample efficiency, while reducing latency and memory usage. These\nfindings demonstrate that LLM-based summarization offers a scalable and\ninterpretable solution for efficient decision-making in context-rich,\nresource-constrained environments.",
      "authors": [
        "Peidong Liu",
        "Junjiang Lin",
        "Shaowen Wang",
        "Yao Xu",
        "Haiqing Li",
        "Xuhao Xie",
        "Siyi Wu",
        "Hao Li"
      ],
      "published": "2025-10-02T02:52:24Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01620v1"
    },
    {
      "arxiv_id": "2510.01612v1",
      "title": "RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical\n  Question Answering",
      "summary": "The exponential growth of biomedical literature creates significant\nchallenges for accessing precise medical information. Current biomedical\nquestion-answering systems primarily focus on short-form answers, failing to\nprovide the comprehensive explanations necessary for clinical decision-making.\nWe present RAG-BioQA, a novel framework combining retrieval-augmented\ngeneration with domain-specific fine-tuning to produce evidence-based,\nlong-form biomedical answers. Our approach integrates BioBERT embeddings with\nFAISS indexing and compares various re-ranking strategies (BM25, ColBERT,\nMonoT5) to optimize context selection before synthesizing evidence through a\nfine-tuned T5 model. Experimental results on the PubMedQA dataset show\nsignificant improvements over baselines, with our best model achieving\nsubstantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state\nof accessible, evidence-based biomedical knowledge retrieval.",
      "authors": [
        "Lovely Yeswanth Panchumarthi",
        "Sai Prasad Gudari",
        "Atharva Negi",
        "Praveen Raj Budime",
        "Harsit Upadhya"
      ],
      "published": "2025-10-02T02:49:09Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01612v1"
    },
    {
      "arxiv_id": "2510.01611v1",
      "title": "PychoBench: Evaluating the Psychology Intelligence of Large Language\n  Models",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: Can LLMs be effectively applied to psychological\ncounseling? To determine whether an LLM can effectively take on the role of a\npsychological counselor, the first step is to assess whether it meets the\nqualifications required for such a role, namely the ability to pass the U.S.\nNational Counselor Certification Exam (NCE). This is because, just as a human\ncounselor must pass a certification exam to practice, an LLM must demonstrate\nsufficient psychological knowledge to meet the standards required for such a\nrole. To address this, we introduce PsychoBench, a benchmark grounded in\nU.S.national counselor examinations, a licensure test for professional\ncounselors that requires about 70% accuracy to pass. PsychoBench comprises\napproximately 2,252 carefully curated single-choice questions, crafted to\nrequire deep understanding and broad enough to cover various sub-disciplines of\npsychology. This benchmark provides a comprehensive assessment of an LLM's\nability to function as a counselor. Our evaluation shows that advanced models\nsuch as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing\nthreshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)\nremain far below it. These results suggest that only frontier LLMs are\ncurrently capable of meeting counseling exam standards, highlighting both the\npromise and the challenges of developing psychology-oriented LLMs.",
      "authors": [
        "Min Zeng"
      ],
      "published": "2025-10-02T02:49:06Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01611v1"
    },
    {
      "arxiv_id": "2510.01609v1",
      "title": "AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative\n  Recommendation with Adaptive Intelligence",
      "summary": "Interactive conversational recommender systems have gained significant\nattention for their ability to capture user preferences through natural\nlanguage interactions. However, existing approaches face substantial challenges\nin handling dynamic user preferences, maintaining conversation coherence, and\nbalancing multiple ranking objectives simultaneously. This paper introduces\nAgentRec, a next-generation LLM-powered multi-agent collaborative\nrecommendation framework that addresses these limitations through hierarchical\nagent networks with adaptive intelligence. Our approach employs specialized\nLLM-powered agents for conversation understanding, preference modeling, context\nawareness, and dynamic ranking, coordinated through an adaptive weighting\nmechanism that learns from interaction patterns. We propose a three-tier\nlearning strategy combining rapid response for simple queries, intelligent\nreasoning for complex preferences, and deep collaboration for challenging\nscenarios. Extensive experiments on three real-world datasets demonstrate that\nAgentRec achieves consistent improvements over state-of-the-art baselines, with\n2.8\\% enhancement in conversation success rate, 1.9\\% improvement in\nrecommendation accuracy (NDCG@10), and 3.2\\% better conversation efficiency\nwhile maintaining comparable computational costs through intelligent agent\ncoordination.",
      "authors": [
        "Bo Ma",
        "Hang Li",
        "ZeHua Hu",
        "XiaoFan Gui",
        "LuYao Liu",
        "Simon Lau"
      ],
      "published": "2025-10-02T02:47:11Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01609v1"
    },
    {
      "arxiv_id": "2510.01606v1",
      "title": "Bridging Collaborative Filtering and Large Language Models with Dynamic\n  Alignment, Multimodal Fusion and Evidence-grounded Explanations",
      "summary": "Recent research has explored using Large Language Models for recommendation\ntasks by transforming user interaction histories and item metadata into text\nprompts, then having the LLM produce rankings or recommendations. A promising\napproach involves connecting collaborative filtering knowledge to LLM\nrepresentations through compact adapter networks, which avoids expensive\nfine-tuning while preserving the strengths of both components. Yet several\nchallenges persist in practice: collaborative filtering models often use static\nsnapshots that miss rapidly changing user preferences; many real-world items\ncontain rich visual and audio content beyond textual descriptions; and current\nsystems struggle to provide trustworthy explanations backed by concrete\nevidence. Our work introduces \\model{}, a framework that tackles these\nlimitations through three key innovations. We develop an online adaptation\nmechanism that continuously incorporates new user interactions through\nlightweight modules, avoiding the need to retrain large models. We create a\nunified representation that seamlessly combines collaborative signals with\nvisual and audio features, handling cases where some modalities may be\nunavailable. Finally, we design an explanation system that grounds\nrecommendations in specific collaborative patterns and item attributes,\nproducing natural language rationales users can verify. Our approach maintains\nthe efficiency of frozen base models while adding minimal computational\noverhead, making it practical for real-world deployment.",
      "authors": [
        "Bo Ma",
        "LuYao Liu",
        "Simon Lau",
        "Chandler Yuan",
        "and XueY Cui",
        "Rosie Zhang"
      ],
      "published": "2025-10-02T02:43:24Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01606v1"
    },
    {
      "arxiv_id": "2510.01600v1",
      "title": "A Comparison of Independent and Joint Fine-tuning Strategies for\n  Retrieval-Augmented Generation",
      "summary": "A Comparison of Independent and Joint Fine-tuning Strategies for\nRetrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,\nAnoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP\n2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0\nKeywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),\nFine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and\ncompare strategies for fine-tuning Retrieval Augmented Generation (RAG)\npipelines, including independent fine-tuning, joint fine-tuning, and two-phase\nfine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular\nframework for question answering that is powered by two large language models\n(LLMs): an embedding model that retrieves context documents from a database\nthat are relevant to a given question, and a generator model that uses the\nretrieved context to generate an answer to the question. Both the embedding and\ngenerator models can be fine-tuned to increase performance of a RAG pipeline on\na new task, but multiple fine-tuning strategies exist with different costs and\nbenefits. In this paper, we evaluate and compare several RAG fine-tuning\nstrategies, including independent, joint, and two-phase fine-tuning. In our\nexperiments, we observe that all of these strategies achieve about equal\nimprovement in EM and F1 generation quality metrics, although they have\nsignificantly different computational costs. We conclude the optimal\nfine-tuning strategy to use depends on whether the training dataset includes\ncontext labels and whether a grid search over the learning rates for the\nembedding and generator models is required.",
      "authors": [
        "Neal Gregory Lawton",
        "Alfy Samuel",
        "Anoop Kumar",
        "Daben Liu"
      ],
      "published": "2025-10-02T02:30:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01600v1"
    },
    {
      "arxiv_id": "2510.01588v1",
      "title": "Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via\n  Contrastive Feature Augmentation",
      "summary": "Parkinson's disease (PD) is one of the most common neurodegenerative\ndisorder. PD telemonitoring emerges as a novel assessment modality enabling\nself-administered at-home tests of Unified Parkinson's Disease Rating Scale\n(UPDRS) scores, enhancing accessibility for PD patients. However, three types\nof noise would occur during measurements: (1) patient-induced measurement\ninaccuracies, (2) environmental noise, and (3) data packet loss during\ntransmission, resulting in higher prediction errors. To address these\nchallenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,\nthe original speech features are grouped into ordered bins, based on the\ncontinuous values of a selected feature, to construct contrastive pairs.\nSecond, the contrastive pairs are employed to train a multilayer perceptron\nencoder for generating noise-robust features. Finally, these features are\nconcatenated with the original features as the augmented features, which are\nthen fed into the UPDRS prediction models. Notably, we further introduces a\nnovel evaluation approach with customizable noise injection module, and\nextensive experiments show that NoRo can successfully enhance the noise\nrobustness of UPDRS prediction across various downstream prediction models\nunder different noisy environments.",
      "authors": [
        "Ziming Tang",
        "Chengbin Hou",
        "Tianyu Zhang",
        "Bangxu Tian",
        "Jinbao Wang",
        "Hairong Lv"
      ],
      "published": "2025-10-02T02:07:41Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01588v1"
    },
    {
      "arxiv_id": "2510.01586v1",
      "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial\n  Co-Evolution in Multi-Agent Reinforcement Learning",
      "summary": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead.",
      "authors": [
        "Zhenyu Pan",
        "Yiting Zhang",
        "Zhuo Liu",
        "Yolo Yunlong Tang",
        "Zeliang Zhang",
        "Haozheng Luo",
        "Yuwei Han",
        "Jianshu Zhang",
        "Dennis Wu",
        "Hong-Yu Chen",
        "Haoran Lu",
        "Haoyang Fang",
        "Manling Li",
        "Chenliang Xu",
        "Philip S. Yu",
        "Han Liu"
      ],
      "published": "2025-10-02T02:06:30Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01586v1"
    },
    {
      "arxiv_id": "2510.01581v1",
      "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,\n  Attentive Compression",
      "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time\ncompute, but this scaling must be allocated in line with task difficulty. On\none hand, short reasoning (underthinking) leads to errors on harder problems\nthat require extended reasoning steps; but, excessively long reasoning\n(overthinking) can be token-inefficient, generating unnecessary steps even\nafter reaching a correct intermediate solution. We refer to this as\nunder-adaptivity, where the model fails to modulate its response length\nappropriately given problems of varying difficulty. To address under-adaptivity\nand strike a balance between under- and overthinking, we propose TRAAC (Think\nRight with Adaptive, Attentive Compression), an online post-training RL method\nthat leverages the model's self-attention over a long reasoning trajectory to\nidentify important steps and prune redundant ones. TRAAC also estimates\ndifficulty and incorporates it into training rewards, thereby learning to\nallocate reasoning budget commensurate with example difficulty. Our approach\nimproves accuracy, reduces reasoning steps, and enables adaptive thinking\ncompared to base models and other RL baselines. Across a variety of tasks\n(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute\naccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%\ncompared to the base model, and a 7.9% accuracy gain paired with a 29.4% length\ndrop compared to the best RL baseline. TRAAC also shows strong generalization:\nalthough our models are trained on math datasets, they show accuracy and\nefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,\nand OptimalThinkingBench. Our analysis further verifies that TRAAC provides\nfine-grained adjustments to thinking budget based on difficulty and that a\ncombination of task-difficulty calibration and attention-based compression\nyields gains across diverse tasks.",
      "authors": [
        "Joykirat Singh",
        "Justin Chih-Yao Chen",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Akshay Nambi",
        "Mohit Bansal"
      ],
      "published": "2025-10-02T02:00:20Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01581v1"
    },
    {
      "arxiv_id": "2510.01576v1",
      "title": "Guiding Multimodal Large Language Models with Blind and Low Vision\n  People Visual Questions for Proactive Visual Interpretations",
      "summary": "Multimodal large language models (MLLMs) have been integrated into visual\ninterpretation applications to support Blind and Low Vision (BLV) users because\nof their accuracy and ability to provide rich, human-like interpretations.\nHowever, these applications often default to comprehensive, lengthy\ndescriptions regardless of context. This leads to inefficient exchanges, as\nusers must go through irrelevant details rather than receiving the specific\ninformation they are likely to seek. To deliver more contextually-relevant\ninformation, we developed a system that draws on historical BLV users\nquestions. When given an image, our system identifies similar past visual\ncontexts from the VizWiz-LF dataset and uses the associated questions to guide\nthe MLLM generate descriptions more relevant to BLV users. An evaluation with\nthree human labelers who revised 92 context-aware and context-free descriptions\nshowed that context-aware descriptions anticipated and answered users'\nquestions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of\ncomparisons (50 out of 92). Our paper reviews, and data analysis are publicly\navailable in a Github repository at\nhttps://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .",
      "authors": [
        "Ricardo Gonzalez Penuela",
        "Felipe Arias-Russi",
        "Victor Capriles"
      ],
      "published": "2025-10-02T01:48:51Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01576v1"
    },
    {
      "arxiv_id": "2510.01574v1",
      "title": "Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query\n  Autocomplete",
      "summary": "We introduce a data-centric approach for mitigating presentation bias in\nreal-time neural query autocomplete systems through the use of synthetic\nprefixes. These prefixes are generated from complete user queries collected\nduring regular search sessions where autocomplete was not active. This allows\nus to enrich the training data for learning to rank models with more diverse\nand less biased examples. This method addresses the inherent bias in engagement\nsignals collected from live query autocomplete interactions, where model\nsuggestions influence user behavior. Our neural ranker is optimized for\nreal-time deployment under strict latency constraints and incorporates a rich\nset of features, including query popularity, seasonality, fuzzy match scores,\nand contextual signals such as department affinity, device type, and vertical\nalignment with previous user queries. To support efficient training, we\nintroduce a task-specific simplification of the listwise loss, reducing\ncomputational complexity from $O(n^2)$ to $O(n)$ by leveraging the query\nautocomplete structure of having only one ground-truth selection per prefix.\nDeployed in a large-scale e-commerce setting, our system demonstrates\nstatistically significant improvements in user engagement, as measured by mean\nreciprocal rank and related metrics. Our findings show that synthetic prefixes\nnot only improve generalization but also provide a scalable path toward bias\nmitigation in other low-latency ranking tasks, including related searches and\nquery recommendations.",
      "authors": [
        "Adithya Rajan",
        "Xiaoyu Liu",
        "Prateek Verma",
        "Vibhu Arora"
      ],
      "published": "2025-10-02T01:44:44Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01574v1"
    },
    {
      "arxiv_id": "2510.01571v1",
      "title": "From Supervision to Exploration: What Does Protein Language Model Learn\n  During Reinforcement Learning?",
      "summary": "Protein language models (PLMs) have advanced computational protein science\nthrough large-scale pretraining and scalable architectures. In parallel,\nreinforcement learning (RL) has broadened exploration and enabled precise\nmulti-objective optimization in protein design. Yet whether RL can push PLMs\nbeyond their pretraining priors to uncover latent sequence-structure-function\nrules remains unclear. We address this by pairing RL with PLMs across four\ndomains: antimicrobial peptide design, kinase variant optimization, antibody\nengineering, and inverse folding. Using diverse RL algorithms and model\nclasses, we ask if RL improves sampling efficiency and, more importantly, if it\nreveals capabilities not captured by supervised learning. Across benchmarks, RL\nconsistently boosts success rates and sample efficiency. Performance follows a\nthree-factor interaction: task headroom, reward fidelity, and policy capacity\njointly determine gains. When rewards are accurate and informative, policies\nhave sufficient capacity, and tasks leave room beyond supervised baselines,\nimprovements scale; when rewards are noisy or capacity is constrained, gains\nsaturate despite exploration. This view yields practical guidance for RL in\nprotein design: prioritize reward modeling and calibration before scaling\npolicy size, match algorithm and regularization strength to task difficulty,\nand allocate capacity where marginal gains are largest. Implementation is\navailable at https://github.com/chq1155/RL-PLM.",
      "authors": [
        "Hanqun Cao",
        "Hongrui Zhang",
        "Junde Xu",
        "Zhou Zhang",
        "Lingdong Shen",
        "Minghao Sun",
        "Ge Liu",
        "Jinbo Xu",
        "Wu-Jun Li",
        "Jinren Ni",
        "Cesar de la Fuente-Nunez",
        "Tianfan Fu",
        "Yejin Choi",
        "Pheng-Ann Heng",
        "Fang Wu"
      ],
      "published": "2025-10-02T01:31:10Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01571v1"
    },
    {
      "arxiv_id": "2510.01569v1",
      "title": "InvThink: Towards AI Safety via Inverse Reasoning",
      "summary": "We present InvThink, a simple yet powerful approach that gives large language\nmodels (LLMs) the capability of inverse thinking: reasoning through failure\nmodes before generating responses. Unlike existing safety alignment methods\nthat optimize directly for safe response, InvThink instructs models to 1)\nenumerate potential harms, 2) analyze their consequences, and 3) generate safe\noutputs that proactively avoid these risks. Our method reveals three key\nfindings: (i) safety improvements show stronger scaling with model size\ncompared to existing safety methods. (ii) InvThink mitigates safety tax; by\ntraining models to systematically consider failure modes, it preserves general\nreasoning capabilities on standard benchmarks. (iii) beyond general safety\ntasks, InvThink excels in high-stakes domains including external-facing\n(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,\nachieving up to 15.7% reduction in harmful responses compared to baseline\nmethods like SafetyPrompt. We further implement InvThink via supervised\nfine-tuning, and reinforcement learning across three LLM families. These\nresults suggest that inverse reasoning provides a scalable and generalizable\npath toward safer, more capable language models.",
      "authors": [
        "Yubin Kim",
        "Taehan Kim",
        "Eugene Park",
        "Chunjong Park",
        "Cynthia Breazeal",
        "Daniel McDuff",
        "Hae Won Park"
      ],
      "published": "2025-10-02T01:26:53Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01569v1"
    },
    {
      "arxiv_id": "2510.01555v1",
      "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient\n  Optimization",
      "summary": "Reinforcement Learning from Human Feedback (RLHF) leverages a\nKullback-Leibler (KL) divergence loss to stabilize training and prevent\noverfitting. However, in methods such as GRPO, its implementation may be guided\nby principles from numerical value estimation-a practice that overlooks the\nterm's functional role as an optimization loss. To analyze this issue, we\nestablish a unified framework that connects two seemingly distinct\nimplementation styles: using the mathematical term $k_n$ as a detached\ncoefficient for the policy's score function ('$k_n$ in reward') or as a direct\nloss function through which gradients are propagated ('$k_n$ as loss'). We show\nthat the latter can always be analyzed via an equivalent gradient coefficient\nin the former, unifying the two perspectives. Through this framework, we prove\nthat the conventional '$k_1$ in reward' (like in PPO) is the principled loss\nfor Reverse KL (RKL) regularization. We further establish a key finding: under\non-policy conditions, the '$k_2$ as loss' formulation is, in fact,\ngradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our\nwork, identifies both as the theoretically sound implementations of the RKL\nobjective. In contrast, we show that the recently adopted '$k_3$ as loss' (like\nin GRPO) is merely a first-order, biased approximation of the principled loss.\nFurthermore, we argue that common off-policy implementations of '$k_n$ as loss'\nmethods are biased due to neglected importance sampling, and we propose a\nprincipled correction. Our findings provide a comprehensive, gradient-based\nrationale for choosing and correctly implementing KL regularization, paving the\nway for more robust and effective RLHF systems.",
      "authors": [
        "Kezhao Liu",
        "Jason Klein Liu",
        "Mingtao Chen",
        "Yiming Liu"
      ],
      "published": "2025-10-02T01:00:02Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01555v1"
    },
    {
      "arxiv_id": "2510.01552v1",
      "title": "POLAR: Automating Cyber Threat Prioritization through LLM-Powered\n  Assessment",
      "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research.",
      "authors": [
        "Luoxi Tang",
        "Yuqiao Meng",
        "Ankita Patra",
        "Weicheng Ma",
        "Muchao Ye",
        "Zhaohan Xi"
      ],
      "published": "2025-10-02T00:49:20Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01552v1"
    },
    {
      "arxiv_id": "2510.01545v1",
      "title": "Predictive Preference Learning from Human Interventions",
      "summary": "Learning from human involvement aims to incorporate the human subject to\nmonitor and correct agent behavior errors. Although most interactive imitation\nlearning methods focus on correcting the agent's action at the current state,\nthey do not adjust its actions in future states, which may be potentially more\nhazardous. To address this, we introduce Predictive Preference Learning from\nHuman Interventions (PPL), which leverages the implicit preference signals\ncontained in human interventions to inform predictions of future rollouts. The\nkey idea of PPL is to bootstrap each human intervention into L future time\nsteps, called the preference horizon, with the assumption that the agent\nfollows the same action and the human makes the same intervention in the\npreference horizon. By applying preference optimization on these future states,\nexpert corrections are propagated into the safety-critical regions where the\nagent is expected to explore, significantly improving learning efficiency and\nreducing human demonstrations needed. We evaluate our approach with experiments\non both autonomous driving and robotic manipulation benchmarks and demonstrate\nits efficiency and generality. Our theoretical analysis further shows that\nselecting an appropriate preference horizon L balances coverage of risky states\nwith label correctness, thereby bounding the algorithmic optimality gap. Demo\nand code are available at: https://metadriverse.github.io/ppl",
      "authors": [
        "Haoyuan Cai",
        "Zhenghao Peng",
        "Bolei Zhou"
      ],
      "published": "2025-10-02T00:38:18Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01545v1"
    },
    {
      "arxiv_id": "2510.01544v1",
      "title": "Step-Aware Policy Optimization for Reasoning in Diffusion Large Language\n  Models",
      "summary": "Diffusion language models (dLLMs) offer a promising, non-autoregressive\nparadigm for text generation, yet training them for complex reasoning remains a\nkey challenge. Current reinforcement learning approaches often rely on sparse,\noutcome-based rewards, which can reinforce flawed reasoning paths that lead to\ncoincidentally correct answers. We argue that this stems from a fundamental\nmismatch with the natural structure of reasoning. We first propose a\ntheoretical framework that formalizes complex problem solving as a hierarchical\nselection process, where an intractable global constraint is decomposed into a\nseries of simpler, localized logical steps. This framework provides a\nprincipled foundation for algorithm design, including theoretical insights into\nthe identifiability of this latent reasoning structure. Motivated by this\ntheory, we identify unstructured refinement -- a failure mode where a model's\niterative steps do not contribute meaningfully to the solution -- as a core\ndeficiency in existing methods. We then introduce Step-Aware Policy\nOptimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising\nprocess with the latent reasoning hierarchy. By using a process-based reward\nfunction that encourages incremental progress, SAPO guides the model to learn\nstructured, coherent reasoning paths. Our empirical results show that this\nprincipled approach significantly improves performance on challenging reasoning\nbenchmarks and enhances the interpretability of the generation process.",
      "authors": [
        "Shaoan Xie",
        "Lingjing Kong",
        "Xiangchen Song",
        "Xinshuai Dong",
        "Guangyi Chen",
        "Eric P. Xing",
        "Kun Zhang"
      ],
      "published": "2025-10-02T00:34:15Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01544v1"
    },
    {
      "arxiv_id": "2510.01531v1",
      "title": "Information Seeking for Robust Decision Making under Partial\n  Observability",
      "summary": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io",
      "authors": [
        "Djengo Cyun-Jyun Fang",
        "Tsung-Wei Ke"
      ],
      "published": "2025-10-02T00:06:32Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01531v1"
    },
    {
      "arxiv_id": "2510.01530v1",
      "title": "LOGicalThought: Logic-Based Ontological Grounding of LLMs for\n  High-Assurance Reasoning",
      "summary": "High-assurance reasoning, particularly in critical domains such as law and\nmedicine, requires conclusions that are accurate, verifiable, and explicitly\ngrounded in evidence. This reasoning relies on premises codified from rules,\nstatutes, and contracts, inherently involving defeasible or non-monotonic logic\ndue to numerous exceptions, where the introduction of a single fact can\ninvalidate general rules, posing significant challenges. While large language\nmodels (LLMs) excel at processing natural language, their capabilities in\nstandard inference tasks do not translate to the rigorous reasoning required\nover high-assurance text guidelines. Core reasoning challenges within such\ntexts often manifest specific logical structures involving negation,\nimplication, and, most critically, defeasible rules and exceptions. In this\npaper, we propose a novel neurosymbolically-grounded architecture called\nLOGicalThought (LogT) that uses an advanced logical language and reasoner in\nconjunction with an LLM to construct a dual symbolic graph context and\nlogic-based context. These two context representations transform the problem\nfrom inference over long-form guidelines into a compact grounded evaluation.\nEvaluated on four multi-domain benchmarks against four baselines, LogT improves\noverall performance by 11.84% across all LLMs. Performance improves\nsignificantly across all three modes of reasoning: by up to +10.2% on negation,\n+13.2% on implication, and +5.5% on defeasible reasoning compared to the\nstrongest baseline.",
      "authors": [
        "Navapat Nananukul",
        "Yue Zhang",
        "Ryan Lee",
        "Eric Boxer",
        "Jonathan May",
        "Vibhav Giridhar Gogate",
        "Jay Pujara",
        "Mayank Kejriwal"
      ],
      "published": "2025-10-02T00:06:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01530v1"
    },
    {
      "arxiv_id": "2510.01528v1",
      "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse\n  Autoencoder-Guided Generation",
      "summary": "We propose a novel method that leverages sparse autoencoders (SAEs) and\nclustering techniques to analyze the internal token representations of large\nlanguage models (LLMs) and guide generations in mathematical reasoning tasks.\nOur approach first trains an SAE to generate sparse vector representations for\ntraining tokens, then applies k-means clustering to construct a graph where\nvertices represent token clusters and weighted edges capture sequential token\ntransitions. Using this graph, we define an edge-weight based reward function\nto quantify adherence to established reasoning traces, thereby identifying\nexploitative reasoning trajectories. Additionally, we measure generation\ndiversity from clustering to assess the extent of exploration. Our findings\nindicate that balancing both exploitation and exploration is crucial for\nachieving high accuracy in mathematical reasoning tasks. During generation, the\nSAE can serve as a scalable reward model to guide generations, ensuring a\nbalanced trade-off between exploitation and exploration. This prevents extreme\nbehaviors in either direction, ultimately fostering a higher-quality reasoning\nprocess in LLMs.",
      "authors": [
        "Daniel Zhao",
        "Abhilash Shankarampeta",
        "Lanxiang Hu",
        "Tajana Rosing",
        "Hao Zhang"
      ],
      "published": "2025-10-02T00:01:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01528v1"
    },
    {
      "arxiv_id": "2510.01524v1",
      "title": "WALT: Web Agents that Learn Tools",
      "summary": "Web agents promise to automate complex browser tasks, but current methods\nremain brittle -- relying on step-by-step UI interactions and heavy LLM\nreasoning that break under dynamic layouts and long horizons. Humans, by\ncontrast, exploit website-provided functionality through high-level operations\nlike search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),\na framework that reverse-engineers latent website functionality into reusable\ninvocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust\nimplementations of automations already designed into websites -- spanning\ndiscovery (search, filter, sort), communication (post, comment, upvote), and\ncontent management (create, edit, delete). Tools abstract away low-level\nexecution: instead of reasoning about how to click and type, agents simply call\nsearch(query) or create(listing). This shifts the computational burden from\nfragile step-by-step reasoning to reliable tool invocation. On VisualWebArena\nand WebArena, WALT achieves higher success with fewer steps and less\nLLM-dependent reasoning, establishing a robust and generalizable paradigm for\nbrowser automation.",
      "authors": [
        "Viraj Prabhu",
        "Yutong Dai",
        "Matthew Fernandez",
        "Jing Gu",
        "Krithika Ramakrishnan",
        "Yanqi Luo",
        "Silvio Savarese",
        "Caiming Xiong",
        "Junnan Li",
        "Zeyuan Chen",
        "Ran Xu"
      ],
      "published": "2025-10-01T23:41:47Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01524v1"
    },
    {
      "arxiv_id": "2510.01520v1",
      "title": "Predictive Modeling and Explainable AI for Veterinary Safety Profiles,\n  Residue Assessment, and Health Outcomes Using Real-World Data and\n  Physicochemical Properties",
      "summary": "The safe use of pharmaceuticals in food-producing animals is vital to protect\nanimal welfare and human food safety. Adverse events (AEs) may signal\nunexpected pharmacokinetic or toxicokinetic effects, increasing the risk of\nviolative residues in the food chain. This study introduces a predictive\nframework for classifying outcomes (Death vs. Recovery) using ~1.28 million\nreports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary\nMedicine. A preprocessing pipeline merged relational tables and standardized\nAEs through VeDDRA ontologies. Data were normalized, missing values imputed,\nand high-cardinality features reduced; physicochemical drug properties were\nintegrated to capture chemical-residue links. We evaluated supervised models,\nincluding Random Forest, CatBoost, XGBoost, ExcelFormer, and large language\nmodels (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as\nundersampling and oversampling, with a focus on prioritizing recall for fatal\noutcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,\nachieving precision, recall, and F1-scores of 0.95. Incorporating Average\nUncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved\nminority-class detection, particularly in ExcelFormer and XGBoost.\nInterpretability via SHAP identified biologically plausible predictors,\nincluding lung, heart, and bronchial disorders, animal demographics, and drug\nphysicochemical properties. These features were strongly linked to fatal\noutcomes. Overall, the framework shows that combining rigorous data\nengineering, advanced machine learning, and explainable AI enables accurate,\ninterpretable predictions of veterinary safety outcomes. The approach supports\nFARAD's mission by enabling early detection of high-risk drug-event profiles,\nstrengthening residue risk assessment, and informing regulatory and clinical\ndecision-making.",
      "authors": [
        "Hossein Sholehrasa",
        "Xuan Xu",
        "Doina Caragea",
        "Jim E. Riviere",
        "Majid Jaberi-Douraki"
      ],
      "published": "2025-10-01T23:34:46Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01520v1"
    },
    {
      "arxiv_id": "2510.01513v1",
      "title": "From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods\n  for Multimodal Content Analysis and Understanding",
      "summary": "Analysis of multi-modal content can be tricky, computationally expensive, and\nrequire a significant amount of engineering efforts. Lots of work with\npre-trained models on static data is out there, yet fusing these opensource\nmodels and methods with complex data such as videos is relatively challenging.\nIn this paper, we present a framework that enables efficiently prototyping\npipelines for multi-modal content analysis. We craft a candidate recipe for a\npipeline, marrying a set of pre-trained models, to convert videos into a\ntemporal semi-structured data format. We translate this structure further to a\nframe-level indexed knowledge graph representation that is query-able and\nsupports continual learning, enabling the dynamic incorporation of new\ndomain-specific knowledge through an interactive medium.",
      "authors": [
        "Basem Rizk",
        "Joel Walsh",
        "Mark Core",
        "Benjamin Nye"
      ],
      "published": "2025-10-01T23:20:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01513v1"
    },
    {
      "arxiv_id": "2510.01500v1",
      "title": "Lateral Tree-of-Thoughts Surpasses ToT by Incorporating\n  Logically-Consistent, Low-Utility Candidates",
      "summary": "Modern deployments increasingly allocate large test-time compute (thousands\nof tokens or many node expansions) to boost reliability. Under such budgets,\nstandard Tree-of-Thoughts-style search exhibits two pathologies: breadth\nsaturation (additional samples mostly produce near-duplicates, so width stops\ngrowing) and depth myopia (noisy short-horizon utilities prune branches whose\npayoff appears after a few more steps). We propose Lateral Tree-of-Thoughts\n(LToT), a drop-in controller that separates utility from logical consistency\nand treats low-utility but consistent candidates as assets rather than waste.\nThe frontier is split into mainlines (high-utility candidates used for\nexploitation) and laterals (consistent, initially low-utility candidates that\nreceive short, cheap probes before judgment). LToT explores laterals via\nLateral Racing with Short-Circuit (LR--SC): a capped successive-halving race\nthat spreads tiny probes across a very wide lateral set, uses width-aware\nthresholds with repeat-to-confirm, and immediately promotes a branch once its\nenvelope clears the mainline bar; mainlines are kept intentionally narrow so\nsurplus compute is invested where width is cheap. We prove a pseudolinear\nlateral cost $\\Theta(N_0 \\log_{\\eta} N_0)$ with logarithmically many rungs\n(initial lateral width $N_0$; culling factor $\\eta>1$), in contrast to the\nexponential growth of uncapped mainlines. Empirical evaluations on benchmark\ntasks are in preparation and will be added in a future revision. In short, LToT\nturns large test-time budgets into principled diversity while preserving\npromotion discipline, mitigating saturation and myopia without inflating\ncompute.",
      "authors": [
        "Abhinav Madahar"
      ],
      "published": "2025-10-01T22:23:58Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01500v1"
    },
    {
      "arxiv_id": "2510.01499v1",
      "title": "Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order\n  Information",
      "summary": "With the rapid progress of multi-agent large language model (LLM) reasoning,\nhow to effectively aggregate answers from multiple LLMs has emerged as a\nfundamental challenge. Standard majority voting treats all answers equally,\nfailing to consider latent heterogeneity and correlation across models. In this\nwork, we design two new aggregation algorithms called Optimal Weight (OW) and\nInverse Surprising Popularity (ISP), leveraging both first-order and\nsecond-order information. Our theoretical analysis shows these methods provably\nmitigate inherent limitations of majority voting under mild assumptions,\nleading to more reliable collective decisions. We empirically validate our\nalgorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as\nUltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all\ncases, our methods consistently outperform majority voting, offering both\npractical performance gains and conceptual insights for the design of robust\nmulti-agent LLM pipelines.",
      "authors": [
        "Rui Ai",
        "Yuqi Pan",
        "David Simchi-Levi",
        "Milind Tambe",
        "Haifeng Xu"
      ],
      "published": "2025-10-01T22:21:50Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01499v1"
    },
    {
      "arxiv_id": "2510.01498v1",
      "title": "AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA\n  Imaging",
      "summary": "While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic\naneurysms (AAA), the required iodinated contrast agents pose significant risks,\nincluding nephrotoxicity, patient allergies, and environmental harm. To reduce\ncontrast agent use, recent deep learning methods have focused on generating\nsynthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a\nmulti-stage pipeline that first generates images and then performs\nsegmentation, which leads to error accumulation and fails to leverage shared\nsemantic and anatomical structures. To address this, we propose a unified deep\nlearning framework that generates synthetic CECT images from NCCT scans while\nsimultaneously segmenting the aortic lumen and thrombus. Our approach\nintegrates conditional diffusion models (CDM) with multi-task learning,\nenabling end-to-end joint optimization of image synthesis and anatomical\nsegmentation. Unlike previous multitask diffusion models, our approach requires\nno initial predictions (e.g., a coarse segmentation mask), shares both encoder\nand decoder parameters across tasks, and employs a semi-supervised training\nstrategy to learn from scans with missing segmentation labels, a common\nconstraint in real-world clinical data. We evaluated our method on a cohort of\n264 patients, where it consistently outperformed state-of-the-art single-task\nand multi-stage models. For image synthesis, our model achieved a PSNR of 25.61\ndB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,\nit improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus\nDice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to\nmore accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm\nfrom 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to\nnnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.",
      "authors": [
        "Yuxuan Ou",
        "Ning Bi",
        "Jiazhen Pan",
        "Jiancheng Yang",
        "Boliang Yu",
        "Usama Zidan",
        "Regent Lee",
        "Vicente Grau"
      ],
      "published": "2025-10-01T22:19:27Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01498v1"
    },
    {
      "arxiv_id": "2510.01494v1",
      "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks\n  Fail Where Data-Space Attacks Succeed",
      "summary": "The field of adversarial robustness has long established that adversarial\nexamples can successfully transfer between image classifiers and that text\njailbreaks can successfully transfer between language models (LMs). However, a\npair of recent studies reported being unable to successfully transfer image\njailbreaks between vision-language models (VLMs). To explain this striking\ndifference, we propose a fundamental distinction regarding the transferability\nof attacks against machine learning models: attacks in the input data-space can\ntransfer, whereas attacks in model representation space do not, at least not\nwithout geometric alignment of representations. We then provide theoretical and\nempirical evidence of this hypothesis in four different settings. First, we\nmathematically prove this distinction in a simple setting where two networks\ncompute the same input-output map but via different representations. Second, we\nconstruct representation-space attacks against image classifiers that are as\nsuccessful as well-known data-space attacks, but fail to transfer. Third, we\nconstruct representation-space attacks against LMs that successfully jailbreak\nthe attacked models but again fail to transfer. Fourth, we construct data-space\nattacks against VLMs that successfully transfer to new VLMs, and we show that\nrepresentation space attacks \\emph{can} transfer when VLMs' latent geometries\nare sufficiently aligned in post-projector space. Our work reveals that\nadversarial transfer is not an inherent property of all attacks but contingent\non their operational domain - the shared data-space versus models' unique\nrepresentation spaces - a critical insight for building more robust models.",
      "authors": [
        "Isha Gupta",
        "Rylan Schaeffer",
        "Joshua Kazdan",
        "Ken Liu",
        "Sanmi Koyejo"
      ],
      "published": "2025-10-01T22:10:58Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01494v1"
    },
    {
      "arxiv_id": "2510.01483v1",
      "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification\n  using Spatiotemporal Knowledge Graphs",
      "summary": "Vision-language models (VLMs) have shown potential for robot navigation but\nencounter fundamental limitations: they lack persistent scene memory, offer\nlimited spatial reasoning, and do not scale effectively with video duration for\nreal-time application. We present VL-KnG, a Visual Scene Understanding system\nthat tackles these challenges using spatiotemporal knowledge graph construction\nand computationally efficient query processing for navigation goal\nidentification. Our approach processes video sequences in chunks utilizing\nmodern VLMs, creates persistent knowledge graphs that maintain object identity\nover time, and enables explainable spatial reasoning through queryable graph\nstructures. We also introduce WalkieKnowledge, a new benchmark with about 200\nmanually annotated questions across 8 diverse trajectories spanning\napproximately 100 minutes of video data, enabling fair comparison between\nstructured approaches and general-purpose VLMs. Real-world deployment on a\ndifferential drive robot demonstrates practical applicability, with our method\nachieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5\nPro performance while providing explainable reasoning supported by the\nknowledge graph, computational efficiency for real-time deployment across\ndifferent tasks, such as localization, navigation and planning. Code and\ndataset will be released after acceptance.",
      "authors": [
        "Mohamad Al Mdfaa",
        "Svetlana Lukina",
        "Timur Akhtyamov",
        "Arthur Nigmatzyanov",
        "Dmitrii Nalberskii",
        "Sergey Zagoruyko",
        "Gonzalo Ferrer"
      ],
      "published": "2025-10-01T21:53:44Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01483v1"
    },
    {
      "arxiv_id": "2510.01480v1",
      "title": "Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules",
      "summary": "The integration of artificial intelligence (AI) in early-stage drug discovery\noffers unprecedented opportunities for exploring chemical space and\naccelerating hit-to-lead optimization. However, docking optimization in\ngenerative approaches is computationally expensive and may lead to inaccurate\nresults. Here, we present a novel generative framework that balances\npharmacophore similarity to reference compounds with structural diversity from\nactive molecules. The framework allows users to provide custom reference sets,\nincluding FDA-approved drugs or clinical candidates, and guides the \\textit{de\nnovo} generation of potential therapeutics. We demonstrate its applicability\nthrough a case study targeting estrogen receptor modulators and antagonists for\nbreast cancer. The generated compounds maintain high pharmacophoric fidelity to\nknown active molecules while introducing substantial structural novelty,\nsuggesting strong potential for functional innovation and patentability.\nComprehensive evaluation of the generated molecules against common drug-like\nproperties confirms the robustness and pharmaceutical relevance of the\napproach.",
      "authors": [
        "Ekaterina Podplutova",
        "Anastasia Vepreva",
        "Olga A. Konovalova",
        "Vladimir Vinogradov",
        "Dmitrii O. Shkil",
        "Andrei Dmitrenko"
      ],
      "published": "2025-10-01T21:45:58Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.01480v1"
    },
    {
      "arxiv_id": "2510.01478v1",
      "title": "Purrception: Variational Flow Matching for Vector-Quantized Image\n  Generation",
      "summary": "We introduce Purrception, a variational flow matching approach for\nvector-quantized image generation that provides explicit categorical\nsupervision while maintaining continuous transport dynamics. Our method adapts\nVariational Flow Matching to vector-quantized latents by learning categorical\nposteriors over codebook indices while computing velocity fields in the\ncontinuous embedding space. This combines the geometric awareness of continuous\nmethods with the discrete supervision of categorical approaches, enabling\nuncertainty quantification over plausible codes and temperature-controlled\ngeneration. We evaluate Purrception on ImageNet-1k 256x256 generation. Training\nconverges faster than both continuous flow matching and discrete flow matching\nbaselines while achieving competitive FID scores with state-of-the-art models.\nThis demonstrates that Variational Flow Matching can effectively bridge\ncontinuous transport and discrete supervision for improved training efficiency\nin image generation.",
      "authors": [
        "Răzvan-Andrei Matişan",
        "Vincent Tao Hu",
        "Grigory Bartosh",
        "Björn Ommer",
        "Cees G. M. Snoek",
        "Max Welling",
        "Jan-Willem van de Meent",
        "Mohammad Mahdi Derakhshani",
        "Floor Eijkelboom"
      ],
      "published": "2025-10-01T21:41:30Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01478v1"
    },
    {
      "arxiv_id": "2510.01474v1",
      "title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation\n  Compliance",
      "summary": "As governments move to regulate AI, there is growing interest in using Large\nLanguage Models (LLMs) to assess whether or not an AI system complies with a\ngiven AI Regulation (AIR). However, there is presently no way to benchmark the\nperformance of LLMs at this task. To fill this void, we introduce AIReg-Bench:\nthe first benchmark dataset designed to test how well LLMs can assess\ncompliance with the EU AI Act (AIA). We created this dataset through a two-step\nprocess: (1) by prompting an LLM with carefully structured instructions, we\ngenerated 120 technical documentation excerpts (samples), each depicting a\nfictional, albeit plausible, AI system - of the kind an AI provider might\nproduce to demonstrate their compliance with AIR; (2) legal experts then\nreviewed and annotated each sample to indicate whether, and in what way, the AI\nsystem described therein violates specific Articles of the AIA. The resulting\ndataset, together with our evaluation of whether frontier LLMs can reproduce\nthe experts' compliance labels, provides a starting point to understand the\nopportunities and limitations of LLM-based AIR compliance assessment tools and\nestablishes a benchmark against which subsequent LLMs can be compared. The\ndataset and evaluation code are available at\nhttps://github.com/camlsys/aireg-bench.",
      "authors": [
        "Bill Marino",
        "Rosco Hunter",
        "Zubair Jamali",
        "Marinos Emmanouil Kalpakos",
        "Mudra Kashyap",
        "Isaiah Hinton",
        "Alexa Hanson",
        "Maahum Nazir",
        "Christoph Schnabl",
        "Felix Steffek",
        "Hongkai Wen",
        "Nicholas D. Lane"
      ],
      "published": "2025-10-01T21:33:33Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01474v1"
    },
    {
      "arxiv_id": "2510.01473v1",
      "title": "From keywords to semantics: Perceptions of large language models in data\n  discovery",
      "summary": "Current approaches to data discovery match keywords between metadata and\nqueries. This matching requires researchers to know the exact wording that\nother researchers previously used, creating a challenging process that could\nlead to missing relevant data. Large Language Models (LLMs) could enhance data\ndiscovery by removing this requirement and allowing researchers to ask\nquestions with natural language. However, we do not currently know if\nresearchers would accept LLMs for data discovery. Using a human-centered\nartificial intelligence (HCAI) focus, we ran focus groups (N = 27) to\nunderstand researchers' perspectives towards LLMs for data discovery. Our\nconceptual model shows that the potential benefits are not enough for\nresearchers to use LLMs instead of current technology. Barriers prevent\nresearchers from fully accepting LLMs, but features around transparency could\novercome them. Using our model will allow developers to incorporate features\nthat result in an increased acceptance of LLMs for data discovery.",
      "authors": [
        "Maura E Halstead",
        "Mark A. Green",
        "Caroline Jay",
        "Richard Kingston",
        "David Topping",
        "Alexander Singleton"
      ],
      "published": "2025-10-01T21:31:54Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.01473v1"
    },
    {
      "arxiv_id": "2510.01462v1",
      "title": "RealClass: A Framework for Classroom Speech Simulation with Public\n  Datasets and Game Engines",
      "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Classroom datasets remain\nlimited and not publicly available, and the absence of dedicated classroom\nnoise or Room Impulse Response (RIR) corpora prevents the use of standard data\naugmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise and RIRs using game engines, a versatile framework that can extend to\nother domains beyond the classroom. Building on this methodology, we present\nRealClass, a dataset that combines a synthesized classroom noise corpus with a\nclassroom speech dataset compiled from publicly available corpora. The speech\ndata pairs a children's speech corpus with instructional speech extracted from\nYouTube videos to approximate real classroom interactions in clean conditions.\nExperiments on clean and noisy speech show that RealClass closely approximates\nreal classroom speech, making it a valuable asset in the absence of abundant\nreal classroom speech.",
      "authors": [
        "Ahmed Adel Attia",
        "Jing Liu",
        "Carol Espy Wilson"
      ],
      "published": "2025-10-01T21:04:51Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.01462v1"
    },
    {
      "arxiv_id": "2510.01460v1",
      "title": "The Three Regimes of Offline-to-Online Reinforcement Learning",
      "summary": "Offline-to-online reinforcement learning (RL) has emerged as a practical\nparadigm that leverages offline datasets for pretraining and online\ninteractions for fine-tuning. However, its empirical behavior is highly\ninconsistent: design choices of online-fine tuning that work well in one\nsetting can fail completely in another. We propose a stability--plasticity\nprinciple that can explain this inconsistency: we should preserve the knowledge\nof pretrained policy or offline dataset during online fine-tuning, whichever is\nbetter, while maintaining sufficient plasticity. This perspective identifies\nthree regimes of online fine-tuning, each requiring distinct stability\nproperties. We validate this framework through a large-scale empirical study,\nfinding that the results strongly align with its predictions in 45 of 63 cases.\nThis work provides a principled framework for guiding design choices in\noffline-to-online RL based on the relative performance of the offline dataset\nand the pretrained policy.",
      "authors": [
        "Lu Li",
        "Tianwei Ni",
        "Yihao Sun",
        "Pierre-Luc Bacon"
      ],
      "published": "2025-10-01T20:58:14Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01460v1"
    },
    {
      "arxiv_id": "2510.01453v1",
      "title": "The Command Line GUIde: Graphical Interfaces from Man Pages via AI",
      "summary": "Although birthed in the era of teletypes, the command line shell survived the\ngraphical interface revolution of the 1980's and lives on in modern desktop\noperating systems. The command line provides access to powerful functionality\nnot otherwise exposed on the computer, but requires users to recall textual\nsyntax and carefully scour documentation. In contrast, graphical interfaces let\nusers organically discover and invoke possible actions through widgets and\nmenus. To better expose the power of the command line, we demonstrate a\nmechanism for automatically creating graphical interfaces for command line\ntools by translating their documentation (in the form of man pages) into\ninterface specifications via AI. Using these specifications, our user-facing\nsystem, called GUIde, presents the command options to the user graphically. We\nevaluate the generated interfaces on a corpus of commands to show to what\ndegree GUIde offers thorough graphical interfaces for users' real-world command\nline tasks.",
      "authors": [
        "Saketh Ram Kasibatla",
        "Kiran Medleri Hiremath",
        "Raven Rothkopf",
        "Sorin Lerner",
        "Haijun Xia",
        "Brian Hempel"
      ],
      "published": "2025-10-01T20:46:53Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.01453v1"
    },
    {
      "arxiv_id": "2510.01450v1",
      "title": "Local Linear Attention: An Optimal Interpolation of Linear and Softmax\n  Attention For Test-Time Regression",
      "summary": "Transformer architectures have achieved remarkable success in various\ndomains. While efficient alternatives to Softmax Attention have been widely\nstudied, the search for more expressive mechanisms grounded in theoretical\ninsight-even at greater computational cost-has been relatively underexplored.\nIn this work, we bridge this gap by proposing Local Linear Attention (LLA), a\nnovel attention mechanism derived from nonparametric statistics through the\nlens of test-time regression. First, we show that LLA offers theoretical\nadvantages over Linear and Softmax Attention for associative memory via a\nbias-variance trade-off analysis. Next, we address its computational challenges\nand propose two memory-efficient primitives to tackle the $\\Theta(n^2 d)$ and\n$\\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,\nblockwise algorithm that enables scalable and parallel computation on modern\naccelerators. In addition, we implement and profile a customized inference\nkernel that significantly reduces memory overheads. Finally, we empirically\nvalidate the advantages and limitations of LLA on test-time regression,\nin-context regression, associative recall and state tracking tasks. Experiment\nresults demonstrate that LLA effectively adapts to non-stationarity,\noutperforming strong baselines in test-time training and in-context learning,\nand exhibiting promising evidence for its scalability and applicability in\nlarge-scale models. Code is available at\nhttps://github.com/Yifei-Zuo/Flash-LLA.",
      "authors": [
        "Yifei Zuo",
        "Yutong Yin",
        "Zhichen Zeng",
        "Ang Li",
        "Banghua Zhu",
        "Zhaoran Wang"
      ],
      "published": "2025-10-01T20:42:21Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01450v1"
    },
    {
      "arxiv_id": "2510.01448v1",
      "title": "GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of\n  Geographic Embeddings",
      "summary": "Worldwide visual geo-localization seeks to determine the geographic location\nof an image anywhere on Earth using only its visual content. Learned\nrepresentations of geography for visual geo-localization remain an active\nresearch topic despite much progress. We formulate geo-localization as aligning\nthe visual representation of the query image with a learned geographic\nrepresentation. Our novel geographic representation explicitly models the world\nas a hierarchy of geographic embeddings. Additionally, we introduce an approach\nto efficiently fuse the appearance features of the query image with its\nsemantic segmentation map, forming a robust visual representation. Our main\nexperiments demonstrate improved all-time bests in 22 out of 25 metrics\nmeasured across five benchmark datasets compared to prior state-of-the-art\n(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional\nablation studies support the claim that these gains are primarily driven by the\ncombination of geographic and visual representations.",
      "authors": [
        "Angel Daruna",
        "Nicholas Meegan",
        "Han-Pang Chiu",
        "Supun Samarasekera",
        "Rakesh Kumar"
      ],
      "published": "2025-10-01T20:39:48Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01448v1"
    },
    {
      "arxiv_id": "2510.01444v1",
      "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
      "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce $\\textbf{VOGUE (Visual Uncertainty Guided\nExploration)}$, a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.",
      "authors": [
        "Rui Liu",
        "Dian Yu",
        "Tong Zheng",
        "Runpeng Dai",
        "Zongxia Li",
        "Wenhao Yu",
        "Zhenwen Liang",
        "Linfeng Song",
        "Haitao Mi",
        "Pratap Tokekar",
        "Dong Yu"
      ],
      "published": "2025-10-01T20:32:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01444v1"
    },
    {
      "arxiv_id": "2510.01433v1",
      "title": "AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for\n  Generalizable and Lightweight Robotic Manipulation",
      "summary": "Vision-based robot learning often relies on dense image or point-cloud\ninputs, which are computationally heavy and entangle irrelevant background\nfeatures. Existing keypoint-based approaches can focus on manipulation-centric\nfeatures and be lightweight, but either depend on manual heuristics or\ntask-coupled selection, limiting scalability and semantic understanding. To\naddress this, we propose AFFORD2ACT, an affordance-guided framework that\ndistills a minimal set of semantic 2D keypoints from a text prompt and a single\nimage. AFFORD2ACT follows a three-stage pipeline: affordance filtering,\ncategory-level keypoint construction, and transformer-based policy learning\nwith embedded gating to reason about the most relevant keypoints, yielding a\ncompact 38-dimensional state policy that can be trained in 15 minutes, which\nperforms well in real-time without proprioception or dense representations.\nAcross diverse real-world manipulation tasks, AFFORD2ACT consistently improves\ndata efficiency, achieving an 82% success rate on unseen objects, novel\ncategories, backgrounds, and distractors.",
      "authors": [
        "Anukriti Singh",
        "Kasra Torshizi",
        "Khuzema Habib",
        "Kelin Yu",
        "Ruohan Gao",
        "Pratap Tokekar"
      ],
      "published": "2025-10-01T20:13:39Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01433v1"
    },
    {
      "arxiv_id": "2510.01432v1",
      "title": "On the Role of Domain Experts in Creating Effective Tutoring Systems",
      "summary": "The role that highly curated knowledge, provided by domain experts, could\nplay in creating effective tutoring systems is often overlooked within the AI\nfor education community. In this paper, we highlight this topic by discussing\ntwo ways such highly curated expert knowledge could help in creating novel\neducational systems. First, we will look at how one could use explainable AI\n(XAI) techniques to automatically create lessons. Most existing XAI methods are\nprimarily aimed at debugging AI systems. However, we will discuss how one could\nuse expert specified rules about solving specific problems along with novel XAI\ntechniques to automatically generate lessons that could be provided to\nlearners. Secondly, we will see how an expert specified curriculum for learning\na target concept can help develop adaptive tutoring systems, that can not only\nprovide a better learning experience, but could also allow us to use more\nefficient algorithms to create these systems. Finally, we will highlight the\nimportance of such methods using a case study of creating a tutoring system for\npollinator identification, where such knowledge could easily be elicited from\nexperts.",
      "authors": [
        "Sarath Sreedharan",
        "Kelsey Sikes",
        "Nathaniel Blanchard",
        "Lisa Mason",
        "Nikhil Krishnaswamy",
        "Jill Zarestky"
      ],
      "published": "2025-10-01T20:12:57Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01432v1"
    },
    {
      "arxiv_id": "2510.01428v1",
      "title": "BioVERSE: Representation Alignment of Biomedical Modalities to LLMs for\n  Multi-Modal Reasoning",
      "summary": "Recent advances in large language models (LLMs) and biomedical foundation\nmodels (BioFMs) have achieved strong results in biological text reasoning,\nmolecular modeling, and single-cell analysis, yet they remain siloed in\ndisjoint embedding spaces, limiting cross-modal reasoning. We present BIOVERSE\n(Biomedical Vector Embedding Realignment for Semantic Engagement), a two-stage\napproach that adapts pretrained BioFMs as modality encoders and aligns them\nwith LLMs through lightweight, modality-specific projection layers. The\napproach first aligns each modality to a shared LLM space through independently\ntrained projections, allowing them to interoperate naturally, and then applies\nstandard instruction tuning with multi-modal data to bring them together for\ndownstream reasoning. By unifying raw biomedical data with knowledge embedded\nin LLMs, the approach enables zero-shot annotation, cross-modal question\nanswering, and interactive, explainable dialogue. Across tasks spanning\ncell-type annotation, molecular description, and protein function reasoning,\ncompact BIOVERSE configurations surpass larger LLM baselines while enabling\nricher, generative outputs than existing BioFMs, establishing a foundation for\nprincipled multi-modal biomedical reasoning.",
      "authors": [
        "Ching-Huei Tsou",
        "Michal Ozery-Flato",
        "Ella Barkan",
        "Diwakar Mahajan",
        "Ben Shapira"
      ],
      "published": "2025-10-01T20:07:36Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.01428v1"
    },
    {
      "arxiv_id": "2510.01427v1",
      "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge\n  Mining",
      "summary": "At the core of Deep Research is knowledge mining, the task of extracting\nstructured information from massive unstructured text in response to user\ninstructions. Large language models (LLMs) excel at interpreting such\ninstructions but are prohibitively expensive to deploy at scale, while\ntraditional pipelines of classifiers and extractors remain efficient yet\nbrittle and unable to generalize to new tasks. We introduce Falconer, a\ncollaborative framework that combines the agentic reasoning of LLMs with\nlightweight proxy models for scalable knowledge mining. In Falconer, LLMs act\nas planners, decomposing user instructions into executable pipelines, and as\nannotators, generating supervision to train small proxies. The framework\nunifies classification and extraction into two atomic operations, get label and\nget span, enabling a single instruction-following model to replace multiple\ntask-specific components. To evaluate the consistency between proxy models\nincubated by Falconer and annotations provided by humans and large models, we\nconstruct new benchmarks covering both planning and end-to-end execution.\nExperiments show that Falconer closely matches state-of-the-art LLMs in\ninstruction-following accuracy while reducing inference cost by up to 90% and\naccelerating large-scale knowledge mining by more than 20x, offering an\nefficient and scalable foundation for Deep Research.",
      "authors": [
        "Sipeng Zhang",
        "Longfei Yun",
        "Zilong Wang",
        "Jingbo Shang",
        "Letian Peng"
      ],
      "published": "2025-10-01T20:06:48Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01427v1"
    },
    {
      "arxiv_id": "2510.01414v1",
      "title": "Risk Phase Transitions in Spiked Regression: Alignment Driven Benign and\n  Catastrophic Overfitting",
      "summary": "This paper analyzes the generalization error of minimum-norm interpolating\nsolutions in linear regression using spiked covariance data models. The paper\ncharacterizes how varying spike strengths and target-spike alignments can\naffect risk, especially in overparameterized settings. The study presents an\nexact expression for the generalization error, leading to a comprehensive\nclassification of benign, tempered, and catastrophic overfitting regimes based\non spike strength, the aspect ratio $c=d/n$ (particularly as $c \\to \\infty$),\nand target alignment. Notably, in well-specified aligned problems, increasing\nspike strength can surprisingly induce catastrophic overfitting before\nachieving benign overfitting. The paper also reveals that target-spike\nalignment is not always advantageous, identifying specific, sometimes\ncounterintuitive, conditions for its benefit or detriment. Alignment with the\nspike being detrimental is empirically demonstrated to persist in nonlinear\nmodels.",
      "authors": [
        "Jiping Li",
        "Rishi Sonthalia"
      ],
      "published": "2025-10-01T19:51:47Z",
      "primary_category": "stat.ML",
      "arxiv_url": "https://arxiv.org/abs/2510.01414v1"
    },
    {
      "arxiv_id": "2510.01409v1",
      "title": "OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity\n  Logs with Large Language Models",
      "summary": "System logs represent a valuable source of Cyber Threat Intelligence (CTI),\ncapturing attacker behaviors, exploited vulnerabilities, and traces of\nmalicious activity. Yet their utility is often limited by lack of structure,\nsemantic inconsistency, and fragmentation across devices and sessions.\nExtracting actionable CTI from logs therefore requires approaches that can\nreconcile noisy, heterogeneous data into coherent and interoperable\nrepresentations. We introduce OntoLogX, an autonomous Artificial Intelligence\n(AI) agent that leverages Large Language Models (LLMs) to transform raw logs\ninto ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a\nlightweight log ontology with Retrieval Augmented Generation (RAG) and\niterative correction steps, ensuring that generated KGs are syntactically and\nsemantically valid. Beyond event-level analysis, the system aggregates KGs into\nsessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level\nlog evidence to higher-level adversarial objectives. We evaluate OntoLogX on\nboth logs from a public benchmark and a real-world honeypot dataset,\ndemonstrating robust KG generation across multiple KGs backends and accurate\nmapping of adversarial activity to ATT&CK tactics. Results highlight the\nbenefits of retrieval and correction for precision and recall, the\neffectiveness of code-oriented models in structured log analysis, and the value\nof ontology-grounded representations for actionable CTI extraction.",
      "authors": [
        "Luca Cotti",
        "Idilio Drago",
        "Anisa Rula",
        "Devis Bianchini",
        "Federico Cerutti"
      ],
      "published": "2025-10-01T19:46:15Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01409v1"
    },
    {
      "arxiv_id": "2510.01398v1",
      "title": "Automating Data-Driven Modeling and Analysis for Engineering\n  Applications using Large Language Model Agents",
      "summary": "Modern engineering increasingly relies on vast datasets generated by\nexperiments and simulations, driving a growing demand for efficient, reliable,\nand broadly applicable modeling strategies. There is also heightened interest\nin developing data-driven approaches, particularly neural network models, for\neffective prediction and analysis of scientific datasets. Traditional\ndata-driven methods frequently involve extensive manual intervention, limiting\ntheir ability to scale effectively and generalize to diverse applications. In\nthis study, we propose an innovative pipeline utilizing Large Language Model\n(LLM) agents to automate data-driven modeling and analysis, with a particular\nemphasis on regression tasks. We evaluate two LLM-agent frameworks: a\nmulti-agent system featuring specialized collaborative agents, and a\nsingle-agent system based on the Reasoning and Acting (ReAct) paradigm. Both\nframeworks autonomously handle data preprocessing, neural network development,\ntraining, hyperparameter optimization, and uncertainty quantification (UQ). We\nvalidate our approach using a critical heat flux (CHF) prediction benchmark,\ninvolving approximately 25,000 experimental data points from the OECD/NEA\nbenchmark dataset. Results indicate that our LLM-agent-developed model\nsurpasses traditional CHF lookup tables and delivers predictive accuracy and UQ\non par with state-of-the-art Bayesian optimized deep neural network models\ndeveloped by human experts. These outcomes underscore the significant potential\nof LLM-based agents to automate complex engineering modeling tasks, greatly\nreducing human workload while meeting or exceeding existing standards of\npredictive performance.",
      "authors": [
        "Yang Liu",
        "Zaid Abulawi",
        "Abhiram Garimidi",
        "Doyeong Lim"
      ],
      "published": "2025-10-01T19:28:35Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01398v1"
    },
    {
      "arxiv_id": "2510.01396v1",
      "title": "Neural Network Surrogates for Free Energy Computation of Complex\n  Chemical Systems",
      "summary": "Free energy reconstruction methods such as Gaussian Process Regression (GPR)\nrequire Jacobians of the collective variables (CVs), a bottleneck that\nrestricts the use of complex or machine-learned CVs. We introduce a neural\nnetwork surrogate framework that learns CVs directly from Cartesian coordinates\nand uses automatic differentiation to provide Jacobians, bypassing analytical\nforms. On an MgCl2 ion-pairing system, our method achieved high accuracy for\nboth a simple distance CV and a complex coordination-number CV. Moreover,\nJacobian errors also followed a near-Gaussian distribution, making them\nsuitable for GPR pipelines. This framework enables gradient-based free energy\nmethods to incorporate complex and machine-learned CVs, broadening the scope of\nbiochemistry and materials simulations.",
      "authors": [
        "Wasut Pornpatcharapong"
      ],
      "published": "2025-10-01T19:28:16Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01396v1"
    },
    {
      "arxiv_id": "2510.01395v1",
      "title": "Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence",
      "summary": "Both the general public and academic communities have raised concerns about\nsycophancy, the phenomenon of artificial intelligence (AI) excessively agreeing\nwith or flattering users. Yet, beyond isolated media reports of severe\nconsequences, like reinforcing delusions, little is known about the extent of\nsycophancy or how it affects people who use AI. Here we show the pervasiveness\nand harmful impacts of sycophancy when people seek advice from AI. First,\nacross 11 state-of-the-art AI models, we find that models are highly\nsycophantic: they affirm users' actions 50% more than humans do, and they do so\neven in cases where user queries mention manipulation, deception, or other\nrelational harms. Second, in two preregistered experiments (N = 1604),\nincluding a live-interaction study where participants discuss a real\ninterpersonal conflict from their life, we find that interaction with\nsycophantic AI models significantly reduced participants' willingness to take\nactions to repair interpersonal conflict, while increasing their conviction of\nbeing in the right. However, participants rated sycophantic responses as higher\nquality, trusted the sycophantic AI model more, and were more willing to use it\nagain. This suggests that people are drawn to AI that unquestioningly validate,\neven as that validation risks eroding their judgment and reducing their\ninclination toward prosocial behavior. These preferences create perverse\nincentives both for people to increasingly rely on sycophantic AI models and\nfor AI model training to favor sycophancy. Our findings highlight the necessity\nof explicitly addressing this incentive structure to mitigate the widespread\nrisks of AI sycophancy.",
      "authors": [
        "Myra Cheng",
        "Cinoo Lee",
        "Pranav Khadpe",
        "Sunny Yu",
        "Dyllan Han",
        "Dan Jurafsky"
      ],
      "published": "2025-10-01T19:26:01Z",
      "primary_category": "cs.CY",
      "arxiv_url": "https://arxiv.org/abs/2510.01395v1"
    },
    {
      "arxiv_id": "2510.01389v1",
      "title": "INSIGHT: INference-time Sequence Introspection for Generating Help\n  Triggers in Vision-Language-Action Models",
      "summary": "Recent Vision-Language-Action (VLA) models show strong generalization\ncapabilities, yet they lack introspective mechanisms for anticipating failures\nand requesting help from a human supervisor. We present \\textbf{INSIGHT}, a\nlearning framework for leveraging token-level uncertainty signals to predict\nwhen a VLA should request help. Using $\\pi_0$-FAST as the underlying model, we\nextract per-token \\emph{entropy}, \\emph{log-probability}, and Dirichlet-based\nestimates of \\emph{aleatoric and epistemic uncertainty}, and train compact\ntransformer classifiers to map these sequences to help triggers. We explore\nsupervision regimes for strong or weak supervision, and extensively compare\nthem across in-distribution and out-of-distribution tasks. Our results show a\ntrade-off: strong labels enable models to capture fine-grained uncertainty\ndynamics for reliable help detection, while weak labels, though noisier, still\nsupport competitive introspection when training and evaluation are aligned,\noffering a scalable path when dense annotation is impractical. Crucially, we\nfind that modeling the temporal evolution of token-level uncertainty signals\nwith transformers provides far greater predictive power than static\nsequence-level scores. This study provides the first systematic evaluation of\nuncertainty-based introspection in VLAs, opening future avenues for active\nlearning and for real-time error mitigation through selective human\nintervention.",
      "authors": [
        "Ulas Berk Karli",
        "Ziyao Shangguan",
        "Tesca FItzgerald"
      ],
      "published": "2025-10-01T19:22:48Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.01389v1"
    },
    {
      "arxiv_id": "2510.01377v1",
      "title": "DeMuon: A Decentralized Muon for Matrix Optimization over Graphs",
      "summary": "In this paper, we propose DeMuon, a method for decentralized matrix\noptimization over a given communication topology. DeMuon incorporates matrix\northogonalization via Newton-Schulz iterations-a technique inherited from its\ncentralized predecessor, Muon-and employs gradient tracking to mitigate\nheterogeneity among local functions. Under heavy-tailed noise conditions and\nadditional mild assumptions, we establish the iteration complexity of DeMuon\nfor reaching an approximate stochastic stationary point. This complexity result\nmatches the best-known complexity bounds of centralized algorithms in terms of\ndependence on the target tolerance. To the best of our knowledge, DeMuon is the\nfirst direct extension of Muon to decentralized optimization over graphs with\nprovable complexity guarantees. We conduct preliminary numerical experiments on\ndecentralized transformer pretraining over graphs with varying degrees of\nconnectivity. Our numerical results demonstrate a clear margin of improvement\nof DeMuon over other popular decentralized algorithms across different network\ntopologies.",
      "authors": [
        "Chuan He",
        "Shuyi Ren",
        "Jingwei Mao",
        "Erik G. Larsson"
      ],
      "published": "2025-10-01T19:06:11Z",
      "primary_category": "math.OC",
      "arxiv_url": "https://arxiv.org/abs/2510.01377v1"
    },
    {
      "arxiv_id": "2510.01375v1",
      "title": "Fine-tuning with RAG for Improving LLM Learning of New Skills",
      "summary": "Large language model (LLM) agents deployed for multi-step tasks frequently\nfail in predictable ways: attempting actions with unmet preconditions, issuing\nredundant commands, or mishandling environment constraints. While\nretrieval-augmented generation (RAG) can improve performance by providing\nruntime guidance, it requires maintaining external knowledge databases and adds\ncomputational overhead at every deployment. We propose a simple pipeline that\nconverts inference-time retrieval into learned competence through distillation.\nOur approach: (1) extracts compact, reusable hints from agent failures, (2)\nuses these hints to generate improved teacher trajectories via one-shot\nretrieval at episode start, and (3) trains student models on these trajectories\nwith hint strings removed, forcing internalization rather than memorization.\nAcross two interactive benchmarks, ALFWorld (household tasks) and WebShop\n(online shopping), distilled students consistently outperform baseline agents,\nachieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving\nWebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens\nthan retrieval-augmented teachers depending on the environment. The approach\ngeneralizes across model scales (7B/14B parameters) and agent architectures\n(ReAct/StateAct), demonstrating that retrieval benefits can be effectively\ninternalized through targeted fine-tuning without permanent runtime\ndependencies.",
      "authors": [
        "Humaid Ibrahim",
        "Nikolai Rozanov",
        "Marek Rei"
      ],
      "published": "2025-10-01T19:03:48Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01375v1"
    },
    {
      "arxiv_id": "2510.01370v1",
      "title": "SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs",
      "summary": "We introduce Small PDE U-Net Solver (SPUS), a compact and efficient\nfoundation model (FM) designed as a unified neural operator for solving a wide\nrange of partial differential equations (PDEs). Unlike existing\nstate-of-the-art PDE FMs-primarily based on large complex transformer\narchitectures with high computational and parameter overhead-SPUS leverages a\nlightweight residual U-Net-based architecture that has been largely\nunderexplored as a foundation model architecture in this domain. To enable\neffective learning in this minimalist framework, we utilize a simple yet\npowerful auto-regressive pretraining strategy which closely replicates the\nbehavior of numerical solvers to learn the underlying physics. SPUS is\npretrained on a diverse set of fluid dynamics PDEs and evaluated across 6\nchallenging unseen downstream PDEs spanning various physical systems.\nExperimental results demonstrate that SPUS using residual U-Net based\narchitecture achieves state-of-the-art generalization on these downstream tasks\nwhile requiring significantly fewer parameters and minimal fine-tuning data,\nhighlighting its potential as a highly parameter-efficient FM for solving\ndiverse PDE systems.",
      "authors": [
        "Abu Bucker Siddik",
        "Diane Oyen",
        "Alexander Most",
        "Michal Kucer",
        "Ayan Biswas"
      ],
      "published": "2025-10-01T18:54:59Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01370v1"
    },
    {
      "arxiv_id": "2510.01367v1",
      "title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by\n  Measuring Reasoning Effort",
      "summary": "Reward hacking, where a reasoning model exploits loopholes in a reward\nfunction to achieve high rewards without solving the intended task, poses a\nsignificant threat. This behavior may be explicit, i.e. verbalized in the\nmodel's chain-of-thought (CoT), or implicit, where the CoT appears benign thus\nbypasses CoT monitors. To detect implicit reward hacking, we propose TRACE\n(Truncated Reasoning AUC Evaluation). Our key observation is that hacking\noccurs when exploiting the loophole is easier than solving the actual task.\nThis means that the model is using less `effort' than required to achieve high\nreward. TRACE quantifies effort by measuring how early a model's reasoning\nbecomes sufficient to pass a verifier. We progressively truncate a model's CoT\nat various lengths, force the model to answer, and measure the verifier-passing\nrate at each cutoff. A hacking model, which takes a shortcut, will achieve a\nhigh passing rate with only a small fraction of its CoT, yielding a large area\nunder the accuracy-vs-length curve. TRACE achieves over 65% gains over our\nstrongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B\nmonitor in coding. We further show that TRACE can discover unknown loopholes\nduring training. Overall, TRACE offers a scalable unsupervised approach for\noversight where current monitoring methods prove ineffective.",
      "authors": [
        "Xinpeng Wang",
        "Nitish Joshi",
        "Barbara Plank",
        "Rico Angell",
        "He He"
      ],
      "published": "2025-10-01T18:49:45Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01367v1"
    },
    {
      "arxiv_id": "2510.01363v1",
      "title": "Retrieval-Augmented Framework for LLM-Based Clinical Decision Support",
      "summary": "The increasing complexity of clinical decision-making, alongside the rapid\nexpansion of electronic health records (EHR), presents both opportunities and\nchallenges for delivering data-informed care. This paper proposes a clinical\ndecision support system powered by Large Language Models (LLMs) to assist\nprescribing clinicians. The system generates therapeutic suggestions by\nanalyzing historical EHR data, including patient demographics, presenting\ncomplaints, clinical symptoms, diagnostic information, and treatment histories.\nThe framework integrates natural language processing with structured clinical\ninputs to produce contextually relevant recommendations. Rather than replacing\nclinician judgment, it is designed to augment decision-making by retrieving and\nsynthesizing precedent cases with comparable characteristics, drawing on local\ndatasets or federated sources where applicable. At its core, the system employs\na retrieval-augmented generation (RAG) pipeline that harmonizes unstructured\nnarratives and codified data to support LLM-based inference. We outline the\nsystem's technical components, including representation representation\nalignment and generation strategies. Preliminary evaluations, conducted with\nde-identified and synthetic clinical datasets, examine the clinical\nplausibility and consistency of the model's outputs. Early findings suggest\nthat LLM-based tools may provide valuable decision support in prescribing\nworkflows when appropriately constrained and rigorously validated. This work\nrepresents an initial step toward integration of generative AI into real-world\nclinical decision-making with an emphasis on transparency, safety, and\nalignment with established practices.",
      "authors": [
        "Leon Garza",
        "Anantaa Kotal",
        "Michael A. Grasso",
        "Emre Umucu"
      ],
      "published": "2025-10-01T18:45:25Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01363v1"
    },
    {
      "arxiv_id": "2510.01359v1",
      "title": "Breaking the Code: Security Assessment of AI Code Agents Through\n  Systematic Jailbreaking Attacks",
      "summary": "Code-capable large language model (LLM) agents are increasingly embedded into\nsoftware engineering workflows where they can read, write, and execute code,\nraising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only\nsettings. Prior evaluations emphasize refusal or harmful-text detection,\nleaving open whether agents actually compile and run malicious programs. We\npresent JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three\nescalating workspace regimes that mirror attacker capability: empty (JAWS-0),\nsingle-file (JAWS-1), and multi-file (JAWS-M). We pair this with a\nhierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)\nattack success, (iii) syntactic correctness, and (iv) runtime executability,\nmoving beyond refusal to measure deployable harm. Using seven LLMs from five\nfamilies as backends, we find that under prompt-only conditions in JAWS-0, code\nagents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%\nrun end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~\n100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the\nmulti-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly\ndeployable attack code. Across models, wrapping an LLM in an agent\nsubstantially increases vulnerability -- ASR raises by 1.6x -- because initial\nrefusals are frequently overturned during later planning/tool-use steps.\nCategory-level analyses identify which attack classes are most vulnerable and\nmost readily deployable, while others exhibit large execution gaps. These\nfindings motivate execution-aware defenses, code-contextual safety filters, and\nmechanisms that preserve refusal decisions throughout the agent's multi-step\nreasoning and tool use.",
      "authors": [
        "Shoumik Saha",
        "Jifan Chen",
        "Sam Mayers",
        "Sanjay Krishna Gouda",
        "Zijian Wang",
        "Varun Kumar"
      ],
      "published": "2025-10-01T18:38:20Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01359v1"
    },
    {
      "arxiv_id": "2510.01354v1",
      "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
      "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.",
      "authors": [
        "Yinuo Liu",
        "Ruohan Xu",
        "Xilong Wang",
        "Yuqi Jia",
        "Neil Zhenqiang Gong"
      ],
      "published": "2025-10-01T18:34:06Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01354v1"
    },
    {
      "arxiv_id": "2510.01353v1",
      "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in\n  Multi-Platform Dynamic Agent Environments",
      "summary": "Recent works on context and memory benchmarking have primarily focused on\nconversational instances but the need for evaluating memory in dynamic\nenterprise environments is crucial for its effective application. We introduce\nMEMTRACK, a benchmark designed to evaluate long-term memory and state tracking\nin multi-platform agent environments. MEMTRACK models realistic organizational\nworkflows by integrating asynchronous events across multiple communication and\nproductivity platforms such as Slack, Linear and Git. Each benchmark instance\nprovides a chronologically platform-interleaved timeline, with noisy,\nconflicting, cross-referring information as well as potential\ncodebase/file-system comprehension and exploration. Consequently, our benchmark\ntests memory capabilities such as acquistion, selection and conflict\nresolution. We curate the MEMTRACK dataset through both manual expert driven\ndesign and scalable agent based synthesis, generating ecologically valid\nscenarios grounded in real world software development processes. We introduce\npertinent metrics for Correctness, Efficiency, and Redundancy that capture the\neffectiveness of memory mechanisms beyond simple QA performance. Experiments\nacross SoTA LLMs and memory backends reveal challenges in utilizing memory\nacross long horizons, handling cross-platform dependencies, and resolving\ncontradictions. Notably, the best performing GPT-5 model only achieves a 60\\%\nCorrectness score on MEMTRACK. This work provides an extensible framework for\nadvancing evaluation research for memory-augmented agents, beyond existing\nfocus on conversational setups, and sets the stage for multi-agent,\nmulti-platform memory benchmarking in complex organizational settings",
      "authors": [
        "Darshan Deshpande",
        "Varun Gangal",
        "Hersh Mehta",
        "Anand Kannappan",
        "Rebecca Qian",
        "Peng Wang"
      ],
      "published": "2025-10-01T18:34:03Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01353v1"
    },
    {
      "arxiv_id": "2510.01346v1",
      "title": "Aristotle: IMO-level Automated Theorem Proving",
      "summary": "We introduce Aristotle, an AI system that combines formal verification with\ninformal reasoning, achieving gold-medal-equivalent performance on the 2025\nInternational Mathematical Olympiad problems. Aristotle integrates three main\ncomponents: a Lean proof search system, an informal reasoning system that\ngenerates and formalizes lemmas, and a dedicated geometry solver. Our system\ndemonstrates state-of-the-art performance with favorable scaling properties for\nautomated theorem proving.",
      "authors": [
        "Tudor Achim",
        "Alex Best",
        "Kevin Der",
        "Mathïs Fédérico",
        "Sergei Gukov",
        "Daniel Halpern-Leister",
        "Kirsten Henningsgard",
        "Yury Kudryashov",
        "Alexander Meiburg",
        "Martin Michelsen",
        "Riley Patterson",
        "Eric Rodriguez",
        "Laura Scharff",
        "Vikram Shanker",
        "Vladmir Sicca",
        "Hari Sowrirajan",
        "Aidan Swope",
        "Matyas Tamas",
        "Vlad Tenev",
        "Jonathan Thomm",
        "Harold Williams",
        "Lawrence Wu"
      ],
      "published": "2025-10-01T18:21:13Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01346v1"
    },
    {
      "arxiv_id": "2510.01336v1",
      "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
      "summary": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.",
      "authors": [
        "Avinash Kumar",
        "Sujay Sanghavi",
        "Poulami Das"
      ],
      "published": "2025-10-01T18:04:14Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01336v1"
    },
    {
      "arxiv_id": "2510.01304v1",
      "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
      "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .",
      "authors": [
        "Yu Zeng",
        "Wenxuan Huang",
        "Shiting Huang",
        "Xikun Bao",
        "Yukun Qi",
        "Yiming Zhao",
        "Qiuchen Wang",
        "Lin Chen",
        "Zehui Chen",
        "Huaian Chen",
        "Wanli Ouyang",
        "Feng Zhao"
      ],
      "published": "2025-10-01T17:58:05Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01304v1"
    },
    {
      "arxiv_id": "2510.01179v1",
      "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
      "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.",
      "authors": [
        "Zhangchen Xu",
        "Adriana Meza Soria",
        "Shawn Tan",
        "Anurag Roy",
        "Ashish Sunil Agrawal",
        "Radha Poovendran",
        "Rameswar Panda"
      ],
      "published": "2025-10-01T17:58:03Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01179v1"
    },
    {
      "arxiv_id": "2510.01178v1",
      "title": "COM-BOM: Bayesian Exemplar Search for Efficiently Exploring the\n  Accuracy-Calibration Pareto Frontier",
      "summary": "Selecting an optimal set of exemplars is critical for good performance of\nin-context learning. However, prior exemplar search methods narrowly optimize\nfor predictive accuracy, critically neglecting model calibration--a key\ndeterminant of trustworthiness and safe deployment. In this paper, we formulate\nexemplar selection as a multi-objective optimization problem, explicitly\ntargeting both the maximization of predictive accuracy and the minimization of\nexpected calibration error. We solve this problem with a sample-efficient\nCombinatorial Bayesian Optimization algorithm (COM-BOM) to find the Pareto\nfront that optimally trades off the two objectives of accuracy and calibration.\nWe evaluate COM-BOM on multiple tasks from unsaturated MMLU-Pro benchmark and\nfind that COM-BOM beats or matches the baselines at jointly optimizing the two\nobjectives, while requiring a minimal number of LLM API calls.",
      "authors": [
        "Gaoxiang Luo",
        "Aryan Deshwal"
      ],
      "published": "2025-10-01T17:57:49Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01178v1"
    },
    {
      "arxiv_id": "2510.01174v1",
      "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
      "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.",
      "authors": [
        "Yanzhe Chen",
        "Kevin Qinghong Lin",
        "Mike Zheng Shou"
      ],
      "published": "2025-10-01T17:56:48Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01174v1"
    },
    {
      "arxiv_id": "2510.01173v1",
      "title": "EditTrack: Detecting and Attributing AI-assisted Image Editing",
      "summary": "In this work, we formulate and study the problem of image-editing detection\nand attribution: given a base image and a suspicious image, detection seeks to\ndetermine whether the suspicious image was derived from the base image using an\nAI editing model, while attribution further identifies the specific editing\nmodel responsible. Existing methods for detecting and attributing AI-generated\nimages are insufficient for this problem, as they focus on determining whether\nan image was AI-generated/edited rather than whether it was edited from a\nparticular base image. To bridge this gap, we propose EditTrack, the first\nframework for this image-editing detection and attribution problem. Building on\nfour key observations about the editing process, EditTrack introduces a novel\nre-editing strategy and leverages carefully designed similarity metrics to\ndetermine whether a suspicious image originates from a base image and, if so,\nby which model. We evaluate EditTrack on five state-of-the-art editing models\nacross six datasets, demonstrating that it consistently achieves accurate\ndetection and attribution, significantly outperforming five baselines.",
      "authors": [
        "Zhengyuan Jiang",
        "Yuyang Zhang",
        "Moyang Guo",
        "Neil Zhenqiang Gong"
      ],
      "published": "2025-10-01T17:56:35Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01173v1"
    },
    {
      "arxiv_id": "2510.01171v1",
      "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity",
      "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.",
      "authors": [
        "Jiayi Zhang",
        "Simon Yu",
        "Derek Chong",
        "Anthony Sicilia",
        "Michael R. Tomz",
        "Christopher D. Manning",
        "Weiyan Shi"
      ],
      "published": "2025-10-01T17:55:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01171v1"
    },
    {
      "arxiv_id": "2510.01169v1",
      "title": "Fiaingen: A financial time series generative method matching real-world\n  data quality",
      "summary": "Data is vital in enabling machine learning models to advance research and\npractical applications in finance, where accurate and robust models are\nessential for investment and trading decision-making. However, real-world data\nis limited despite its quantity, quality, and variety. The data shortage of\nvarious financial assets directly hinders the performance of machine learning\nmodels designed to trade and invest in these assets. Generative methods can\nmitigate this shortage. In this paper, we introduce a set of novel techniques\nfor time series data generation (we name them Fiaingen) and assess their\nperformance across three criteria: (a) overlap of real-world and synthetic data\non a reduced dimensionality space, (b) performance on downstream machine\nlearning tasks, and (c) runtime performance. Our experiments demonstrate that\nthe methods achieve state-of-the-art performance across the three criteria\nlisted above. Synthetic data generated with Fiaingen methods more closely\nmirrors the original time series data while keeping data generation time close\nto seconds - ensuring the scalability of the proposed approach. Furthermore,\nmodels trained on it achieve performance close to those trained with real-world\ndata.",
      "authors": [
        "Jože M. Rožanec",
        "Tina Žezlin",
        "Laurentiu Vasiliu",
        "Dunja Mladenić",
        "Radu Prodan",
        "Dumitru Roman"
      ],
      "published": "2025-10-01T17:55:08Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01169v1"
    },
    {
      "arxiv_id": "2510.01167v1",
      "title": "Simultaneous Multi-objective Alignment Across Verifiable and\n  Non-verifiable Rewards",
      "summary": "Aligning large language models to human preferences is inherently\nmultidimensional, yet most pipelines collapse heterogeneous signals into a\nsingle optimizeable objective. We seek to answer what it would take to\nsimultaneously align a model across various domains spanning those with:\nverifiable rewards (mathematical accuracy), non-verifiable subjective\npreferences (human values), and complex interactive scenarios (multi-turn AI\ntutoring dialogues). Such multi-objective reinforcement learning setups are\noften plagued by the individual objectives being at odds with each other,\nresulting in inefficient training and little user control during inference. We\npropose a unified framework that: (i) standardizes {process reward model} (PRM)\ntraining across both verifiable and non-verifiable settings to better supervise\nmodels' chain-of-thought reasoning; (ii) performs {multi-objective alignment}\nby training the LLM with our $\\textbf{M}$ulti-$\\textbf{A}$ction-$\\textbf{H}$ead\n$\\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the\nvector correspond to the various objectives instead of a single scalar; and\n(iii) demonstrates how such a system provides fine-grained inference-time user\ncontrol. Experiments across math reasoning, value alignment, and multi-turn\ndialogue show that our framework improves performance across multiple\nobjectives simultaneously, while minimizing cross-objective trade-offs and\nenabling flexible inference time user control. The code can be found at\nhttps://github.com/pearls-lab/multiobj-align.",
      "authors": [
        "Yiran Shen",
        "Yu Xia",
        "Jonathan Chang",
        "Prithviraj Ammanabrolu"
      ],
      "published": "2025-10-01T17:54:15Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01167v1"
    },
    {
      "arxiv_id": "2510.01165v1",
      "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient\n  Few-Shot Reasoning",
      "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks,\nbut their effectiveness often depends on the quality of the provided context.\nRetrieval-Augmented Generation (RAG) enriches prompts with external\ninformation, but its reliance on static databases constrains adaptability and\ncan result in irrelevant demonstrations. In this work, we propose a Generative\nRetrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach\nwhere an LLM model is trained to generate input-specific concise\ndemonstrations. By tailoring demonstrations to each input, our method offers\nbetter contextual support than traditional RAG approaches. We demonstrate the\nsuperiority of GRAD under budget constraints, where we limit both the number of\ntokens used per demonstration and the number of tokens used for the final\noutput. Trained solely on a math dataset, GRAD consistently outperforms strong\nbaselines on Qwen2.5-14B across mathematical reasoning and advanced STEM\nquestions, highlighting GRAD's robust generalization to out-of-distribution\n(OOD) domains such as physics, chemistry, and computer science. Furthermore, we\nshow that demonstrations generated by trained smaller models can effectively\nguide larger target models, reducing training costs while maintaining\ncompetitive accuracy. Overall, this work introduces a scalable demonstration\ngenerator model presenting the first step toward a dynamic few-shot learning\nparadigm in resource-constrained settings. We release the code used for the\nproject.",
      "authors": [
        "Oussama Gabouj",
        "Kamel Charaf",
        "Ivan Zakazov",
        "Nicolas Baldwin",
        "Robert West"
      ],
      "published": "2025-10-01T17:52:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01165v1"
    },
    {
      "arxiv_id": "2510.01164v1",
      "title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social\n  Welfare",
      "summary": "Large language models (LLMs) are increasingly entrusted with high-stakes\ndecisions that affect human welfare. However, the principles and values that\nguide these models when distributing scarce societal resources remain largely\nunexamined. To address this, we introduce the Social Welfare Function (SWF)\nBenchmark, a dynamic simulation environment where an LLM acts as a sovereign\nallocator, distributing tasks to a heterogeneous community of recipients. The\nbenchmark is designed to create a persistent trade-off between maximizing\ncollective efficiency (measured by Return on Investment) and ensuring\ndistributive fairness (measured by the Gini coefficient). We evaluate 20\nstate-of-the-art LLMs and present the first leaderboard for social welfare\nallocation. Our findings reveal three key insights: (i) A model's general\nconversational ability, as measured by popular leaderboards, is a poor\npredictor of its allocation skill. (ii) Most LLMs exhibit a strong default\nutilitarian orientation, prioritizing group productivity at the expense of\nsevere inequality. (iii) Allocation strategies are highly vulnerable, easily\nperturbed by output-length constraints and social-influence framing. These\nresults highlight the risks of deploying current LLMs as societal\ndecision-makers and underscore the need for specialized benchmarks and targeted\nalignment for AI governance.",
      "authors": [
        "Zhengliang Shi",
        "Ruotian Ma",
        "Jen-tse Huang",
        "Xinbei Ma",
        "Xingyu Chen",
        "Mengru Wang",
        "Qu Yang",
        "Yue Wang",
        "Fanghua Ye",
        "Ziyang Chen",
        "Shanyi Wang",
        "Cixing Li",
        "Wenxuan Wang",
        "Zhaopeng Tu",
        "Xiaolong Li",
        "Zhaochun Ren",
        "Linus"
      ],
      "published": "2025-10-01T17:52:31Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01164v1"
    },
    {
      "arxiv_id": "2510.01161v1",
      "title": "Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale\n  Data on LLMs?",
      "summary": "Reinforcement learning has been central to recent advances in large language\nmodel reasoning, but most algorithms rely on on-policy training that demands\nfresh rollouts at every update, limiting efficiency and scalability.\nAsynchronous RL systems alleviate this by decoupling rollout generation from\ntraining, yet their effectiveness hinges on tolerating large staleness in\nrollout data, a setting where existing methods either degrade in performance or\ncollapse. We revisit this challenge and uncover a prosperity-before-collapse\nphenomenon: stale data can be as informative as on-policy data if exploited\nproperly. Building on this insight, we introduce M2PO (Second-Moment Trust\nPolicy Optimization), which constrains the second moment of importance weights\nto suppress only extreme outliers while preserving informative updates.\nNotably, M2PO sharply reduces the fraction of clipped tokens under high\nstaleness (from 1.22% to 0.06% over training), precisely masking high-variance\ntokens while maintaining stable optimization. Extensive evaluation across six\nmodels (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable\noff-policy training even with data stale by at least 256 model updates and\nmatches on-policy performance.",
      "authors": [
        "Haizhong Zheng",
        "Jiawei Zhao",
        "Bedi Chen"
      ],
      "published": "2025-10-01T17:48:23Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01161v1"
    },
    {
      "arxiv_id": "2510.01146v1",
      "title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models",
      "summary": "Evaluation using Large Language Model (LLM) judges has been widely adopted in\nEnglish and shown to be effective for automatic evaluation. However, their\nperformance does not generalize well to non-English settings, and it remains\nunclear what constitutes effective multilingual training for such judges. In\nthis paper, we introduce mR3, a massively multilingual, rubric-agnostic reward\nreasoning model trained on 72 languages, achieving the broadest language\ncoverage in reward modeling to date. We present a comprehensive study of data\nand curriculum selection for training to identify effective strategies and data\nsources for building high-quality reward models, including the integration of\ntarget-language reasoning datasets. Our approach attains state-of-the-art\nperformance on multilingual reward model benchmarks, surpassing much larger\nmodels (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness\nis further confirmed through extensive ablation studies. Our models, data, and\ncode are available as open source at https://github.com/rubricreward/mr3.",
      "authors": [
        "David Anugraha",
        "Shou-Yi Hung",
        "Zilu Tang",
        "Annie En-Shiun Lee",
        "Derry Tanti Wijaya",
        "Genta Indra Winata"
      ],
      "published": "2025-10-01T17:36:59Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01146v1"
    },
    {
      "arxiv_id": "2510.01143v1",
      "title": "Generalized Parallel Scaling with Interdependent Generations",
      "summary": "Parallel LLM inference scaling involves sampling a set of $N>1$ responses for\na single input prompt. However, these $N$ parallel responses tend to be\ngenerated independently from each other, partitioning compute resources and\nleaving potentially useful information in one generation untapped by others.\nThis is in contrast to response length scaling where past computation is used\nin all future steps. For higher quality responses and response sets, we propose\nBridge to generate interdependent responses in parallel by rethinking batched\nLLM hidden states as holistic tensors rather than independent slices. With only\na small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean\naccuracy gains from reinforcement learning with verifiable rewards by up to 50%\nand boosts consistency of correct responses. Trained once, Bridge scales to any\ngeneration width, all with greater performance than independent generations,\nunlocking a more general mode of parallel scaling that effectively leverages\ninformation between sequences, compatible with any post-generation aggregation\ntechnique.",
      "authors": [
        "Harry Dong",
        "David Brandfonbrener",
        "Eryk Helenowski",
        "Yun He",
        "Mrinal Kumar",
        "Han Fang",
        "Yuejie Chi",
        "Karthik Abinav Sankararaman"
      ],
      "published": "2025-10-01T17:33:35Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01143v1"
    },
    {
      "arxiv_id": "2510.01141v1",
      "title": "Apriel-1.5-15b-Thinker",
      "summary": "We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights\nmultimodal reasoning model that achieves frontier-level performance through\ntraining design rather than sheer scale. Starting from Pixtral-12B, we apply a\nprogressive three-stage methodology: (1) depth upscaling to expand reasoning\ncapacity without pretraining from scratch, (2) staged continual pre-training\nthat first develops foundational text and vision understanding, then enhances\nvisual reasoning through targeted synthetic data generation addressing spatial\nstructure, compositional understanding, and fine-grained perception, and (3)\nhigh-quality text-only supervised fine-tuning on curated instruction-response\npairs with explicit reasoning traces spanning mathematics, coding, science, and\ntool use. Notably, our model achieves competitive results without reinforcement\nlearning or preference optimization, isolating the contribution of our\ndata-centric continual pre-training approach. On the Artificial Analysis\nIntelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching\nDeepSeek-R1-0528 despite requiring significantly fewer computational resources.\nAcross ten image benchmarks, its performance is on average within five points\nof Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model\noperating within single-GPU deployment constraints. Our results demonstrate\nthat thoughtful mid-training 2 design can close substantial capability gaps\nwithout massive scale, making frontier-level multimodal reasoning accessible to\norganizations with limited infrastructure. We release the model checkpoint, all\ntraining recipes, and evaluation protocols under the MIT license to to advance\nopen-source research.",
      "authors": [
        "Shruthan Radhakrishna",
        "Aman Tiwari",
        "Aanjaneya Shukla",
        "Masoud Hashemi",
        "Rishabh Maheshwary",
        "Shiva Krishna Reddy Malay",
        "Jash Mehta",
        "Pulkit Pattnaik",
        "Saloni Mittal",
        "Khalil Slimi",
        "Kelechi Ogueji",
        "Akintunde Oladipo",
        "Soham Parikh",
        "Oluwanifemi Bamgbose",
        "Toby Liang",
        "Ahmed Masry",
        "Khyati Mahajan",
        "Sai Rajeswar Mudumba",
        "Vikas Yadav",
        "Sathwik Tejaswi Madhusudhan",
        "Torsten Scholak",
        "Sagar Davasam",
        "Srinivas Sunkara",
        "Nicholas Chapados"
      ],
      "published": "2025-10-01T17:29:35Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01141v1"
    },
    {
      "arxiv_id": "2510.01136v1",
      "title": "TabINR: An Implicit Neural Representation Framework for Tabular Data\n  Imputation",
      "summary": "Tabular data builds the basis for a wide range of applications, yet\nreal-world datasets are frequently incomplete due to collection errors, privacy\nrestrictions, or sensor failures. As missing values degrade the performance or\nhinder the applicability of downstream models, and while simple imputing\nstrategies tend to introduce bias or distort the underlying data distribution,\nwe require imputers that provide high-quality imputations, are robust across\ndataset sizes and yield fast inference. We therefore introduce TabINR, an\nauto-decoder based Implicit Neural Representation (INR) framework that models\ntables as neural functions. Building on recent advances in generalizable INRs,\nwe introduce learnable row and feature embeddings that effectively deal with\nthe discrete structure of tabular data and can be inferred from partial\nobservations, enabling instance adaptive imputations without modifying the\ntrained model. We evaluate our framework across a diverse range of twelve\nreal-world datasets and multiple missingness mechanisms, demonstrating\nconsistently strong imputation accuracy, mostly matching or outperforming\nclassical (KNN, MICE, MissForest) and deep learning based models (GAIN,\nReMasker), with the clearest gains on high-dimensional datasets.",
      "authors": [
        "Vincent Ochs",
        "Florentin Bieder",
        "Sidaty el Hadramy",
        "Paul Friedrich",
        "Stephanie Taha-Mehlitz",
        "Anas Taha",
        "Philippe C. Cattin"
      ],
      "published": "2025-10-01T17:24:35Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01136v1"
    },
    {
      "arxiv_id": "2510.01132v1",
      "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
      "summary": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
      "authors": [
        "Ruiyi Wang",
        "Prithviraj Ammanabrolu"
      ],
      "published": "2025-10-01T17:23:04Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01132v1"
    },
    {
      "arxiv_id": "2510.01123v1",
      "title": "Rethinking Thinking Tokens: LLMs as Improvement Operators",
      "summary": "Reasoning training incentivizes LLMs to produce long chains of thought (long\nCoT), which among other things, allows them to explore solution strategies with\nself-checking. This results in higher accuracy, but inflates context length,\ntoken/compute cost, and answer latency. We ask: Can current models leverage\ntheir metacognition to provide other combinations on this Pareto frontier,\ne.g., better accuracy with lower context length and/or latency? Abstractly, we\nview the model as an improvement operator on its own \"thoughts\" with a\ncontinuum of possible strategies. We identify an interesting inference family\nParallel-Distill-Refine (PDR), which performs the following: (i) generate\ndiverse drafts in parallel; (ii) distill them into a bounded, textual\nworkspace; and (iii) refine conditioned on this workspace, producing an output\nthat seeds the next round. Importantly, context length (hence compute cost) is\ncontrollable via degree of parallelism, and is no longer conflated with the\ntotal number of generated tokens. We report PDR instantiations of current\nmodels that give better accuracy than long CoT while incurring lower latency.\nSetting degree of parallelism to 1 yields an interesting subcase, Sequential\nRefinement (SR) (iteratively improve a single candidate answer) which provides\nperformance superior to long CoT. Success of such model orchestrations raises\nthe question whether further training could shift the Pareto frontier. To this\nend, we train an 8B thinking model with Reinforcement Learning (RL) to make it\nconsistent with PDR as the inference method. On math tasks with verifiable\nanswers, iterative pipelines surpass single-pass baselines at matched\nsequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME\n2024 and +9% on AIME 2025).",
      "authors": [
        "Lovish Madaan",
        "Aniket Didolkar",
        "Suchin Gururangan",
        "John Quan",
        "Ruan Silva",
        "Ruslan Salakhutdinov",
        "Manzil Zaheer",
        "Sanjeev Arora",
        "Anirudh Goyal"
      ],
      "published": "2025-10-01T17:08:59Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01123v1"
    },
    {
      "arxiv_id": "2510.01115v1",
      "title": "Exploring Network-Knowledge Graph Duality: A Case Study in Agentic\n  Supply Chain Risk Analysis",
      "summary": "Large Language Models (LLMs) struggle with the complex, multi-modal, and\nnetwork-native data underlying financial risk. Standard Retrieval-Augmented\nGeneration (RAG) oversimplifies relationships, while specialist models are\ncostly and static. We address this gap with an LLM-centric agent framework for\nsupply chain risk analysis. Our core contribution is to exploit the inherent\nduality between networks and knowledge graphs (KG). We treat the supply chain\nnetwork as a KG, allowing us to use structural network science principles for\nretrieval. A graph traverser, guided by network centrality scores, efficiently\nextracts the most economically salient risk paths. An agentic architecture\norchestrates this graph retrieval alongside data from numerical factor tables\nand news streams. Crucially, it employs novel ``context shells'' -- descriptive\ntemplates that embed raw figures in natural language -- to make quantitative\ndata fully intelligible to the LLM. This lightweight approach enables the model\nto generate concise, explainable, and context-rich risk narratives in real-time\nwithout costly fine-tuning or a dedicated graph database.",
      "authors": [
        "Evan Heus",
        "Rick Bookstaber",
        "Dhruv Sharma"
      ],
      "published": "2025-10-01T17:02:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01115v1"
    },
    {
      "arxiv_id": "2510.01114v1",
      "title": "PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned\n  Diagnosis",
      "summary": "We present PRISM-Consult, a clinician-aligned panel-of-experts architecture\nthat extends the compact PRISM sequence model into a routed family of domain\nspecialists. Episodes are tokenized as structured clinical events; a\nlight-weight router reads the first few tokens and dispatches to specialist\nmodels (Cardiac-Vascular, Pulmonary, Gastro-Oesophageal, Musculoskeletal,\nPsychogenic). Each specialist inherits PRISM's small transformer backbone and\ntoken template, enabling parameter efficiency and interpretability. On\nreal-world Emergency Department cohorts, specialists exhibit smooth convergence\nwith low development perplexities across domains, while the router achieves\nhigh routing quality and large compute savings versus consult-all under a\nsafety-first policy. We detail the data methodology (initial vs. conclusive\nICD-9 families), routing thresholds and calibration, and report per-domain\nresults to avoid dominance by common events. The framework provides a practical\npath to safe, auditable, and low-latency consult at scale, and we outline\nvalidation steps-external/temporal replication, asymmetric life-threat\nthresholds, and multi-label arbitration-to meet prospective clinical deployment\nstandards.",
      "authors": [
        "Lionel Levine",
        "John Santerre",
        "Alexander S. Young",
        "T. Barry Levine",
        "Francis Campion",
        "Majid Sarrafzadeh"
      ],
      "published": "2025-10-01T17:00:05Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01114v1"
    },
    {
      "arxiv_id": "2510.01094v1",
      "title": "Optimizing Fairness in Production Planning: A Human-Centric Approach to\n  Machine and Workforce Allocation",
      "summary": "This work presents a two-layer, human-centric production planning framework\ndesigned to optimize both operational efficiency and workforce fairness in\nindustrial manufacturing. The first layer formulates the Order-Line allocation\nas a Constraint Programming (CP) problem, generating high-utilization\nproduction schedules that respect machine capacities, processing times, and due\ndates. The second layer models Worker-Line allocation as a Markov Decision\nProcess (MDP), integrating human factors such as worker preference, experience,\nresilience, and medical constraints into the assignment process. Three solution\nstrategies, greedy allocation, MCTS, and RL, are implemented and compared\nacross multiple evaluation scenarios. The proposed system is validated through\n16 test sessions with domain experts from the automotive industry, combining\nquantitative key performance indicators (KPIs) with expert ratings. Results\nindicate that the CP-based scheduling approach produces compact, feasible\nproduction plans with low tardiness, while the MDP-based worker allocation\nsignificantly improves fairness and preference alignment compared to baseline\napproaches. Domain experts rated both the Order-Line and Worker-Line components\nas effective and highlighted opportunities to further refine the objective\nfunction to penalize excessive earliness and improve continuity in worker\nassignments. Overall, the findings demonstrate that combining CP with\nlearning-based decision-making provides a robust approach for human-centric\nproduction planning. The approach enables simultaneous optimization of\nthroughput and workforce well-being, offering a practical foundation for fair\nand efficient manufacturing scheduling in industrial settings.",
      "authors": [
        "Alexander Nasuta",
        "Alessandro Cisi",
        "Sylwia Olbrych",
        "Gustavo Vieira",
        "Rui Fernandes",
        "Lucas Paletta",
        "Marlene Mayr",
        "Rishyank Chevuri",
        "Robert Woitsch",
        "Hans Aoyang Zhou",
        "Anas Abdelrazeq",
        "Robert H. Schmitt"
      ],
      "published": "2025-10-01T16:41:18Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01094v1"
    },
    {
      "arxiv_id": "2510.01088v1",
      "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for\n  Self-Defense",
      "summary": "Ensuring Large Language Model (LLM) safety remains challenging due to the\nabsence of universal standards and reliable content validators, making it\ndifficult to obtain effective training signals. We discover that aligned models\nalready possess robust internal safety beliefs: they consistently produce\nhigh-confidence refusals to harmful requests while exhibiting high entropy when\ngenerating potentially dangerous content. This entropy gap reveals an untapped\nsignal--models intrinsically \"know\" when to refuse. We introduce Safety\nInstincts Reinforcement Learning (SIRL), which transforms this internal\nconfidence into a self-generated reward signal, eliminating dependence on\nexternal validators or human annotations. SIRL teaches models to trust their\nsafety instincts by reinforcing low-entropy refusal behaviors. Evaluated on\nLlama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against\n20+ jailbreak methods, from static prompts to adaptive attacks. Using only\n15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods\nwhile preserving performance on mathematics, coding, and conversation\nbenchmarks. Our work demonstrates that effective alignment can emerge from\nwithin, paving the way for more autonomous and robust AI safety mechanisms that\nscale without extensive human oversight.",
      "authors": [
        "Guobin Shen",
        "Dongcheng Zhao",
        "Haibo Tong",
        "Jindong Li",
        "Feifei Zhao",
        "Yi Zeng"
      ],
      "published": "2025-10-01T16:35:03Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01088v1"
    },
    {
      "arxiv_id": "2510.01077v1",
      "title": "CodeGenLink: A Tool to Find the Likely Origin and License of\n  Automatically Generated Code",
      "summary": "Large Language Models (LLMs) are widely used in software development tasks\nnowadays. Unlike reusing code taken from the Web, for LLMs' generated code,\ndevelopers are concerned about its lack of trustworthiness and possible\ncopyright or licensing violations, due to the lack of code provenance\ninformation. This paper proposes CodeGenLink, a GitHub CoPilot extension for\nVisual Studio Code aimed at (i) suggesting links containing code very similar\nto automatically generated code, and (ii) whenever possible, indicating the\nlicense of the likely origin of the code. CodeGenLink retrieves candidate links\nby combining LLMs with their web search features and then performs similarity\nanalysis between the generated and retrieved code. Preliminary results show\nthat CodeGenLink effectively filters unrelated links via similarity analysis\nand provides licensing information when available. Tool URL:\nhttps://github.com/danielebifolco/CodeGenLink Tool Video:\nhttps://youtu.be/M6nqjBf9_pw",
      "authors": [
        "Daniele Bifolco",
        "Guido Annicchiarico",
        "Pierluigi Barbiero",
        "Massimiliano Di Penta",
        "Fiorella Zampetti"
      ],
      "published": "2025-10-01T16:21:13Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.01077v1"
    },
    {
      "arxiv_id": "2510.01303v1",
      "title": "Low Rank Gradients and Where to Find Them",
      "summary": "This paper investigates low-rank structure in the gradients of the training\nloss for two-layer neural networks while relaxing the usual isotropy\nassumptions on the training data and parameters. We consider a spiked data\nmodel in which the bulk can be anisotropic and ill-conditioned, we do not\nrequire independent data and weight matrices and we also analyze both the\nmean-field and neural-tangent-kernel scalings. We show that the gradient with\nrespect to the input weights is approximately low rank and is dominated by two\nrank-one terms: one aligned with the bulk data-residue , and another aligned\nwith the rank one spike in the input data. We characterize how properties of\nthe training data, the scaling regime and the activation function govern the\nbalance between these two components. Additionally, we also demonstrate that\nstandard regularizers, such as weight decay, input noise and Jacobian\npenalties, also selectively modulate these components. Experiments on synthetic\nand real data corroborate our theoretical predictions.",
      "authors": [
        "Rishi Sonthalia",
        "Michael Murray",
        "Guido Montúfar"
      ],
      "published": "2025-10-01T16:20:19Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01303v1"
    },
    {
      "arxiv_id": "2510.01069v1",
      "title": "Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM\n  Reasoning",
      "summary": "While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of\nlarge language models, the faithfulness of the generated rationales remains an\nopen problem for model interpretability. We propose a novel theoretical lens\nfor this problem grounded in the Curry-Howard correspondence, which posits a\ndirect relationship between formal proofs and computer programs. Under this\nparadigm, a faithful reasoning trace is analogous to a well-typed program,\nwhere each intermediate step corresponds to a typed logical inference. We\noperationalise this analogy, presenting methods to extract and map the\ninformal, natural language steps of CoT into a formal, typed proof structure.\nSuccessfully converting a CoT trace into a well-typed proof serves as a strong,\nverifiable certificate of its computational faithfulness, moving beyond\nheuristic interpretability towards formal verification. Our framework provides\na methodology to transform plausible narrative explanations into formally\nverifiable programs, offering a path towards building more reliable and\ntrustworthy AI systems.",
      "authors": [
        "Elija Perrier"
      ],
      "published": "2025-10-01T16:06:40Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01069v1"
    },
    {
      "arxiv_id": "2510.01052v1",
      "title": "Hybrid Dialogue State Tracking for Persian Chatbots: A Language\n  Model-Based Approach",
      "summary": "Dialogue State Tracking (DST) is an essential element of conversational AI\nwith the objective of deeply understanding the conversation context and leading\nit toward answering user requests. Due to high demands for open-domain and\nmulti-turn chatbots, the traditional rule-based DST is not efficient enough,\nsince it cannot provide the required adaptability and coherence for human-like\nexperiences in complex conversations. This study proposes a hybrid DST model\nthat utilizes rule-based methods along with language models, including BERT for\nslot filling and intent detection, XGBoost for intent validation, GPT for DST,\nand online agents for real-time answer generation. This model is uniquely\ndesigned to be evaluated on a comprehensive Persian multi-turn dialogue dataset\nand demonstrated significantly improved accuracy and coherence over existing\nmethods in Persian-based chatbots. The results demonstrate how effectively a\nhybrid approach may improve DST capabilities, paving the way for conversational\nAI systems that are more customized, adaptable, and human-like.",
      "authors": [
        "Samin Mahdipour Aghabagher",
        "Saeedeh Momtazi"
      ],
      "published": "2025-10-01T15:57:19Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01052v1"
    },
    {
      "arxiv_id": "2510.01051v1",
      "title": "GEM: A Gym for Agentic LLMs",
      "summary": "The training paradigm for large language models (LLMs) is moving from static\ndatasets to experience-based learning, where agents acquire skills via\ninteracting with complex environments. To facilitate this transition we\nintroduce GEM (General Experience Maker), an open-source environment simulator\ndesigned for the age of LLMs. Analogous to OpenAI-Gym for traditional\nreinforcement learning (RL), GEM provides a standardized framework for the\nenvironment-agent interface, including asynchronous vectorized execution for\nhigh throughput, and flexible wrappers for easy extensibility. GEM also\nfeatures a diverse suite of environments, robust integrated tools, and\nsingle-file example scripts demonstrating using GEM with five popular RL\ntraining frameworks. Along with this, we also provide a set of baselines across\n24 environments using REINFORCE with Return Batch Normalization (ReBN), which\n-- unlike GRPO -- is compatible with the full RL setting of dense per-turn\nrewards and offers better credit assignment. We further conduct apple-to-apple\nbenchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings\nusing GEM to shed light on the algorithmic designs. Lastly, GEM also functions\nas a convenient evaluation toolkit besides a training environment. We hope this\nframework can help accelerate future agentic LLM research.",
      "authors": [
        "Zichen Liu",
        "Anya Sims",
        "Keyu Duan",
        "Changyu Chen",
        "Simon Yu",
        "Xiangxin Zhou",
        "Haotian Xu",
        "Shaopan Xiong",
        "Bo Liu",
        "Chenmien Tan",
        "Chuen Yang Beh",
        "Weixun Wang",
        "Hao Zhu",
        "Weiyan Shi",
        "Diyi Yang",
        "Michael Shieh",
        "Yee Whye Teh",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "published": "2025-10-01T15:55:57Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01051v1"
    },
    {
      "arxiv_id": "2510.01048v1",
      "title": "Interpreting Language Models Through Concept Descriptions: A Survey",
      "summary": "Understanding the decision-making processes of neural networks is a central\ngoal of mechanistic interpretability. In the context of Large Language Models\n(LLMs), this involves uncovering the underlying mechanisms and identifying the\nroles of individual model components such as neurons and attention heads, as\nwell as model abstractions such as the learned sparse features extracted by\nSparse Autoencoders (SAEs). A rapidly growing line of work tackles this\nchallenge by using powerful generator models to produce open-vocabulary,\nnatural language concept descriptions for these components. In this paper, we\nprovide the first survey of the emerging field of concept descriptions for\nmodel components and abstractions. We chart the key methods for generating\nthese descriptions, the evolving landscape of automated and human metrics for\nevaluating them, and the datasets that underpin this research. Our synthesis\nreveals a growing demand for more rigorous, causal evaluation. By outlining the\nstate of the art and identifying key challenges, this survey provides a roadmap\nfor future research toward making models more transparent.",
      "authors": [
        "Nils Feldhus",
        "Laura Kopf"
      ],
      "published": "2025-10-01T15:51:44Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01048v1"
    },
    {
      "arxiv_id": "2510.01047v1",
      "title": "Authentic Discrete Diffusion Model",
      "summary": "We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally\nredefines prior pseudo-discrete approaches by preserving core diffusion\ncharacteristics directly in the one-hot space through a suite of coordinated\nmechanisms. Unlike conventional \"pseudo\" discrete diffusion (PDD) methods, ADD\nreformulates the diffusion input by directly using float-encoded one-hot class\ndata, without relying on diffusing in the continuous latent spaces or masking\npolicies. At its core, a timestep-conditioned cross-entropy loss is introduced\nbetween the diffusion model's outputs and the original one-hot labels. This\nsynergistic design establishes a bridge between discriminative and generative\nlearning. Our experiments demonstrate that ADD not only achieves superior\nperformance on classification tasks compared to the baseline, but also exhibits\nexcellent text generation capabilities on Image captioning. Extensive ablations\nvalidate the measurable gains of each component.",
      "authors": [
        "Xiao Li",
        "Jiaqi Zhang",
        "Shuxiang Zhang",
        "Tianshui Chen",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "published": "2025-10-01T15:51:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01047v1"
    },
    {
      "arxiv_id": "2510.01038v1",
      "title": "Activation-Deactivation: A General Framework for Robust Post-hoc\n  Explainable AI",
      "summary": "Black-box explainability methods are popular tools for explaining the\ndecisions of image classifiers. A major drawback of these tools is their\nreliance on mutants obtained by occluding parts of the input, leading to\nout-of-distribution images. This raises doubts about the quality of the\nexplanations. Moreover, choosing an appropriate occlusion value often requires\ndomain knowledge. In this paper we introduce a novel forward-pass paradigm\nActivation-Deactivation (AD), which removes the effects of occluded input\nfeatures from the model's decision-making by switching off the parts of the\nmodel that correspond to the occlusions. We introduce ConvAD, a drop-in\nmechanism that can be easily added to any trained Convolutional Neural Network\n(CNN), and which implements the AD paradigm. This leads to more robust\nexplanations without any additional training or fine-tuning. We prove that the\nConvAD mechanism does not change the decision-making process of the network. We\nprovide experimental evaluation across several datasets and model\narchitectures. We compare the quality of AD-explanations with explanations\nachieved using a set of masking values, using the proxies of robustness, size,\nand confidence drop-off. We observe a consistent improvement in robustness of\nAD explanations (up to 62.5%) compared to explanations obtained with\nocclusions, demonstrating that ConvAD extracts more robust explanations without\nthe need for domain knowledge.",
      "authors": [
        "Akchunya Chanchal",
        "David A. Kelly",
        "Hana Chockler"
      ],
      "published": "2025-10-01T15:42:58Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01038v1"
    },
    {
      "arxiv_id": "2510.01037v1",
      "title": "CurES: From Gradient Analysis to Efficient Curriculum Learning for\n  Reasoning LLMs",
      "summary": "Curriculum learning plays a crucial role in enhancing the training efficiency\nof large language models (LLMs) on reasoning tasks. However, existing methods\noften fail to adequately account for variations in prompt difficulty or rely on\nsimplistic filtering mechanisms to select prompt datasets within a narrow\ncriterion range, resulting in significant computational waste. In this work, we\napproach the problem from the perspective of reinforcement learning gradient\noptimization, offering a systematic and theoretical investigation into how to\nimprove the training efficiency of LLMs. We identify two key factors\ninfluencing training efficiency: the selection of training prompts and the\nallocation of rollout quantities across different prompts. Our theoretical\nanalysis reveals that the sampling distribution of prompts dictates the\nconvergence rate of gradient descent, while the allocation of the rollout\nquantity influences the consistency and stability of overall gradient updates.\nBased on these insights, we propose CurES, an efficient training method that\naccelerates convergence and employs Bayesian posterior estimation to minimize\ncomputational overhead. Experiments demonstrate that our CurES outperforms\nGroup Relative Policy Optimization (GRPO) by \\textbf{+3.30} points and\n\\textbf{+4.82} points with 1.5B and 7B models, respectively. Additionally,\nCurES exhibits faster convergence compared to baselines, including GRPO.",
      "authors": [
        "Yongcheng Zeng",
        "Zexu Sun",
        "Bokai Ji",
        "Erxue Min",
        "Hengyi Cai",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Haifeng Zhang",
        "Xu Chen",
        "Jun Wang"
      ],
      "published": "2025-10-01T15:41:27Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01037v1"
    },
    {
      "arxiv_id": "2510.01030v1",
      "title": "Uncovering the Computational Ingredients of Human-Like Representations\n  in LLMs",
      "summary": "The ability to translate diverse patterns of inputs into structured patterns\nof behavior has been thought to rest on both humans' and machines' ability to\nlearn robust representations of relevant concepts. The rapid advancement of\ntransformer-based large language models (LLMs) has led to a diversity of\ncomputational ingredients -- architectures, fine tuning methods, and training\ndatasets among others -- but it remains unclear which of these ingredients are\nmost crucial for building models that develop human-like representations.\nFurther, most current LLM benchmarks are not suited to measuring\nrepresentational alignment between humans and models, making benchmark scores\nunreliable for assessing if current LLMs are making progress towards becoming\nuseful cognitive models. We address these limitations by first evaluating a set\nof over 70 models that widely vary in their computational ingredients on a\ntriplet similarity task, a method well established in the cognitive sciences\nfor measuring human conceptual representations, using concepts from the THINGS\ndatabase. Comparing human and model representations, we find that models that\nundergo instruction-finetuning and which have larger dimensionality of\nattention heads are among the most human aligned, while multimodal pretraining\nand parameter size have limited bearing on alignment. Correlations between\nalignment scores and scores on existing benchmarks reveal that while some\nbenchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for\ncapturing representational alignment, no existing benchmark is capable of fully\naccounting for the variance of alignment scores, demonstrating their\ninsufficiency in capturing human-AI alignment. Taken together, our findings\nhelp highlight the computational ingredients most essential for advancing LLMs\ntowards models of human conceptual representation and address a key\nbenchmarking gap in LLM evaluation.",
      "authors": [
        "Zach Studdiford",
        "Timothy T. Rogers",
        "Kushin Mukherjee",
        "Siddharth Suresh"
      ],
      "published": "2025-10-01T15:37:19Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01030v1"
    },
    {
      "arxiv_id": "2510.01025v1",
      "title": "Shape Happens: Automatic Feature Manifold Discovery in LLMs via\n  Supervised Multi-Dimensional Scaling",
      "summary": "The linear representation hypothesis states that language models (LMs) encode\nconcepts as directions in their latent space, forming organized,\nmultidimensional manifolds. Prior efforts focus on discovering specific\ngeometries for specific features, and thus lack generalization. We introduce\nSupervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to\nautomatically discover feature manifolds. We apply SMDS to temporal reasoning\nas a case study, finding that different features form various geometric\nstructures such as circles, lines, and clusters. SMDS reveals many insights on\nthese structures: they consistently reflect the properties of the concepts they\nrepresent; are stable across model families and sizes; actively support\nreasoning in models; and dynamically reshape in response to context changes.\nTogether, our findings shed light on the functional role of feature manifolds,\nsupporting a model of entity-based reasoning in which LMs encode and transform\nstructured representations.",
      "authors": [
        "Federico Tiblias",
        "Irina Bigoulaeva",
        "Jingcheng Niu",
        "Simone Balloccu",
        "Iryna Gurevych"
      ],
      "published": "2025-10-01T15:30:47Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01025v1"
    },
    {
      "arxiv_id": "2510.01020v1",
      "title": "The Good, the Bad, and the Sampled: a No-Regret Approach to Safe Online\n  Classification",
      "summary": "We study the problem of sequentially testing individuals for a binary disease\noutcome whose true risk is governed by an unknown logistic model. At each\nround, a patient arrives with feature vector $x_t$, and the decision maker may\neither pay to administer a (noiseless) diagnostic test--revealing the true\nlabel--or skip testing and predict the patient's disease status based on their\nfeature vector and prior history. Our goal is to minimize the total number of\ncostly tests required while guaranteeing that the fraction of\nmisclassifications does not exceed a prespecified error tolerance $\\alpha$,\nwith probability at least $1-\\delta$. To address this, we develop a novel\nalgorithm that interleaves label-collection and distribution estimation to\nestimate both $\\theta^{*}$ and the context distribution $P$, and computes a\nconservative, data-driven threshold $\\tau_t$ on the logistic score\n$|x_t^\\top\\theta|$ to decide when testing is necessary. We prove that, with\nprobability at least $1-\\delta$, our procedure does not exceed the target\nmisclassification rate, and requires only $O(\\sqrt{T})$ excess tests compared\nto the oracle baseline that knows both $\\theta^{*}$ and the patient feature\ndistribution $P$. This establishes the first no-regret guarantees for\nerror-constrained logistic testing, with direct applications to cost-sensitive\nmedical screening. Simulations corroborate our theoretical guarantees, showing\nthat in practice our procedure efficiently estimates $\\theta^{*}$ while\nretaining safety guarantees, and does not require too many excess tests.",
      "authors": [
        "Tavor Z. Baharav",
        "Spyros Dragazis",
        "Aldo Pacchiano"
      ],
      "published": "2025-10-01T15:28:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01020v1"
    },
    {
      "arxiv_id": "2510.01006v1",
      "title": "Integrating AI and Ensemble Forecasting: Explainable Materials Planning\n  with Scorecards and Trend Insights for a Large-Scale Manufacturer",
      "summary": "This paper presents a practical architecture for after-sales demand\nforecasting and monitoring that unifies a revenue- and cluster-aware ensemble\nof statistical, machine-learning, and deep-learning models with a role-driven\nanalytics layer for scorecards and trend diagnostics. The framework ingests\nexogenous signals (installed base, pricing, macro indicators, life cycle,\nseasonality) and treats COVID-19 as a distinct regime, producing country-part\nforecasts with calibrated intervals. A Pareto-aware segmentation forecasts\nhigh-revenue items individually and pools the long tail via clusters, while\nhorizon-aware ensembling aligns weights with business-relevant losses (e.g.,\nWMAPE). Beyond forecasts, a performance scorecard delivers decision-focused\ninsights: accuracy within tolerance thresholds by revenue share and count, bias\ndecomposition (over- vs under-forecast), geographic and product-family\nhotspots, and ranked root causes tied to high-impact part-country pairs. A\ntrend module tracks trajectories of MAPE/WMAPE and bias across recent months,\nflags entities that are improving or deteriorating, detects change points\naligned with known regimes, and attributes movements to lifecycle and seasonal\nfactors. LLMs are embedded in the analytics layer to generate role-aware\nnarratives and enforce reporting contracts. They standardize business\ndefinitions, automate quality checks and reconciliations, and translate\nquantitative results into concise, explainable summaries for planners and\nexecutives. The system exposes a reproducible workflow -- request\nspecification, model execution, database-backed artifacts, and AI-generated\nnarratives -- so planners can move from \"How accurate are we now?\" to \"Where is\naccuracy heading and which levers should we pull?\", closing the loop between\nforecasting, monitoring, and inventory decisions across more than 90 countries\nand about 6,000 parts.",
      "authors": [
        "Saravanan Venkatachalam"
      ],
      "published": "2025-10-01T15:14:10Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01006v1"
    },
    {
      "arxiv_id": "2510.01004v1",
      "title": "TextCAM: Explaining Class Activation Map with Text",
      "summary": "Deep neural networks (DNNs) have achieved remarkable success across domains\nbut remain difficult to interpret, limiting their trustworthiness in\nhigh-stakes applications. This paper focuses on deep vision models, for which a\ndominant line of explainability methods are Class Activation Mapping (CAM) and\nits variants working by highlighting spatial regions that drive predictions. We\nfigure out that CAM provides little semantic insight into what attributes\nunderlie these activations. To address this limitation, we propose TextCAM, a\nnovel explanation framework that enriches CAM with natural languages. TextCAM\ncombines the precise spatial localization of CAM with the semantic alignment of\nvision-language models (VLMs). Specifically, we derive channel-level semantic\nrepresentations using CLIP embeddings and linear discriminant analysis, and\naggregate them with CAM weights to produce textual descriptions of salient\nvisual evidence. This yields explanations that jointly specify where the model\nattends and what visual attributes likely support its decision. We further\nextend TextCAM to generate feature channels into semantically coherent groups,\nenabling more fine-grained visual-textual explanations. Experiments on\nImageNet, CLEVR, and CUB demonstrate that TextCAM produces faithful and\ninterpretable rationales that improve human understanding, detect spurious\ncorrelations, and preserve model fidelity.",
      "authors": [
        "Qiming Zhao",
        "Xingjian Li",
        "Xiaoyu Cao",
        "Xiaolong Wu",
        "Min Xu"
      ],
      "published": "2025-10-01T15:11:14Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01004v1"
    },
    {
      "arxiv_id": "2510.00976v1",
      "title": "Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware\n  Secure Aggregation",
      "summary": "Rare-disease diagnosis remains one of the most pressing challenges in digital\nhealth, hindered by extreme data scarcity, privacy concerns, and the limited\nresources of edge devices. This paper proposes the Adaptive Federated Few-Shot\nRare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)\nfew-shot federated optimization with meta-learning to generalize from limited\npatient samples, (ii) energy-aware client scheduling to mitigate device\ndropouts and ensure balanced participation, and (iii) secure aggregation with\ncalibrated differential privacy to safeguard sensitive model updates. Unlike\nprior work that addresses these aspects in isolation, AFFR unifies them into a\nmodular pipeline deployable on real-world clinical networks. Experimental\nevaluation on simulated rare-disease detection datasets demonstrates up to 10%\nimprovement in accuracy compared with baseline FL, while reducing client\ndropouts by over 50% without degrading convergence. Furthermore,\nprivacy-utility trade-offs remain within clinically acceptable bounds. These\nfindings highlight AFFR as a practical pathway for equitable and trustworthy\nfederated diagnosis of rare conditions.",
      "authors": [
        "Aueaphum Aueawatthanaphisut"
      ],
      "published": "2025-10-01T14:52:07Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00976v1"
    },
    {
      "arxiv_id": "2510.00967v1",
      "title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via\n  Agentic RL",
      "summary": "Designing and optimizing task-specific quantum circuits are crucial to\nleverage the advantage of quantum computing. Recent large language model\n(LLM)-based quantum circuit generation has emerged as a promising automatic\nsolution. However, the fundamental challenges remain unaddressed: (i)\nparameterized quantum gates require precise numerical values for optimal\nperformance, which also depend on multiple aspects, including the number of\nquantum gates, their parameters, and the layout/depth of the circuits. (ii)\nLLMs often generate low-quality or incorrect quantum circuits due to the lack\nof quantum domain-specific knowledge. We propose QUASAR, an agentic\nreinforcement learning (RL) framework for quantum circuits generation and\noptimization based on tool-augmented LLMs. To align the LLM with\nquantum-specific knowledge and improve the generated quantum circuits, QUASAR\ndesigns (i) a quantum circuit verification approach with external quantum\nsimulators and (ii) a sophisticated hierarchical reward mechanism in RL\ntraining. Extensive evaluation shows improvements in both syntax and semantic\nperformance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR\nhas achieved the validity of 99.31% in Pass@1 and 100% in Pass@10,\noutperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several\nsupervised-fine-tuning (SFT)-only and RL-only baselines.",
      "authors": [
        "Cong Yu",
        "Valter Uotila",
        "Shilong Deng",
        "Qingyuan Wu",
        "Tuo Shi",
        "Songlin Jiang",
        "Lei You",
        "Bo Zhao"
      ],
      "published": "2025-10-01T14:40:04Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00967v1"
    },
    {
      "arxiv_id": "2510.00966v1",
      "title": "Deep Learning-Based Approach for Improving Relational Aggregated Search",
      "summary": "Due to an information explosion on the internet, there is a need for the\ndevelopment of aggregated search systems that can boost the retrieval and\nmanagement of content in various formats. To further improve the clustering of\nArabic text data in aggregated search environments, this research investigates\nthe application of advanced natural language processing techniques, namely\nstacked autoencoders and AraBERT embeddings. By transcending the limitations of\ntraditional search engines, which are imprecise, not contextually relevant, and\nnot personalized, we offer more enriched, context-aware characterizations of\nsearch results, so we used a K-means clustering algorithm to discover\ndistinctive features and relationships in these results, we then used our\napproach on different Arabic queries to evaluate its effectiveness. Our model\nillustrates that using stacked autoencoders in representation learning suits\nclustering tasks and can significantly improve clustering search results. It\nalso demonstrates improved accuracy and relevance of search results.",
      "authors": [
        "Sara Saad Soliman",
        "Ahmed Younes",
        "Islam Elkabani",
        "Ashraf Elsayed"
      ],
      "published": "2025-10-01T14:37:38Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.00966v1"
    },
    {
      "arxiv_id": "2510.00960v1",
      "title": "A Neuro-Fuzzy System for Interpretable Long-Term Stock Market\n  Forecasting",
      "summary": "In the complex landscape of multivariate time series forecasting, achieving\nboth accuracy and interpretability remains a significant challenge. This paper\nintroduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network\narchitecture combined with multi-head self-attention and fuzzy inference\nsystems to analyze multivariate stock market data and conduct long-term time\nseries forecasting. The method leverages LSTM networks and temporal attention\nto condense multivariate data into interpretable features suitable for fuzzy\ninference systems. The resulting architecture offers comparable forecasting\nperformance to conventional models such as ARIMA and LSTM while providing\nmeaningful information flow within the network. The method was examined on the\nreal world stock market index S\\&P500. Initial results show potential for\ninterpretable forecasting and identify current performance tradeoffs,\nsuggesting practical application in understanding and forecasting stock market\nbehavior.",
      "authors": [
        "Miha Ožbot",
        "Igor Škrjanc",
        "Vitomir Štruc"
      ],
      "published": "2025-10-01T14:33:07Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00960v1"
    },
    {
      "arxiv_id": "2510.00958v1",
      "title": "Test-Time Search in Neural Graph Coarsening Procedures for the\n  Capacitated Vehicle Routing Problem",
      "summary": "The identification of valid inequalities, such as the rounded capacity\ninequalities (RCIs), is a key component of cutting plane methods for the\nCapacitated Vehicle Routing Problem (CVRP). While a deep learning-based\nseparation method can learn to find high-quality cuts, our analysis reveals\nthat the model produces fewer cuts than expected because it is insufficiently\nsensitive to generate a diverse set of generated subsets. This paper proposes\nan alternative: enhancing the performance of a trained model at inference time\nthrough a new test-time search with stochasticity. First, we introduce\nstochastic edge selection into the graph coarsening procedure, replacing the\npreviously proposed greedy approach. Second, we propose the Graph Coarsening\nHistory-based Partitioning (GraphCHiP) algorithm, which leverages coarsening\nhistory to identify not only RCIs but also, for the first time, the Framed\ncapacity inequalities (FCIs). Experiments on randomly generated CVRP instances\ndemonstrate the effectiveness of our approach in reducing the dual gap compared\nto the existing neural separation method. Additionally, our method discovers\neffective FCIs on a specific instance, despite the challenging nature of\nidentifying such cuts.",
      "authors": [
        "Yoonju Sim",
        "Hyeonah Kim",
        "Changhyun Kwon"
      ],
      "published": "2025-10-01T14:31:36Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00958v1"
    },
    {
      "arxiv_id": "2510.00956v1",
      "title": "Bridging the Gap Between Simulated and Real Network Data Using Transfer\n  Learning",
      "summary": "Machine Learning (ML)-based network models provide fast and accurate\npredictions for complex network behaviors but require substantial training\ndata. Collecting such data from real networks is often costly and limited,\nespecially for critical scenarios like failures. As a result, researchers\ncommonly rely on simulated data, which reduces accuracy when models are\ndeployed in real environments. We propose a hybrid approach leveraging transfer\nlearning to combine simulated and real-world data. Using RouteNet-Fermi, we\nshow that fine-tuning a pre-trained model with a small real dataset\nsignificantly improves performance. Our experiments with OMNeT++ and a custom\ntestbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay\nprediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and\nwith 50 scenarios, by 48%.",
      "authors": [
        "Carlos Güemes-Palau",
        "Miquel Ferriol-Galmés",
        "Jordi Paillisse-Vilanova",
        "Albert López-Brescó",
        "Pere Barlet-Ros",
        "Albert Cabellos-Aparicio"
      ],
      "published": "2025-10-01T14:29:47Z",
      "primary_category": "cs.NI",
      "arxiv_url": "https://arxiv.org/abs/2510.00956v1"
    },
    {
      "arxiv_id": "2510.01299v1",
      "title": "Enhancing the development of Cherenkov Telescope Array control software\n  with Large Language Models",
      "summary": "We develop AI agents based on instruction-finetuned large language models\n(LLMs) to assist in the engineering and operation of the Cherenkov Telescope\nArray Observatory (CTAO) Control and Data Acquisition Software (ACADA). These\nagents align with project-specific documentation and codebases, understand\ncontextual information, interact with external APIs, and communicate with users\nin natural language. We present our progress in integrating these features into\nCTAO pipelines for operations and offline data analysis.",
      "authors": [
        "Dmitriy Kostunin",
        "Elisa Jones",
        "Vladimir Sotnikov",
        "Valery Sotnikov",
        "Sergo Golovachev",
        "Alexandre Strube"
      ],
      "published": "2025-10-01T14:14:41Z",
      "primary_category": "astro-ph.IM",
      "arxiv_url": "https://arxiv.org/abs/2510.01299v1"
    },
    {
      "arxiv_id": "2510.00922v1",
      "title": "On Discovering Algorithms for Adversarial Imitation Learning",
      "summary": "Adversarial Imitation Learning (AIL) methods, while effective in settings\nwith limited expert demonstrations, are often considered unstable. These\napproaches typically decompose into two components: Density Ratio (DR)\nestimation $\\frac{\\rho_E}{\\rho_{\\pi}}$, where a discriminator estimates the\nrelative occupancy of state-action pairs under the policy versus the expert;\nand Reward Assignment (RA), where this ratio is transformed into a reward\nsignal used to train the policy. While significant research has focused on\nimproving density estimation, the role of reward assignment in influencing\ntraining dynamics and final policy performance has been largely overlooked. RA\nfunctions in AIL are typically derived from divergence minimization objectives,\nrelying heavily on human design and ingenuity. In this work, we take a\ndifferent approach: we investigate the discovery of data-driven RA functions,\ni.e, based directly on the performance of the resulting imitation policy. To\nthis end, we leverage an LLM-guided evolutionary framework that efficiently\nexplores the space of RA functions, yielding \\emph{Discovered Adversarial\nImitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably,\nDAIL generalises across unseen environments and policy optimization algorithms,\noutperforming the current state-of-the-art of \\emph{human-designed} baselines.\nFinally, we analyse why DAIL leads to more stable training, offering novel\ninsights into the role of RA functions in the stability of AIL. Code is\npublicly available: https://github.com/shshnkreddy/DAIL.",
      "authors": [
        "Shashank Reddy Chirra",
        "Jayden Teoh",
        "Praveen Paruchuri",
        "Pradeep Varakantham"
      ],
      "published": "2025-10-01T14:02:05Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00922v1"
    },
    {
      "arxiv_id": "2510.00919v2",
      "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in\n  Olympic-Level Physics Problem Solving",
      "summary": "Retrieval-augmented generation (RAG) with foundation models has achieved\nstrong performance across diverse tasks, but their capacity for expert-level\nreasoning-such as solving Olympiad-level physics problems-remains largely\nunexplored. Inspired by the way students prepare for competitions by reviewing\npast problems, we investigate the potential of RAG to enhance physics reasoning\nin foundation models. We introduce PhoPile, a high-quality multimodal dataset\nspecifically designed for Olympiad-level physics, enabling systematic study of\nretrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,\ncapturing the inherently multimodal nature of physics problem solving. Using\nPhoPile, we benchmark RAG-augmented foundation models, covering both large\nlanguage models (LLMs) and large multimodal models (LMMs) with multiple\nretrievers. Our results demonstrate that integrating retrieval with physics\ncorpora can improve model performance, while also highlighting challenges that\nmotivate further research in retrieval-augmented physics reasoning.",
      "authors": [
        "Shunfeng Zheng",
        "Yudi Zhang",
        "Meng Fang",
        "Zihan Zhang",
        "Zhitan Wu",
        "Mykola Pechenizkiy",
        "Ling Chen"
      ],
      "published": "2025-10-01T13:57:53Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00919v2"
    },
    {
      "arxiv_id": "2510.00915v1",
      "title": "Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect\n  Verifiers",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) trains policies against\nautomated verifiers to avoid costly human labeling. To reduce vulnerability to\nverifier hacking, many RLVR systems collapse rewards to binary $\\{0,1\\}$ during\ntraining. This choice carries a cost: it introduces \\textit{false negatives}\n(rejecting correct answers, FNs) and \\textit{false positives} (accepting\nincorrect ones, FPs). For instance, a rule-based checker may mark the correct\nfraction $\\frac{12}{36}$ as wrong when compared against the canonical\n$\\frac{1}{3}$ due to brittle parsing/equivalence rules (FN), while a large\nlanguage model (LLM) judges can be gamed by superficial cues or even a single\nadversarial token, yielding inflated correctness for wrong solutions (FP). We\nformalize verifier unreliability by modeling the verifier as a stochastic\nreward channel with asymmetric noise rates. From this abstraction, we derive\ntwo correction algorithms for verifier errors. The first is a \\textit{backward}\ncorrection that de-biases the observed binary reward to recover an\n\\textit{unbiased} estimator of the clean policy gradient. The second is a\n\\textit{forward} correction that reweights score-function terms so that the\nexpected update direction aligns with the \\textit{clean gradient}; notably, it\nrequires only the FN rate. We implement both as lightweight hooks in a group\nrelative policy optimization (GRPO)-based RLVR pipeline and evaluate them on\nmath-reasoning models and benchmarks. Across models and datasets, both\ncorrections improve over uncorrected training; the forward variant converges\nfaster and remains stable under heavier noise. Finally, we show a practical\nappeal mechanism in which a lightweight LLM verifier estimates the FN rate\nonline by rechecking rule-based negatives, obtaining outperformance compared\nwith other state-of-the-art contenders.",
      "authors": [
        "Xin-Qiang Cai",
        "Wei Wang",
        "Feng Liu",
        "Tongliang Liu",
        "Gang Niu",
        "Masashi Sugiyama"
      ],
      "published": "2025-10-01T13:56:44Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00915v1"
    },
    {
      "arxiv_id": "2510.00911v1",
      "title": "RiskPO: Risk-based Policy Optimization via Verifiable Reward for LLM\n  Post-Training",
      "summary": "Reinforcement learning with verifiable reward has recently emerged as a\ncentral paradigm for post-training large language models (LLMs); however,\nprevailing mean-based methods, such as Group Relative Policy Optimization\n(GRPO), suffer from entropy collapse and limited reasoning gains. We argue that\nthese issues stem from overemphasizing high-probability output sequences while\nneglecting rare but informative reasoning paths. To address these challenges,\nwe propose Risk-based Policy Optimization (RiskPO), which substitutes classical\nmean-based objectives with principled risk measures. Specifically, we introduce\na Mixed Value-at-Risk objective that integrates weighted attention over\nmultiple regions of the reward distribution, thereby amplifying gradient\nsignals on challenging instances and preventing overconfident convergence. We\nfurther design a bundling scheme that aggregates multiple questions into\nbundles, thus enriching the feedback signal and yielding more stable and\ninformative training dynamics. Theoretically, we prove that the risk-averse\nupdate alleviates entropy collapse and promotes exploration. Numerically,\nRiskPO achieves consistent and significant improvements in mathematical\nreasoning, multi-modal reasoning, and code generation benchmarks, surpassing\nGRPO and its variants on both Pass@1 and Pass@k metrics. Our results\ndemonstrate that risk-based optimization provides a rigorous and effective\nparadigm for enhancing LLM reasoning capabilities.",
      "authors": [
        "Tao Ren",
        "Jinyang Jiang",
        "Hui Yang",
        "Wan Tian",
        "Minhao Zou",
        "Guanghao Li",
        "Zishi Zhang",
        "Qinghao Wang",
        "Shentao Qin",
        "Yanjun Zhao",
        "Rui Tao",
        "Hui Shao",
        "Yijie Peng"
      ],
      "published": "2025-10-01T13:53:09Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00911v1"
    },
    {
      "arxiv_id": "2510.00909v1",
      "title": "\"We are not Future-ready\": Understanding AI Privacy Risks and Existing\n  Mitigation Strategies from the Perspective of AI Developers in Europe",
      "summary": "The proliferation of AI has sparked privacy concerns related to training\ndata, model interfaces, downstream applications, and more. We interviewed 25 AI\ndevelopers based in Europe to understand which privacy threats they believe\npose the greatest risk to users, developers, and businesses and what protective\nstrategies, if any, would help to mitigate them. We find that there is little\nconsensus among AI developers on the relative ranking of privacy risks. These\ndifferences stem from salient reasoning patterns that often relate to human\nrather than purely technical factors. Furthermore, while AI developers are\naware of proposed mitigation strategies for addressing these risks, they\nreported minimal real-world adoption. Our findings highlight both gaps and\nopportunities for empowering AI developers to better address privacy risks in\nAI.",
      "authors": [
        "Alexandra Klymenko",
        "Stephen Meisenbacher",
        "Patrick Gage Kelley",
        "Sai Teja Peddinti",
        "Kurt Thomas",
        "Florian Matthes"
      ],
      "published": "2025-10-01T13:51:33Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.00909v1"
    },
    {
      "arxiv_id": "2510.00908v1",
      "title": "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval\n  with Multilingual LLMs",
      "summary": "Cross-lingual information retrieval (CLIR) addresses the challenge of\nretrieving relevant documents written in languages different from that of the\noriginal query. Research in this area has typically framed the task as\nmonolingual retrieval augmented by translation, treating retrieval methods and\ncross-lingual capabilities in isolation. Both monolingual and cross-lingual\nretrieval usually follow a pipeline of query expansion, ranking, re-ranking\nand, increasingly, question answering. Recent advances, however, have shifted\nfrom translation-based methods toward embedding-based approaches and leverage\nmultilingual large language models (LLMs), for which aligning representations\nacross languages remains a central challenge. The emergence of cross-lingual\nembeddings and multilingual LLMs has introduced a new paradigm, offering\nimproved retrieval performance and enabling answer generation. This survey\nprovides a comprehensive overview of developments from early translation-based\nmethods to state-of-the-art embedding-driven and generative techniques. It\npresents a structured account of core CLIR components, evaluation practices,\nand available resources. Persistent challenges such as data imbalance and\nlinguistic variation are identified, while promising directions are suggested\nfor advancing equitable and effective cross-lingual information retrieval. By\nsituating CLIR within the broader landscape of information retrieval and\nmultilingual language processing, this work not only reviews current\ncapabilities but also outlines future directions for building retrieval systems\nthat are robust, inclusive, and adaptable.",
      "authors": [
        "Roksana Goworek",
        "Olivia Macmillan-Scott",
        "Eda B. Özyiğit"
      ],
      "published": "2025-10-01T13:50:05Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.00908v1"
    },
    {
      "arxiv_id": "2510.00906v1",
      "title": "TubeDAgger: Reducing the Number of Expert Interventions with Stochastic\n  Reach-Tubes",
      "summary": "Interactive Imitation Learning deals with training a novice policy from\nexpert demonstrations in an online fashion. The established DAgger algorithm\ntrains a robust novice policy by alternating between interacting with the\nenvironment and retraining of the network. Many variants thereof exist, that\ndiffer in the method of discerning whether to allow the novice to act or return\ncontrol to the expert. We propose the use of stochastic reachtubes - common in\nverification of dynamical systems - as a novel method for estimating the\nnecessity of expert intervention. Our approach does not require fine-tuning of\ndecision thresholds per environment and effectively reduces the number of\nexpert interventions, especially when compared with related approaches that\nmake use of a doubt classification model.",
      "authors": [
        "Julian Lemmel",
        "Manuel Kranzl",
        "Adam Lamine",
        "Philipp Neubauer",
        "Radu Grosu",
        "Sophie A. Neubauer"
      ],
      "published": "2025-10-01T13:45:16Z",
      "primary_category": "eess.SY",
      "arxiv_url": "https://arxiv.org/abs/2510.00906v1"
    },
    {
      "arxiv_id": "2510.00894v1",
      "title": "FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge\n  Graphs",
      "summary": "Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including\ntext and images, to enhance entity and relation representations. Notably,\ndifferent modalities for the same entity often present complementary and\ndiverse information. However, existing MMKG methods primarily align modalities\ninto a shared space, which tends to overlook the distinct contributions of\nspecific modalities, limiting their performance particularly in low-resource\nsettings. To address this challenge, we propose FusionAdapter for the learning\nof few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an\nadapter module that enables efficient adaptation of each modality to unseen\nrelations and (2) a fusion strategy that integrates multimodal entity\nrepresentations while preserving diverse modality-specific characteristics. By\neffectively adapting and fusing information from diverse modalities,\nFusionAdapter improves generalization to novel relations with minimal\nsupervision. Extensive experiments on two benchmark MMKG datasets demonstrate\nthat FusionAdapter achieves superior performance over state-of-the-art methods.",
      "authors": [
        "Ran Liu",
        "Yuan Fang",
        "Xiaoli Li"
      ],
      "published": "2025-10-01T13:36:56Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00894v1"
    },
    {
      "arxiv_id": "2510.00890v1",
      "title": "Span-level Detection of AI-generated Scientific Text via Contrastive\n  Learning and Structural Calibration",
      "summary": "The rapid adoption of large language models (LLMs) in scientific writing\nraises serious concerns regarding authorship integrity and the reliability of\nscholarly publications. Existing detection approaches mainly rely on\ndocument-level classification or surface-level statistical cues; however, they\nneglect fine-grained span localization, exhibit weak calibration, and often\nfail to generalize across disciplines and generators. To address these\nlimitations, we present Sci-SpanDet, a structure-aware framework for detecting\nAI-generated scholarly texts. The proposed method combines section-conditioned\nstylistic modeling with multi-level contrastive learning to capture nuanced\nhuman-AI differences while mitigating topic dependence, thereby enhancing\ncross-domain robustness. In addition, it integrates BIO-CRF sequence labeling\nwith pointer-based boundary decoding and confidence calibration to enable\nprecise span-level detection and reliable probability estimates. Extensive\nexperiments on a newly constructed cross-disciplinary dataset of 100,000\nannotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,\nLLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with\nF1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows\nstrong resilience under adversarial rewriting and maintains balanced accuracy\nacross IMRaD sections and diverse disciplines, substantially surpassing\nexisting baselines. To ensure reproducibility and to foster further research on\nAI-generated text detection in scholarly documents, the curated dataset and\nsource code will be publicly released upon publication.",
      "authors": [
        "Zhen Yin",
        "Shenghua Wang"
      ],
      "published": "2025-10-01T13:35:14Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00890v1"
    },
    {
      "arxiv_id": "2510.00883v1",
      "title": "GLAI: GreenLightningAI for Accelerated Training through Knowledge\n  Decoupling",
      "summary": "In this work we introduce GreenLightningAI (GLAI), a new architectural block\ndesigned as an alternative to conventional MLPs. The central idea is to\nseparate two types of knowledge that are usually entangled during training: (i)\n*structural knowledge*, encoded by the stable activation patterns induced by\nReLU activations; and (ii) *quantitative knowledge*, carried by the numerical\nweights and biases. By fixing the structure once stabilized, GLAI reformulates\nthe MLP as a combination of paths, where only the quantitative component is\noptimized. This reformulation retains the universal approximation capabilities\nof MLPs, yet achieves a more efficient training process, reducing training time\nby ~40% on average across the cases examined in this study. Crucially, GLAI is\nnot just another classifier, but a generic block that can replace MLPs wherever\nthey are used, from supervised heads with frozen backbones to projection layers\nin self-supervised learning or few-shot classifiers. Across diverse\nexperimental setups, GLAI consistently matches or exceeds the accuracy of MLPs\nwith an equivalent number of parameters, while converging faster. Overall, GLAI\nestablishes a new design principle that opens a direction for future\nintegration into large-scale architectures such as Transformers, where MLP\nblocks dominate the computational footprint.",
      "authors": [
        "Jose I. Mestre",
        "Alberto Fernández-Hernández",
        "Cristian Pérez-Corral",
        "Manuel F. Dolz",
        "Jose Duato",
        "Enrique S. Quintana-Ortí"
      ],
      "published": "2025-10-01T13:31:34Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00883v1"
    },
    {
      "arxiv_id": "2510.00881v1",
      "title": "Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of\n  LLM Reasoning",
      "summary": "Large Language Models (LLMs) are increasingly integrated into software\nengineering (SE) tools for tasks that extend beyond code synthesis, including\njudgment under uncertainty and reasoning in ethically significant contexts. We\npresent a fully automated framework for assessing ethical reasoning\ncapabilities across 16 LLMs in a zero-shot setting, using 30 real-world\nethically charged scenarios. Each model is prompted to identify the most\napplicable ethical theory to an action, assess its moral acceptability, and\nexplain the reasoning behind their choice. Responses are compared against\nexpert ethicists' choices using inter-model agreement metrics. Our results show\nthat LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary\nAgreement Rate (BAR) on moral acceptability of 86.7%, with interpretable\ndivergences concentrated in ethically ambiguous cases. A qualitative analysis\nof free-text explanations reveals strong conceptual convergence across models\ndespite surface-level lexical diversity. These findings support the potential\nviability of LLMs as ethical inference engines within SE pipelines, enabling\nscalable, auditable, and adaptive integration of user-aligned ethical\nreasoning. Our focus is the Ethical Interpreter component of a broader\nprofiling pipeline: we evaluate whether current LLMs exhibit sufficient\ninterpretive stability and theory-consistent reasoning to support automated\nprofiling.",
      "authors": [
        "Patrizio Migliarini",
        "Mashal Afzal Memon",
        "Marco Autili",
        "Paola Inverardi"
      ],
      "published": "2025-10-01T13:28:26Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.00881v1"
    },
    {
      "arxiv_id": "2510.00876v1",
      "title": "Unveiling Interesting Insights: Monte Carlo Tree Search for Knowledge\n  Discovery",
      "summary": "Organizations are increasingly focused on leveraging data from their\nprocesses to gain insights and drive decision-making. However, converting this\ndata into actionable knowledge remains a difficult and time-consuming task.\nThere is often a gap between the volume of data collected and the ability to\nprocess and understand it, which automated knowledge discovery aims to fill.\nAutomated knowledge discovery involves complex open problems, including\neffectively navigating data, building models to extract implicit relationships,\nand considering subjective goals and knowledge. In this paper, we introduce a\nnovel method for Automated Insights and Data Exploration (AIDE), that serves as\na robust foundation for tackling these challenges through the use of Monte\nCarlo Tree Search (MCTS). We evaluate AIDE using both real-world and synthetic\ndata, demonstrating its effectiveness in identifying data transformations and\nmodels that uncover interesting data patterns. Among its strengths, AIDE's\nMCTS-based framework offers significant extensibility, allowing for future\nintegration of additional pattern extraction strategies and domain knowledge.\nThis makes AIDE a valuable step towards developing a comprehensive solution for\nautomated knowledge discovery.",
      "authors": [
        "Pietro Totis",
        "Alberto Pozanco",
        "Daniel Borrajo"
      ],
      "published": "2025-10-01T13:25:15Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00876v1"
    },
    {
      "arxiv_id": "2510.00862v1",
      "title": "Gather-Scatter Mamba: Accelerating Propagation with Efficient State\n  Space Model",
      "summary": "State Space Models (SSMs)-most notably RNNs-have historically played a\ncentral role in sequential modeling. Although attention mechanisms such as\nTransformers have since dominated due to their ability to model global context,\ntheir quadratic complexity and limited scalability make them less suited for\nlong sequences. Video super-resolution (VSR) methods have traditionally relied\non recurrent architectures to propagate features across frames. However, such\napproaches suffer from well-known issues including vanishing gradients, lack of\nparallelism, and slow inference speed. Recent advances in selective SSMs like\nMamba offer a compelling alternative: by enabling input-dependent state\ntransitions with linear-time complexity, Mamba mitigates these issues while\nmaintaining strong long-range modeling capabilities. Despite this potential,\nMamba alone struggles to capture fine-grained spatial dependencies due to its\ncausal nature and lack of explicit context aggregation. To address this, we\npropose a hybrid architecture that combines shifted window self-attention for\nspatial context aggregation with Mamba-based selective scanning for efficient\ntemporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an\nalignment-aware mechanism that warps features toward a center anchor frame\nwithin the temporal window before Mamba propagation and scatters them back\nafterward, effectively reducing occlusion artifacts and ensuring effective\nredistribution of aggregated information across all frames. The official\nimplementation is provided at: https://github.com/Ko-Lani/GSMamba.",
      "authors": [
        "Hyun-kyu Ko",
        "Youbin Kim",
        "Jihyeon Park",
        "Dongheok Park",
        "Gyeongjin Kang",
        "Wonjun Cho",
        "Hyung Yi",
        "Eunbyung Park"
      ],
      "published": "2025-10-01T13:11:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00862v1"
    },
    {
      "arxiv_id": "2510.00861v1",
      "title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented\n  LLMs",
      "summary": "While search-augmented large language models (LLMs) exhibit impressive\ncapabilities, their reliability in complex multi-hop reasoning remains limited.\nThis limitation arises from three fundamental challenges: decomposition errors,\nwhere tasks are incorrectly broken down; retrieval missing, where key evidence\nfails to be retrieved; and reasoning errors, where flawed logic propagates\nthrough the reasoning chain. A single failure in any of these stages can derail\nthe final answer. We propose Erasable Reinforcement Learning (ERL), a novel\nframework that transforms fragile reasoning into a robust process. ERL\nexplicitly identifies faulty steps, erases them, and regenerates reasoning in\nplace, preventing defective logic from propagating through the reasoning chain.\nThis targeted correction mechanism turns brittle reasoning into a more\nresilient process. Models trained with ERL, termed ESearch, achieve substantial\nimprovements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model\nachieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and\n+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest\nthat erasable reinforcement learning provides a powerful paradigm shift for\nrobust multi-step reasoning in LLMs.",
      "authors": [
        "Ziliang Wang",
        "Kang An",
        "Xuhui Zheng",
        "Faqiang Qian",
        "Weikun Zhang",
        "Cijun Ouyang",
        "Jialu Cai",
        "Yuhang Wang",
        "Yichao Wu"
      ],
      "published": "2025-10-01T13:10:36Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00861v1"
    },
    {
      "arxiv_id": "2510.00855v1",
      "title": "Can World Models Benefit VLMs for World Dynamics?",
      "summary": "Trained on internet-scale video data, generative world models are\nincreasingly recognized as powerful world simulators that can generate\nconsistent and plausible dynamics over structure, motion, and physics. This\nraises a natural question: with the advent of strong video foundational models,\nmight they supplant conventional vision encoder paradigms for general-purpose\nmultimodal understanding? While recent studies have begun to explore the\npotential of world models on common vision tasks, these explorations typically\nlack a systematic investigation of generic, multimodal tasks. In this work, we\nstrive to investigate the capabilities when world model priors are transferred\ninto Vision-Language Models: we re-purpose a video diffusion model as a\ngenerative encoder to perform a single denoising step and treat the resulting\nlatents as a set of visual embedding. We empirically investigate this class of\nmodels, which we refer to as World-Language Models (WorldLMs), and we find that\ngenerative encoders can capture latents useful for downstream understanding\nthat show distinctions from conventional encoders. Naming our best-performing\nvariant Dynamic Vision Aligner (DyVA), we further discover that this method\nsignificantly enhances spatial reasoning abilities and enables single-image\nmodels to perform multi-frame reasoning. Through the curation of a suite of\nvisual reasoning tasks, we find DyVA to surpass both open-source and\nproprietary baselines, achieving state-of-the-art or comparable performance. We\nattribute these gains to WorldLM's inherited motion-consistency internalization\nfrom video pre-training. Finally, we systematically explore extensive model\ndesigns to highlight promising directions for future work. We hope our study\ncan pave the way for a new family of VLMs that leverage priors from world\nmodels and are on a promising path towards generalist vision learners.",
      "authors": [
        "Kevin Zhang",
        "Kuangzhi Ge",
        "Xiaowei Chi",
        "Renrui Zhang",
        "Shaojun Shi",
        "Zhen Dong",
        "Sirui Han",
        "Shanghang Zhang"
      ],
      "published": "2025-10-01T13:07:05Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00855v1"
    },
    {
      "arxiv_id": "2510.00844v1",
      "title": "Learning Compact Representations of LLM Abilities via Item Response\n  Theory",
      "summary": "Recent years have witnessed a surge in the number of large language models\n(LLMs), yet efficiently managing and utilizing these vast resources remains a\nsignificant challenge. In this work, we explore how to learn compact\nrepresentations of LLM abilities that can facilitate downstream tasks, such as\nmodel routing and performance prediction on new benchmarks. We frame this\nproblem as estimating the probability that a given model will correctly answer\na specific query. Inspired by the item response theory (IRT) in psychometrics,\nwe model this probability as a function of three key factors: (i) the model's\nmulti-skill ability vector, (2) the query's discrimination vector that\nseparates models of differing skills, and (3) the query's difficulty scalar. To\nlearn these parameters jointly, we introduce a Mixture-of-Experts (MoE) network\nthat couples model- and query-level embeddings. Extensive experiments\ndemonstrate that our approach leads to state-of-the-art performance in both\nmodel routing and benchmark accuracy prediction. Moreover, analysis validates\nthat the learned parameters encode meaningful, interpretable information about\nmodel capabilities and query characteristics.",
      "authors": [
        "Jianhao Chen",
        "Chenxu Wang",
        "Gengrui Zhang",
        "Peng Ye",
        "Lei Bai",
        "Wei Hu",
        "Yuzhong Qu",
        "Shuyue Hu"
      ],
      "published": "2025-10-01T12:55:34Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00844v1"
    },
    {
      "arxiv_id": "2510.00845v2",
      "title": "Mechanistic Interpretability as Statistical Estimation: A Variance\n  Analysis of EAP-IG",
      "summary": "The development of trustworthy artificial intelligence requires moving beyond\nblack-box performance metrics toward an understanding of models' internal\ncomputations. Mechanistic Interpretability (MI) aims to meet this need by\nidentifying the algorithmic mechanisms underlying model behaviors. Yet, the\nscientific rigor of MI critically depends on the reliability of its findings.\nIn this work, we argue that interpretability methods, such as circuit\ndiscovery, should be viewed as statistical estimators, subject to questions of\nvariance and robustness. To illustrate this statistical framing, we present a\nsystematic stability analysis of a state-of-the-art circuit discovery method:\nEAP-IG. We evaluate its variance and robustness through a comprehensive suite\nof controlled perturbations, including input resampling, prompt paraphrasing,\nhyperparameter variation, and injected noise within the causal analysis itself.\nAcross a diverse set of models and tasks, our results demonstrate that EAP-IG\nexhibits high structural variance and sensitivity to hyperparameters,\nquestioning the stability of its findings. Based on these results, we offer a\nset of best-practice recommendations for the field, advocating for the routine\nreporting of stability metrics to promote a more rigorous and statistically\ngrounded science of interpretability.",
      "authors": [
        "Maxime Méloux",
        "François Portet",
        "Maxime Peyrard"
      ],
      "published": "2025-10-01T12:55:34Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00845v2"
    },
    {
      "arxiv_id": "2510.00837v1",
      "title": "Feature Identification for Hierarchical Contrastive Learning",
      "summary": "Hierarchical classification is a crucial task in many applications, where\nobjects are organized into multiple levels of categories. However, conventional\nclassification approaches often neglect inherent inter-class relationships at\ndifferent hierarchy levels, thus missing important supervisory signals. Thus,\nwe propose two novel hierarchical contrastive learning (HMLC) methods. The\nfirst, leverages a Gaussian Mixture Model (G-HMLC) and the second uses an\nattention mechanism to capture hierarchy-specific features (A-HMLC), imitating\nhuman processing. Our approach explicitly models inter-class relationships and\nimbalanced class distribution at higher hierarchy levels, enabling fine-grained\nclustering across all hierarchy levels. On the competitive CIFAR100 and\nModelNet40 datasets, our method achieves state-of-the-art performance in linear\nevaluation, outperforming existing hierarchical contrastive learning methods by\n2 percentage points in terms of accuracy. The effectiveness of our approach is\nbacked by both quantitative and qualitative results, highlighting its potential\nfor applications in computer vision and beyond.",
      "authors": [
        "Julius Ott",
        "Nastassia Vysotskaya",
        "Huawei Sun",
        "Lorenzo Servadei",
        "Robert Wille"
      ],
      "published": "2025-10-01T12:46:47Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00837v1"
    },
    {
      "arxiv_id": "2510.00836v1",
      "title": "Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based\n  Models and Synthetic Oversampling Techniques",
      "summary": "This study aims to detect pump and dump (P&D) manipulation in cryptocurrency\nmarkets, where the scarcity of such events causes severe class imbalance and\nhinders accurate detection. To address this issue, the Synthetic Minority\nOversampling Technique (SMOTE) was applied, and advanced ensemble learning\nmodels were evaluated to distinguish manipulative trading behavior from normal\nmarket activity. The experimental results show that applying SMOTE greatly\nenhanced the ability of all models to detect P&D events by increasing recall\nand improving the overall balance between precision and recall. In particular,\nXGBoost and LightGBM achieved high recall rates (94.87% and 93.59%,\nrespectively) with strong F1-scores and demonstrated fast computational\nperformance, making them suitable for near real time surveillance. These\nfindings indicate that integrating data balancing techniques with ensemble\nmethods significantly improves the early detection of manipulative activities,\ncontributing to a fairer, more transparent, and more stable cryptocurrency\nmarket.",
      "authors": [
        "Jieun Yu",
        "Minjung Park",
        "Sangmi Chai"
      ],
      "published": "2025-10-01T12:46:45Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00836v1"
    },
    {
      "arxiv_id": "2510.00833v1",
      "title": "Towards Verifiable Federated Unlearning: Framework, Challenges, and The\n  Road Ahead",
      "summary": "Federated unlearning (FUL) enables removing the data influence from the model\ntrained across distributed clients, upholding the right to be forgotten as\nmandated by privacy regulations. FUL facilitates a value exchange where clients\ngain privacy-preserving control over their data contributions, while service\nproviders leverage decentralized computing and data freshness. However, this\nentire proposition is undermined because clients have no reliable way to verify\nthat their data influence has been provably removed, as current metrics and\nsimple notifications offer insufficient assurance. We envision unlearning\nverification becoming a pivotal and trust-by-design part of the FUL life-cycle\ndevelopment, essential for highly regulated and data-sensitive services and\napplications like healthcare. This article introduces veriFUL, a reference\nframework for verifiable FUL that formalizes verification entities, goals,\napproaches, and metrics. Specifically, we consolidate existing efforts and\ncontribute new insights, concepts, and metrics to this domain. Finally, we\nhighlight research challenges and identify potential applications and\ndevelopments for verifiable FUL and veriFUL.",
      "authors": [
        "Thanh Linh Nguyen",
        "Marcela Tuler de Oliveira",
        "An Braeken",
        "Aaron Yi Ding",
        "Quoc-Viet Pham"
      ],
      "published": "2025-10-01T12:45:46Z",
      "primary_category": "cs.DC",
      "arxiv_url": "https://arxiv.org/abs/2510.00833v1"
    },
    {
      "arxiv_id": "2510.00831v1",
      "title": "Benchmarking Machine Learning Models for Fault Classification and\n  Localization in Power System Protection",
      "summary": "The increasing integration of distributed energy resources (DERs),\nparticularly renewables, poses significant challenges for power system\nprotection, with fault classification (FC) and fault localization (FL) being\namong the most critical tasks. Conventional protection schemes, based on fixed\nthresholds, cannot reliably identify and localize short circuits with the\nincreasing complexity of the grid under dynamic conditions. Machine learning\n(ML) offers a promising alternative; however, systematic benchmarks across\nmodels and settings remain limited. This work presents, for the first time, a\ncomparative benchmarking study of classical ML models for FC and FL in power\nsystem protection based on EMT data. Using voltage and current waveforms\nsegmented into sliding windows of 10 ms to 50 ms, we evaluate models under\nrealistic real-time constraints. Performance is assessed in terms of accuracy,\nrobustness to window size, and runtime efficiency. The best-performing FC model\nachieved an F1 score of 0.992$\\pm$0.001, while the top FL model reached an R2\nof 0.806$\\pm$0.008 with a mean processing time of 0.563 ms.",
      "authors": [
        "Julian Oelhaf",
        "Georg Kordowich",
        "Changhun Kim",
        "Paula Andrea Pérez-Toro",
        "Christian Bergler",
        "Andreas Maier",
        "Johann Jäger",
        "Siming Bayer"
      ],
      "published": "2025-10-01T12:44:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00831v1"
    },
    {
      "arxiv_id": "2510.00821v1",
      "title": "Logical Consistency Between Disagreeing Experts and Its Role in AI\n  Safety",
      "summary": "If two experts disagree on a test, we may conclude both cannot be 100 per\ncent correct. But if they completely agree, no possible evaluation can be\nexcluded. This asymmetry in the utility of agreements versus disagreements is\nexplored here by formalizing a logic of unsupervised evaluation for\nclassifiers. Its core problem is computing the set of group evaluations that\nare logically consistent with how we observe them agreeing and disagreeing in\ntheir decisions. Statistical summaries of their aligned decisions are inputs\ninto a Linear Programming problem in the integer space of possible correct or\nincorrect responses given true labels. Obvious logical constraints, such as,\nthe number of correct responses cannot exceed the number of observed responses,\nare inequalities. But in addition, there are axioms, universally applicable\nlinear equalities that apply to all finite tests. The practical and immediate\nutility of this approach to unsupervised evaluation using only logical\nconsistency is demonstrated by building no-knowledge alarms that can detect\nwhen one or more LLMs-as-Judges are violating a minimum grading threshold\nspecified by the user.",
      "authors": [
        "Andrés Corrada-Emmanuel"
      ],
      "published": "2025-10-01T12:30:01Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00821v1"
    },
    {
      "arxiv_id": "2510.00819v1",
      "title": "Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning\n  in LLM Reasoning",
      "summary": "Reinforcement Learning, particularly through policy gradient methods, has\nplayed a central role in enabling reasoning capabilities of Large Language\nModels. However, the optimization stability of policy gradients in this setting\nremains understudied. As a result, existing implementations often resort to\nconservative hyperparameter choices to ensure stability, which requires more\ntraining samples and increases computational costs. Hence, developing models\nfor reliably tracking the underlying optimization dynamics and leveraging them\ninto training enables more sample-efficient regimes and further unleashes\nscalable post-training. We address this gap by formalizing the stochastic\noptimization problem of policy gradients with explicit consideration of\nsecond-order geometry. We propose a tractable computational framework that\ntracks and leverages curvature information during policy updates. We further\nemploy this framework to design interventions in the optimization process\nthrough data selection. The resultant algorithm, Curvature-Aware Policy\nOptimization (CAPO), identifies samples that contribute to unstable updates and\nmasks them out. Theoretically, we establish monotonic improvement guarantees\nunder realistic assumptions. On standard math reasoning benchmarks, we\nempirically show that CAPO ensures stable updates under aggressive learning\nregimes where baselines catastrophically fail. With minimal intervention\n(rejecting fewer than 8% of tokens), CAPO achieves up to 30x improvement in\nsample efficiency over standard GRPO for LLM reasoning.",
      "authors": [
        "Luckeciano C. Melo",
        "Alessandro Abate",
        "Yarin Gal"
      ],
      "published": "2025-10-01T12:29:32Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00819v1"
    },
    {
      "arxiv_id": "2510.00817v2",
      "title": "Semantic Bridges Between First Order c-Representations and Cost-Based\n  Semantics: An Initial Perspective",
      "summary": "Weighted-knowledge bases and cost-based semantics represent a recent\nformalism introduced by Bienvenu et al. for Ontology Mediated Data Querying in\nthe case where a given knowledge base is inconsistent. This is done by adding a\nweight to each statement in the knowledge base (KB), and then giving each DL\ninterpretation a cost based on how often it breaks rules in the KB. In this\npaper we compare this approach with c-representations, a form of non-monotonic\nreasoning originally introduced by Kern-Isberner. c-Representations describe a\nmeans to interpret defeasible concept inclusions in the first-order case. This\nis done by assigning a numerical ranking to each interpretations via penalties\nfor each violated conditional. We compare these two approaches on a semantic\nlevel. In particular, we show that under certain conditions a weighted\nknowledge base and a set of defeasible conditionals can generate the same\nordering on interpretations, and therefore an equivalence of semantic\nstructures up to relative cost. Moreover, we compare entailment described in\nboth cases, where certain notions are equivalently expressible in both\nformalisms. Our results have the potential to benefit further work on both\ncost-based semantics and c-representations",
      "authors": [
        "Nicholas Leisegang",
        "Giovanni Casini",
        "Thomas Meyer"
      ],
      "published": "2025-10-01T12:27:19Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00817v2"
    },
    {
      "arxiv_id": "2510.00808v1",
      "title": "What You See is What You Ask: Evaluating Audio Descriptions",
      "summary": "Audio descriptions (ADs) narrate important visual details in movies, enabling\nBlind and Low Vision (BLV) users to understand narratives and appreciate visual\ndetails. Existing works in automatic AD generation mostly focus on few-second\ntrimmed clips, and evaluate them by comparing against a single ground-truth\nreference AD. However, writing ADs is inherently subjective. Through alignment\nand analysis of two independent AD tracks for the same movies, we quantify the\nsubjectivity in when and whether to describe, and what and how to highlight.\nThus, we show that working with trimmed clips is inadequate. We propose ADQA, a\nQA benchmark that evaluates ADs at the level of few-minute long, coherent video\nsegments, testing whether they would help BLV users understand the story and\nappreciate visual details. ADQA features visual appreciation (VA) questions\nabout visual facts and narrative understanding (NU) questions based on the\nplot. Through ADQA, we show that current AD generation methods lag far behind\nhuman-authored ADs. We conclude with several recommendations for future work\nand introduce a public leaderboard for benchmarking.",
      "authors": [
        "Divy Kala",
        "Eshika Khandelwal",
        "Makarand Tapaswi"
      ],
      "published": "2025-10-01T12:14:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00808v1"
    },
    {
      "arxiv_id": "2510.00805v1",
      "title": "MG2FlowNet: Accelerating High-Reward Sample Generation via Enhanced MCTS\n  and Greediness Control",
      "summary": "Generative Flow Networks (GFlowNets) have emerged as a powerful tool for\ngenerating diverse and high-reward structured objects by learning to sample\nfrom a distribution proportional to a given reward function. Unlike\nconventional reinforcement learning (RL) approaches that prioritize\noptimization of a single trajectory, GFlowNets seek to balance diversity and\nreward by modeling the entire trajectory distribution. This capability makes\nthem especially suitable for domains such as molecular design and combinatorial\noptimization. However, existing GFlowNets sampling strategies tend to\noverexplore and struggle to consistently generate high-reward samples,\nparticularly in large search spaces with sparse high-reward regions. Therefore,\nimproving the probability of generating high-reward samples without sacrificing\ndiversity remains a key challenge under this premise. In this work, we\nintegrate an enhanced Monte Carlo Tree Search (MCTS) into the GFlowNets\nsampling process, using MCTS-based policy evaluation to guide the generation\ntoward high-reward trajectories and Polynomial Upper Confidence Trees (PUCT) to\nbalance exploration and exploitation adaptively, and we introduce a\ncontrollable mechanism to regulate the degree of greediness. Our method\nenhances exploitation without sacrificing diversity by dynamically balancing\nexploration and reward-driven guidance. The experimental results show that our\nmethod can not only accelerate the speed of discovering high-reward regions but\nalso continuously generate high-reward samples, while preserving the diversity\nof the generative distribution. All implementations are available at\nhttps://github.com/ZRNB/MG2FlowNet.",
      "authors": [
        "Rui Zhu",
        "Xuan Yu",
        "Yudong Zhang",
        "Chen Zhang",
        "Xu Wang",
        "Yang Wang"
      ],
      "published": "2025-10-01T12:09:04Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00805v1"
    },
    {
      "arxiv_id": "2510.00799v1",
      "title": "Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text\n  Vectors",
      "summary": "Most image watermarking systems focus on robustness, capacity, and\nimperceptibility while treating the embedded payload as meaningless bits. This\nbit-centric view imposes a hard ceiling on capacity and prevents watermarks\nfrom carrying useful information. We propose LatentSeal, which reframes\nwatermarking as semantic communication: a lightweight text autoencoder maps\nfull-sentence messages into a compact 256-dimensional unit-norm latent vector,\nwhich is robustly embedded by a finetuned watermark model and secured through a\nsecret, invertible rotation. The resulting system hides full-sentence messages,\ndecodes in real time, and survives valuemetric and geometric attacks. It\nsurpasses prior state of the art in BLEU-4 and Exact Match on several\nbenchmarks, while breaking through the long-standing 256-bit payload ceiling.\nIt also introduces a statistically calibrated score that yields a ROC AUC score\nof 0.97-0.99, and practical operating points for deployment. By shifting from\nbit payloads to semantic latent vectors, LatentSeal enables watermarking that\nis not only robust and high-capacity, but also secure and interpretable,\nproviding a concrete path toward provenance, tamper explanation, and\ntrustworthy AI governance. Models, training and inference code, and data splits\nwill be available upon publication.",
      "authors": [
        "Gautier Evennou",
        "Vivien Chappelier",
        "Ewa Kijak"
      ],
      "published": "2025-10-01T11:56:40Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.00799v1"
    },
    {
      "arxiv_id": "2510.00797v1",
      "title": "Solar PV Installation Potential Assessment on Building Facades Based on\n  Vision and Language Foundation Models",
      "summary": "Building facades represent a significant untapped resource for solar energy\ngeneration in dense urban environments, yet assessing their photovoltaic (PV)\npotential remains challenging due to complex geometries and semantic com\nponents. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), an\nautomated framework that transforms street-view photographs into quantitative\nPV deployment assessments. The approach combines com puter vision and\nartificial intelligence techniques to address three key challenges: perspective\ndistortion correction, semantic understanding of facade elements, and spatial\nreasoning for PV layout optimization. Our four-stage pipeline processes images\nthrough geometric rectification, zero-shot semantic segmentation, Large\nLanguage Model (LLM) guided spatial reasoning, and energy simulation.\nValidation across 80 buildings in four countries demonstrates ro bust\nperformance with mean area estimation errors of 6.2% &#177; 2.8% compared to\nexpert annotations. The auto mated assessment requires approximately 100\nseconds per building, a substantial gain in efficiency over manual methods.\nSimulated energy yield predictions confirm the method's reliability and\napplicability for regional poten tial studies, urban energy planning, and\nbuilding-integrated photovoltaic (BIPV) deployment. Code is available at:\nhttps:github.com/CodeAXu/Solar-PV-Installation",
      "authors": [
        "Ruyu Liu",
        "Dongxu Zhuang",
        "Jianhua Zhang",
        "Arega Getaneh Abate",
        "Per Sieverts Nielsen",
        "Ben Wang",
        "Xiufeng Liu"
      ],
      "published": "2025-10-01T11:51:28Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00797v1"
    },
    {
      "arxiv_id": "2510.00796v1",
      "title": "MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically\n  Equivalent Prompts",
      "summary": "Recent advances in text-to-image (T2I) models, especially diffusion-based\narchitectures, have significantly improved the visual quality of generated\nimages. However, these models continue to struggle with a critical limitation:\nmaintaining semantic consistency when input prompts undergo minor linguistic\nvariations. Despite being logically equivalent, such prompt pairs often yield\nmisaligned or semantically inconsistent images, exposing a lack of robustness\nin reasoning and generalisation. To address this, we propose MetaLogic, a novel\nevaluation framework that detects T2I misalignment without relying on ground\ntruth images. MetaLogic leverages metamorphic testing, generating image pairs\nfrom prompts that differ grammatically but are semantically identical. By\ndirectly comparing these image pairs, the framework identifies inconsistencies\nthat signal failures in preserving the intended meaning, effectively diagnosing\nrobustness issues in the model's logic understanding. Unlike existing\nevaluation methods that compare a generated image to a single prompt, MetaLogic\nevaluates semantic equivalence between paired images, offering a scalable,\nground-truth-free approach to identifying alignment failures. It categorises\nthese alignment errors (e.g., entity omission, duplication, positional\nmisalignment) and surfaces counterexamples that can be used for model debugging\nand refinement. We evaluate MetaLogic across multiple state-of-the-art T2I\nmodels and reveal consistent robustness failures across a range of logical\nconstructs. We find that even the SOTA text-to-image models like Flux.dev and\nDALLE-3 demonstrate a 59 percent and 71 percent misalignment rate,\nrespectively. Our results show that MetaLogic is not only efficient and\nscalable, but also effective in uncovering fine-grained logical inconsistencies\nthat are overlooked by existing evaluation metrics.",
      "authors": [
        "Yifan Shen",
        "Yangyang Shu",
        "Hye-young Paik",
        "Yulei Sui"
      ],
      "published": "2025-10-01T11:51:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00796v1"
    },
    {
      "arxiv_id": "2510.00795v1",
      "title": "Benchmarking Agentic Systems in Automated Scientific Information\n  Extraction with ChemX",
      "summary": "The emergence of agent-based systems represents a significant advancement in\nartificial intelligence, with growing applications in automated data\nextraction. However, chemical information extraction remains a formidable\nchallenge due to the inherent heterogeneity of chemical data. Current\nagent-based approaches, both general-purpose and domain-specific, exhibit\nlimited performance in this domain. To address this gap, we present ChemX, a\ncomprehensive collection of 10 manually curated and domain-expert-validated\ndatasets focusing on nanomaterials and small molecules. These datasets are\ndesigned to rigorously evaluate and enhance automated extraction methodologies\nin chemistry. To demonstrate their utility, we conduct an extensive\nbenchmarking study comparing existing state-of-the-art agentic systems such as\nChatGPT Agent and chemical-specific data extraction agents. Additionally, we\nintroduce our own single-agent approach that enables precise control over\ndocument preprocessing prior to extraction. We further evaluate the performance\nof modern baselines, such as GPT-5 and GPT-5 Thinking, to compare their\ncapabilities with agentic approaches. Our empirical findings reveal persistent\nchallenges in chemical information extraction, particularly in processing\ndomain-specific terminology, complex tabular and schematic representations, and\ncontext-dependent ambiguities. The ChemX benchmark serves as a critical\nresource for advancing automated information extraction in chemistry,\nchallenging the generalization capabilities of existing methods, and providing\nvaluable insights into effective evaluation strategies.",
      "authors": [
        "Anastasia Vepreva",
        "Julia Razlivina",
        "Maria Eremeeva",
        "Nina Gubina",
        "Anastasia Orlova",
        "Aleksei Dmitrenko",
        "Ksenya Kapranova",
        "Susan Jyakhwo",
        "Nikita Vasilev",
        "Arsen Sarkisyan",
        "Ivan Yu. Chernyshov",
        "Vladimir Vinogradov",
        "Andrei Dmitrenko"
      ],
      "published": "2025-10-01T11:50:11Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00795v1"
    },
    {
      "arxiv_id": "2510.00793v1",
      "title": "AI in data science education: experiences from the classroom",
      "summary": "This study explores the integration of AI, particularly large language models\n(LLMs) like ChatGPT, into educational settings, focusing on the implications\nfor teaching and learning. Through interviews with course coordinators from\ndata science courses at Wageningen University, this research identifies both\nthe benefits and challenges associated with AI in the classroom. While AI tools\ncan streamline tasks and enhance learning, concerns arise regarding students'\noverreliance on these technologies, potentially hindering the development of\nessential cognitive and problem solving skills. The study highlights the\nimportance of responsible AI usage, ethical considerations, and the need for\nadapting assessment methods to ensure educational outcomes are met. With\ncareful integration, AI can be a valuable asset in education, provided it is\nused to complement rather than replace fundamental learning processes.",
      "authors": [
        "J. A. Hageman",
        "C. F. W. Peeters"
      ],
      "published": "2025-10-01T11:45:25Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00793v1"
    },
    {
      "arxiv_id": "2510.00778v1",
      "title": "DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion\n  Models",
      "summary": "Diffusion models have shown to be strong representation learners, showcasing\nstate-of-the-art performance across multiple domains. Aside from accelerated\nsampling, DDIM also enables the inversion of real images back to their latent\ncodes. A direct inheriting application of this inversion operation is real\nimage editing, where the inversion yields latent trajectories to be utilized\nduring the synthesis of the edited image. Unfortunately, this practical tool\nhas enabled malicious users to freely synthesize misinformative or deepfake\ncontents with greater ease, which promotes the spread of unethical and abusive,\nas well as privacy-, and copyright-infringing contents. While defensive\nalgorithms such as AdvDM and Photoguard have been shown to disrupt the\ndiffusion process on these images, the misalignment between their objectives\nand the iterative denoising trajectory at test time results in weak disruptive\nperformance.In this work, we present the DDIM Inversion Attack (DIA) that\nattacks the integrated DDIM trajectory path. Our results support the effective\ndisruption, surpassing previous defensive methods across various editing\nmethods. We believe that our frameworks and results can provide practical\ndefense methods against the malicious use of AI for both the industry and the\nresearch community. Our code is available here:\nhttps://anonymous.4open.science/r/DIA-13419/.",
      "authors": [
        "Seunghoo Hong",
        "Geonho Son",
        "Juhun Lee",
        "Simon S. Woo"
      ],
      "published": "2025-10-01T11:20:03Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00778v1"
    },
    {
      "arxiv_id": "2510.00773v1",
      "title": "Uncertainty-Aware Concept Bottleneck Models with Enhanced\n  Interpretability",
      "summary": "In the context of image classification, Concept Bottleneck Models (CBMs)\nfirst embed images into a set of human-understandable concepts, followed by an\nintrinsically interpretable classifier that predicts labels based on these\nintermediate representations. While CBMs offer a semantically meaningful and\ninterpretable classification pipeline, they often sacrifice predictive\nperformance compared to end-to-end convolutional neural networks. Moreover, the\npropagation of uncertainty from concept predictions to final label decisions\nremains underexplored. In this paper, we propose a novel uncertainty-aware and\ninterpretable classifier for the second stage of CBMs. Our method learns a set\nof binary class-level concept prototypes and uses the distances between\npredicted concept vectors and each class prototype as both a classification\nscore and a measure of uncertainty. These prototypes also serve as\ninterpretable classification rules, indicating which concepts should be present\nin an image to justify a specific class prediction. The proposed framework\nenhances both interpretability and robustness by enabling conformal prediction\nfor uncertain or outlier inputs based on their deviation from the learned\nbinary class-level concept prototypes.",
      "authors": [
        "Haifei Zhang",
        "Patrick Barry",
        "Eduardo Brandao"
      ],
      "published": "2025-10-01T11:11:18Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00773v1"
    },
    {
      "arxiv_id": "2510.00771v1",
      "title": "UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free\n  Flow Matching",
      "summary": "In this paper, we present a vocoder-free framework for audio super-resolution\nthat employs a flow matching generative model to capture the conditional\ndistribution of complex-valued spectral coefficients. Unlike conventional\ntwo-stage diffusion-based approaches that predict a mel-spectrogram and then\nrely on a pre-trained neural vocoder to synthesize waveforms, our method\ndirectly reconstructs waveforms via the inverse Short-Time Fourier Transform\n(iSTFT), thereby eliminating the dependence on a separate vocoder. This design\nnot only simplifies end-to-end optimization but also overcomes a critical\nbottleneck of two-stage pipelines, where the final audio quality is\nfundamentally constrained by vocoder performance. Experiments show that our\nmodel consistently produces high-fidelity 48 kHz audio across diverse\nupsampling factors, achieving state-of-the-art performance on both speech and\ngeneral audio datasets.",
      "authors": [
        "Woongjib Choi",
        "Sangmin Lee",
        "Hyungseob Lim",
        "Hong-Goo Kang"
      ],
      "published": "2025-10-01T11:04:53Z",
      "primary_category": "eess.AS",
      "arxiv_url": "https://arxiv.org/abs/2510.00771v1"
    },
    {
      "arxiv_id": "2510.00766v1",
      "title": "Multi-Objective Task-Aware Predictor for Image-Text Alignment",
      "summary": "Evaluating image-text alignment while reflecting human preferences across\nmultiple aspects is a significant issue for the development of reliable\nvision-language applications. It becomes especially crucial in real-world\nscenarios where multiple valid descriptions exist depending on contexts or user\nneeds. However, research progress is hindered by the lack of comprehensive\nbenchmarks and existing evaluation predictors lacking at least one of these key\nproperties: (1) Alignment with human judgments, (2) Long-sequence processing,\n(3) Inference efficiency, and (4) Applicability to multi-objective scoring. To\naddress these challenges, we propose a plug-and-play architecture to build a\nrobust predictor, MULTI-TAP (Multi-Objective Task-Aware Predictor), capable of\nboth multi and single-objective scoring. MULTI-TAP can produce a single overall\nscore, utilizing a reward head built on top of a large vision-language model\n(LVLMs). We show that MULTI-TAP is robust in terms of application to different\nLVLM architectures, achieving significantly higher performance than existing\nmetrics and even on par with the GPT-4o-based predictor, G-VEval, with a\nsmaller size (7-8B). By training a lightweight ridge regression layer on the\nfrozen hidden states of a pre-trained LVLM, MULTI-TAP can produce fine-grained\nscores for multiple human-interpretable objectives. MULTI-TAP performs better\nthan VisionREWARD, a high-performing multi-objective reward model, in both\nperformance and efficiency on multi-objective benchmarks and our newly released\ntext-image-to-text dataset, EYE4ALL. Our new dataset, consisting of\nchosen/rejected human preferences (EYE4ALLPref) and human-annotated\nfine-grained scores across seven dimensions (EYE4ALLMulti), can serve as a\nfoundation for developing more accessible AI systems by capturing the\nunderlying preferences of users, including blind and low-vision (BLV)\nindividuals.",
      "authors": [
        "Eunki Kim",
        "Na Min An",
        "James Thorne",
        "Hyunjung Shim"
      ],
      "published": "2025-10-01T10:55:33Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00766v1"
    },
    {
      "arxiv_id": "2510.00743v1",
      "title": "From Scores to Preferences: Redefining MOS Benchmarking for Speech\n  Quality Reward Modeling",
      "summary": "Assessing the perceptual quality of synthetic speech is crucial for guiding\nthe development and refinement of speech generation models. However, it has\ntraditionally relied on human subjective ratings such as the Mean Opinion Score\n(MOS), which depend on manual annotations and often suffer from inconsistent\nrating standards and poor reproducibility. To address these limitations, we\nintroduce MOS-RMBench, a unified benchmark that reformulates diverse MOS\ndatasets into a preference-comparison setting, enabling rigorous evaluation\nacross different datasets. Building on MOS-RMBench, we systematically construct\nand evaluate three paradigms for reward modeling: scalar reward models,\nsemi-scalar reward models, and generative reward models (GRMs). Our experiments\nreveal three key findings: (1) scalar models achieve the strongest overall\nperformance, consistently exceeding 74% accuracy; (2) most models perform\nconsiderably worse on synthetic speech than on human speech; and (3) all models\nstruggle on pairs with very small MOS differences. To improve performance on\nthese challenging pairs, we propose a MOS-aware GRM that incorporates an\nMOS-difference-based reward function, enabling the model to adaptively scale\nrewards according to the difficulty of each sample pair. Experimental results\nshow that the MOS-aware GRM significantly improves fine-grained quality\ndiscrimination and narrows the gap with scalar models on the most challenging\ncases. We hope this work will establish both a benchmark and a methodological\nframework to foster more rigorous and scalable research in automatic speech\nquality assessment.",
      "authors": [
        "Yifei Cao",
        "Changhao Jiang",
        "Jiabao Zhuang",
        "Jiajun Sun",
        "Ming Zhang",
        "Zhiheng Xi",
        "Hui Li",
        "Shihan Dou",
        "Yuran Wang",
        "Yunke Zhang",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "published": "2025-10-01T10:27:51Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00743v1"
    },
    {
      "arxiv_id": "2510.00733v2",
      "title": "Neural Diffusion Processes for Physically Interpretable Survival\n  Prediction",
      "summary": "We introduce DeepFHT, a survival-analysis framework that couples deep neural\nnetworks with first hitting time (FHT) distributions from stochastic process\ntheory. Time to event is represented as the first passage of a latent diffusion\nprocess to an absorbing boundary. A neural network maps input variables to\nphysically meaningful parameters including initial condition, drift, and\ndiffusion, within a chosen FHT process such as Brownian motion, both with drift\nand driftless. This yields closed-form survival and hazard functions and\ncaptures time-varying risk without assuming proportional-hazards.\n  We compare DeepFHT with Cox survival model using synthetic and real-world\ndatasets. The method achieves predictive accuracy on par with state-of-the-art\napproaches, while maintaining a physics-based interpretable parameterization\nthat elucidates the relation between input features and risk. This combination\nof stochastic process theory and deep learning provides a principled avenue for\nmodeling survival phenomena in complex systems.",
      "authors": [
        "Alessio Cristofoletto",
        "Cesare Rollo",
        "Giovanni Birolo",
        "Piero Fariselli"
      ],
      "published": "2025-10-01T10:16:29Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00733v2"
    },
    {
      "arxiv_id": "2510.00732v1",
      "title": "EvolProver: Advancing Automated Theorem Proving by Evolving Formalized\n  Problems via Symmetry and Difficulty",
      "summary": "Large Language Models (LLMs) for formal theorem proving have shown\nsignificant promise, yet they often lack generalizability and are fragile to\neven minor transformations of problem statements. To address this limitation,\nwe introduce a novel data augmentation pipeline designed to enhance model\nrobustness from two perspectives: symmetry and difficulty. From the symmetry\nperspective, we propose two complementary methods: EvolAST, an Abstract Syntax\nTree (AST) based approach that targets syntactic symmetry to generate\nsemantically equivalent problem variants, and EvolDomain, which leverages LLMs\nto address semantic symmetry by translating theorems across mathematical\ndomains. From the difficulty perspective, we propose EvolDifficulty, which uses\ncarefully designed evolutionary instructions to guide LLMs in generating new\ntheorems with a wider range of difficulty. We then use the evolved data to\ntrain EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver\nestablishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8%\npass@32 rate, surpassing all models of comparable size, including\nreasoning-based models. It also sets new SOTA records for non-reasoning models\non MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and\nIneq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our\ndata augmentation pipeline's effectiveness across multiple benchmarks.",
      "authors": [
        "Yuchen Tian",
        "Ruiyuan Huang",
        "Xuanwu Wang",
        "Jing Ma",
        "Zengfeng Huang",
        "Ziyang Luo",
        "Hongzhan Lin",
        "Da Zheng",
        "Lun Du"
      ],
      "published": "2025-10-01T10:15:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00732v1"
    },
    {
      "arxiv_id": "2510.00728v1",
      "title": "Extreme Blind Image Restoration via Prompt-Conditioned Information\n  Bottleneck",
      "summary": "Blind Image Restoration (BIR) methods have achieved remarkable success but\nfalter when faced with Extreme Blind Image Restoration (EBIR), where inputs\nsuffer from severe, compounded degradations beyond their training scope.\nDirectly learning a mapping from extremely low-quality (ELQ) to high-quality\n(HQ) images is challenging due to the massive domain gap, often leading to\nunnatural artifacts and loss of detail. To address this, we propose a novel\nframework that decomposes the intractable ELQ-to-HQ restoration process. We\nfirst learn a projector that maps an ELQ image onto an intermediate,\nless-degraded LQ manifold. This intermediate image is then restored to HQ using\na frozen, off-the-shelf BIR model. Our approach is grounded in information\ntheory; we provide a novel perspective of image restoration as an Information\nBottleneck problem and derive a theoretically-driven objective to train our\nprojector. This loss function effectively stabilizes training by balancing a\nlow-quality reconstruction term with a high-quality prior-matching term. Our\nframework enables Look Forward Once (LFO) for inference-time prompt refinement,\nand supports plug-and-play strengthening of existing image restoration models\nwithout need for finetuning. Extensive experiments under severe degradation\nregimes provide a thorough analysis of the effectiveness of our work.",
      "authors": [
        "Hongeun Kim",
        "Bryan Sangwoo Kim",
        "Jong Chul Ye"
      ],
      "published": "2025-10-01T10:13:27Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00728v1"
    },
    {
      "arxiv_id": "2510.00726v1",
      "title": "CroSTAta: Cross-State Transition Attention Transformer for Robotic\n  Manipulation",
      "summary": "Learning robotic manipulation policies through supervised learning from\ndemonstrations remains challenging when policies encounter execution variations\nnot explicitly covered during training. While incorporating historical context\nthrough attention mechanisms can improve robustness, standard approaches\nprocess all past states in a sequence without explicitly modeling the temporal\nstructure that demonstrations may include, such as failure and recovery\npatterns. We propose a Cross-State Transition Attention Transformer that\nemploys a novel State Transition Attention (STA) mechanism to modulate standard\nattention weights based on learned state evolution patterns, enabling policies\nto better adapt their behavior based on execution history. Our approach\ncombines this structured attention with temporal masking during training, where\nvisual information is randomly removed from recent timesteps to encourage\ntemporal reasoning from historical context. Evaluation in simulation shows that\nSTA consistently outperforms standard cross-attention and temporal modeling\napproaches like TCN and LSTM networks across all tasks, achieving more than 2x\nimprovement over cross-attention on precision-critical tasks.",
      "authors": [
        "Giovanni Minelli",
        "Giulio Turrisi",
        "Victor Barasuol",
        "Claudio Semini"
      ],
      "published": "2025-10-01T10:09:05Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00726v1"
    },
    {
      "arxiv_id": "2510.01296v1",
      "title": "From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic\n  Resonance Imaging: A Review",
      "summary": "Deep learning-based 3-dimensional (3D) shape reconstruction from\n2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly\nimportant in medical disease diagnosis, treatment planning, and computational\nmodeling. This review surveys the methodological landscape of 3D MRI\nreconstruction, focusing on 4 primary approaches: point cloud, mesh-based,\nshape-aware, and volumetric models. For each category, we analyze the current\nstate-of-the-art techniques, their methodological foundation, limitations, and\napplications across anatomical structures. We provide an extensive overview\nranging from cardiac to neurological to lung imaging. We also focus on the\nclinical applicability of models to diseased anatomy, and the influence of\ntheir training and testing data. We examine publicly available datasets,\ncomputational demands, and evaluation metrics. Finally, we highlight the\nemerging research directions including multimodal integration and\ncross-modality frameworks. This review aims to provide researchers with a\nstructured overview of current 3D reconstruction methodologies to identify\nopportunities for advancing deep learning towards more robust, generalizable,\nand clinically impactful solutions.",
      "authors": [
        "Emma McMillian",
        "Abhirup Banerjee",
        "Alfonso Bueno-Orovio"
      ],
      "published": "2025-10-01T09:57:29Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01296v1"
    },
    {
      "arxiv_id": "2510.00706v1",
      "title": "AttentionDep: Domain-Aware Attention for Explainable Depression Severity\n  Assessment",
      "summary": "In today's interconnected society, social media platforms provide a window\ninto individuals' thoughts, emotions, and mental states. This paper explores\nthe use of platforms like Facebook, X (formerly Twitter), and Reddit for\ndepression severity detection. We propose AttentionDep, a domain-aware\nattention model that drives explainable depression severity estimation by\nfusing contextual and domain knowledge. Posts are encoded hierarchically using\nunigrams and bigrams, with attention mechanisms highlighting clinically\nrelevant tokens. Domain knowledge from a curated mental health knowledge graph\nis incorporated through a cross-attention mechanism, enriching the contextual\nfeatures. Finally, depression severity is predicted using an ordinal regression\nframework that respects the clinical-relevance and natural ordering of severity\nlevels. Our experiments demonstrate that AttentionDep outperforms\nstate-of-the-art baselines by over 5% in graded F1 score across datasets, while\nproviding interpretable insights into its predictions. This work advances the\ndevelopment of trustworthy and transparent AI systems for mental health\nassessment from social media.",
      "authors": [
        "Yusif Ibrahimov",
        "Tarique Anwar",
        "Tommy Yuan",
        "Turan Mutallimov",
        "Elgun Hasanov"
      ],
      "published": "2025-10-01T09:20:53Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00706v1"
    },
    {
      "arxiv_id": "2510.00694v1",
      "title": "ALARB: An Arabic Legal Argument Reasoning Benchmark",
      "summary": "We introduce ALARB, a dataset and suite of tasks designed to evaluate the\nreasoning capabilities of large language models (LLMs) within the Arabic legal\ndomain. While existing Arabic benchmarks cover some knowledge-intensive tasks\nsuch as retrieval and understanding, substantial datasets focusing specifically\non multistep reasoning for Arabic LLMs, especially in open-ended contexts, are\nlacking. The dataset comprises over 13K commercial court cases from Saudi\nArabia, with each case including the facts presented, the reasoning of the\ncourt, the verdict, as well as the cited clauses extracted from the regulatory\ndocuments. We define a set of challenging tasks leveraging this dataset and\nreflecting the complexity of real-world legal reasoning, including verdict\nprediction, completion of reasoning chains in multistep legal arguments, and\nidentification of relevant regulations based on case facts. We benchmark a\nrepresentative selection of current open and closed Arabic LLMs on these tasks\nand demonstrate the dataset's utility for instruction tuning. Notably, we show\nthat instruction-tuning a modest 12B parameter model using ALARB significantly\nenhances its performance in verdict prediction and Arabic verdict generation,\nreaching a level comparable to that of GPT-4o.",
      "authors": [
        "Harethah Abu Shairah",
        "Somayah AlHarbi",
        "Abdulaziz AlHussein",
        "Sameer Alsabea",
        "Omar Shaqaqi",
        "Hebah AlShamlan",
        "Omar Knio",
        "George Turkiyyah"
      ],
      "published": "2025-10-01T09:15:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00694v1"
    },
    {
      "arxiv_id": "2510.00691v1",
      "title": "Inclusive Easy-to-Read Generation for Individuals with Cognitive\n  Impairments",
      "summary": "Ensuring accessibility for individuals with cognitive impairments is\nessential for autonomy, self-determination, and full citizenship. However,\nmanual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to\nscale, limiting access to crucial information in healthcare, education, and\ncivic life. AI-driven ETR generation offers a scalable solution but faces key\nchallenges, including dataset scarcity, domain adaptation, and balancing\nlightweight learning of Large Language Models (LLMs). In this paper, we\nintroduce ETR-fr, the first dataset for ETR text generation fully compliant\nwith European ETR guidelines. We implement parameter-efficient fine-tuning on\nPLMs and LLMs to establish generative baselines. To ensure high-quality and\naccessible outputs, we introduce an evaluation framework based on automatic\nmetrics supplemented by human assessments. The latter is conducted using a\n36-question evaluation form that is aligned with the guidelines. Overall\nresults show that PLMs perform comparably to LLMs and adapt effectively to\nout-of-domain texts.",
      "authors": [
        "François Ledoyen",
        "Gaël Dias",
        "Alexis Lechervy",
        "Jeremie Pantin",
        "Fabrice Maurel",
        "Youssef Chahir",
        "Elisa Gouzonnat",
        "Mélanie Berthelot",
        "Stanislas Moravac",
        "Armony Altinier",
        "Amy Khairalla"
      ],
      "published": "2025-10-01T09:13:18Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00691v1"
    },
    {
      "arxiv_id": "2510.00690v1",
      "title": "ACPO: Adaptive Curriculum Policy Optimization for Aligning\n  Vision-Language Models in Complex Reasoning",
      "summary": "Aligning large-scale vision-language models (VLMs) for complex reasoning via\nreinforcement learning is often hampered by the limitations of existing policy\noptimization algorithms, such as static training schedules and the rigid,\nuniform clipping mechanism in Proximal Policy Optimization (PPO). In this work,\nwe introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework\nthat addresses these challenges through a dual-component adaptive learning\nstrategy. First, ACPO employs a dynamic curriculum that orchestrates a\nprincipled transition from a stable, near on-policy exploration phase to an\nefficient, off-policy exploitation phase by progressively increasing sample\nreuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism\nthat replaces the fixed clipping hyperparameter with dynamic, sample-wise\nbounds modulated by the normalized advantage of each token. This allows for\nmore granular and robust policy updates, enabling larger gradients for\nhigh-potential samples while safeguarding against destructive ones. We conduct\nextensive experiments on a suite of challenging multimodal reasoning\nbenchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate\nthat ACPO consistently outperforms strong baselines such as DAPO and PAPO,\nachieving state-of-the-art performance, accelerated convergence, and superior\ntraining stability.",
      "authors": [
        "Yunhao Wang",
        "Ziting Li",
        "Shuai Chen",
        "Tao Liu",
        "Chao Song",
        "Junjie Jiang",
        "Jian Zhu",
        "Peng Gao",
        "Bin Qin"
      ],
      "published": "2025-10-01T09:11:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00690v1"
    },
    {
      "arxiv_id": "2510.00689v1",
      "title": "Relevance-Zone Reduction in Game Solving",
      "summary": "Game solving aims to find the optimal strategies for all players and\ndetermine the theoretical outcome of a game. However, due to the exponential\ngrowth of game trees, many games remain unsolved, even though methods like\nAlphaZero have demonstrated super-human level in game playing. The\nRelevance-Zone (RZ) is a local strategy reuse technique that restricts the\nsearch to only the regions relevant to the outcome, significantly reducing the\nsearch space. However, RZs are not unique. Different solutions may result in\nRZs of varying sizes. Smaller RZs are generally more favorable, as they\nincrease the chance of reuse and improve pruning efficiency. To this end, we\npropose an iterative RZ reduction method that repeatedly solves the same\nposition while gradually restricting the region involved, guiding the solver\ntoward smaller RZs. We design three constraint generation strategies and\nintegrate an RZ Pattern Table to fully leverage past solutions. In experiments\non 7x7 Killall-Go, our method reduces the average RZ size to 85.95% of the\noriginal. Furthermore, the reduced RZs can be permanently stored as reusable\nknowledge for future solving tasks, especially for larger board sizes or\ndifferent openings.",
      "authors": [
        "Chi-Huang Lin",
        "Ting Han Wei",
        "Chun-Jui Wang",
        "Hung Guei",
        "Chung-Chin Shih",
        "Yun-Jui Tsai",
        "I-Chen Wu",
        "Ti-Rong Wu"
      ],
      "published": "2025-10-01T09:10:32Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00689v1"
    },
    {
      "arxiv_id": "2510.00664v1",
      "title": "Batch-CAM: Introduction to better reasoning in convolutional deep\n  learning models",
      "summary": "Understanding the inner workings of deep learning models is crucial for\nadvancing artificial intelligence, particularly in high-stakes fields such as\nhealthcare, where accurate explanations are as vital as precision. This paper\nintroduces Batch-CAM, a novel training paradigm that fuses a batch\nimplementation of the Grad-CAM algorithm with a prototypical reconstruction\nloss. This combination guides the model to focus on salient image features,\nthereby enhancing its performance across classification tasks. Our results\ndemonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and\nimage reconstruction quality while reducing training and inference times. By\nensuring models learn from evidence-relevant information,this approach makes a\nrelevant contribution to building more transparent, explainable, and\ntrustworthy AI systems.",
      "authors": [
        "Giacomo Ignesti",
        "Davide Moroni",
        "Massimo Martinelli"
      ],
      "published": "2025-10-01T08:47:00Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00664v1"
    },
    {
      "arxiv_id": "2510.00662v1",
      "title": "Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to\n  Easy-to-Read Text Generation",
      "summary": "Simplifying complex texts is essential for ensuring equitable access to\ninformation, especially for individuals with cognitive impairments. The\nEasy-to-Read (ETR) initiative offers a framework for making content accessible\nto the neurodivergent population, but the manual creation of such texts remains\ntime-consuming and resource-intensive. In this work, we investigate the\npotential of large language models (LLMs) to automate the generation of ETR\ncontent. To address the scarcity of aligned corpora and the specificity of ETR\nconstraints, we propose a multi-task learning (MTL) approach that trains models\njointly on text summarization, text simplification, and ETR generation. We\nexplore two different strategies: multi-task retrieval-augmented generation\n(RAG) for in-context learning, and MTL-LoRA for parameter-efficient\nfine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a\nnew high-quality dataset, demonstrate the benefits of multi-task setups over\nsingle-task baselines across all configurations. Moreover, results show that\nthe RAG-based strategy enables generalization in out-of-domain settings, while\nMTL-LoRA outperforms all learning strategies within in-domain configurations.",
      "authors": [
        "François Ledoyen",
        "Gaël Dias",
        "Jeremie Pantin",
        "Alexis Lechervy",
        "Fabrice Maurel",
        "Youssef Chahir"
      ],
      "published": "2025-10-01T08:44:05Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00662v1"
    },
    {
      "arxiv_id": "2510.00658v1",
      "title": "Align Your Tangent: Training Better Consistency Models via\n  Manifold-Aligned Tangents",
      "summary": "With diffusion and flow matching models achieving state-of-the-art generating\nperformance, the interest of the community now turned to reducing the inference\ntime without sacrificing sample quality. Consistency Models (CMs), which are\ntrained to be consistent on diffusion or probability flow ordinary differential\nequation (PF-ODE) trajectories, enable one or two-step flow or diffusion\nsampling. However, CMs typically require prolonged training with large batch\nsizes to obtain competitive sample quality. In this paper, we examine the\ntraining dynamics of CMs near convergence and discover that CM tangents -- CM\noutput update directions -- are quite oscillatory, in the sense that they move\nparallel to the data manifold, not towards the manifold. To mitigate\noscillatory tangents, we propose a new loss function, called the manifold\nfeature distance (MFD), which provides manifold-aligned tangents that point\ntoward the data manifold. Consequently, our method -- dubbed Align Your Tangent\n(AYT) -- can accelerate CM training by orders of magnitude and even out-perform\nthe learned perceptual image patch similarity metric (LPIPS). Furthermore, we\nfind that our loss enables training with extremely small batch sizes without\ncompromising sample quality. Code: https://github.com/1202kbs/AYT",
      "authors": [
        "Beomsu Kim",
        "Byunghee Cha",
        "Jong Chul Ye"
      ],
      "published": "2025-10-01T08:35:18Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00658v1"
    },
    {
      "arxiv_id": "2510.00636v1",
      "title": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution",
      "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.",
      "authors": [
        "Alessio Devoto",
        "Maximilian Jeblick",
        "Simon Jégou"
      ],
      "published": "2025-10-01T08:12:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00636v1"
    },
    {
      "arxiv_id": "2510.00629v2",
      "title": "Tenyidie Syllabification corpus creation and deep learning applications",
      "summary": "The Tenyidie language is a low-resource language of the Tibeto-Burman family\nspoken by the Tenyimia Community of Nagaland in the north-eastern part of India\nand is considered a major language in Nagaland. It is tonal,\nSubject-Object-Verb, and highly agglutinative in nature. Being a low-resource\nlanguage, very limited research on Natural Language Processing (NLP) has been\nconducted. To the best of our knowledge, no work on syllabification has been\nreported for this language. Among the many NLP tasks, syllabification or\nsyllabication is an important task in which the given word syllables are\nidentified. The contribution of this work is the creation of 10,120 syllabified\nTenyidie words and the application of the Deep Learning techniques on the\ncreated corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and\nEncoder-decoder deep learning architectures on our created dataset. In our\ndataset split of 80:10:10 (train:validation:test) set, we achieved the highest\naccuracy of 99.21% with BLSTM model on the test set. This work will find its\napplication in numerous other NLP applications, such as morphological analysis,\npart-of-speech tagging, machine translation, etc, for the Tenyidie Language.\n  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF;\nEncoder-decoder",
      "authors": [
        "Teisovi Angami",
        "Kevisino Khate"
      ],
      "published": "2025-10-01T08:00:59Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00629v2"
    },
    {
      "arxiv_id": "2510.00627v1",
      "title": "Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and\n  Lightweight Trajectory Prediction",
      "summary": "Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and\nIntelligent Transportation Systems (ITS), supporting efficient motion planning\nand real-time traffic safety management. Diffusion models have recently\ndemonstrated strong performance in probabilistic trajectory prediction, but\ntheir large model size and slow sampling process hinder real-world deployment.\nThis paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel\nmethod for real-time and lightweight trajectory prediction. Built upon\nCollaborative Progressive Distillation (CPD), CDDM progressively transfers\nknowledge from a high-capacity teacher diffusion model to a lightweight student\nmodel, jointly reducing both the number of sampling steps and the model size\nacross distillation iterations. A dual-signal regularized distillation loss is\nfurther introduced to incorporate guidance from both the teacher and\nground-truth data, mitigating potential overfitting and ensuring robust\nperformance. Extensive experiments on the ETH-UCY pedestrian benchmark and the\nnuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art\nprediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the\nbaseline model's ADE and FDE performance on pedestrian trajectories, while\nrequiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x\ncompression, 31x acceleration, and 9 ms latency. Qualitative results further\nshow that CDDM generates diverse and accurate trajectories under dynamic agent\nbehaviors and complex social interactions. By bridging high-performing\ngenerative models with practical deployment constraints, CDDM enables\nresource-efficient probabilistic prediction for AVs and ITS. Code is available\nat https://github.com/bingzhangw/CDDM.",
      "authors": [
        "Bingzhang Wang",
        "Kehua Chen",
        "Yinhai Wang"
      ],
      "published": "2025-10-01T08:00:31Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00627v1"
    },
    {
      "arxiv_id": "2510.00625v1",
      "title": "Is Model Editing Built on Sand? Revealing Its Illusory Success and\n  Fragile Foundation",
      "summary": "Large language models (LLMs) inevitably encode outdated or incorrect\nknowledge. Updating, deleting, and forgetting such knowledge is important for\nalignment, safety, and other issues. To address this issue, model editing has\nemerged as a promising paradigm: by precisely editing a small subset of\nparameters such that a specific fact is updated while preserving other\nknowledge. Despite its great success reported in previous papers, we find the\napparent reliability of editing rests on a fragile foundation and the current\nliterature is largely driven by illusory success. The fundamental goal of\nsteering the model's output toward a target with minimal modification would\nencourage exploiting hidden shortcuts, rather than utilizing real semantics.\nThis problem directly challenges the feasibility of the current model editing\nliterature at its very foundation, as shortcuts are inherently at odds with\nrobust knowledge integration. Coincidentally, this issue has long been obscured\nby evaluation frameworks that lack the design of negative examples. To uncover\nit, we systematically develop a suite of new evaluation methods. Strikingly, we\nfind that state-of-the-art approaches collapse even under the simplest negation\nqueries. Our empirical evidence shows that editing is likely to be based on\nshortcuts rather than full semantics, calling for an urgent reconsideration of\nthe very basis of model editing before further advancements can be meaningfully\npursued.",
      "authors": [
        "Wei Liu",
        "Haomei Xu",
        "Bingqing Liu",
        "Zhiying Deng",
        "Haozhao Wang",
        "Jun Wang",
        "Ruixuan Li",
        "Yee Whye Teh",
        "Wee Sun Lee"
      ],
      "published": "2025-10-01T07:59:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00625v1"
    },
    {
      "arxiv_id": "2510.00621v1",
      "title": "FAME: Adaptive Functional Attention with Expert Routing for\n  Function-on-Function Regression",
      "summary": "Functional data play a pivotal role across science and engineering, yet their\ninfinite-dimensional nature makes representation learning challenging.\nConventional statistical models depend on pre-chosen basis expansions or\nkernels, limiting the flexibility of data-driven discovery, while many\ndeep-learning pipelines treat functions as fixed-grid vectors, ignoring\ninherent continuity. In this paper, we introduce Functional Attention with a\nMixture-of-Experts (FAME), an end-to-end, fully data-driven framework for\nfunction-on-function regression. FAME forms continuous attention by coupling a\nbidirectional neural controlled differential equation with MoE-driven vector\nfields to capture intra-functional continuity, and further fuses change to\ninter-functional dependencies via multi-head cross attention. Extensive\nexperiments on synthetic and real-world functional-regression benchmarks show\nthat FAME achieves state-of-the-art accuracy, strong robustness to arbitrarily\nsampled discrete observations of functions.",
      "authors": [
        "Yifei Gao",
        "Yong Chen",
        "Chen Zhang"
      ],
      "published": "2025-10-01T07:53:55Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00621v1"
    },
    {
      "arxiv_id": "2510.00620v1",
      "title": "HARPA: A Testability-Driven, Literature-Grounded Framework for Research\n  Ideation",
      "summary": "While there has been a surge of interest in automated scientific discovery\n(ASD), especially with the emergence of LLMs, it remains challenging for tools\nto generate hypotheses that are both testable and grounded in the scientific\nliterature. Additionally, existing ideation tools are not adaptive to prior\nexperimental outcomes. We developed HARPA to address these challenges by\nincorporating the ideation workflow inspired by human researchers. HARPA first\nidentifies emerging research trends through literature mining, then explores\nhypothesis design spaces, and finally converges on precise, testable hypotheses\nby pinpointing research gaps and justifying design choices. Our evaluations\nshow that HARPA-generated hypothesis-driven research proposals perform\ncomparably to a strong baseline AI-researcher across most qualitative\ndimensions (e.g., specificity, novelty, overall quality), but achieve\nsignificant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness\n(+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the\nASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11\nout of 40) and fewer failures (16 vs. 21 out of 40), showing that expert\nfeasibility judgments track with actual execution success. Furthermore, to\nsimulate how researchers continuously refine their understanding of what\nhypotheses are both testable and potentially interesting from experience, HARPA\nlearns a reward model that scores new hypotheses based on prior experimental\noutcomes, achieving approx. a 28\\% absolute gain over HARPA's untrained\nbaseline scorer. Together, these methods represent a step forward in the field\nof AI-driven scientific discovery.",
      "authors": [
        "Rosni Vasu",
        "Peter Jansen",
        "Pao Siangliulue",
        "Cristina Sarasua",
        "Abraham Bernstein",
        "Peter Clark",
        "Bhavana Dalvi Mishra"
      ],
      "published": "2025-10-01T07:52:19Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00620v1"
    },
    {
      "arxiv_id": "2510.00619v1",
      "title": "What Did I Learn? Operational Competence Assessment for AI-Based\n  Trajectory Planners",
      "summary": "Automated driving functions increasingly rely on machine learning for tasks\nlike perception and trajectory planning, requiring large, relevant datasets.\nThe performance of these algorithms depends on how closely the training data\nmatches the task. To ensure reliable functioning, it is crucial to know what is\nincluded in the dataset to assess the trained model's operational risk. We aim\nto enhance the safe use of machine learning in automated driving by developing\na method to recognize situations that an automated vehicle has not been\nsufficiently trained on. This method also improves explainability by describing\nthe dataset at a human-understandable level. We propose modeling driving data\nas knowledge graphs, representing driving scenes with entities and their\nrelationships. These graphs are queried for specific sub-scene configurations\nto check their occurrence in the dataset. We estimate a vehicle's competence in\na driving scene by considering the coverage and complexity of sub-scene\nconfigurations in the training set. Higher complexity scenes require greater\ncoverage for high competence. We apply this method to the NuPlan dataset,\nmodeling it with knowledge graphs and analyzing the coverage of specific\ndriving scenes. This approach helps monitor the competence of machine learning\nmodels trained on the dataset, which is essential for trustworthy AI to be\ndeployed in automated driving.",
      "authors": [
        "Michiel Braat",
        "Maren Buermann",
        "Marijke van Weperen",
        "Jan-Pieter Paardekooper"
      ],
      "published": "2025-10-01T07:46:50Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00619v1"
    },
    {
      "arxiv_id": "2510.00615v1",
      "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
      "summary": "Large language models (LLMs) are increasingly deployed as agents in dynamic,\nreal-world environments, where success requires both reasoning and effective\ntool use. A central challenge for agentic tasks is the growing context length,\nas agents must accumulate long histories of actions and observations. This\nexpansion raises costs and reduces efficiency in long-horizon tasks, yet prior\nwork on context compression has mostly focused on single-step tasks or narrow\napplications. We introduce Agent Context Optimization (ACON), a unified\nframework that optimally compresses both environment observations and\ninteraction histories into concise yet informative condensations. ACON\nleverages compression guideline optimization in natural language space: given\npaired trajectories where full context succeeds but compressed context fails,\ncapable LLMs analyze the causes of failure, and the compression guideline is\nupdated accordingly. Furthermore, we propose distilling the optimized LLM\ncompressor into smaller models to reduce the overhead of the additional module.\nExperiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON\nreduces memory usage by 26-54% (peak tokens) while largely preserving task\nperformance, preserves over 95% of accuracy when distilled into smaller\ncompressors, and enhances smaller LMs as long-horizon agents with up to 46%\nperformance improvement.",
      "authors": [
        "Minki Kang",
        "Wei-Ning Chen",
        "Dongge Han",
        "Huseyin A. Inan",
        "Lukas Wutschitz",
        "Yanzhi Chen",
        "Robert Sim",
        "Saravan Rajmohan"
      ],
      "published": "2025-10-01T07:43:49Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00615v1"
    },
    {
      "arxiv_id": "2510.00600v1",
      "title": "Hybrid Training for Vision-Language-Action Models",
      "summary": "Using Large Language Models to produce intermediate thoughts, a.k.a.\nChain-of-thought (CoT), before providing an answer has been a successful recipe\nfor solving complex language tasks. In robotics, similar embodied CoT\nstrategies, generating thoughts before actions, have also been shown to lead to\nimproved performance when using Vision-Language-Action models (VLAs). As these\ntechniques increase the length of the model's generated outputs to include the\nthoughts, the inference time is negatively affected. Delaying an agent's\nactions in real-world executions, as in robotic manipulation settings, strongly\naffects the usability of a method, as tasks require long sequences of actions.\nHowever, is the generation of long chains-of-thought a strong prerequisite for\nachieving performance improvements? In this work, we explore the idea of Hybrid\nTraining (HyT), a framework that enables VLAs to learn from thoughts and\nbenefit from the associated performance gains, while enabling the possibility\nto leave out CoT generation during inference. Furthermore, by learning to\nconditionally predict a diverse set of outputs, HyT supports flexibility at\ninference time, enabling the model to either predict actions directly, generate\nthoughts or follow instructions. We evaluate the proposed method in a series of\nsimulated benchmarks and real-world experiments.",
      "authors": [
        "Pietro Mazzaglia",
        "Cansu Sancaktar",
        "Markus Peschl",
        "Daniel Dijkman"
      ],
      "published": "2025-10-01T07:27:15Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00600v1"
    },
    {
      "arxiv_id": "2510.00591v1",
      "title": "AI-Driven Self-Evolving Software: A Promising Path Toward Software\n  Automation",
      "summary": "Software automation has long been a central goal of software engineering,\nstriving for software development that proceeds without human intervention.\nRecent efforts have leveraged Artificial Intelligence (AI) to advance software\nautomation with notable progress. However, current AI functions primarily as\nassistants to human developers, leaving software development still dependent on\nexplicit human intervention. This raises a fundamental question: Can AI move\nbeyond its role as an assistant to become a core component of software, thereby\nenabling genuine software automation? To investigate this vision, we introduce\nAI-Driven Self-Evolving Software, a new form of software that evolves\ncontinuously through direct interaction with users. We demonstrate the\nfeasibility of this idea with a lightweight prototype built on a multi-agent\narchitecture that autonomously interprets user requirements, generates and\nvalidates code, and integrates new functionalities. Case studies across\nmultiple representative scenarios show that the prototype can reliably\nconstruct and reuse functionality, providing early evidence that such software\nsystems can scale to more sophisticated applications and pave the way toward\ntruly automated software development. We make code and cases in this work\npublicly available at https://anonymous.4open.science/r/live-software.",
      "authors": [
        "Liyi Cai",
        "Yijie Ren",
        "Yitong Zhang",
        "Jia Li"
      ],
      "published": "2025-10-01T07:17:51Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.00591v1"
    },
    {
      "arxiv_id": "2510.01295v1",
      "title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM\n  Evaluation",
      "summary": "As Large Language Models (LLMs) transition from static tools to autonomous\nagents, traditional evaluation benchmarks that measure performance on\ndownstream tasks are becoming insufficient. These methods fail to capture the\nemergent social and cognitive dynamics that arise when agents communicate,\npersuade, and collaborate in interactive environments. To address this gap, we\nintroduce a novel evaluation framework that uses multi-agent debate as a\ncontrolled \"social laboratory\" to discover and quantify these behaviors. In our\nframework, LLM-based agents, instantiated with distinct personas and\nincentives, deliberate on a wide range of challenging topics under the\nsupervision of an LLM moderator. Our analysis, enabled by a new suite of\npsychometric and semantic metrics, reveals several key findings. Across\nhundreds of debates, we uncover a powerful and robust emergent tendency for\nagents to seek consensus, consistently reaching high semantic agreement ({\\mu}\n> 0.88) even without explicit instruction and across sensitive topics. We show\nthat assigned personas induce stable, measurable psychometric profiles,\nparticularly in cognitive effort, and that the moderators persona can\nsignificantly alter debate outcomes by structuring the environment, a key\nfinding for external AI alignment. This work provides a blueprint for a new\nclass of dynamic, psychometrically grounded evaluation protocols designed for\nthe agentic setting, offering a crucial methodology for understanding and\nshaping the social behaviors of the next generation of AI agents. We have\nreleased the code and results at\nhttps://github.com/znreza/multi-agent-LLM-eval-for-debate.",
      "authors": [
        "Zarreen Reza"
      ],
      "published": "2025-10-01T07:10:28Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01295v1"
    },
    {
      "arxiv_id": "2510.00585v1",
      "title": "U-DFA: A Unified DINOv2-Unet with Dual Fusion Attention for\n  Multi-Dataset Medical Segmentation",
      "summary": "Accurate medical image segmentation plays a crucial role in overall diagnosis\nand is one of the most essential tasks in the diagnostic pipeline. CNN-based\nmodels, despite their extensive use, suffer from a local receptive field and\nfail to capture the global context. A common approach that combines CNNs with\ntransformers attempts to bridge this gap but fails to effectively fuse the\nlocal and global features. With the recent emergence of VLMs and foundation\nmodels, they have been adapted for downstream medical imaging tasks; however,\nthey suffer from an inherent domain gap and high computational cost. To this\nend, we propose U-DFA, a unified DINOv2-Unet encoder-decoder architecture that\nintegrates a novel Local-Global Fusion Adapter (LGFA) to enhance segmentation\nperformance. LGFA modules inject spatial features from a CNN-based Spatial\nPattern Adapter (SPA) module into frozen DINOv2 blocks at multiple stages,\nenabling effective fusion of high-level semantic and spatial features. Our\nmethod achieves state-of-the-art performance on the Synapse and ACDC datasets\nwith only 33\\% of the trainable model parameters. These results demonstrate\nthat U-DFA is a robust and scalable framework for medical image segmentation\nacross multiple modalities.",
      "authors": [
        "Zulkaif Sajjad",
        "Furqan Shaukat",
        "Junaid Mir"
      ],
      "published": "2025-10-01T07:06:49Z",
      "primary_category": "eess.IV",
      "arxiv_url": "https://arxiv.org/abs/2510.00585v1"
    },
    {
      "arxiv_id": "2510.00582v1",
      "title": "SAGE-LD: Towards Scalable and Generalizable End-to-End Language\n  Diarization via Simulated Data Augmentation",
      "summary": "In this paper, we present a neural spoken language diarization model that\nsupports an unconstrained span of languages within a single framework. Our\napproach integrates a learnable query-based architecture grounded in\nmultilingual awareness, with large-scale pretraining on simulated\ncode-switching data. By jointly leveraging these two components, our method\novercomes the limitations of conventional approaches in data scarcity and\narchitecture optimization, and generalizes effectively to real-world\nmultilingual settings across diverse environments. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance on several\nlanguage diarization benchmarks, with a relative performance improvement of 23%\nto 52% over previous methods. We believe that this work not only advances\nresearch in language diarization but also establishes a foundational framework\nfor code-switching speech technologies.",
      "authors": [
        "Sangmin Lee",
        "Woongjib Choi",
        "Jihyun Kim",
        "Hong-Goo Kang"
      ],
      "published": "2025-10-01T07:01:33Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00582v1"
    },
    {
      "arxiv_id": "2510.00570v1",
      "title": "Adaptive Shared Experts with LoRA-Based Mixture of Experts for\n  Multi-Task Learning",
      "summary": "Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task\nlearning (MTL). However, existing MoE-MTL methods often rely on single-task\npretrained backbones and suffer from redundant adaptation and inefficient\nknowledge sharing during the transition from single-task to multi-task learning\n(STL to MTL). To address these limitations, we propose adaptive shared experts\n(ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are\nassigned router-computed gating weights jointly normalized with sparse experts.\nThis design facilitates STL to MTL transition, enhances expert specialization,\nand cooperation. Furthermore, we incorporate fine-grained experts by increasing\nthe number of LoRA experts while proportionally reducing their rank, enabling\nmore effective knowledge sharing under a comparable parameter budget. Extensive\nexperiments on the PASCAL-Context benchmark, under unified training settings,\ndemonstrate that ASE consistently improves performance across diverse\nconfigurations and validates the effectiveness of fine-grained designs for MTL.",
      "authors": [
        "Minghao Yang",
        "Ren Togo",
        "Guang Li",
        "Takahiro Ogawa",
        "Miki Haseyama"
      ],
      "published": "2025-10-01T06:49:19Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00570v1"
    },
    {
      "arxiv_id": "2510.00566v1",
      "title": "Panorama: Fast-Track Nearest Neighbors",
      "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
      "authors": [
        "Vansh Ramani",
        "Alexis Schlomer",
        "Akash Nayar",
        "Panagiotis Karras",
        "Sayan Ranu",
        "Jignesh M. Patel"
      ],
      "published": "2025-10-01T06:38:45Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00566v1"
    },
    {
      "arxiv_id": "2510.00565v1",
      "title": "Toward Safer Diffusion Language Models: Discovery and Mitigation of\n  Priming Vulnerability",
      "summary": "Diffusion language models (DLMs) generate tokens in parallel through\niterative denoising, which can reduce latency and enable bidirectional\nconditioning. However, the safety risks posed by jailbreak attacks that exploit\nthis inference mechanism are not well understood. In this paper, we reveal that\nDLMs have a critical vulnerability stemming from their iterative denoising\nprocess and propose a countermeasure. Specifically, our investigation shows\nthat if an affirmative token for a harmful query appears at an intermediate\nstep, subsequent denoising can be steered toward a harmful response even in\naligned models. As a result, simply injecting such affirmative tokens can\nreadily bypass the safety guardrails. Furthermore, we demonstrate that the\nvulnerability allows existing optimization-based jailbreak attacks to succeed\non DLMs. Building on this analysis, we propose a novel safety alignment method\ntailored to DLMs that trains models to generate safe responses from\ncontaminated intermediate states that contain affirmative tokens. Our\nexperiments indicate that the proposed method significantly mitigates the\nvulnerability with minimal impact on task performance. Furthermore, our method\nimproves robustness against conventional jailbreak attacks. Our work\nunderscores the need for DLM-specific safety research.",
      "authors": [
        "Shojiro Yamabe",
        "Jun Sakuma"
      ],
      "published": "2025-10-01T06:35:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00565v1"
    },
    {
      "arxiv_id": "2510.00563v1",
      "title": "Memory Determines Learning Direction: A Theory of Gradient-Based\n  Optimization in State Space Models",
      "summary": "State space models (SSMs) have gained attention by showing potential to\noutperform Transformers. However, previous studies have not sufficiently\naddressed the mechanisms underlying their high performance owing to a lack of\ntheoretical explanation of SSMs' learning dynamics. In this study, we provide\nsuch an explanation and propose an improved training strategy. The memory\ncapacity of SSMs can be evaluated by examining how input time series are stored\nin their current state. Such an examination reveals a tradeoff between memory\naccuracy and length, as well as the theoretical equivalence between the\nstructured state space sequence model (S4) and a simplified S4 with diagonal\nrecurrent weights. This theoretical foundation allows us to elucidate the\nlearning dynamics, proving the importance of initial parameters. Our analytical\nresults suggest that successful learning requires the initial memory structure\nto be the longest possible even if memory accuracy may deteriorate or the\ngradient lose the teacher information. Experiments on tasks requiring long\nmemory confirmed that extending memory is difficult, emphasizing the importance\nof initialization. Furthermore, we found that fixing recurrent weights can be\nmore advantageous than adapting them because it achieves comparable or even\nhigher performance with faster convergence. Our results provide a new\ntheoretical foundation for SSMs and potentially offer a novel optimization\nstrategy.",
      "authors": [
        "JingChuan Guan",
        "Tomoyuki Kubota",
        "Yasuo Kuniyoshi",
        "Kohei Nakajima"
      ],
      "published": "2025-10-01T06:30:42Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00563v1"
    },
    {
      "arxiv_id": "2510.00555v1",
      "title": "PromptPilot: Improving Human-AI Collaboration Through LLM-Enhanced\n  Prompt Engineering",
      "summary": "Effective prompt engineering is critical to realizing the promised\nproductivity gains of large language models (LLMs) in knowledge-intensive\ntasks. Yet, many users struggle to craft prompts that yield high-quality\noutputs, limiting the practical benefits of LLMs. Existing approaches, such as\nprompt handbooks or automated optimization pipelines, either require\nsubstantial effort, expert knowledge, or lack interactive guidance. To address\nthis gap, we design and evaluate PromptPilot, an interactive prompting\nassistant grounded in four empirically derived design objectives for\nLLM-enhanced prompt engineering. We conducted a randomized controlled\nexperiment with 80 participants completing three realistic, work-related\nwriting tasks. Participants supported by PromptPilot achieved significantly\nhigher performance (median: 78.3 vs. 61.7; p = .045, d = 0.56), and reported\nenhanced efficiency, ease-of-use, and autonomy during interaction. These\nfindings empirically validate the effectiveness of our proposed design\nobjectives, establishing LLM-enhanced prompt engineering as a viable technique\nfor improving human-AI collaboration.",
      "authors": [
        "Niklas Gutheil",
        "Valentin Mayer",
        "Leopold Müller",
        "Jörg Rommelt",
        "Niklas Kühl"
      ],
      "published": "2025-10-01T06:14:42Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.00555v1"
    },
    {
      "arxiv_id": "2510.00553v2",
      "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
      "summary": "Recent advances in reasoning capabilities of large language models (LLMs) are\nlargely driven by reinforcement learning (RL), yet the underlying parameter\ndynamics during RL training remain poorly understood. This work identifies two\nfundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1\nDominance, where the top singular subspace of the parameter update matrix\nnearly fully determines reasoning improvements, recovering over 99\\% of\nperformance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace\nevolves linearly throughout training, enabling accurate prediction from early\ncheckpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the\ngeneralizability of these properties. More importantly, based on these\nfindings, we propose AlphaRL, a plug-in acceleration framework that\nextrapolates the final parameter update using a short early training window,\nachieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning\nperformance without extra modules or hyperparameter tuning. This positions our\nfinding as a versatile and practical tool for large-scale RL, opening a path\ntoward principled, interpretable, and efficient training paradigm for LLMs.",
      "authors": [
        "Yuchen Cai",
        "Ding Cao",
        "Xin Xu",
        "Zijun Yao",
        "Yuqing Huang",
        "Zhenyu Tan",
        "Benyi Zhang",
        "Guiquan Liu",
        "Junfeng Fang"
      ],
      "published": "2025-10-01T06:13:50Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00553v2"
    },
    {
      "arxiv_id": "2510.00552v1",
      "title": "Data Quality Challenges in Retrieval-Augmented Generation",
      "summary": "Organizations increasingly adopt Retrieval-Augmented Generation (RAG) to\nenhance Large Language Models with enterprise-specific knowledge. However,\ncurrent data quality (DQ) frameworks have been primarily developed for static\ndatasets, and only inadequately address the dynamic, multi-stage nature of RAG\nsystems. This study aims to develop DQ dimensions for this new type of AI-based\nsystems. We conduct 16 semi-structured interviews with practitioners of leading\nIT service companies. Through a qualitative content analysis, we inductively\nderive 15 distinct DQ dimensions across the four processing stages of RAG\nsystems: data extraction, data transformation, prompt & search, and generation.\nOur findings reveal that (1) new dimensions have to be added to traditional DQ\nframeworks to also cover RAG contexts; (2) these new dimensions are\nconcentrated in early RAG steps, suggesting the need for front-loaded quality\nmanagement strategies, and (3) DQ issues transform and propagate through the\nRAG pipeline, necessitating a dynamic, step-aware approach to quality\nmanagement.",
      "authors": [
        "Leopold Müller",
        "Joshua Holstein",
        "Sarah Bause",
        "Gerhard Satzger",
        "Niklas Kühl"
      ],
      "published": "2025-10-01T06:13:40Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00552v1"
    },
    {
      "arxiv_id": "2510.00549v2",
      "title": "EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases",
      "summary": "Machine learning models for clinical prediction rely on structured data\nextracted from Electronic Medical Records (EMRs), yet this process remains\ndominated by hardcoded, database-specific pipelines for cohort definition,\nfeature selection, and code mapping. These manual efforts limit scalability,\nreproducibility, and cross-institutional generalization. To address this, we\nintroduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an\nagent-based framework that replaces manual rule writing with dynamic, language\nmodel-driven interaction to extract and standardize structured clinical data.\nOur framework automates cohort selection, feature extraction, and code mapping\nthrough interactive querying of databases. Our modular agents iteratively\nobserve query results and reason over schema and documentation, using SQL not\njust for data retrieval but also as a tool for database observation and\ndecision making. This eliminates the need for hand-crafted, schema-specific\nlogic. To enable rigorous evaluation, we develop a benchmarking codebase for\nthree EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen\nschema settings. Our results demonstrate strong performance and generalization\nacross these databases, highlighting the feasibility of automating a process\npreviously thought to require expert-driven design. The code will be released\npublicly at https://github.com/AITRICS/EMR-AGENT/tree/main. For a\ndemonstration, please visit our anonymous demo page:\nhttps://anonymoususer-max600.github.io/EMR_AGENT/",
      "authors": [
        "Kwanhyung Lee",
        "Sungsoo Hong",
        "Joonhyung Park",
        "Jeonghyeop Lim",
        "Juhwan Choi",
        "Donghwee Yoon",
        "Eunho Yang"
      ],
      "published": "2025-10-01T06:10:04Z",
      "primary_category": "cs.DB",
      "arxiv_url": "https://arxiv.org/abs/2510.00549v2"
    },
    {
      "arxiv_id": "2510.00547v1",
      "title": "Forestpest-YOLO: A High-Performance Detection Framework for Small\n  Forestry Pests",
      "summary": "Detecting agricultural pests in complex forestry environments using remote\nsensing imagery is fundamental for ecological preservation, yet it is severely\nhampered by practical challenges. Targets are often minuscule, heavily\noccluded, and visually similar to the cluttered background, causing\nconventional object detection models to falter due to the loss of fine-grained\nfeatures and an inability to handle extreme data imbalance. To overcome these\nobstacles, this paper introduces Forestpest-YOLO, a detection framework\nmeticulously optimized for the nuances of forestry remote sensing. Building\nupon the YOLOv8 architecture, our framework introduces a synergistic trio of\ninnovations. We first integrate a lossless downsampling module, SPD-Conv, to\nensure that critical high-resolution details of small targets are preserved\nthroughout the network. This is complemented by a novel cross-stage feature\nfusion block, CSPOK, which dynamically enhances multi-scale feature\nrepresentation while suppressing background noise. Finally, we employ\nVarifocalLoss to refine the training objective, compelling the model to focus\non high-quality and hard-to-classify samples. Extensive experiments on our\nchallenging, self-constructed ForestPest dataset demonstrate that\nForestpest-YOLO achieves state-of-the-art performance, showing marked\nimprovements in detecting small, occluded pests and significantly outperforming\nestablished baseline models.",
      "authors": [
        "Aoduo Li",
        "Peikai Lin",
        "Jiancheng Li",
        "Zhen Zhang",
        "Shiting Wu",
        "Zexiao Liang",
        "Zhifa Jiang"
      ],
      "published": "2025-10-01T06:06:40Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00547v1"
    },
    {
      "arxiv_id": "2510.01293v1",
      "title": "Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town\n  for Self-Directed Research Evolution and Emergent Scientific Discovery",
      "summary": "The rapid advancement of artificial intelligence (AI) has demonstrated\nsubstantial potential in chemical engineering, yet existing AI systems remain\nlimited in interdisciplinary collaboration and exploration of uncharted\nproblems. To address these issues, we present the Cyber Academia-Chemical\nEngineering (CA-ChemE) system, a living digital town that enables self-directed\nresearch evolution and emergent scientific discovery through multi-agent\ncollaboration. By integrating domain-specific knowledge bases, knowledge\nenhancement technologies, and collaboration agents, the system successfully\nconstructs an intelligent ecosystem capable of deep professional reasoning and\nefficient interdisciplinary collaboration. Our findings demonstrate that\nknowledge base-enabled enhancement mechanisms improved dialogue quality scores\nby 10-15% on average across all seven expert agents, fundamentally ensuring\ntechnical judgments are grounded in verifiable scientific evidence. However, we\nobserved a critical bottleneck in cross-domain collaboration efficiency,\nprompting the introduction of a Collaboration Agent (CA) equipped with ontology\nengineering capabilities. CA's intervention achieved 8.5% improvements for\ndistant-domain expert pairs compared to only 0.8% for domain-proximate pairs -\na 10.6-fold difference - unveiling the \"diminished collaborative efficiency\ncaused by knowledge-base gaps\" effect. This study demonstrates how carefully\ndesigned multi-agent architectures can provide a viable pathway toward\nautonomous scientific discovery in chemical engineering.",
      "authors": [
        "Zekun Jiang",
        "Chunming Xu",
        "Tianhang Zhou"
      ],
      "published": "2025-10-01T05:26:55Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01293v1"
    },
    {
      "arxiv_id": "2510.00523v1",
      "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
      "summary": "Multimodal representation learning models have demonstrated successful\noperation across complex tasks, and the integration of vision-language models\n(VLMs) has further enabled embedding models with instruction-following\ncapabilities. However, existing embedding models lack visual-interactive\ncapabilities to specify regions of interest from users (e.g., point, bounding\nbox, mask), which have been explored in generative models to broaden their\nhuman-interactive applicability. Equipping embedding models with visual\ninteractions not only would unlock new applications with localized grounding of\nuser intent, which remains unexplored, but also enable the models to learn\nentity-level information within images to complement their global\nrepresentations for conventional embedding tasks. In this paper, we propose a\nnovel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends\nthe capabilities of the segmentation model and the vision-language model to the\nrealm of representation learning. In VIRTUE, the segmentation model can process\nvisual prompts that pinpoint specific regions within an image, thereby enabling\nthe embedder to handle complex and ambiguous scenarios more precisely. To\nevaluate the visual-interaction ability of VIRTUE, we introduce a large-scale\nSegmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples\nthat aims to retrieve the text caption by jointly considering the entity with a\nspecific object and image scene. VIRTUE consistently achieves a\nstate-of-the-art performance with significant improvements across 36 universal\nMMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
      "authors": [
        "Wei-Yao Wang",
        "Kazuya Tateishi",
        "Qiyu Wu",
        "Shusuke Takahashi",
        "Yuki Mitsufuji"
      ],
      "published": "2025-10-01T05:11:54Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00523v1"
    },
    {
      "arxiv_id": "2510.00519v1",
      "title": "Architectural Transformations and Emerging Verification Demands in\n  AI-Enabled Cyber-Physical Systems",
      "summary": "In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion\noccurs where digital technology meets the physical world. This synergy has been\nsignificantly transformed by the integration of artificial intelligence (AI), a\nmove that dramatically enhances system adaptability and introduces a layer of\ncomplexity that impacts CPS control optimization and reliability. Despite\nadvancements in AI integration, a significant gap remains in understanding how\nthis shift affects CPS architecture, operational complexity, and verification\npractices. The extended abstract addresses this gap by investigating\narchitectural distinctions between AI-driven and traditional control models\ndesigned in Simulink and their respective implications for system verification.",
      "authors": [
        "Hadiza Umar Yusuf",
        "Khouloud Gaaloul"
      ],
      "published": "2025-10-01T05:09:12Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.00519v1"
    },
    {
      "arxiv_id": "2510.00512v1",
      "title": "Adaptive Data-Knowledge Alignment in Genetic Perturbation Prediction",
      "summary": "The transcriptional response to genetic perturbation reveals fundamental\ninsights into complex cellular systems. While current approaches have made\nprogress in predicting genetic perturbation responses, they provide limited\nbiological understanding and cannot systematically refine existing knowledge.\nOvercoming these limitations requires an end-to-end integration of data-driven\nlearning and existing knowledge. However, this integration is challenging due\nto inconsistencies between data and knowledge bases, such as noise,\nmisannotation, and incompleteness. To address this challenge, we propose\nALIGNED (Adaptive aLignment for Inconsistent Genetic kNowledgE and Data), a\nneuro-symbolic framework based on the Abductive Learning (ABL) paradigm. This\nend-to-end framework aligns neural and symbolic components and performs\nsystematic knowledge refinement. We introduce a balanced consistency metric to\nevaluate the predictions' consistency against both data and knowledge. Our\nresults show that ALIGNED outperforms state-of-the-art methods by achieving the\nhighest balanced consistency, while also re-discovering biologically meaningful\nknowledge. Our work advances beyond existing methods to enable both the\ntransparency and the evolution of mechanistic biological understanding.",
      "authors": [
        "Yuanfang Xiang",
        "Lun Ai"
      ],
      "published": "2025-10-01T04:48:43Z",
      "primary_category": "q-bio.MN",
      "arxiv_url": "https://arxiv.org/abs/2510.00512v1"
    },
    {
      "arxiv_id": "2510.00508v1",
      "title": "Copy-Paste to Mitigate Large Language Model Hallucinations",
      "summary": "While Retrieval-Augmented Generation (RAG) enables large language models\n(LLMs) to generate contextually grounded responses, contextual faithfulness\nremains challenging as LLMs may not consistently trust provided context,\nleading to hallucinations that undermine reliability. We observe an inverse\ncorrelation between response copying degree and context-unfaithful\nhallucinations on RAGTruth, suggesting that higher copying degrees reduce\nhallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,\nobtained through two-stage high-copying response preference training. We design\nthree prompting methods to enhance copying degree, demonstrating that\nhigh-copying responses achieve superior contextual faithfulness and\nhallucination control. These approaches enable a fully automated pipeline that\ntransforms generated responses into high-copying preference data for training\nCopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best\nperformance in both counterfactual and original contexts, remarkably with 12.2%\nto 24.5% accuracy improvements on FaithEval over the best baseline, while\nrequiring only 365 training samples -- 1/50th of baseline data. To elucidate\nCopyPasteLLM's effectiveness, we propose the Context-Parameter Copying\nCapturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates\nreliance on internal parametric knowledge rather than external knowledge during\ngeneration. All codes are available at\nhttps://github.com/longyongchao/CopyPasteLLM",
      "authors": [
        "Yongchao Long",
        "Xian Wu",
        "Yingying Zhang",
        "Xianbin Wen",
        "Yuxi Zhou",
        "Shenda Hong"
      ],
      "published": "2025-10-01T04:40:04Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00508v1"
    },
    {
      "arxiv_id": "2510.00507v1",
      "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via\n  Knowledge Graphs",
      "summary": "As multimodal LLM-driven agents continue to advance in autonomy and\ngeneralization, evaluation based on static datasets can no longer adequately\nassess their true capabilities in dynamic environments and diverse tasks.\nExisting LLM-based synthetic data methods are largely designed for LLM training\nand evaluation, and thus cannot be directly applied to agent tasks that require\ntool use and interactive capabilities. While recent studies have explored\nautomatic agent task generation with LLMs, most efforts remain limited to text\nor image analysis, without systematically modeling multi-step interactions in\nweb environments. To address these challenges, we propose Graph2Eval, a\nknowledge graph-based framework that automatically generates both multimodal\ndocument comprehension tasks and web interaction tasks, enabling comprehensive\nevaluation of agents' reasoning, collaboration, and interactive capabilities.\nIn our approach, knowledge graphs constructed from multi-source external data\nserve as the task space, where we translate semantic relations into structured\nmultimodal tasks using subgraph sampling, task templates, and meta-paths. A\nmulti-stage filtering pipeline based on node reachability, LLM scoring, and\nsimilarity analysis is applied to guarantee the quality and executability of\nthe generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of\nmultiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures\nreasoning, collaboration, and interaction capabilities. We instantiate the\nframework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning\ndocument comprehension and web interaction scenarios. Experiments show that\nGraph2Eval efficiently generates tasks that differentiate agent and model\nperformance, revealing gaps in reasoning, collaboration, and web interaction\nacross different settings and offering a new perspective for agent evaluation.",
      "authors": [
        "Yurun Chen",
        "Xavier Hu",
        "Yuhan Liu",
        "Ziqi Wang",
        "Zeyi Liao",
        "Lin Chen",
        "Feng Wei",
        "Yuxi Qian",
        "Bo Zheng",
        "Keting Yin",
        "Shengyu Zhang"
      ],
      "published": "2025-10-01T04:37:54Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00507v1"
    },
    {
      "arxiv_id": "2510.00500v1",
      "title": "Relative-Absolute Fusion: Rethinking Feature Extraction in Image-Based\n  Iterative Method Selection for Solving Sparse Linear Systems",
      "summary": "Iterative method selection is crucial for solving sparse linear systems\nbecause these methods inherently lack robustness. Though image-based selection\napproaches have shown promise, their feature extraction techniques might encode\ndistinct matrices into identical image representations, leading to the same\nselection and suboptimal method. In this paper, we introduce RAF\n(Relative-Absolute Fusion), an efficient feature extraction technique to\nenhance image-based selection approaches. By simultaneously extracting and\nfusing image representations as relative features with corresponding numerical\nvalues as absolute features, RAF achieves comprehensive matrix representations\nthat prevent feature ambiguity across distinct matrices, thus improving\nselection accuracy and unlocking the potential of image-based selection\napproaches. We conducted comprehensive evaluations of RAF on SuiteSparse and\nour developed BMCMat (Balanced Multi-Classification Matrix dataset),\ndemonstrating solution time reductions of 0.08s-0.29s for sparse linear\nsystems, which is 5.86%-11.50% faster than conventional image-based selection\napproaches and achieves state-of-the-art (SOTA) performance. BMCMat is\navailable at https://github.com/zkqq/BMCMat.",
      "authors": [
        "Kaiqi Zhang",
        "Mingguan Yang",
        "Dali Chang",
        "Chun Chen",
        "Yuxiang Zhang",
        "Kexun He",
        "Jing Zhao"
      ],
      "published": "2025-10-01T04:33:23Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00500v1"
    },
    {
      "arxiv_id": "2510.00499v2",
      "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
      "summary": "Spoken dialogue systems often rely on cascaded pipelines that transcribe,\nprocess, and resynthesize speech. While effective, this design discards\nparalinguistic cues and limits expressivity. Recent end-to-end methods reduce\nlatency and better preserve these cues, yet still rely on text intermediates,\ncreating a fundamental bottleneck. We present MOSS-Speech, a true\nspeech-to-speech large language model that directly understands and generates\nspeech without relying on text guidance. Our approach combines a modality-based\nlayer-splitting architecture with a frozen pre-training strategy, preserving\nthe reasoning and knowledge of pretrained text LLMs while adding native speech\ncapabilities. Experiments show that our model achieves state-of-the-art results\nin spoken question answering and delivers comparable speech-to-speech\nperformance relative to existing text-guided systems, while still maintaining\ncompetitive text performance. By narrowing the gap between text-guided and\ndirect speech generation, our work establishes a new paradigm for expressive\nand efficient end-to-end speech interaction.",
      "authors": [
        "Xingjian Zhao",
        "Zhe Xu",
        "Qinyuan Cheng",
        "Zhaoye Fei",
        "Luozhijie Jin",
        "Yang Wang",
        "Hanfu Chen",
        "Yaozhou Jiang",
        "Qinghui Gao",
        "Ke Chen",
        "Ruixiao Li",
        "Mingshu Chen",
        "Ruiming Wang",
        "Wenbo Zhang",
        "Yiyang Zhang",
        "Donghua Yu",
        "Yang Gao",
        "Xiaogui Yang",
        "Yitian Gong",
        "Yuanfan Xu",
        "Yaqian Zhou",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "published": "2025-10-01T04:32:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00499v2"
    },
    {
      "arxiv_id": "2510.00495v2",
      "title": "Normal-Abnormal Guided Generalist Anomaly Detection",
      "summary": "Generalist Anomaly Detection (GAD) aims to train a unified model on an\noriginal domain that can detect anomalies in new target domains. Previous GAD\nmethods primarily use only normal samples as references, overlooking the\nvaluable information contained in anomalous samples that are often available in\nreal-world scenarios. To address this limitation, we propose a more practical\napproach: normal-abnormal-guided generalist anomaly detection, which leverages\nboth normal and anomalous samples as references to guide anomaly detection\nacross diverse domains. We introduce the Normal-Abnormal Generalist Learning\n(NAGL) framework, consisting of two key components: Residual Mining (RM) and\nAnomaly Feature Learning (AFL). RM extracts abnormal patterns from\nnormal-abnormal reference residuals to establish transferable anomaly\nrepresentations, while AFL adaptively learns anomaly features in query images\nthrough residual mapping to identify instance-aware anomalies. Our approach\neffectively utilizes both normal and anomalous references for more accurate and\nefficient cross-domain anomaly detection. Extensive experiments across multiple\nbenchmarks demonstrate that our method significantly outperforms existing GAD\napproaches. This work represents the first to adopt a mixture of normal and\nabnormal samples as references in generalist anomaly detection. The code and\ndatasets are available at https://github.com/JasonKyng/NAGL.",
      "authors": [
        "Yuexin Wang",
        "Xiaolei Wang",
        "Yizheng Gong",
        "Jimin Xiao"
      ],
      "published": "2025-10-01T04:27:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00495v2"
    },
    {
      "arxiv_id": "2510.00494v1",
      "title": "Exploring System 1 and 2 communication for latent reasoning in LLMs",
      "summary": "Should LLM reasoning live in a separate module, or within a single model's\nforward pass and representational space? We study dual-architecture latent\nreasoning, where a fluent Base exchanges latent messages with a Coprocessor,\nand test two hypotheses aimed at improving latent communication over Liu et al.\n(2024): (H1) increase channel capacity; (H2) learn communication via joint\nfinetuning. Under matched latent-token budgets on GPT-2 and Qwen-3, H2 is\nconsistently strongest while H1 yields modest gains. A unified soft-embedding\nbaseline, a single model with the same forward pass and shared representations,\nusing the same latent-token budget, nearly matches H2 and surpasses H1,\nsuggesting current dual designs mostly add compute rather than qualitatively\nimproving reasoning. Across GSM8K, ProsQA, and a Countdown stress test with\nincreasing branching factor, scaling the latent-token budget beyond small\nvalues fails to improve robustness. Latent analyses show overlapping subspaces\nwith limited specialization, consistent with weak reasoning gains. We conclude\ndual-model latent reasoning remains promising in principle, but likely requires\nobjectives and communication mechanisms that explicitly shape latent spaces for\nalgorithmic planning.",
      "authors": [
        "Julian Coda-Forno",
        "Zhuokai Zhao",
        "Qiang Zhang",
        "Dipesh Tamboli",
        "Weiwei Li",
        "Xiangjun Fan",
        "Lizhu Zhang",
        "Eric Schulz",
        "Hsiao-Ping Tseng"
      ],
      "published": "2025-10-01T04:26:09Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00494v1"
    },
    {
      "arxiv_id": "2510.00492v2",
      "title": "Rethinking Reward Models for Multi-Domain Test-Time Scaling",
      "summary": "The reliability of large language models (LLMs) during test-time scaling is\noften assessed with \\emph{external verifiers} or \\emph{reward models} that\ndistinguish correct reasoning from flawed logic. Prior work generally assumes\nthat process reward models (PRMs), which score every intermediate reasoning\nstep, outperform outcome reward models (ORMs) that assess only the final\nanswer. This view is based mainly on evidence from narrow, math-adjacent\ndomains. We present the first unified evaluation of four reward model variants,\ndiscriminative ORM and PRM (\\DisORM, \\DisPRM) and generative ORM and PRM\n(\\GenORM, \\GenPRM), across 14 diverse domains. Contrary to conventional wisdom,\nwe find that (i) \\DisORM performs on par with \\DisPRM, (ii) \\GenPRM is not\ncompetitive, and (iii) overall, \\GenORM is the most robust, yielding\nsignificant and consistent gains across every tested domain. We attribute this\nto PRM-style stepwise scoring, which inherits label noise from LLM\nauto-labeling and has difficulty evaluating long reasoning trajectories,\nincluding those involving self-correcting reasoning. Our theoretical analysis\nshows that step-wise aggregation compounds errors as reasoning length grows,\nand our empirical observations confirm this effect. These findings challenge\nthe prevailing assumption that fine-grained supervision is always better and\nsupport generative outcome verification for multi-domain deployment. We\npublicly release our code, datasets, and checkpoints at\n\\href{https://github.com/db-Lee/Multi-RM}{\\underline{\\small\\texttt{https://github.com/db-Lee/Multi-RM}}}\nto facilitate future research in multi-domain settings.",
      "authors": [
        "Dong Bok Lee",
        "Seanie Lee",
        "Sangwoo Park",
        "Minki Kang",
        "Jinheon Baek",
        "Dongki Kim",
        "Dominik Wagner",
        "Jiongdao Jin",
        "Heejun Lee",
        "Tobias Bocklet",
        "Jinyu Wang",
        "Jingjing Fu",
        "Sung Ju Hwang",
        "Jiang Bian",
        "Lei Song"
      ],
      "published": "2025-10-01T04:21:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00492v2"
    },
    {
      "arxiv_id": "2510.00491v1",
      "title": "From Human Hands to Robot Arms: Manipulation Skills Transfer via\n  Trajectory Alignment",
      "summary": "Learning diverse manipulation skills for real-world robots is severely\nbottlenecked by the reliance on costly and hard-to-scale teleoperated\ndemonstrations. While human videos offer a scalable alternative, effectively\ntransferring manipulation knowledge is fundamentally hindered by the\nsignificant morphological gap between human and robotic embodiments. To address\nthis challenge and facilitate skill transfer from human to robot, we introduce\nTraj2Action,a novel framework that bridges this embodiment gap by using the 3D\ntrajectory of the operational endpoint as a unified intermediate\nrepresentation, and then transfers the manipulation knowledge embedded in this\ntrajectory to the robot's actions. Our policy first learns to generate a coarse\ntrajectory, which forms an high-level motion plan by leveraging both human and\nrobot data. This plan then conditions the synthesis of precise, robot-specific\nactions (e.g., orientation and gripper state) within a co-denoising framework.\nExtensive real-world experiments on a Franka robot demonstrate that Traj2Action\nboosts the performance by up to 27% and 22.25% over $\\pi_0$ baseline on short-\nand long-horizon real-world tasks, and achieves significant gains as human data\nscales in robot policy learning. Our project website, featuring code and video\ndemonstrations, is available at\nhttps://anonymous.4open.science/w/Traj2Action-4A45/.",
      "authors": [
        "Han Zhou",
        "Jinjin Cao",
        "Liyuan Ma",
        "Xueji Fang",
        "Guo-jun Qi"
      ],
      "published": "2025-10-01T04:21:12Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00491v1"
    },
    {
      "arxiv_id": "2510.00487v1",
      "title": "Black-Box Time-Series Domain Adaptation via Cross-Prompt Foundation\n  Models",
      "summary": "The black-box domain adaptation (BBDA) topic is developed to address the\nprivacy and security issues where only an application programming interface\n(API) of the source model is available for domain adaptations. Although the\nBBDA topic has attracted growing research attentions, existing works mostly\ntarget the vision applications and are not directly applicable to the\ntime-series applications possessing unique spatio-temporal characteristics. In\naddition, none of existing approaches have explored the strength of foundation\nmodel for black box time-series domain adaptation (BBTSDA). This paper proposes\na concept of Cross-Prompt Foundation Model (CPFM) for the BBTSDA problems. CPFM\nis constructed under a dual branch network structure where each branch is\nequipped with a unique prompt to capture different characteristics of data\ndistributions. In the domain adaptation phase, the reconstruction learning\nphase in the prompt and input levels is developed. All of which are built upon\na time-series foundation model to overcome the spatio-temporal dynamic. Our\nrigorous experiments substantiate the advantage of CPFM achieving improved\nresults with noticeable margins from its competitors in three time-series\ndatasets of different application domains.",
      "authors": [
        "M. T. Furqon",
        "Mahardhika Pratama",
        "Igor Skrjanc",
        "Lin Liu",
        "Habibullah Habibullah",
        "Kutluyil Dogancay"
      ],
      "published": "2025-10-01T04:09:01Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00487v1"
    },
    {
      "arxiv_id": "2510.00485v1",
      "title": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
      "summary": "Recently, an increasing number of multimodal (text and audio) benchmarks have\nemerged, primarily focusing on evaluating models' understanding capability.\nHowever, exploration into assessing generative capabilities remains limited,\nespecially for open-ended long-form content generation. Significant challenges\nlie in no reference standard answer, no unified evaluation metrics and\nuncontrollable human judgments. In this work, we take podcast-like audio\ngeneration as a starting point and propose PodEval, a comprehensive and\nwell-designed open-source evaluation framework. In this framework: 1) We\nconstruct a real-world podcast dataset spanning diverse topics, serving as a\nreference for human-level creative quality. 2) We introduce a multimodal\nevaluation strategy and decompose the complex task into three dimensions: text,\nspeech and audio, with different evaluation emphasis on \"Content\" and \"Format\".\n3) For each modality, we design corresponding evaluation methods, involving\nboth objective metrics and subjective listening test. We leverage\nrepresentative podcast generation systems (including open-source, close-source,\nand human-made) in our experiments. The results offer in-depth analysis and\ninsights into podcast generation, demonstrating the effectiveness of PodEval in\nevaluating open-ended long-form audio. This project is open-source to\nfacilitate public use: https://github.com/yujxx/PodEval.",
      "authors": [
        "Yujia Xiao",
        "Liumeng Xue",
        "Lei He",
        "Xinyi Chen",
        "Aemon Yat Fei Chiu",
        "Wenjie Tian",
        "Shaofei Zhang",
        "Qiuqiang Kong",
        "Xinfa Zhu",
        "Wei Xue",
        "Tan Lee"
      ],
      "published": "2025-10-01T04:08:08Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00485v1"
    },
    {
      "arxiv_id": "2510.00481v1",
      "title": "Make a Video Call with LLM: A Measurement Campaign over Five Mainstream\n  Apps",
      "summary": "In 2025, Large Language Model (LLM) services have launched a new feature --\nAI video chat -- allowing users to interact with AI agents via real-time video\ncommunication (RTC), just like chatting with real people. Despite its\nsignificance, no systematic study has characterized the performance of existing\nAI video chat systems. To address this gap, this paper proposes a comprehensive\nbenchmark with carefully designed metrics across four dimensions: quality,\nlatency, internal mechanisms, and system overhead. Using custom testbeds, we\nfurther evaluate five mainstream AI video chatbots with this benchmark. This\nwork provides the research community a baseline of real-world performance and\nidentifies unique system bottlenecks. In the meantime, our benchmarking results\nalso open up several research questions for future optimizations of AI video\nchatbots.",
      "authors": [
        "Jiayang Xu",
        "Xiangjie Huang",
        "Zijie Li",
        "Zili Meng"
      ],
      "published": "2025-10-01T04:03:51Z",
      "primary_category": "cs.NI",
      "arxiv_url": "https://arxiv.org/abs/2510.00481v1"
    },
    {
      "arxiv_id": "2510.00480v1",
      "title": "Expandable Decision-Making States for Multi-Agent Deep Reinforcement\n  Learning in Soccer Tactical Analysis",
      "summary": "Invasion team sports such as soccer produce a high-dimensional, strongly\ncoupled state space as many players continuously interact on a shared field,\nchallenging quantitative tactical analysis. Traditional rule-based analyses are\nintuitive, while modern predictive machine learning models often perform\npattern-matching without explicit agent representations. The problem we address\nis how to build player-level agent models from data, whose learned values and\npolicies are both tactically interpretable and robust across heterogeneous data\nsources. Here, we propose Expandable Decision-Making States (EDMS), a\nsemantically enriched state representation that augments raw positions and\nvelocities with relational variables (e.g., scoring of space, pass, and score),\ncombined with an action-masking scheme that gives on-ball and off-ball agents\ndistinct decision sets. Compared to prior work, EDMS maps learned value\nfunctions and action policies to human-interpretable tactical concepts (e.g.,\nmarking pressure, passing lanes, ball accessibility) instead of raw coordinate\nfeatures, and aligns agent choices with the rules of play. In the experiments,\nEDMS with action masking consistently reduced both action-prediction loss and\ntemporal-difference (TD) error compared to the baseline. Qualitative case\nstudies and Q-value visualizations further indicate that EDMS highlights\nhigh-risk, high-reward tactical patterns (e.g., fast counterattacks and\ndefensive breakthroughs). We also integrated our approach into an open-source\nlibrary and demonstrated compatibility with multiple commercial and open\ndatasets, enabling cross-provider evaluation and reproducible experiments.",
      "authors": [
        "Kenjiro Ide",
        "Taiga Someya",
        "Kohei Kawaguchi",
        "Keisuke Fujii"
      ],
      "published": "2025-10-01T04:01:51Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00480v1"
    },
    {
      "arxiv_id": "2510.00476v1",
      "title": "Analyzing Latent Concepts in Code Language Models",
      "summary": "Interpreting the internal behavior of large language models trained on code\nremains a critical challenge, particularly for applications demanding trust,\ntransparency, and semantic robustness. We propose Code Concept Analysis\n(CoCoA): a global post-hoc interpretability framework that uncovers emergent\nlexical, syntactic, and semantic structures in a code language model's\nrepresentation space by clustering contextualized token embeddings into\nhuman-interpretable concept groups. We propose a hybrid annotation pipeline\nthat combines static analysis tool-based syntactic alignment with\nprompt-engineered large language models (LLMs), enabling scalable labeling of\nlatent concepts across abstraction levels. We analyse the distribution of\nconcepts across layers and across three finetuning tasks. Emergent concept\nclusters can help identify unexpected latent interactions and be used to\nidentify trends and biases within the model's learned representations. We\nfurther integrate LCA with local attribution methods to produce\nconcept-grounded explanations, improving the coherence and interpretability of\ntoken-level saliency. Empirical evaluations across multiple models and tasks\nshow that LCA discovers concepts that remain stable under semantic-preserving\nperturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve\npredictably with fine-tuning. In a user study, concept-augmented explanations\ndisambiguate token roles. In a user study on the programming-language\nclassification task, concept-augmented explanations disambiguated token roles\nand improved human-centric explainability by 37 percentage points compared with\ntoken-level attributions using Integrated Gradients.",
      "authors": [
        "Arushi Sharma",
        "Vedant Pungliya",
        "Christopher J. Quinn",
        "Ali Jannesari"
      ],
      "published": "2025-10-01T03:53:21Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.00476v1"
    },
    {
      "arxiv_id": "2510.00468v1",
      "title": "Feature Identification via the Empirical NTK",
      "summary": "We provide evidence that eigenanalysis of the empirical neural tangent kernel\n(eNTK) can surface the features used by trained neural networks. Across two\nstandard toy models for mechanistic interpretability, Toy Models of\nSuperposition (TMS) and a 1-layer MLP trained on modular addition, we find that\nthe eNTK exhibits sharp spectral cliffs whose top eigenspaces align with\nground-truth features. In TMS, the eNTK recovers the ground-truth features in\nboth the sparse (high superposition) and dense regimes. In modular arithmetic,\nthe eNTK can be used to recover Fourier feature families. Moreover, we provide\nevidence that a layerwise eNTK localizes features to specific layers and that\nthe evolution of the eNTK eigenspectrum can be used to diagnose the grokking\nphase transition. These results suggest that eNTK analysis may provide a\npractical handle for feature discovery and for detecting phase changes in small\nmodels.",
      "authors": [
        "Jennifer Lin"
      ],
      "published": "2025-10-01T03:39:48Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00468v1"
    },
    {
      "arxiv_id": "2510.00466v1",
      "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A\n  Reinforcement Learning Approach for Robot Social Navigation",
      "summary": "Offline reinforcement learning (RL) has emerged as a promising framework for\naddressing robot social navigation challenges. However, inherent uncertainties\nin pedestrian behavior and limited environmental interaction during training\noften lead to suboptimal exploration and distributional shifts between offline\ntraining and online deployment. To overcome these limitations, this paper\nproposes a novel offline-to-online fine-tuning RL algorithm for robot social\nnavigation by integrating Return-to-Go (RTG) prediction into a causal\nTransformer architecture. Our algorithm features a spatiotem-poral fusion model\ndesigned to precisely estimate RTG values in real-time by jointly encoding\ntemporal pedestrian motion patterns and spatial crowd dynamics. This RTG\nprediction framework mitigates distribution shift by aligning offline policy\ntraining with online environmental interactions. Furthermore, a hybrid\noffline-online experience sampling mechanism is built to stabilize policy\nupdates during fine-tuning, ensuring balanced integration of pre-trained\nknowledge and real-time adaptation. Extensive experiments in simulated social\nnavigation environments demonstrate that our method achieves a higher success\nrate and lower collision rate compared to state-of-the-art baselines. These\nresults underscore the efficacy of our algorithm in enhancing navigation policy\nrobustness and adaptability. This work paves the way for more reliable and\nadaptive robotic navigation systems in real-world applications.",
      "authors": [
        "Run Su",
        "Hao Fu",
        "Shuai Zhou",
        "Yingao Fu"
      ],
      "published": "2025-10-01T03:37:02Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00466v1"
    },
    {
      "arxiv_id": "2510.00461v1",
      "title": "TimeEmb: A Lightweight Static-Dynamic Disentanglement Framework for Time\n  Series Forecasting",
      "summary": "Temporal non-stationarity, the phenomenon that time series distributions\nchange over time, poses fundamental challenges to reliable time series\nforecasting. Intuitively, the complex time series can be decomposed into two\nfactors, \\ie time-invariant and time-varying components, which indicate static\nand dynamic patterns, respectively. Nonetheless, existing methods often\nconflate the time-varying and time-invariant components, and jointly learn the\ncombined long-term patterns and short-term fluctuations, leading to suboptimal\nperformance facing distribution shifts. To address this issue, we initiatively\npropose a lightweight static-dynamic decomposition framework, TimeEmb, for time\nseries forecasting. TimeEmb innovatively separates time series into two\ncomplementary components: (1) time-invariant component, captured by a novel\nglobal embedding module that learns persistent representations across time\nseries, and (2) time-varying component, processed by an efficient\nfrequency-domain filtering mechanism inspired by full-spectrum analysis in\nsignal processing. Experiments on real-world datasets demonstrate that TimeEmb\noutperforms state-of-the-art baselines and requires fewer computational\nresources. We conduct comprehensive quantitative and qualitative analyses to\nverify the efficacy of static-dynamic disentanglement. This lightweight\nframework can also improve existing time-series forecasting methods with simple\nintegration. To ease reproducibility, the code is available at\nhttps://github.com/showmeon/TimeEmb.",
      "authors": [
        "Mingyuan Xia",
        "Chunxu Zhang",
        "Zijian Zhang",
        "Hao Miao",
        "Qidong Liu",
        "Yuanshao Zhu",
        "Bo Yang"
      ],
      "published": "2025-10-01T03:28:49Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00461v1"
    },
    {
      "arxiv_id": "2510.00457v1",
      "title": "UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous\n  Graphs for Urban Microclimate Prediction",
      "summary": "With rapid urbanization, predicting urban microclimates has become critical,\nas it affects building energy demand and public health risks. However, existing\ngenerative and homogeneous graph approaches fall short in capturing physical\nconsistency, spatial dependencies, and temporal variability. To address this,\nwe introduce UrbanGraph, a physics-informed framework integrating heterogeneous\nand dynamic spatio-temporal graphs. It encodes key physical processes --\nvegetation evapotranspiration, shading, and convective diffusion -- while\nmodeling complex spatial dependencies among diverse urban entities and their\ntemporal evolution. We evaluate UrbanGraph on UMC4/12, a physics-based\nsimulation dataset covering diverse urban configurations and climates. Results\nshow that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0%\nover all baselines, with heterogeneous and dynamic graphs contributing 3.5% and\n7.1% gains. Our dataset provides the first high-resolution benchmark for\nspatio-temporal microclimate modeling, and our method extends to broader urban\nheterogeneous dynamic computing tasks.",
      "authors": [
        "Weilin Xin",
        "Chenyu Huang",
        "Peilin Li",
        "Jing Zhong",
        "Jiawei Yao"
      ],
      "published": "2025-10-01T03:14:05Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00457v1"
    },
    {
      "arxiv_id": "2510.00454v1",
      "title": "Measuring and Controlling the Spectral Bias for Self-Supervised Image\n  Denoising",
      "summary": "Current self-supervised denoising methods for paired noisy images typically\ninvolve mapping one noisy image through the network to the other noisy image.\nHowever, after measuring the spectral bias of such methods using our proposed\nImage Pair Frequency-Band Similarity, it suffers from two practical\nlimitations. Firstly, the high-frequency structural details in images are not\npreserved well enough. Secondly, during the process of fitting high\nfrequencies, the network learns high-frequency noise from the mapped noisy\nimages. To address these challenges, we introduce a Spectral Controlling\nnetwork (SCNet) to optimize self-supervised denoising of paired noisy images.\nFirst, we propose a selection strategy to choose frequency band components for\nnoisy images, to accelerate the convergence speed of training. Next, we present\na parameter optimization method that restricts the learning ability of\nconvolutional kernels to high-frequency noise using the Lipschitz constant,\nwithout changing the network structure. Finally, we introduce the Spectral\nSeparation and low-rank Reconstruction module (SSR module), which separates\nnoise and high-frequency details through frequency domain separation and\nlow-rank space reconstruction, to retain the high-frequency structural details\nof images. Experiments performed on synthetic and real-world datasets verify\nthe effectiveness of SCNet.",
      "authors": [
        "Wang Zhang",
        "Huaqiu Li",
        "Xiaowan Hu",
        "Tao Jiang",
        "Zikang Chen",
        "Haoqian Wang"
      ],
      "published": "2025-10-01T03:07:05Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00454v1"
    },
    {
      "arxiv_id": "2510.00452v1",
      "title": "Cloud Investigation Automation Framework (CIAF): An AI-Driven Approach\n  to Cloud Forensics",
      "summary": "Large Language Models (LLMs) have gained prominence in domains including\ncloud security and forensics. Yet cloud forensic investigations still rely on\nmanual analysis, making them time-consuming and error-prone. LLMs can mimic\nhuman reasoning, offering a pathway to automating cloud log analysis. To\naddress this, we introduce the Cloud Investigation Automation Framework (CIAF),\nan ontology-driven framework that systematically investigates cloud forensic\nlogs while improving efficiency and accuracy. CIAF standardizes user inputs\nthrough semantic validation, eliminating ambiguity and ensuring consistency in\nlog interpretation. This not only enhances data quality but also provides\ninvestigators with reliable, standardized information for decision-making. To\nevaluate security and performance, we analyzed Microsoft Azure logs containing\nransomware-related events. By simulating attacks and assessing CIAF's impact,\nresults showed significant improvement in ransomware detection, achieving\nprecision, recall, and F1 scores of 93 percent. CIAF's modular, adaptable\ndesign extends beyond ransomware, making it a robust solution for diverse\ncyberattacks. By laying the foundation for standardized forensic methodologies\nand informing future AI-driven automation, this work underscores the role of\ndeterministic prompt engineering and ontology-based validation in enhancing\ncloud forensic investigations. These advancements improve cloud security while\npaving the way for efficient, automated forensic workflows.",
      "authors": [
        "Dalal Alharthi",
        "Ivan Roberto Kawaminami Garcia"
      ],
      "published": "2025-10-01T03:05:47Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.00452v1"
    },
    {
      "arxiv_id": "2510.00451v1",
      "title": "A Call to Action for a Secure-by-Design Generative AI Paradigm",
      "summary": "Large language models have gained widespread prominence, yet their\nvulnerability to prompt injection and other adversarial attacks remains a\ncritical concern. This paper argues for a security-by-design AI paradigm that\nproactively mitigates LLM vulnerabilities while enhancing performance. To\nachieve this, we introduce PromptShield, an ontology-driven framework that\nensures deterministic and secure prompt interactions. It standardizes user\ninputs through semantic validation, eliminating ambiguity and mitigating\nadversarial manipulation. To assess PromptShield's security and performance\ncapabilities, we conducted an experiment on an agent-based system to analyze\ncloud logs within Amazon Web Services (AWS), containing 493 distinct events\nrelated to malicious activities and anomalies. By simulating prompt injection\nattacks and assessing the impact of deploying PromptShield, our results\ndemonstrate a significant improvement in model security and performance,\nachieving precision, recall, and F1 scores of approximately 94%. Notably, the\nontology-based framework not only mitigates adversarial threats but also\nenhances the overall performance and reliability of the system. Furthermore,\nPromptShield's modular and adaptable design ensures its applicability beyond\ncloud security, making it a robust solution for safeguarding generative AI\napplications across various domains. By laying the groundwork for AI safety\nstandards and informing future policy development, this work stimulates a\ncrucial dialogue on the pivotal role of deterministic prompt engineering and\nontology-based validation in ensuring the safe and responsible deployment of\nLLMs in high-stakes environments.",
      "authors": [
        "Dalal Alharthi",
        "Ivan Roberto Kawaminami Garcia"
      ],
      "published": "2025-10-01T03:05:07Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.00451v1"
    },
    {
      "arxiv_id": "2510.00436v1",
      "title": "Automated Evaluation can Distinguish the Good and Bad AI Responses to\n  Patient Questions about Hospitalization",
      "summary": "Automated approaches to answer patient-posed health questions are rising, but\nselecting among systems requires reliable evaluation. The current gold standard\nfor evaluating the free-text artificial intelligence (AI) responses--human\nexpert review--is labor-intensive and slow, limiting scalability. Automated\nmetrics are promising yet variably aligned with human judgments and often\ncontext-dependent. To address the feasibility of automating the evaluation of\nAI responses to hospitalization-related questions posed by patients, we\nconducted a large systematic study of evaluation approaches. Across 100 patient\ncases, we collected responses from 28 AI systems (2800 total) and assessed them\nalong three dimensions: whether a system response (1) answers the question, (2)\nappropriately uses clinical note evidence, and (3) uses general medical\nknowledge. Using clinician-authored reference answers to anchor metrics,\nautomated rankings closely matched expert ratings. Our findings suggest that\ncarefully designed automated evaluation can scale comparative assessment of AI\nsystems and support patient-clinician communication.",
      "authors": [
        "Sarvesh Soni",
        "Dina Demner-Fushman"
      ],
      "published": "2025-10-01T02:39:37Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00436v1"
    },
    {
      "arxiv_id": "2510.00430v1",
      "title": "Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model\n  Alignment",
      "summary": "Despite the recent progress, reinforcement learning (RL)-based fine-tuning of\ndiffusion models often struggles with generalization, composability, and\nrobustness against reward hacking. Recent studies have explored prompt\nrefinement as a modular alternative, but most adopt a feed-forward approach\nthat applies a single refined prompt throughout the entire sampling trajectory,\nthereby failing to fully leverage the sequential nature of reinforcement\nlearning. To address this, here we introduce PromptLoop, a plug-and-play RL\nframework that incorporates latent feedback into step-wise prompt refinement.\nRather than modifying diffusion model weights, a multimodal large language\nmodel (MLLM) is trained with RL to iteratively update prompts based on\nintermediate latent states of diffusion models. This design achieves a\nstructural analogy to the Diffusion RL approach, while retaining the\nflexibility and generality of prompt-based alignment. Extensive experiments\nacross diverse reward functions and diffusion backbones demonstrate that\nPromptLoop (i) achieves effective reward optimization, (ii) generalizes\nseamlessly to unseen models, (iii) composes orthogonally with existing\nalignment methods, and (iv) mitigates over-optimization and reward hacking.",
      "authors": [
        "Suhyeon Lee",
        "Jong Chul Ye"
      ],
      "published": "2025-10-01T02:18:58Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00430v1"
    },
    {
      "arxiv_id": "2510.00428v1",
      "title": "Automated Structured Radiology Report Generation with Rich Clinical\n  Context",
      "summary": "Automated structured radiology report generation (SRRG) from chest X-ray\nimages offers significant potential to reduce workload of radiologists by\ngenerating reports in structured formats that ensure clarity, consistency, and\nadherence to clinical reporting standards. While radiologists effectively\nutilize available clinical contexts in their diagnostic reasoning, existing\nSRRG systems overlook these essential elements. This fundamental gap leads to\ncritical problems including temporal hallucinations when referencing\nnon-existent clinical contexts. To address these limitations, we propose\ncontextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical\ncontext for SRRG. We curate C-SRRG dataset by integrating comprehensive\nclinical context encompassing 1) multi-view X-ray images, 2) clinical\nindication, 3) imaging techniques, and 4) prior studies with corresponding\ncomparisons based on patient histories. Through extensive benchmarking with\nstate-of-the-art multimodal large language models, we demonstrate that\nincorporating clinical context with the proposed C-SRRG significantly improves\nreport generation quality. We publicly release dataset, code, and checkpoints\nto facilitate future research for clinically-aligned automated RRG at\nhttps://github.com/vuno/contextualized-srrg.",
      "authors": [
        "Seongjae Kang",
        "Dong Bok Lee",
        "Juho Jung",
        "Dongseop Kim",
        "Won Hwa Kim",
        "Sunghoon Joo"
      ],
      "published": "2025-10-01T02:14:23Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00428v1"
    },
    {
      "arxiv_id": "2510.00416v1",
      "title": "Domain-Specialized Interactive Segmentation Framework for Meningioma\n  Radiotherapy Planning",
      "summary": "Precise delineation of meningiomas is crucial for effective radiotherapy (RT)\nplanning, directly influencing treatment efficacy and preservation of adjacent\nhealthy tissues. While automated deep learning approaches have demonstrated\nconsiderable potential, achieving consistently accurate clinical segmentation\nremains challenging due to tumor heterogeneity. Interactive Medical Image\nSegmentation (IMIS) addresses this challenge by integrating advanced AI\ntechniques with clinical input. However, generic segmentation tools, despite\nwidespread applicability, often lack the specificity required for clinically\ncritical and disease-specific tasks like meningioma RT planning. To overcome\nthese limitations, we introduce Interactive-MEN-RT, a dedicated IMIS tool\nspecifically developed for clinician-assisted 3D meningioma segmentation in RT\nworkflows. The system incorporates multiple clinically relevant interaction\nmethods, including point annotations, bounding boxes, lasso tools, and\nscribbles, enhancing usability and clinical precision. In our evaluation\ninvolving 500 contrast-enhanced T1-weighted MRI scans from the BraTS 2025\nMeningioma RT Segmentation Challenge, Interactive-MEN-RT demonstrated\nsubstantial improvement compared to other segmentation methods, achieving Dice\nsimilarity coefficients of up to 77.6\\% and Intersection over Union scores of\n64.8\\%. These results emphasize the need for clinically tailored segmentation\nsolutions in critical applications such as meningioma RT planning. The code is\npublicly available at: https://github.com/snuh-rad-aicon/Interactive-MEN-RT",
      "authors": [
        "Junhyeok Lee",
        "Han Jang",
        "Kyu Sung Choi"
      ],
      "published": "2025-10-01T01:57:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00416v1"
    },
    {
      "arxiv_id": "2510.00415v1",
      "title": "Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via\n  Test-Time Exploration under Validate-by-Reproduce Paradigm",
      "summary": "Recent advances in large language models (LLMs) and agent system designs have\nempowered agents with unprecedented levels of capability. However, existing\nagent benchmarks are showing a trend of rapid ceiling-hitting by newly\ndeveloped agents, making it difficult to meet the demands for evaluating agent\nabilities. To address this problem, we propose the Trajectory-based\nValidated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE)\nframework. This framework takes an original task from an existing benchmark and\nencourages agents to freely explore and evolve it into a new task with higher\ndifficulty while recording validatable agent trajectories. The framework\nproceeds in three stages: (1) evolutionary proposal mining, which provides task\nevolution proposals through preliminary exploration and divergent thinking; (2)\nproblem formation and free exploration, where proposals are conceptualized into\nfeasible problem candidates and the agents then explore them freely while\nrecording their execution trajectories; and (3) multi-level validation, which\nensures that the evolved tasks are accompanied by validatable and reproducible\ntrajectories. Experiments on the GAIA benchmark demonstrate that the TRACE\nframework consistently enhances task complexity while improving the reliability\nof correctness through validatable execution trajectories. This work marks a\nparadigm shift from static, manually curated benchmarks to dynamic,\nself-evolving evaluation systems, providing a sustainable and challenging\nrunway for agent development.",
      "authors": [
        "Dadi Guo",
        "Tianyi Zhou",
        "Dongrui Liu",
        "Chen Qian",
        "Qihan Ren",
        "Shuai Shao",
        "Zhiyuan Fan",
        "Yi R. Fung",
        "Kun Wang",
        "Linfeng Zhang",
        "Jing Shao"
      ],
      "published": "2025-10-01T01:52:52Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00415v1"
    },
    {
      "arxiv_id": "2510.00411v2",
      "title": "Does Bigger Mean Better? Comparitive Analysis of CNNs and Biomedical\n  Vision Language Modles in Medical Diagnosis",
      "summary": "The accurate interpretation of chest radiographs using automated methods is a\ncritical task in medical imaging. This paper presents a comparative analysis\nbetween a supervised lightweight Convolutional Neural Network (CNN) and a\nstate-of-the-art, zero-shot medical Vision-Language Model (VLM), BiomedCLIP,\nacross two distinct diagnostic tasks: pneumonia detection on the PneumoniaMNIST\nbenchmark and tuberculosis detection on the Shenzhen TB dataset. Our\nexperiments show that supervised CNNs serve as highly competitive baselines in\nboth cases. While the default zero-shot performance of the VLM is lower, we\ndemonstrate that its potential can be unlocked via a simple yet crucial remedy:\ndecision threshold calibration. By optimizing the classification threshold on a\nvalidation set, the performance of BiomedCLIP is significantly boosted across\nboth datasets. For pneumonia detection, calibration enables the zero-shot VLM\nto achieve a superior F1-score of 0.8841, surpassing the supervised CNN's\n0.8803. For tuberculosis detection, calibration dramatically improves the\nF1-score from 0.4812 to 0.7684, bringing it close to the supervised baseline's\n0.7834. This work highlights a key insight: proper calibration is essential for\nleveraging the full diagnostic power of zero-shot VLMs, enabling them to match\nor even outperform efficient, task-specific supervised models.",
      "authors": [
        "Ran Tong",
        "Jiaqi Liu",
        "Su Liu",
        "Jiexi Xu",
        "Lanruo Wang",
        "Tong Wang"
      ],
      "published": "2025-10-01T01:46:09Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00411v2"
    },
    {
      "arxiv_id": "2510.00405v1",
      "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy\n  Observations",
      "summary": "Reliable trajectory prediction from an ego-centric perspective is crucial for\nrobotic navigation in human-centric environments. However, existing methods\ntypically assume idealized observation histories, failing to account for the\nperceptual artifacts inherent in first-person vision, such as occlusions, ID\nswitches, and tracking drift. This discrepancy between training assumptions and\ndeployment reality severely limits model robustness. To bridge this gap, we\nintroduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,\nfirst-person visual histories in clean, bird's-eye-view future trajectories,\nenabling robust learning under realistic perceptual constraints. Building on\nthis benchmark, we propose BiFlow, a dual-stream flow matching model that\nconcurrently denoises historical observations and forecasts future motion by\nleveraging a shared latent representation. To better model agent intent, BiFlow\nincorporates our EgoAnchor mechanism, which conditions the prediction decoder\non distilled historical features via feature modulation. Extensive experiments\nshow that BiFlow achieves state-of-the-art performance, reducing minADE and\nminFDE by 10-15% on average and demonstrating superior robustness. We\nanticipate that our benchmark and model will provide a critical foundation for\ndeveloping trajectory forecasting systems truly resilient to the challenges of\nreal-world, ego-centric perception.",
      "authors": [
        "Jiayi Liu",
        "Jiaming Zhou",
        "Ke Ye",
        "Kun-Yu Lin",
        "Allan Wang",
        "Junwei Liang"
      ],
      "published": "2025-10-01T01:30:13Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00405v1"
    },
    {
      "arxiv_id": "2510.00404v2",
      "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features",
      "summary": "Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs.",
      "authors": [
        "Xudong Zhu",
        "Mohammad Mahdi Khalili",
        "Zhihui Zhu"
      ],
      "published": "2025-10-01T01:29:31Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00404v2"
    },
    {
      "arxiv_id": "2510.00401v1",
      "title": "Physics-Informed Neural Controlled Differential Equations for Scalable\n  Long Horizon Multi-Agent Motion Forecasting",
      "summary": "Long-horizon motion forecasting for multiple autonomous robots is challenging\ndue to non-linear agent interactions, compounding prediction errors, and\ncontinuous-time evolution of dynamics. Learned dynamics of such a system can be\nuseful in various applications such as travel time prediction,\nprediction-guided planning and generative simulation. In this work, we aim to\ndevelop an efficient trajectory forecasting model conditioned on multi-agent\ngoals. Motivated by the recent success of physics-guided deep learning for\npartially known dynamical systems, we develop a model based on neural\nControlled Differential Equations (CDEs) for long-horizon motion forecasting.\nUnlike discrete-time methods such as RNNs and transformers, neural CDEs operate\nin continuous time, allowing us to combine physics-informed constraints and\nbiases to jointly model multi-robot dynamics. Our approach, named PINCoDE\n(Physics-Informed Neural Controlled Differential Equations), learns\ndifferential equation parameters that can be used to predict the trajectories\nof a multi-agent system starting from an initial condition. PINCoDE is\nconditioned on future goals and enforces physics constraints for robot motion\nover extended periods of time. We adopt a strategy that scales our model from\n10 robots to 100 robots without the need for additional model parameters, while\nproducing predictions with an average ADE below 0.5 m for a 1-minute horizon.\nFurthermore, progressive training with curriculum learning for our PINCoDE\nmodel results in a 2.7X reduction of forecasted pose error over 4 minute\nhorizons compared to analytical models.",
      "authors": [
        "Shounak Sural",
        "Charles Kekeh",
        "Wenliang Liu",
        "Federico Pecora",
        "Mouhacine Benosman"
      ],
      "published": "2025-10-01T01:27:07Z",
      "primary_category": "cs.RO",
      "arxiv_url": "https://arxiv.org/abs/2510.00401v1"
    },
    {
      "arxiv_id": "2510.01288v1",
      "title": "Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal\n  LLM Misbehaviours",
      "summary": "We draw inspiration from microsaccades, tiny involuntary eye movements that\nreveal hidden dynamics of human perception, to propose an analogous probing\nmethod for large language models (LLMs). Just as microsaccades expose subtle\nbut informative shifts in vision, we show that lightweight position encoding\nperturbations elicit latent signals that indicate model misbehaviour. Our\nmethod requires no fine-tuning or task-specific supervision, yet detects\nfailures across diverse settings including factuality, safety, toxicity, and\nbackdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate\nthat these perturbation-based probes surface misbehaviours while remaining\ncomputationally efficient. These findings suggest that pretrained LLMs already\nencode the internal evidence needed to flag their own failures, and that\nmicrosaccade-inspired interventions provide a pathway for detecting and\nmitigating undesirable behaviours.",
      "authors": [
        "Rui Melo",
        "Rui Abreu",
        "Corina S. Pasareanu"
      ],
      "published": "2025-10-01T01:24:59Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01288v1"
    },
    {
      "arxiv_id": "2510.00395v1",
      "title": "SAGE-Music: Low-Latency Symbolic Music Generation via\n  Attribute-Specialized Key-Value Head Sharing",
      "summary": "Low-latency symbolic music generation is essential for real-time\nimprovisation and human-AI co-creation. Existing transformer-based models,\nhowever, face a trade-off between inference speed and musical quality.\nTraditional acceleration techniques such as embedding pooling significantly\ndegrade quality, while recently proposed Byte Pair Encoding (BPE) methods -\nthough effective on single-track piano data - suffer large performance drops in\nmulti-track settings, as revealed by our analysis. We propose\nAttribute-Specialized Key-Value Head Sharing (AS-KVHS), adapted to music's\nstructured symbolic representation, achieving about 30% inference speedup with\nonly a negligible (about 0.4%) quality drop in objective evaluations and slight\nimprovements in subjective listening tests. Our main contributions are (1) the\nfirst systematic study of BPE's generalizability in multi-track symbolic music,\nand (2) the introduction of AS-KVHS for low-latency symbolic music generation.\nBeyond these, we also release SAGE-Music, an open-source benchmark that matches\nor surpasses state-of-the-art models in generation quality.",
      "authors": [
        "Jiaye Tan",
        "Haonan Luo",
        "Linfeng Song",
        "Shuaiqi Chen",
        "Yishan Lyu",
        "Zian Zhong",
        "Roujia Wang",
        "Daniel Jiang",
        "Haoran Zhang",
        "Jiaming Bai",
        "Haoran Cheng",
        "Q. Vera Liao",
        "Hao-Wen Dong"
      ],
      "published": "2025-10-01T01:11:43Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00395v1"
    },
    {
      "arxiv_id": "2510.00386v1",
      "title": "Train on Validation (ToV): Fast data selection with applications to\n  fine-tuning",
      "summary": "State-of-the-art machine learning often follows a two-stage process:\n$(i)$~pre-training on large, general-purpose datasets; $(ii)$~fine-tuning on\ntask-specific data. In fine-tuning, selecting training examples that closely\nreflect the target distribution is crucial. However, it is often the case that\nonly a few samples are available from the target distribution. Existing data\nselection methods treat these target samples as a validation set and estimate\nthe effect of adding or removing a single sample from the training pool by\nperforming inference on the validation set.\n  We propose a simpler and faster alternative that inverts the usual role of\ntrain and validation: we perform inference on the training pool before and\nafter fine-tuning on the validation set. We then select samples whose\npredictions change the most. Our key insight is that the training samples most\naffected by fine-tuning on a small validation set tend to be the most\nbeneficial for reducing test loss on the target distribution. Experiments on\ninstruction tuning and named entity recognition tasks show that, in most cases,\nour method achieves lower test log-loss than state-of-the-art approaches. We\nsupport our findings with theoretical analysis.",
      "authors": [
        "Ayush Jain",
        "Andrea Montanari",
        "Eren Sasoglu"
      ],
      "published": "2025-10-01T00:55:39Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00386v1"
    },
    {
      "arxiv_id": "2510.00381v1",
      "title": "Semantic-Driven AI Agent Communications: Challenges and Solutions",
      "summary": "With the rapid growth of intelligent services, communication targets are\nshifting from humans to artificial intelligent (AI) agents, which require new\nparadigms to enable real-time perception, decision-making, and collaboration.\nSemantic communication, which conveys task-relevant meaning rather than raw\ndata, offers a promising solution. However, its practical deployment remains\nconstrained by dynamic environments and limited resources. To address these\nissues, this article proposes a semantic-driven AI agent communication\nframework and develops three enabling techniques. First, semantic adaptation\ntransmission applies fine-tuning with real or generative samples to efficiently\nadapt models to varying environments. Second, semantic lightweight transmission\nincorporates pruning, quantization, and perception-aware sampling to reduce\nmodel complexity and alleviate computational burden on edge agents. Third,\nsemantic self-evolution control employs distributed hierarchical\ndecision-making to optimize multi-dimensional resources, enabling robust\nmulti-agent collaboration in dynamic environments. Simulation results show that\nthe proposed solutions achieve faster convergence and stronger robustness,\nwhile the proposed distributed hierarchical optimization method significantly\noutperforms conventional decision-making schemes, highlighting its potential\nfor AI agent communication networks.",
      "authors": [
        "Kaiwen Yu",
        "Mengying Sun",
        "Zhijin Qin",
        "Xiaodong Xu",
        "Ping Yang",
        "Yue Xiao",
        "Gang Wu"
      ],
      "published": "2025-10-01T00:52:37Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00381v1"
    },
    {
      "arxiv_id": "2510.00376v1",
      "title": "Discrete Wavelet Transform as a Facilitator for Expressive Latent Space\n  Representation in Variational Autoencoders in Satellite Imagery",
      "summary": "Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the\ncomputational complexity of pixel-space diffusion by operating within a\ncompressed latent space constructed by Variational Autoencoders (VAEs),\ndemonstrating significant advantages in Remote Sensing (RS) applications.\nThough numerous studies enhancing LDMs have been conducted, investigations\nexplicitly targeting improvements within the intrinsic latent space remain\nscarce. This paper proposes an innovative perspective, utilizing the Discrete\nWavelet Transform (DWT) to enhance the VAE's latent space representation,\ndesigned for satellite imagery. The proposed method, ExpDWT-VAE, introduces\ndual branches: one processes spatial domain input through convolutional\noperations, while the other extracts and processes frequency-domain features\nvia 2D Haar wavelet decomposition, convolutional operation, and inverse DWT\nreconstruction. These branches merge to create an integrated spatial-frequency\nrepresentation, further refined through convolutional and diagonal Gaussian\nmapping into a robust latent representation. We utilize a new satellite imagery\ndataset housed by the TerraFly mapping system to validate our method.\nExperimental results across several performance metrics highlight the efficacy\nof the proposed method at enhancing latent space representation.",
      "authors": [
        "Arpan Mahara",
        "Md Rezaul Karim Khan",
        "Naphtali Rishe",
        "Wenjia Wang",
        "Seyed Masoud Sadjadi"
      ],
      "published": "2025-10-01T00:49:41Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00376v1"
    },
    {
      "arxiv_id": "2510.00373v1",
      "title": "Combining Large Language Models and Gradient-Free Optimization for\n  Automatic Control Policy Synthesis",
      "summary": "Large Language models (LLMs) have shown promise as generators of symbolic\ncontrol policies, producing interpretable program-like representations through\niterative search. However, these models are not capable of separating the\nfunctional structure of a policy from the numerical values it is parametrized\nby, thus making the search process slow and inefficient. We propose a hybrid\napproach that decouples structural synthesis from parameter optimization by\nintroducing an additional optimization layer for local parameter search. In our\nmethod, the numerical parameters of LLM-generated programs are extracted and\noptimized numerically to maximize task performance. With this integration, an\nLLM iterates over the functional structure of programs, while a separate\noptimization loop is used to find a locally optimal set of parameters\naccompanying candidate programs. We evaluate our method on a set of control\ntasks, showing that it achieves higher returns and improved sample efficiency\ncompared to purely LLM-guided search. We show that combining symbolic program\nsynthesis with numerical optimization yields interpretable yet high-performing\npolicies, bridging the gap between language-model-guided design and classical\ncontrol tuning. Our code is available at\nhttps://sites.google.com/berkeley.edu/colmo.",
      "authors": [
        "Carlo Bosio",
        "Matteo Guarrera",
        "Alberto Sangiovanni-Vincentelli",
        "Mark W. Mueller"
      ],
      "published": "2025-10-01T00:42:15Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00373v1"
    },
    {
      "arxiv_id": "2510.01287v1",
      "title": "Evaluating New AI Cell Foundation Models on Challenging Kidney Pathology\n  Cases Unaddressed by Previous Foundation Models",
      "summary": "Accurate cell nuclei segmentation is critical for downstream tasks in kidney\npathology and remains a major challenge due to the morphological diversity and\nimaging variability of renal tissues. While our prior work has evaluated\nearly-generation AI cell foundation models in this domain, the effectiveness of\nrecent cell foundation models remains unclear. In this study, we benchmark\nadvanced AI cell foundation models (2025), including CellViT++ variants and\nCellpose-SAM, against three widely used cell foundation models developed prior\nto 2024, using a diverse large-scale set of kidney image patches within a\nhuman-in-the-loop rating framework. We further performed fusion-based ensemble\nevaluation and model agreement analysis to assess the segmentation capabilities\nof the different models. Our results show that CellViT++ [Virchow] yields the\nhighest standalone performance with 40.3% of predictions rated as \"Good\" on a\ncurated set of 2,091 challenging samples, outperforming all prior models. In\naddition, our fused model achieves 62.2% \"Good\" predictions and only 0.4%\n\"Bad\", substantially reducing segmentation errors. Notably, the fusion model\n(2025) successfully resolved the majority of challenging cases that remained\nunaddressed in our previous study. These findings demonstrate the potential of\nAI cell foundation model development in renal pathology and provide a curated\ndataset of challenging samples to support future kidney-specific model\nrefinement.",
      "authors": [
        "Runchen Wang",
        "Junlin Guo",
        "Siqi Lu",
        "Ruining Deng",
        "Zhengyi Lu",
        "Yanfan Zhu",
        "Yuechen Yang",
        "Chongyu Qu",
        "Yu Wang",
        "Shilin Zhao",
        "Catie Chang",
        "Mitchell Wilkes",
        "Mengmeng Yin",
        "Haichun Yang",
        "Yuankai Huo"
      ],
      "published": "2025-10-01T00:38:36Z",
      "primary_category": "q-bio.QM",
      "arxiv_url": "https://arxiv.org/abs/2510.01287v1"
    },
    {
      "arxiv_id": "2510.00361v1",
      "title": "Attribution Gradients: Incrementally Unfolding Citations for Critical\n  Examination of Attributed AI Answers",
      "summary": "AI question answering systems increasingly generate responses with\nattributions to sources. However, the task of verifying the actual content of\nthese attributions is in most cases impractical. In this paper, we present\nattribution gradients as a solution. Attribution gradients provide integrated,\nincremental affordances for diving into an attributed passage. A user can\ndecompose a sentence of an answer into its claims. For each claim, the user can\nview supporting and contradictory excerpts mined from sources. Those excerpts\nserve as clickable conduits into the source (in our application, scientific\npapers). When evidence itself contains more citations, the UI unpacks the\nevidence into excerpts from the cited sources. These features of attribution\ngradients facilitate concurrent interconnections among answer, claim, excerpt,\nand context. In a usability study, we observed greater engagement with sources\nand richer revision in a task where participants revised an attributed AI\nanswer with attribution gradients and a baseline.",
      "authors": [
        "Hita Kambhamettu",
        "Alyssa Hwang",
        "Philippe Laban",
        "Andrew Head"
      ],
      "published": "2025-10-01T00:07:28Z",
      "primary_category": "cs.HC",
      "arxiv_url": "https://arxiv.org/abs/2510.00361v1"
    }
  ]
}