{
  "generated_at": "2025-10-05T06:20:06.782657Z",
  "categories": [
    "cs.CL"
  ],
  "paper_count": 157,
  "papers": [
    {
      "arxiv_id": "2510.02306v1",
      "title": "Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation",
      "summary": "In arena-style evaluation of large language models (LLMs), two LLMs respond\nto a user query, and the user chooses the winning response or deems the\n\"battle\" a draw, resulting in an adjustment to the ratings of both models. The\nprevailing approach for modeling these rating dynamics is to view battles as\ntwo-player game matches, as in chess, and apply the Elo rating system and its\nderivatives. In this paper, we critically examine this paradigm. Specifically,\nwe question whether a draw genuinely means that the two models are equal and\nhence whether their ratings should be equalized. Instead, we conjecture that\ndraws are more indicative of query difficulty: if the query is too easy, then\nboth models are more likely to succeed equally. On three real-world arena\ndatasets, we show that ignoring rating updates for draws yields a 1-3% relative\nincrease in battle outcome prediction accuracy (which includes draws) for all\nfour rating systems studied. Further analyses suggest that draws occur more for\nqueries rated as very easy and those as highly objective, with risk ratios of\n1.37 and 1.35, respectively. We recommend future rating systems to reconsider\nexisting draw semantics and to account for query properties in rating updates.",
      "authors": [
        "Raphael Tang",
        "Crystina Zhang",
        "Wenyan Li",
        "Carmen Lai",
        "Pontus Stenetorp",
        "Yao Lu"
      ],
      "published": "2025-10-02T17:59:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02306v1"
    },
    {
      "arxiv_id": "2510.02297v1",
      "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
      "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
      "authors": [
        "Wentao Zhang",
        "Yang Young Lu",
        "Yuntian Deng"
      ],
      "published": "2025-10-02T17:59:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02297v1"
    },
    {
      "arxiv_id": "2510.02294v1",
      "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data",
      "summary": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of\nstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike\nprevious top-ranking embedding models that require massive contrastive\npretraining, sophisticated training pipelines, and costly synthetic training\ndata, F2LLM is directly finetuned from foundation models on 6 million\nquery-document-negative tuples curated from open-source, non-synthetic\ndatasets, striking a strong balance between training cost, model size, and\nembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd\namong models with approximately 4B parameters and 7th overall, while F2LLM-1.7B\nranks 1st among models in the 1B-2B size range. To facilitate future research\nin the field, we release the models, training dataset, and code, positioning\nF2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
      "authors": [
        "Ziyin Zhang",
        "Zihan Liao",
        "Hang Yu",
        "Peng Di",
        "Rui Wang"
      ],
      "published": "2025-10-02T17:58:49Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02294v1"
    },
    {
      "arxiv_id": "2510.02292v1",
      "title": "From Behavioral Performance to Internal Competence: Interpreting\n  Vision-Language Models with VLM-Lens",
      "summary": "We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,\nanalysis, and interpretation of vision-language models (VLMs) by supporting the\nextraction of intermediate outputs from any layer during the forward pass of\nopen-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that\nabstracts away model-specific complexities and supports user-friendly operation\nacross diverse VLMs. It currently supports 16 state-of-the-art base VLMs and\ntheir over 30 variants, and is extensible to accommodate new models without\nchanging the core logic.\n  The toolkit integrates easily with various interpretability and analysis\nmethods. We demonstrate its usage with two simple analytical experiments,\nrevealing systematic differences in the hidden representations of VLMs across\nlayers and target concepts. VLM-Lens is released as an open-sourced project to\naccelerate community efforts in understanding and improving VLMs.",
      "authors": [
        "Hala Sheta",
        "Eric Huang",
        "Shuyu Wu",
        "Ilia Alenabi",
        "Jiajun Hong",
        "Ryker Lin",
        "Ruoxi Ning",
        "Daniel Wei",
        "Jialin Yang",
        "Jiawei Zhou",
        "Ziqiao Ma",
        "Freda Shi"
      ],
      "published": "2025-10-02T17:58:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02292v1"
    },
    {
      "arxiv_id": "2510.02286v1",
      "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
      "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
      "authors": [
        "Ruohao Guo",
        "Afshin Oroojlooy",
        "Roshan Sridhar",
        "Miguel Ballesteros",
        "Alan Ritter",
        "Dan Roth"
      ],
      "published": "2025-10-02T17:57:05Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02286v1"
    },
    {
      "arxiv_id": "2510.02272v1",
      "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective",
      "summary": "Recent advancements in Reinforcement Post-Training (RPT) have significantly\nenhanced the capabilities of Large Reasoning Models (LRMs), sparking increased\ninterest in the generalization of RL-based reasoning. While existing work has\nprimarily focused on investigating its generalization across tasks or\nmodalities, this study proposes a novel cross-linguistic perspective to\ninvestigate reasoning generalization. This raises a crucial question:\n$\\textit{Does the reasoning capability achieved from English RPT effectively\ntransfer to other languages?}$ We address this by systematically evaluating\nEnglish-centric LRMs on multilingual reasoning benchmarks and introducing a\nmetric to quantify cross-lingual transferability. Our findings reveal that\ncross-lingual transferability varies significantly across initial model, target\nlanguage, and training paradigm. Through interventional studies, we find that\nmodels with stronger initial English capabilities tend to over-rely on\nEnglish-specific patterns, leading to diminished cross-lingual generalization.\nTo address this, we conduct a thorough parallel training study. Experimental\nresults yield three key findings: $\\textbf{First-Parallel Leap}$, a substantial\nleap in performance when transitioning from monolingual to just a single\nparallel language, and a predictable $\\textbf{Parallel Scaling Law}$, revealing\nthat cross-lingual reasoning transfer follows a power-law with the number of\ntraining parallel languages. Moreover, we identify the discrepancy between\nactual monolingual performance and the power-law prediction as\n$\\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs\nfail to fully generalize across languages. Our study challenges the assumption\nthat LRM reasoning mirrors human cognition, providing critical insights for the\ndevelopment of more language-agnostic LRMs.",
      "authors": [
        "Wen Yang",
        "Junhong Wu",
        "Chong Li",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "published": "2025-10-02T17:49:49Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02272v1"
    },
    {
      "arxiv_id": "2510.02271v1",
      "title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in\n  Tool-Augmented Agents",
      "summary": "Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling.",
      "authors": [
        "Yaxin Du",
        "Yuanshuo Zhang",
        "Xiyuan Yang",
        "Yifan Zhou",
        "Cheng Wang",
        "Gongyi Zou",
        "Xianghe Pang",
        "Wenhao Wang",
        "Menglan Chen",
        "Shuo Tang",
        "Zhiyu Li",
        "Siheng Chen"
      ],
      "published": "2025-10-02T17:48:03Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02271v1"
    },
    {
      "arxiv_id": "2510.02263v1",
      "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
      "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.",
      "authors": [
        "Yuxiao Qu",
        "Anikait Singh",
        "Yoonho Lee",
        "Amrith Setlur",
        "Ruslan Salakhutdinov",
        "Chelsea Finn",
        "Aviral Kumar"
      ],
      "published": "2025-10-02T17:44:23Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02263v1"
    },
    {
      "arxiv_id": "2510.02250v1",
      "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
      "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
      "authors": [
        "Gonzalo Gonzalez-Pumariega",
        "Vincent Tu",
        "Chih-Lun Lee",
        "Jiachen Yang",
        "Ang Li",
        "Xin Eric Wang"
      ],
      "published": "2025-10-02T17:37:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02250v1"
    },
    {
      "arxiv_id": "2510.02249v1",
      "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process.",
      "authors": [
        "Tianyi Jiang",
        "Yi Bin",
        "Yujuan Ding",
        "Kainian Zhu",
        "Fei Ma",
        "Jingkuan Song",
        "Heng Tao Shen"
      ],
      "published": "2025-10-02T17:36:50Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02249v1"
    },
    {
      "arxiv_id": "2510.02245v1",
      "title": "ExGRPO: Learning to Reason from Experience",
      "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.",
      "authors": [
        "Runzhe Zhan",
        "Yafu Li",
        "Zhi Wang",
        "Xiaoye Qu",
        "Dongrui Liu",
        "Jing Shao",
        "Derek F. Wong",
        "Yu Cheng"
      ],
      "published": "2025-10-02T17:31:30Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02245v1"
    },
    {
      "arxiv_id": "2510.02243v1",
      "title": "AccurateRAG: A Framework for Building Accurate Retrieval-Augmented\n  Question-Answering Applications",
      "summary": "We introduce AccurateRAG -- a novel framework for constructing\nhigh-performance question-answering applications based on retrieval-augmented\ngeneration (RAG). Our framework offers a pipeline for development efficiency\nwith tools for raw dataset processing, fine-tuning data generation, text\nembedding & LLM fine-tuning, output evaluation, and building RAG systems\nlocally. Experimental results show that our framework outperforms previous\nstrong baselines and obtains new state-of-the-art question-answering\nperformance on benchmark datasets.",
      "authors": [
        "Linh The Nguyen",
        "Chi Tran",
        "Dung Ngoc Nguyen",
        "Van-Cuong Pham",
        "Hoang Ngo",
        "Dat Quoc Nguyen"
      ],
      "published": "2025-10-02T17:30:08Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02243v1"
    },
    {
      "arxiv_id": "2510.02241v1",
      "title": "Study on LLMs for Promptagator-Style Dense Retriever Training",
      "summary": "Promptagator demonstrated that Large Language Models (LLMs) with few-shot\nprompts can be used as task-specific query generators for fine-tuning\ndomain-specialized dense retrieval models. However, the original Promptagator\napproach relied on proprietary and large-scale LLMs which users may not have\naccess to or may be prohibited from using with sensitive data. In this work, we\nstudy the impact of open-source LLMs at accessible scales ($\\leq$14B\nparameters) as an alternative. Our results demonstrate that open-source LLMs as\nsmall as 3B parameters can serve as effective Promptagator-style query\ngenerators. We hope our work will inform practitioners with reliable\nalternatives for synthetic data generation and give insights to maximize\nfine-tuning results for domain-specific applications.",
      "authors": [
        "Daniel Gwon",
        "Nour Jedidi",
        "Jimmy Lin"
      ],
      "published": "2025-10-02T17:29:51Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.02241v1"
    },
    {
      "arxiv_id": "2510.02232v1",
      "title": "Enhanced Arabic-language cyberbullying detection: deep embedding and\n  transformer (BERT) approaches",
      "summary": "Recent technological advances in smartphones and communications, including\nthe growth of such online platforms as massive social media networks such as X\n(formerly known as Twitter) endangers young people and their emotional\nwell-being by exposing them to cyberbullying, taunting, and bullying content.\nMost proposed approaches for automatically detecting cyberbullying have been\ndeveloped around the English language, and methods for detecting\nArabic-language cyberbullying are scarce. Methods for detecting Arabic-language\ncyberbullying are especially scarce. This paper aims to enhance the\neffectiveness of methods for detecting cyberbullying in Arabic-language\ncontent. We assembled a dataset of 10,662 X posts, pre-processed the data, and\nused the kappa tool to verify and enhance the quality of our annotations. We\nconducted four experiments to test numerous deep learning models for\nautomatically detecting Arabic-language cyberbullying. We first tested a long\nshort-term memory (LSTM) model and a bidirectional long short-term memory\n(Bi-LSTM) model with several experimental word embeddings. We also tested the\nLSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder from\nrepresentations (BERT) and then tested them on a different experimental models\nBERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTM\nwith FastText embedding word performed even better, achieving 98% accuracy. As\na result, the outcomes are generalize",
      "authors": [
        "Ebtesam Jaber Aljohani",
        "Wael M. S. Yafoo"
      ],
      "published": "2025-10-02T17:20:02Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02232v1"
    },
    {
      "arxiv_id": "2510.02230v1",
      "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.",
      "authors": [
        "Phuc Minh Nguyen",
        "Chinh D. La",
        "Duy M. H. Nguyen",
        "Nitesh V. Chawla",
        "Binh T. Nguyen",
        "Khoa D. Doan"
      ],
      "published": "2025-10-02T17:17:27Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02230v1"
    },
    {
      "arxiv_id": "2510.02227v1",
      "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.",
      "authors": [
        "Xiaoyang Yuan",
        "Yujuan Ding",
        "Yi Bin",
        "Wenqi Shao",
        "Jinyu Cai",
        "Jingkuan Song",
        "Yang Yang",
        "Hengtao Shen"
      ],
      "published": "2025-10-02T17:14:00Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02227v1"
    },
    {
      "arxiv_id": "2510.02209v1",
      "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
      "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
      "authors": [
        "Yanxu Chen",
        "Zijun Yao",
        "Yantao Liu",
        "Jin Ye",
        "Jianing Yu",
        "Lei Hou",
        "Juanzi Li"
      ],
      "published": "2025-10-02T16:54:57Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.02209v1"
    },
    {
      "arxiv_id": "2510.02204v1",
      "title": "Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in\n  VLM-Powered Mobile-Use Agents",
      "summary": "Mobile-use agents powered by vision-language models (VLMs) have shown great\npotential in interpreting natural language instructions and generating\ncorresponding actions based on mobile graphical user interface. Recent studies\nsuggest that incorporating chain-of-thought (CoT) reasoning tends to improve\nthe execution accuracy. However, existing evaluations emphasize execution\naccuracy while neglecting whether CoT reasoning aligns with ground-truth\nactions. This oversight fails to assess potential reasoning-execution gaps,\nwhich in turn foster over-trust: users relying on seemingly plausible CoTs may\nunknowingly authorize harmful actions, potentially resulting in financial loss\nor trust crisis. In this work, we introduce a new evaluation framework to\ndiagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment\n(GTA), which measures whether the action implied by a CoT matches the\nground-truth action. By combining GTA with the standard Exact Match (EM)\nmetric, we jointly assess both the reasoning accuracy and execution accuracy.\nThis joint perspective reveals two types of reasoning-execution gaps: (i)\nExecution Gap (EG), where the reasoning correctly identifies the correct action\nbut execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but\nreasoning process conflicts with the actual execution. Experimental results\nacross a wide range of mobile interaction tasks reveal that reasoning-execution\ngaps are prevalent, with execution gaps occurring more frequently than\nreasoning gaps. Moreover, while scaling up model size reduces the overall gap,\nsizable execution gaps persist even in the largest models. Further analysis\nshows that our framework reliably reflects systematic EG/RG patterns in\nstate-of-the-art models. These findings offer concrete diagnostics and support\nthe development of more trustworthy mobile-use agents.",
      "authors": [
        "Lingzhong Dong",
        "Ziqi Zhou",
        "Shuaibo Yang",
        "Haiyue Sheng",
        "Pengzhou Cheng",
        "Zongru Wu",
        "Zheng Wu",
        "Gongshen Liu",
        "Zhuosheng Zhang"
      ],
      "published": "2025-10-02T16:51:19Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02204v1"
    },
    {
      "arxiv_id": "2510.02200v1",
      "title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge\n  Graph Exploration Utilities",
      "summary": "Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain.",
      "authors": [
        "Felix Brei",
        "Lorenz Bühmann",
        "Johannes Frey",
        "Daniel Gerber",
        "Lars-Peter Meyer",
        "Claus Stadler",
        "Kirill Bulert"
      ],
      "published": "2025-10-02T16:49:27Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02200v1"
    },
    {
      "arxiv_id": "2510.02190v1",
      "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
      "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.",
      "authors": [
        "Yang Yao",
        "Yixu Wang",
        "Yuxuan Zhang",
        "Yi Lu",
        "Tianle Gu",
        "Lingyu Li",
        "Dingyi Zhao",
        "Keming Wu",
        "Haozhe Wang",
        "Ping Nie",
        "Yan Teng",
        "Yingchun Wang"
      ],
      "published": "2025-10-02T16:40:02Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02190v1"
    },
    {
      "arxiv_id": "2510.02173v1",
      "title": "Learning to Reason for Hallucination Span Detection",
      "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
      "authors": [
        "Hsuan Su",
        "Ting-Yao Hu",
        "Hema Swetha Koppula",
        "Kundan Krishna",
        "Hadi Pouransari",
        "Cheng-Yu Hsieh",
        "Cem Koc",
        "Joseph Yitan Cheng",
        "Oncel Tuzel",
        "Raviteja Vemulapalli"
      ],
      "published": "2025-10-02T16:24:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02173v1"
    },
    {
      "arxiv_id": "2510.02172v1",
      "title": "RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with\n  Self-Penalization",
      "summary": "Reinforcement learning with human-annotated data has boosted chain-of-thought\nreasoning in large reasoning models, but these gains come at high costs in\nlabeled data while faltering on harder tasks. A natural next step is\nexperience-driven learning, where models improve without curated labels by\nadapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with\nSelf-restraint), a self-penalizing RL framework that converts the absence of\ngold labels into a useful learning signal. Instead of overcommitting to\nspurious majority votes, RESTRAIN exploits signals from the model's entire\nanswer distribution: penalizing overconfident rollouts and low-consistency\nexamples while preserving promising reasoning chains. The self-penalization\nmechanism integrates seamlessly into policy optimization methods such as GRPO,\nenabling continual self-improvement without supervision. On challenging\nreasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.\nWith Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to\n+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on\nGPQA-Diamond, nearly matching gold-label training while using no gold labels.\nThese results demonstrate that RESTRAIN establishes a scalable path toward\nstronger reasoning without gold labels.",
      "authors": [
        "Zhaoning Yu",
        "Will Su",
        "Leitian Tao",
        "Haozhu Wang",
        "Aashu Singh",
        "Hanchao Yu",
        "Jianyu Wang",
        "Hongyang Gao",
        "Weizhe Yuan",
        "Jason Weston",
        "Ping Yu",
        "Jing Xu"
      ],
      "published": "2025-10-02T16:24:01Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02172v1"
    },
    {
      "arxiv_id": "2510.02128v1",
      "title": "The Disparate Impacts of Speculative Decoding",
      "summary": "The practice of speculative decoding, whereby inference is probabilistically\nsupported by a smaller, cheaper, ``drafter'' model, has become a standard\ntechnique for systematically reducing the decoding time of large language\nmodels. This paper conducts an analysis of speculative decoding through the\nlens of its potential disparate speed-up rates across tasks. Crucially, the\npaper shows that speed-up gained from speculative decoding is not uniformly\ndistributed across tasks, consistently diminishing for under-fit, and often\nunderrepresented tasks. To better understand this phenomenon, we derive an\nanalysis to quantify this observed ``unfairness'' and draw attention to the\nfactors that motivate such disparate speed-ups to emerge. Further, guided by\nthese insights, the paper proposes a mitigation strategy designed to reduce\nspeed-up disparities and validates the approach across several model pairs,\nrevealing on average a 12% improvement in our fairness metric.",
      "authors": [
        "Jameson Sandler",
        "Ahmet Üstün",
        "Marco Romanelli",
        "Sara Hooker",
        "Ferdinando Fioretto"
      ],
      "published": "2025-10-02T15:38:57Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02128v1"
    },
    {
      "arxiv_id": "2510.02125v1",
      "title": "Do AI Models Perform Human-like Abstract Reasoning Across Modalities?",
      "summary": "OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI\nbenchmark, but does that mean state-of-the-art models recognize and reason with\nthe abstractions that the task creators intended? We investigate models'\nabstraction abilities on ConceptARC. We evaluate models under settings that\nvary the input modality (textual vs. visual), whether the model is permitted to\nuse external Python tools, and, for reasoning models, the amount of reasoning\neffort. In addition to measuring output accuracy, we perform fine-grained\nevaluation of the natural-language rules that models generate to explain their\nsolutions. This dual evaluation lets us assess whether models solve tasks using\nthe abstractions ConceptARC was designed to elicit, rather than relying on\nsurface-level patterns. Our results show that, while some models using\ntext-based representations match human output accuracy, the best models' rules\nare often based on surface-level ``shortcuts'' and capture intended\nabstractions far less often than humans. Thus their capabilities for general\nabstract reasoning may be overestimated by evaluations based on accuracy alone.\nIn the visual modality, AI models' output accuracy drops sharply, yet our\nrule-level analysis reveals that models might be underestimated, as they still\nexhibit a substantial share of rules that capture intended abstractions, but\nare often unable to correctly apply these rules. In short, our results show\nthat models still lag humans in abstract reasoning, and that using accuracy\nalone to evaluate abstract reasoning on ARC-like tasks may overestimate\nabstract-reasoning capabilities in textual modalities and underestimate it in\nvisual modalities. We believe that our evaluation framework offers a more\nfaithful picture of multimodal models' abstract reasoning abilities and a more\nprincipled way to track progress toward human-like, abstraction-centered\nintelligence.",
      "authors": [
        "Claas Beger",
        "Ryan Yi",
        "Shuhao Fu",
        "Arseny Moskvichev",
        "Sarah W. Tsai",
        "Sivasankaran Rajamanickam",
        "Melanie Mitchell"
      ],
      "published": "2025-10-02T15:35:10Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.02125v1"
    },
    {
      "arxiv_id": "2510.02066v1",
      "title": "Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken\n  Dialogue Systems",
      "summary": "Most end-to-end (E2E) spoken dialogue systems (SDS) rely on voice activity\ndetection (VAD) for turn-taking, but VAD fails to distinguish between pauses\nand turn completions. Duplex SDS models address this by predicting output\ncontinuously, including silence tokens, thus removing the need for explicit\nVAD. However, they often have complex dual-channel architecture and lag behind\ncascaded models in semantic reasoning. To overcome these challenges, we propose\nSCoT: a Streaming Chain-of-Thought (CoT) framework for Duplex SDS, alternating\nbetween processing fixed-duration user input and generating responses in a\nblockwise manner. Using frame-level alignments, we create intermediate\ntargets-aligned user transcripts and system responses for each block.\nExperiments show that our approach produces more coherent and interpretable\nresponses than existing duplex methods while supporting lower-latency and\noverlapping interactions compared to turn-by-turn systems.",
      "authors": [
        "Siddhant Arora",
        "Jinchuan Tian",
        "Hayato Futami",
        "Jiatong Shi",
        "Yosuke Kashiwagi",
        "Emiru Tsunoo",
        "Shinji Watanabe"
      ],
      "published": "2025-10-02T14:33:05Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02066v1"
    },
    {
      "arxiv_id": "2510.02044v1",
      "title": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming\n  Tool Usage",
      "summary": "End-to-end speech-in speech-out dialogue systems are emerging as a powerful\nalternative to traditional ASR-LLM-TTS pipelines, generating more natural,\nexpressive responses with significantly lower latency. However, these systems\nremain prone to hallucinations due to limited factual grounding. While\ntext-based dialogue systems address this challenge by integrating tools such as\nweb search and knowledge graph APIs, we introduce the first approach to extend\ntool use directly into speech-in speech-out systems. A key challenge is that\ntool integration substantially increases response latency, disrupting\nconversational flow. To mitigate this, we propose Streaming Retrieval-Augmented\nGeneration (Streaming RAG), a novel framework that reduces user-perceived\nlatency by predicting tool queries in parallel with user speech, even before\nthe user finishes speaking. Specifically, we develop a post-training pipeline\nthat teaches the model when to issue tool calls during ongoing speech and how\nto generate spoken summaries that fuse audio queries with retrieved text\nresults, thereby improving both accuracy and responsiveness. To evaluate our\napproach, we construct AudioCRAG, a benchmark created by converting queries\nfrom the publicly available CRAG dataset into speech form. Experimental results\ndemonstrate that our streaming RAG approach increases QA accuracy by up to 200%\nrelative (from 11.1% to 34.2% absolute) and further enhances user experience by\nreducing tool use latency by 20%. Importantly, our streaming RAG approach is\nmodality-agnostic and can be applied equally to typed input, paving the way for\nmore agentic, real-time AI assistants.",
      "authors": [
        "Siddhant Arora",
        "Haidar Khan",
        "Kai Sun",
        "Xin Luna Dong",
        "Sajal Choudhary",
        "Seungwhan Moon",
        "Xinyuan Zhang",
        "Adithya Sagar",
        "Surya Teja Appini",
        "Kaushik Patnaik",
        "Sanat Sharma",
        "Shinji Watanabe",
        "Anuj Kumar",
        "Ahmed Aly",
        "Yue Liu",
        "Florian Metze",
        "Zhaojiang Lin"
      ],
      "published": "2025-10-02T14:18:20Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02044v1"
    },
    {
      "arxiv_id": "2510.02025v1",
      "title": "Style Over Story: A Process-Oriented Study of Authorial Creativity in\n  Large Language Models",
      "summary": "Evaluations of large language models (LLMs)' creativity have focused\nprimarily on the quality of their outputs rather than the processes that shape\nthem. This study takes a process-oriented approach, drawing on narratology to\nexamine LLMs as computational authors. We introduce constraint-based\ndecision-making as a lens for authorial creativity. Using controlled prompting\nto assign authorial personas, we analyze the creative preferences of the\nmodels. Our findings show that LLMs consistently emphasize Style over other\nelements, including Character, Event, and Setting. By also probing the\nreasoning the models provide for their choices, we show that distinctive\nprofiles emerge across models and argue that our approach provides a novel\nsystematic tool for analyzing AI's authorial creativity.",
      "authors": [
        "Donghoon Jung",
        "Jiwoo Choi",
        "Songeun Chae",
        "Seohyon Jung"
      ],
      "published": "2025-10-02T13:57:14Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.02025v1"
    },
    {
      "arxiv_id": "2510.01995v1",
      "title": "LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and\n  Target",
      "summary": "Online social media platforms are central to everyday communication and\ninformation seeking. While these platforms serve positive purposes, they also\nprovide fertile ground for the spread of hate speech, offensive language, and\nbullying content targeting individuals, organizations, and communities. Such\ncontent undermines safety, participation, and equity online. Reliable detection\nsystems are therefore needed, especially for low-resource languages where\nmoderation tools are limited. In Bangla, prior work has contributed resources\nand models, but most are single-task (e.g., binary hate/offense) with limited\ncoverage of multi-facet signals (type, severity, target). We address these gaps\nby introducing the first multi-task Bangla hate-speech dataset,\nBanglaMultiHate, one of the largest manually annotated corpus to date. Building\non this resource, we conduct a comprehensive, controlled comparison spanning\nclassical baselines, monolingual pretrained models, and LLMs under zero-shot\nprompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a\nlow-resource setting and reveal a consistent trend: although LoRA-tuned LLMs\nare competitive with BanglaBERT, culturally and linguistically grounded\npretraining remains critical for robust performance. Together, our dataset and\nfindings establish a stronger benchmark for developing culturally aligned\nmoderation tools in low-resource contexts. For reproducibility, we will release\nthe dataset and all related scripts.",
      "authors": [
        "Md Arid Hasan",
        "Firoj Alam",
        "Md Fahad Hossain",
        "Usman Naseem",
        "Syed Ishtiaque Ahmed"
      ],
      "published": "2025-10-02T13:17:11Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01995v1"
    },
    {
      "arxiv_id": "2510.01989v1",
      "title": "Exploring Database Normalization Effects on SQL Generation",
      "summary": "Schema design, particularly normalization, is a critical yet often overlooked\nfactor in natural language to SQL (NL2SQL) systems. Most prior research\nevaluates models on fixed schemas, overlooking the influence of design on\nperformance. We present the first systematic study of schema normalization's\nimpact, evaluating eight leading large language models on synthetic and\nreal-world datasets with varied normalization levels. We construct controlled\nsynthetic datasets with formal normalization (1NF-3NF) and real academic paper\ndatasets with practical schemes. Our results show that denormalized schemas\noffer high accuracy on simple retrieval queries, even with cost-effective\nmodels in zero-shot settings. In contrast, normalized schemas (2NF/3NF)\nintroduce challenges such as errors in base table selection and join type\nprediction; however, these issues are substantially mitigated by providing\nfew-shot examples. For aggregation queries, normalized schemas yielded better\nperformance, mainly due to their robustness against the data duplication and\nNULL value issues that cause errors in denormalized schemas. These findings\nsuggest that the optimal schema design for NL2SQL applications depends on the\ntypes of queries to be supported. Our study demonstrates the importance of\nconsidering schema design when developing NL2SQL interfaces and integrating\nadaptive schema selection for real-world scenarios.",
      "authors": [
        "Ryosuke Kohita"
      ],
      "published": "2025-10-02T13:11:30Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01989v1"
    },
    {
      "arxiv_id": "2510.01976v1",
      "title": "Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion,\n  Argument, and Topic Annotations",
      "summary": "Our interpretation of value concepts is shaped by our sociocultural\nbackground and lived experiences, and is thus subjective. Recognizing\nindividual value interpretations is important for developing AI systems that\ncan align with diverse human perspectives and avoid bias toward majority\nviewpoints. To this end, we investigate whether a language model can predict\nindividual value interpretations by leveraging multi-dimensional subjective\nannotations as a proxy for their interpretive lens. That is, we evaluate\nwhether providing examples of how an individual annotates Sentiment, Emotion,\nArgument, and Topics (SEAT dimensions) helps a language model in predicting\ntheir value interpretations. Our experiment across different zero- and few-shot\nsettings demonstrates that providing all SEAT dimensions simultaneously yields\nsuperior performance compared to individual dimensions and a baseline where no\ninformation about the individual is provided. Furthermore, individual\nvariations across annotators highlight the importance of accounting for the\nincorporation of individual subjective annotators. To the best of our\nknowledge, this controlled setting, although small in size, is the first\nattempt to go beyond demographics and investigate the impact of annotation\nbehavior on value prediction, providing a solid foundation for future\nlarge-scale validation.",
      "authors": [
        "Adina Nicola Dobrinoiu",
        "Ana Cristiana Marcu",
        "Amir Homayounirad",
        "Luciano Cavalcante Siebert",
        "Enrico Liscio"
      ],
      "published": "2025-10-02T12:51:33Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01976v1"
    },
    {
      "arxiv_id": "2510.01932v1",
      "title": "Veri-R1: Toward Precise and Faithful Claim Verification via Online\n  Reinforcement Learning",
      "summary": "Claim verification with large language models (LLMs) has recently attracted\nconsiderable attention, owing to their superior reasoning capabilities and\ntransparent verification pathways compared to traditional answer-only\njudgments. Online claim verification requires iterative evidence retrieval and\nreasoning, yet existing approaches mainly rely on prompt engineering or\npredesigned reasoning workflows without offering a unified training paradigm to\nimprove necessary skills. Therefore, we introduce Veri-R1, an online\nreinforcement learning (RL) framework that enables an LLM to interact with a\nsearch engine and to receive reward signals that explicitly shape its planning,\nretrieval, and reasoning behaviors. The dynamic interaction between models and\nretrieval systems more accurately reflects real-world verification scenarios\nand fosters comprehensive verification skills. Empirical results show that\nVeri-R1 improves joint accuracy by up to 30% and doubles evidence score, often\nsurpassing larger-scale counterparts. Ablation studies further reveal the\nimpact of reward components and the link between output logits and label\naccuracy. Our results highlight the effectiveness of online RL for precise and\nfaithful claim verification and provide a foundation for future research. We\nrelease our code to support community progress in LLM empowered claim\nverification.",
      "authors": [
        "Qi He",
        "Cheng Qian",
        "Xiusi Chen",
        "Bingxiang He",
        "Yi R.",
        "Fung",
        "Heng Ji"
      ],
      "published": "2025-10-02T11:49:48Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01932v1"
    },
    {
      "arxiv_id": "2510.01929v1",
      "title": "Inverse Language Modeling towards Robust and Grounded LLMs",
      "summary": "The current landscape of defensive mechanisms for LLMs is fragmented and\nunderdeveloped, unlike prior work on classifiers. To further promote\nadversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a\nunified framework that simultaneously 1) improves the robustness of LLMs to\ninput perturbations, and, at the same time, 2) enables native grounding by\ninverting model outputs to identify potentially toxic or unsafe input triggers.\nILM transforms LLMs from static generators into analyzable and robust systems,\npotentially helping RED teaming. ILM can lay the foundation for next-generation\nLLMs that are not only robust and grounded but also fundamentally more\ncontrollable and trustworthy. The code is publicly available at\ngithub.com/davegabe/pag-llm.",
      "authors": [
        "Davide Gabrielli",
        "Simone Sestito",
        "Iacopo Masi"
      ],
      "published": "2025-10-02T11:47:18Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01929v1"
    },
    {
      "arxiv_id": "2510.01925v1",
      "title": "Enhancing Large Language Model Reasoning with Reward Models: An\n  Analytical Survey",
      "summary": "Reward models (RMs) play a critical role in enhancing the reasoning\nperformance of LLMs. For example, they can provide training signals to finetune\nLLMs during reinforcement learning (RL) and help select the best answer from\nmultiple candidates during inference. In this paper, we provide a systematic\nintroduction to RMs, along with a comprehensive survey of their applications in\nLLM reasoning. We first review fundamental concepts of RMs, including their\narchitectures, training methodologies, and evaluation techniques. Then, we\nexplore their key applications: (1) guiding generation and selecting optimal\noutputs during LLM inference, (2) facilitating data synthesis and iterative\nself-improvement for LLMs, and (3) providing training signals in RL-based\nfinetuning. Finally, we address critical open questions regarding the\nselection, generalization, evaluation, and enhancement of RMs, based on\nexisting research and our own empirical findings. Our analysis aims to provide\nactionable insights for the effective deployment and advancement of RMs for LLM\nreasoning.",
      "authors": [
        "Qiyuan Liu",
        "Hao Xu",
        "Xuhong Chen",
        "Wei Chen",
        "Yee Whye Teh",
        "Ning Miao"
      ],
      "published": "2025-10-02T11:42:17Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01925v1"
    },
    {
      "arxiv_id": "2510.01902v1",
      "title": "Constrained Adaptive Rejection Sampling",
      "summary": "Language Models (LMs) are increasingly used in applications where generated\noutputs must satisfy strict semantic or syntactic constraints. Existing\napproaches to constrained generation fall along a spectrum: greedy constrained\ndecoding methods enforce validity during decoding but distort the LM's\ndistribution, while rejection sampling (RS) preserves fidelity but wastes\ncomputation by discarding invalid outputs. Both extremes are problematic in\ndomains such as program fuzzing, where both validity and diversity of samples\nare essential. We present Constrained Adaptive Rejection Sampling (CARS), an\napproach that strictly improves the sample-efficiency of RS without\ndistributional distortion. CARS begins with unconstrained LM sampling and\nadaptively rules out constraint-violating continuations by recording them in a\ntrie and subtracting their probability mass from future draws. This adaptive\npruning ensures that prefixes proven invalid are never revisited, acceptance\nrates improve monotonically, and the resulting samples exactly follow the\nconstrained distribution. In experiments on a variety of domains -- e.g.,\nprogram fuzzing and molecular generation -- CARS consistently achieves higher\nefficiency -- measured in the number of LM forward passes per valid sample --\nwhile also producing stronger sample diversity than both GCD and methods that\napproximate the LM's distribution.",
      "authors": [
        "Paweł Parys",
        "Sairam Vaidya",
        "Taylor Berg-Kirkpatrick",
        "Loris D'Antoni"
      ],
      "published": "2025-10-02T11:17:26Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01902v1"
    },
    {
      "arxiv_id": "2510.01879v1",
      "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration",
      "summary": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs.",
      "authors": [
        "Yisu Wang",
        "Ming Wang",
        "Haoyuan Song",
        "Wenjie Huang",
        "Chaozheng Wang",
        "Yi Xie",
        "Xuming Ran"
      ],
      "published": "2025-10-02T10:35:39Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01879v1"
    },
    {
      "arxiv_id": "2510.01845v1",
      "title": "Model Merging to Maintain Language-Only Performance in Developmentally\n  Plausible Multimodal Models",
      "summary": "State-of-the-art vision-and-language models consist of many parameters and\nlearn from enormous datasets, surpassing the amounts of linguistic data that\nchildren are exposed to as they acquire a language. This paper presents our\napproach to the multimodal track of the BabyLM challenge addressing this\ndiscrepancy. We develop language-only and multimodal models in low-resource\nsettings using developmentally plausible datasets, with our multimodal models\noutperforming previous BabyLM baselines. One finding in the multimodal language\nmodel literature is that these models tend to underperform in\n\\textit{language-only} tasks. Therefore, we focus on maintaining language-only\nabilities in multimodal models. To this end, we experiment with \\textit{model\nmerging}, where we fuse the parameters of multimodal models with those of\nlanguage-only models using weighted linear interpolation. Our results\ncorroborate the findings that multimodal models underperform in language-only\nbenchmarks that focus on grammar, and model merging with text-only models can\nhelp alleviate this problem to some extent, while maintaining multimodal\nperformance.",
      "authors": [
        "Ece Takmaz",
        "Lisa Bylinina",
        "Jakub Dotlacil"
      ],
      "published": "2025-10-02T09:38:25Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01845v1"
    },
    {
      "arxiv_id": "2510.01833v1",
      "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for\n  LLM Reasoning",
      "summary": "Large language models (LLMs) have demonstrated remarkable reasoning abilities\nin complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,\ndue to their autoregressive token-level generation, the reasoning process is\nlargely constrained to local decision-making and lacks global planning. This\nlimitation frequently results in redundant, incoherent, or inaccurate\nreasoning, which significantly degrades overall performance. Existing\napproaches, such as tree-based algorithms and reinforcement learning (RL),\nattempt to address this issue but suffer from high computational costs and\noften fail to produce optimal reasoning trajectories. To tackle this challenge,\nwe propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy\nOptimization PTA-GRPO, a two-stage framework designed to improve both\nhigh-level planning and fine-grained CoT reasoning. In the first stage, we\nleverage advanced LLMs to distill CoT into compact high-level guidance, which\nis then used for supervised fine-tuning (SFT). In the second stage, we\nintroduce a guidance-aware RL method that jointly optimizes the final output\nand the quality of high-level guidance, thereby enhancing reasoning\neffectiveness. We conduct extensive experiments on multiple mathematical\nreasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across\ndiverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and\nLLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently\nachieves stable and significant improvements across different models and tasks,\nvalidating its effectiveness and generalization.",
      "authors": [
        "Zhihao Dou",
        "Qinjian Zhao",
        "Zhongwei Wan",
        "Dinggen Zhang",
        "Weida Wang",
        "Towsif Raiyan",
        "Benteng Chen",
        "Qingtao Pan",
        "Yang Ouyang",
        "Zhiqiang Gao",
        "Shufei Zhang",
        "Sumon Biswas"
      ],
      "published": "2025-10-02T09:28:13Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01833v1"
    },
    {
      "arxiv_id": "2510.01832v1",
      "title": "SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with\n  Reinforcement Learning",
      "summary": "Semi-structured content in HTML tables, lists, and infoboxes accounts for a\nsubstantial share of factual data on the web, yet the formatting complicates\nusage, and reliably extracting structured information from them remains\nchallenging. Existing methods either lack generalization or are\nresource-intensive due to per-page LLM inference. In this paper, we introduce\nSCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novel\nreinforcement learning framework that leverages layout similarity across\nwebpages within the same site as a reward signal. Instead of processing each\npage individually, SCRIBES generates reusable extraction scripts that can be\napplied to groups of structurally similar webpages. Our approach further\nimproves by iteratively training on synthetic annotations from in-the-wild\nCommonCrawl data. Experiments show that our approach outperforms strong\nbaselines by over 13% in script quality and boosts downstream question\nanswering accuracy by more than 4% for GPT-4o, enabling scalable and\nresource-efficient web information extraction.",
      "authors": [
        "Shicheng Liu",
        "Kai Sun",
        "Lisheng Fu",
        "Xilun Chen",
        "Xinyuan Zhang",
        "Zhaojiang Lin",
        "Rulin Shao",
        "Yue Liu",
        "Anuj Kumar",
        "Wen-tau Yih",
        "Xin Luna Dong"
      ],
      "published": "2025-10-02T09:27:15Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01832v1"
    },
    {
      "arxiv_id": "2510.01831v1",
      "title": "Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical\n  Errors",
      "summary": "Large Language Models (LLMs) demonstrate strong mathematical problem-solving\nabilities but frequently fail on problems that deviate syntactically from their\ntraining distribution. We identify a systematic failure mode, syntactic blind\nspots, in which models misapply familiar reasoning strategies to problems that\nare semantically straightforward but phrased in unfamiliar ways. These errors\nare not due to gaps in mathematical competence, but rather reflect a brittle\ncoupling between surface form and internal representation. To test this, we\nrephrase incorrectly answered questions using syntactic templates drawn from\ncorrect examples. These rephrasings, which preserve semantics while reducing\nstructural complexity, often lead to correct answers. We quantify syntactic\ncomplexity using a metric based on Dependency Locality Theory (DLT), and show\nthat higher DLT scores are associated with increased failure rates across\nmultiple datasets. Our findings suggest that many reasoning errors stem from\nstructural misalignment rather than conceptual difficulty, and that\nsyntax-aware interventions can reveal and mitigate these inductive failures.",
      "authors": [
        "Dane Williamson",
        "Yangfeng Ji",
        "Matthew Dwyer"
      ],
      "published": "2025-10-02T09:26:26Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01831v1"
    },
    {
      "arxiv_id": "2510.01817v1",
      "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention\n  Mechanism with Query Heads Reduction",
      "summary": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA)\nmechanism, has become the de facto standard for state-of-the-art models in\nartificial intelligence. However, the quadratic computational complexity of MHA\nwith respect to sequence length presents a significant barrier to scaling,\nparticularly for applications involving long contexts. Prevailing solutions,\nsuch as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have\neffectively addressed the memory bandwidth bottleneck that dominates\nautoregressive inference latency by sharing Key and Value projections. While\nhighly successful, these methods do not reduce the fundamental number of\nfloating-point operations (FLOPs) required for the attention score computation,\nwhich remains a critical bottleneck for training and full-sequence processing.\nThis paper introduces Sparse Query Attention (SQA), a novel attention\narchitecture that pursues an alternative and complementary optimization path.\nInstead of reducing Key/Value heads, SQA reduces the number of Query heads.\nThis architectural modification directly decreases the computational complexity\nof the attention mechanism by a factor proportional to the reduction in query\nheads, thereby lowering the overall FLOPs. This work presents the theoretical\nfoundation of SQA, its mathematical formulation, and a family of architectural\nvariants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate\nthat SQA can achieve significant throughput improvements of up to 3x in\ncomputation-bound scenarios such as model pre-training, fine-tuning, and\nencoder-based tasks, with only a minimal impact on model quality in preliminary\nsmallscale experiments. SQA was discovered serendipitously during the\ndevelopment of the upcoming Reactive Transformer architecture, suggesting its\npotential as a powerful tool for building more efficient and scalable models",
      "authors": [
        "Adam Filipek"
      ],
      "published": "2025-10-02T09:01:38Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01817v1"
    },
    {
      "arxiv_id": "2510.01801v1",
      "title": "Detecting LLM-Generated Spam Reviews by Integrating Language Model\n  Embeddings and Graph Neural Network",
      "summary": "The rise of large language models (LLMs) has enabled the generation of highly\npersuasive spam reviews that closely mimic human writing. These reviews pose\nsignificant challenges for existing detection systems and threaten the\ncredibility of online platforms. In this work, we first create three realistic\nLLM-generated spam review datasets using three distinct LLMs, each guided by\nproduct metadata and genuine reference reviews. Evaluations by GPT-4.1 confirm\nthe high persuasion and deceptive potential of these reviews. To address this\nthreat, we propose FraudSquad, a hybrid detection model that integrates text\nembeddings from a pre-trained language model with a gated graph transformer for\nspam node classification. FraudSquad captures both semantic and behavioral\nsignals without relying on manual feature engineering or massive training\nresources. Experiments show that FraudSquad outperforms state-of-the-art\nbaselines by up to 44.22% in precision and 43.01% in recall on three\nLLM-generated datasets, while also achieving promising results on two\nhuman-written spam datasets. Furthermore, FraudSquad maintains a modest model\nsize and requires minimal labeled training data, making it a practical solution\nfor real-world applications. Our contributions include new synthetic datasets,\na practical detection framework, and empirical evidence highlighting the\nurgency of adapting spam detection to the LLM era. Our code and datasets are\navailable at: https://anonymous.4open.science/r/FraudSquad-5389/.",
      "authors": [
        "Xin Liu",
        "Rongwu Xu",
        "Xinyi Jia",
        "Jason Liao",
        "Jiao Sun",
        "Ling Huang",
        "Wei Xu"
      ],
      "published": "2025-10-02T08:42:35Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01801v1"
    },
    {
      "arxiv_id": "2510.01792v1",
      "title": "Comparison of Unsupervised Metrics for Evaluating Judicial Decision\n  Extraction",
      "summary": "The rapid advancement of artificial intelligence in legal natural language\nprocessing demands scalable methods for evaluating text extraction from\njudicial decisions. This study evaluates 16 unsupervised metrics, including\nnovel formulations, to assess the quality of extracting seven semantic blocks\nfrom 1,000 anonymized Russian judicial decisions, validated against 7,168\nexpert reviews on a 1--5 Likert scale. These metrics, spanning document-based,\nsemantic, structural, pseudo-ground truth, and legal-specific categories,\noperate without pre-annotated ground truth. Bootstrapped correlations, Lin's\nconcordance correlation coefficient (CCC), and mean absolute error (MAE) reveal\nthat Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =\n0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =\n0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density\n(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative\ncorrelations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin\nCCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using\ngpt-4.1-mini via g4f, suggests limited specialization for legal textse. These\nfindings highlight that unsupervised metrics, including LLM-based approaches,\nenable scalable screening but, with moderate correlations and low CCC values,\ncannot fully replace human judgment in high-stakes legal contexts. This work\nadvances legal NLP by providing annotation-free evaluation tools, with\nimplications for judicial analytics and ethical AI deployment.",
      "authors": [
        "Ivan Leonidovich Litvak",
        "Anton Kostin",
        "Fedor Lashkin",
        "Tatiana Maksiyan",
        "Sergey Lagutin"
      ],
      "published": "2025-10-02T08:32:16Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01792v1"
    },
    {
      "arxiv_id": "2510.01782v1",
      "title": "Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware\n  Refusal in Factual Tasks",
      "summary": "Large Language Models (LLMs) should refuse to answer questions beyond their\nknowledge. This capability, which we term knowledge-aware refusal, is crucial\nfor factual reliability. However, existing metrics fail to faithfully measure\nthis ability. On the one hand, simple refusal-based metrics are biased by\nrefusal rates and yield inconsistent scores when models exhibit different\nrefusal tendencies. On the other hand, existing calibration metrics are\nproxy-based, capturing the performance of auxiliary calibration processes\nrather than the model's actual refusal behavior. In this work, we propose the\nRefusal Index (RI), a principled metric that measures how accurately LLMs\nrefuse questions they do not know. We define RI as Spearman's rank correlation\nbetween refusal probability and error probability. To make RI practically\nmeasurable, we design a lightweight two-pass evaluation method that efficiently\nestimates RI from observed refusal rates across two standard evaluation runs.\nExtensive experiments across 16 models and 5 datasets demonstrate that RI\naccurately quantifies a model's intrinsic knowledge-aware refusal capability in\nfactual tasks. Notably, RI remains stable across different refusal rates and\nprovides consistent model rankings independent of a model's overall accuracy\nand refusal rates. More importantly, RI provides insight into an important but\npreviously overlooked aspect of LLM factuality: while LLMs achieve high\naccuracy on factual tasks, their refusal behavior can be unreliable and\nfragile. This finding highlights the need to complement traditional accuracy\nmetrics with the Refusal Index for comprehensive factuality evaluation.",
      "authors": [
        "Wenbo Pan",
        "Jie Xu",
        "Qiguang Chen",
        "Junhao Dong",
        "Libo Qin",
        "Xinfeng Li",
        "Haining Yu",
        "Xiaohua Jia"
      ],
      "published": "2025-10-02T08:20:36Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01782v1"
    },
    {
      "arxiv_id": "2510.01736v1",
      "title": "Machine-interpretable Engineering Design Standards for Valve\n  Specification",
      "summary": "Engineering design processes use technical specifications and must comply\nwith standards. Product specifications, product type data sheets, and design\nstandards are still mainly document-centric despite the ambition to digitalize\nindustrial work. In this paper, we demonstrate how to transform information\nheld in engineering design standards into modular, reusable,\nmachine-interpretable ontologies and use the ontologies in quality assurance of\nthe plant design and equipment selection process. We use modelling patterns to\ncreate modular ontologies for knowledge captured in the text and in frequently\nreferenced tables in International Standards for piping, material and valve\ndesign. These modules are exchangeable, as stored in a W3C compliant format,\nand interoperable as they are aligned with the top-level ontology ISO DIS\n23726-3: Industrial Data Ontology (IDO).\n  We test these ontologies, created based on international material and piping\nstandards and industry norms, on a valve selection process. Valves are\ninstantiated in semantic asset models as individuals along with a semantic\nrepresentation of the environmental condition at their location on the asset.\nWe create \"functional location tags\" as OWL individuals that become instances\nof OWL class Valve Data Sheet (VDS) specified valves. Similarly we create\ninstances of manufacturer product type. Our approach enables automated\nvalidation that a specific VDS is compliant with relevant industry standards.\nUsing semantic reasoning and executable design rules, we also determine whether\nthe product type meets the valve specification. Creation of shared, reusable\nIDO-based modular ontologies for design standards enables semantic reasoning to\nbe applied to equipment selection processes and demonstrates the potential of\nthis approach for Standards Bodies wanting to transition to digitized Smart\nStandards.",
      "authors": [
        "Anders Gjerver",
        "Rune Frostad",
        "Vedrana Barisic",
        "Melinda Hodkiewicz",
        "Caitlin Woods",
        "Mihaly Fekete",
        "Arild Braathen Torjusen",
        "Johan Wilhelm Kluwer"
      ],
      "published": "2025-10-02T07:20:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01736v1"
    },
    {
      "arxiv_id": "2510.01719v1",
      "title": "What MLLMs Learn about When they Learn about Multimodal Reasoning:\n  Perception, Reasoning, or their Integration?",
      "summary": "Multimodal reasoning models have recently shown promise on challenging\ndomains such as olympiad-level geometry, yet their evaluation remains dominated\nby aggregate accuracy, a single score that obscures where and how models are\nimproving. We introduce MathLens, a benchmark designed to disentangle the\nsubskills of multimodal reasoning while preserving the complexity of\ntextbook-style geometry problems. The benchmark separates performance into\nthree components: Perception: extracting information from raw inputs,\nReasoning: operating on available information, and Integration: selecting\nrelevant perceptual evidence and applying it within reasoning. To support each\ntest, we provide annotations: visual diagrams, textual descriptions to evaluate\nreasoning in isolation, controlled questions that require both modalities, and\nprobes for fine-grained perceptual skills, all derived from symbolic\nspecifications of the problems to ensure consistency and robustness. Our\nanalysis reveals that different training approaches have uneven effects: First,\nreinforcement learning chiefly strengthens perception, especially when\nsupported by textual supervision, while textual SFT indirectly improves\nperception through reflective reasoning. Second, reasoning improves only in\ntandem with perception. Third, integration remains the weakest capacity, with\nresidual errors concentrated there once other skills advance. Finally,\nrobustness diverges: RL improves consistency under diagram variation, whereas\nmultimodal SFT reduces it through overfitting. We will release all data and\nexperimental logs.",
      "authors": [
        "Jiwan Chung",
        "Neel Joshi",
        "Pratyusha Sharma",
        "Youngjae Yu",
        "Vibhav Vineet"
      ],
      "published": "2025-10-02T06:58:29Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01719v1"
    },
    {
      "arxiv_id": "2510.01688v1",
      "title": "Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation",
      "summary": "Recent advances in Large Language Models (LLMs) have brought significant\nimprovements to various service domains, including chatbots and medical\npre-consultation applications. In the healthcare domain, the most common\napproach for adapting LLMs to multi-turn dialogue generation is Supervised\nFine-Tuning (SFT). However, datasets for SFT in tasks like medical\npre-consultation typically exhibit a skewed turn-count distribution. Training\non such data induces a novel failure mechanism we term **Format Inertia**,\nwhere models tend to generate repetitive, format-correct, but diagnostically\nuninformative questions in long medical dialogues. To mitigate this observed\nfailure mechanism, we adopt a simple, data-centric method that rebalances the\nturn-count distribution of the training dataset. Experimental results show that\nour approach substantially alleviates Format Inertia in medical\npre-consultation.",
      "authors": [
        "Seungseop Lim",
        "Gibaeg Kim",
        "Wooseok Han",
        "Jean Seo",
        "Hyunkyung Lee",
        "Jaehyo Yoo",
        "Eunho Yang"
      ],
      "published": "2025-10-02T05:29:38Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01688v1"
    },
    {
      "arxiv_id": "2510.01687v1",
      "title": "Improving AGI Evaluation: A Data Science Perspective",
      "summary": "Evaluation of potential AGI systems and methods is difficult due to the\nbreadth of the engineering goal. We have no methods for perfect evaluation of\nthe end state, and instead measure performance on small tests designed to\nprovide directional indication that we are approaching AGI. In this work we\nargue that AGI evaluation methods have been dominated by a design philosophy\nthat uses our intuitions of what intelligence is to create synthetic tasks,\nthat have performed poorly in the history of AI. Instead we argue for an\nalternative design philosophy focused on evaluating robust task execution that\nseeks to demonstrate AGI through competence. This perspective is developed from\ncommon practices in data science that are used to show that a system can be\nreliably deployed. We provide practical examples of what this would mean for\nAGI evaluation.",
      "authors": [
        "John Hawkins"
      ],
      "published": "2025-10-02T05:27:29Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01687v1"
    },
    {
      "arxiv_id": "2510.01685v1",
      "title": "How Do Language Models Compose Functions?",
      "summary": "While large language models (LLMs) appear to be increasingly capable of\nsolving compositional tasks, it is an open question whether they do so using\ncompositional mechanisms. In this work, we investigate how feedforward LLMs\nsolve two-hop factual recall tasks, which can be expressed compositionally as\n$g(f(x))$. We first confirm that modern LLMs continue to suffer from the\n\"compositionality gap\": i.e. their ability to compute both $z = f(x)$ and $y =\ng(z)$ does not entail their ability to compute the composition $y = g(f(x))$.\nThen, using logit lens on their residual stream activations, we identify two\nprocessing mechanisms, one which solves tasks $\\textit{compositionally}$,\ncomputing $f(x)$ along the way to computing $g(f(x))$, and one which solves\nthem $\\textit{directly}$, without any detectable signature of the intermediate\nvariable $f(x)$. Finally, we find that which mechanism is employed appears to\nbe related to the embedding space geometry, with the idiomatic mechanism being\ndominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in\nthe embedding spaces. We fully release our data and code at:\nhttps://github.com/apoorvkh/composing-functions .",
      "authors": [
        "Apoorv Khandelwal",
        "Ellie Pavlick"
      ],
      "published": "2025-10-02T05:21:34Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01685v1"
    },
    {
      "arxiv_id": "2510.01674v1",
      "title": "FOR-Prompting: From Objection to Revision via an Asymmetric Prompting\n  Protocol",
      "summary": "Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)\norganize internal deliberation but lack an explicit mechanism for external\nquestioning that elicits self-revision. We present FOR-Prompting (From\nObjection to Revision Prompting), an asymmetric protocol where a Defender\nproposes an answer, an Objectioner raises question-style objections with no\ndirect fixes, and a Host enforces consistency and closure. On GSM8K we observe\nabout a 22% point gain over single-prompt and accuracy on par with CoT, with\nmore than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1\njudge. FOR-Prompting also corrects mistakes without tools or human supervision\non tricky queries, and improves performance for small-scale model (approx. 19%\naccuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for\nsmall models and on personal device use. Beyond factual QA, qualitative\nanalyses on open-ended tasks show enhanced exploration and refinement, with\ndialogue traces that make assumptions and trade-offs explicit. The protocol is\nmodel agnostic and operates purely at the prompt level through role-structured\nturns, so it works with hosted and local models of different sizes without\nretraining, and it supports large-scale study of objection-guided reasoning.",
      "authors": [
        "He Zhang",
        "Anzhou Zhang",
        "Jian Dai"
      ],
      "published": "2025-10-02T04:57:58Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01674v1"
    },
    {
      "arxiv_id": "2510.01670v1",
      "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
      "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.",
      "authors": [
        "Erfan Shayegani",
        "Keegan Hines",
        "Yue Dong",
        "Nael Abu-Ghazaleh",
        "Roman Lutz",
        "Spencer Whitehead",
        "Vidhisha Balachandran",
        "Besmira Nushi",
        "Vibhav Vineet"
      ],
      "published": "2025-10-02T04:52:15Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01670v1"
    },
    {
      "arxiv_id": "2510.01659v1",
      "title": "MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue\n  Summarization",
      "summary": "Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging\napplications. To support the development of effective MDS models, robust\nautomatic evaluation methods are essential for reducing both cost and human\neffort. However, such methods require a strong meta-evaluation benchmark\ngrounded in human annotations. In this work, we introduce MDSEval, the first\nmeta-evaluation benchmark for MDS, consisting image-sharing dialogues,\ncorresponding summaries, and human judgments across eight well-defined quality\naspects. To ensure data quality and richfulness, we propose a novel filtering\nframework leveraging Mutually Exclusive Key Information (MEKI) across\nmodalities. Our work is the first to identify and formalize key evaluation\ndimensions specific to MDS. We benchmark state-of-the-art modal evaluation\nmethods, revealing their limitations in distinguishing summaries from advanced\nMLLMs and their susceptibility to various bias.",
      "authors": [
        "Yinhong Liu",
        "Jianfeng He",
        "Hang Su",
        "Ruixue Lian",
        "Yi Nian",
        "Jake Vincent",
        "Srikanth Vishnubhotla",
        "Robinson Piramuthu",
        "Saab Mansour"
      ],
      "published": "2025-10-02T04:38:27Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01659v1"
    },
    {
      "arxiv_id": "2510.01654v1",
      "title": "SoK: Measuring What Matters for Closed-Loop Security Agents",
      "summary": "Cybersecurity is a relentless arms race, with AI driven offensive systems\nevolving faster than traditional defenses can adapt. Research and tooling\nremain fragmented across isolated defensive functions, creating blind spots\nthat adversaries exploit. Autonomous agents capable of integrating, exploit\nconfirmation, remediation, and validation into a single closed loop offer\npromise, but the field lacks three essentials: a framework defining the agentic\ncapabilities of security systems across security life cycle, a principled\nmethod for evaluating closed loop agents, and a benchmark for measuring their\nperformance in practice. We introduce CLASP: the Closed-Loop Autonomous\nSecurity Performance framework which aligns the security lifecycle\n(reconnaissance, exploitation, root cause analysis, patch synthesis,\nvalidation) with core agentic capabilities (planning, tool use, memory,\nreasoning, reflection & perception) providing a common vocabulary and rubric\nfor assessing agentic capabilities in security tasks. By applying CLASP to 21\nrepresentative works, we map where systems demonstrate strengths, and where\ncapability gaps persist. We then define the Closed-Loop Capability (CLC) Score,\na composite metric quantifying both degree of loop closure and operational\neffectiveness, and outline the requirements for a closed loop benchmark.\nTogether, CLASP and the CLC Score, provide the vocabulary, diagnostics, and\nmeasurements needed to advance both function level performance and measure\nclosed loop security agents.",
      "authors": [
        "Mudita Khurana",
        "Raunak Jain"
      ],
      "published": "2025-10-02T04:20:35Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01654v1"
    },
    {
      "arxiv_id": "2510.01652v1",
      "title": "Learning to Look at the Other Side: A Semantic Probing Study of Word\n  Embeddings in LLMs with Enabled Bidirectional Attention",
      "summary": "Autoregressive Large Language Models (LLMs) demonstrate exceptional\nperformance in language understanding and generation. However, their\napplication in text embedding tasks has been relatively slow, along with the\nanalysis of their semantic representation in probing tasks, due to the\nconstraints of the unidirectional attention mechanism.\n  This paper aims to explore whether such constraints can be overcome by\nenabling bidirectional attention in LLMs. We tested different variants of the\nLlama architecture through additional training steps, progressively enabling\nbidirectional attention and unsupervised/supervised contrastive learning.",
      "authors": [
        "Zhaoxin Feng",
        "Jianfei Ma",
        "Emmanuele Chersoni",
        "Xiaojing Zhao",
        "Xiaoyi Bao"
      ],
      "published": "2025-10-02T04:18:13Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01652v1"
    },
    {
      "arxiv_id": "2510.01645v1",
      "title": "Position: Privacy Is Not Just Memorization!",
      "summary": "The discourse on privacy risks in Large Language Models (LLMs) has\ndisproportionately focused on verbatim memorization of training data, while a\nconstellation of more immediate and scalable privacy threats remain\nunderexplored. This position paper argues that the privacy landscape of LLM\nsystems extends far beyond training data extraction, encompassing risks from\ndata collection practices, inference-time context leakage, autonomous agent\ncapabilities, and the democratization of surveillance through deep inference\nattacks. We present a comprehensive taxonomy of privacy risks across the LLM\nlifecycle -- from data collection through deployment -- and demonstrate through\ncase studies how current privacy frameworks fail to address these multifaceted\nthreats. Through a longitudinal analysis of 1,322 AI/ML privacy papers\npublished at leading conferences over the past decade (2016--2025), we reveal\nthat while memorization receives outsized attention in technical research, the\nmost pressing privacy harms lie elsewhere, where current technical approaches\noffer little traction and viable paths forward remain unclear. We call for a\nfundamental shift in how the research community approaches LLM privacy, moving\nbeyond the narrow focus of current technical solutions and embracing\ninterdisciplinary approaches that address the sociotechnical nature of these\nemerging threats.",
      "authors": [
        "Niloofar Mireshghallah",
        "Tianshi Li"
      ],
      "published": "2025-10-02T04:02:06Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01645v1"
    },
    {
      "arxiv_id": "2510.01644v1",
      "title": "NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with\n  BERT",
      "summary": "Large Language Models (LLMs) suffer from a range of vulnerabilities that\nallow malicious users to solicit undesirable responses through manipulation of\nthe input text. These so-called jailbreak prompts are designed to trick the LLM\ninto circumventing the safety guardrails put in place to keep responses\nacceptable to the developer's policies. In this study, we analyse the ability\nof different machine learning models to distinguish jailbreak prompts from\ngenuine uses, including looking at our ability to identify jailbreaks that use\npreviously unseen strategies. Our results indicate that using current datasets\nthe best performance is achieved by fine tuning a Bidirectional Encoder\nRepresentations from Transformers (BERT) model end-to-end for identifying\njailbreaks. We visualise the keywords that distinguish jailbreak from genuine\nprompts and conclude that explicit reflexivity in prompt structure could be a\nsignal of jailbreak intention.",
      "authors": [
        "John Hawkins",
        "Aditya Pramar",
        "Rodney Beard",
        "Rohitash Chandra"
      ],
      "published": "2025-10-02T03:55:29Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01644v1"
    },
    {
      "arxiv_id": "2510.01631v1",
      "title": "Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of\n  Scaling Laws, Benefits, and Pitfalls",
      "summary": "Training data plays a crucial role in Large Language Models (LLM) scaling,\nyet high quality data is of limited supply. Synthetic data techniques offer a\npotential path toward sidestepping these limitations. We conduct a large-scale\nempirical investigation (>1000 LLMs with >100k GPU hours) using a unified\nprotocol and scaling laws, comparing natural web data, diverse synthetic types\n(rephrased text, generated textbooks), and mixtures of natural and synthetic\ndata. Specifically, we found pre-training on rephrased synthetic data\n\\textit{alone} is not faster than pre-training on natural web texts; while\npre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts\ncan speed up 5-10x (to reach the same validation loss) at larger data budgets.\nPre-training on textbook-style synthetic data \\textit{alone} results in notably\nhigher loss on many downstream domains especially at small data budgets. \"Good\"\nratios of synthetic data in training data mixtures depend on the model size and\ndata budget, empirically converging to ~30% for rephrased synthetic data.\nLarger generator models do not necessarily yield better pre-training data than\n~8B-param models. These results contribute mixed evidence on \"model collapse\"\nduring large-scale single-round (n=1) model training on synthetic\ndata--training on rephrased synthetic data shows no degradation in performance\nin foreseeable scales whereas training on mixtures of textbook-style\npure-generated synthetic data shows patterns predicted by \"model collapse\". Our\nwork demystifies synthetic data in pre-training, validates its conditional\nbenefits, and offers practical guidance.",
      "authors": [
        "Feiyang Kang",
        "Newsha Ardalani",
        "Michael Kuchnik",
        "Youssef Emad",
        "Mostafa Elhoushi",
        "Shubhabrata Sengupta",
        "Shang-Wen Li",
        "Ramya Raghavendra",
        "Ruoxi Jia",
        "Carole-Jean Wu"
      ],
      "published": "2025-10-02T03:24:42Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01631v1"
    },
    {
      "arxiv_id": "2510.01624v1",
      "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What\n  to Use Instead",
      "summary": "In post-training for reasoning Large Language Models (LLMs), the current\nstate of practice trains LLMs in two independent stages: Supervised Fine-Tuning\n(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as\n``RL'' below). In this work, we challenge whether high SFT scores translate to\nimproved performance after RL. We provide extensive counter-examples where this\nis not true. We find high SFT scores can be biased toward simpler or more\nhomogeneous data and are not reliably predictive of subsequent RL gains or\nscaled-up post-training effectiveness. In some cases, RL training on models\nwith improved SFT performance could lead to substantially worse outcome\ncompared to RL on the base model without SFT. We study alternative metrics and\nidentify generalization loss on held-out reasoning examples and Pass@large k\nperformance to provide strong proxies for the RL outcome. We trained hundreds\nof models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive\nevaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU\nhours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple\nstate-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL\nperformance, prediction based on generalization loss and Pass@large k achieves\nsubstantial higher precision, improving $R^2$ coefficient and Spearman's rank\ncorrelation coefficient by up to 0.5 (2x). This provides strong utility for\nbroad use cases. For example, in most experiments, we find SFT training on\nunique examples for a one epoch underperforms training on half examples for two\nepochs, either after SFT or SFT-then-RL; With the same SFT budget, training\nonly on short examples may lead to better SFT performance, though, it often\nleads to worse outcome after RL compared to training on examples with varying\nlengths. Evaluation tool will be open-sourced.",
      "authors": [
        "Feiyang Kang",
        "Michael Kuchnik",
        "Karthik Padthe",
        "Marin Vlastelica",
        "Ruoxi Jia",
        "Carole-Jean Wu",
        "Newsha Ardalani"
      ],
      "published": "2025-10-02T02:57:00Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01624v1"
    },
    {
      "arxiv_id": "2510.01622v1",
      "title": "LLM4Rec: Large Language Models for Multimodal Generative Recommendation\n  with Causal Debiasing",
      "summary": "Contemporary generative recommendation systems face significant challenges in\nhandling multimodal data, eliminating algorithmic biases, and providing\ntransparent decision-making processes. This paper introduces an enhanced\ngenerative recommendation framework that addresses these limitations through\nfive key innovations: multimodal fusion architecture, retrieval-augmented\ngeneration mechanisms, causal inference-based debiasing, explainable\nrecommendation generation, and real-time adaptive learning capabilities. Our\nframework leverages advanced large language models as the backbone while\nincorporating specialized modules for cross-modal understanding, contextual\nknowledge integration, bias mitigation, explanation synthesis, and continuous\nmodel adaptation. Extensive experiments on three benchmark datasets\n(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistent\nimprovements in recommendation accuracy, fairness, and diversity compared to\nexisting approaches. The proposed framework achieves up to 2.3% improvement in\nNDCG@10 and 1.4% enhancement in diversity metrics while maintaining\ncomputational efficiency through optimized inference strategies.",
      "authors": [
        "Bo Ma",
        "Hang Li",
        "ZeHua Hu",
        "XiaoFan Gui",
        "LuYao Liu",
        "Simon Lau"
      ],
      "published": "2025-10-02T02:53:05Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01622v1"
    },
    {
      "arxiv_id": "2510.01617v1",
      "title": "AMAS: Adaptively Determining Communication Topology for LLM-based\n  Multi-Agent System",
      "summary": "Although large language models (LLMs) have revolutionized natural language\nprocessing capabilities, their practical implementation as autonomous\nmulti-agent systems (MAS) for industrial problem-solving encounters persistent\nbarriers. Conventional MAS architectures are fundamentally restricted by\ninflexible, hand-crafted graph topologies that lack contextual responsiveness,\nresulting in diminished efficacy across varied academic and commercial\nworkloads. To surmount these constraints, we introduce AMAS, a\nparadigm-shifting framework that redefines LLM-based MAS through a novel\ndynamic graph designer. This component autonomously identifies task-specific\noptimal graph configurations via lightweight LLM adaptation, eliminating the\nreliance on monolithic, universally applied structural templates. Instead, AMAS\nexploits the intrinsic properties of individual inputs to intelligently direct\nquery trajectories through task-optimized agent pathways. Rigorous validation\nacross question answering, mathematical deduction, and code generation\nbenchmarks confirms that AMAS systematically exceeds state-of-the-art\nsingle-agent and multi-agent approaches across diverse LLM architectures. Our\ninvestigation establishes that context-sensitive structural adaptability\nconstitutes a foundational requirement for high-performance LLM MAS\ndeployments.",
      "authors": [
        "Hui Yi Leong",
        "Yuheng Li",
        "Yuqing Wu",
        "Wenwen Ouyang",
        "Wei Zhu",
        "Jiechao Gao"
      ],
      "published": "2025-10-02T02:50:22Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01617v1"
    },
    {
      "arxiv_id": "2510.01616v1",
      "title": "Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single\n  Consumer GPU: Continual Pre-training, SFT, and DPO",
      "summary": "Small Language Models (SLMs) enable cost-effective, on-device and\nlatency-sensitive AI applications, yet their deployment in Traditional Chinese\n(TC) remains hindered by token-level instability - models unpredictably emit\nnon-TC characters or code-switch into other languages. We address this\npractical reliability gap by creating PureTC-1B, a three-stage stabilization\npipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned model\nreleased by Meta) using parameter-efficient LoRA adapters. Our method combines\nContinual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning\n(SFT) with instruction data, and Direct Preference Optimization (DPO) using\nTC-adherence preferences to improve monolingual robustness without full-model\nretraining. On a benchmark designed to simulate real-world usage, PureTC-1B\nachieves a 51.3% relative reduction (micro-average) in non-TC output tokens\nversus the base model. On a Named Entity Translation (NET) task, PureTC-1B\nfurther reduces incorrect-language tokens by 77.2% relative to Llama-3B and\n57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainable\neven at the 1B scale. The pipeline is reproducible, adapter-only, and\nhardware-friendly, offering practitioners a practical recipe to enhance\nlanguage stability for TC and potentially other non-English languages.",
      "authors": [
        "Yu-Cheng Chih",
        "Ming-Tao Duan",
        "Yong-Hao Hou"
      ],
      "published": "2025-10-02T02:50:12Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01616v1"
    },
    {
      "arxiv_id": "2510.01612v1",
      "title": "RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical\n  Question Answering",
      "summary": "The exponential growth of biomedical literature creates significant\nchallenges for accessing precise medical information. Current biomedical\nquestion-answering systems primarily focus on short-form answers, failing to\nprovide the comprehensive explanations necessary for clinical decision-making.\nWe present RAG-BioQA, a novel framework combining retrieval-augmented\ngeneration with domain-specific fine-tuning to produce evidence-based,\nlong-form biomedical answers. Our approach integrates BioBERT embeddings with\nFAISS indexing and compares various re-ranking strategies (BM25, ColBERT,\nMonoT5) to optimize context selection before synthesizing evidence through a\nfine-tuned T5 model. Experimental results on the PubMedQA dataset show\nsignificant improvements over baselines, with our best model achieving\nsubstantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state\nof accessible, evidence-based biomedical knowledge retrieval.",
      "authors": [
        "Lovely Yeswanth Panchumarthi",
        "Sai Prasad Gudari",
        "Atharva Negi",
        "Praveen Raj Budime",
        "Harsit Upadhya"
      ],
      "published": "2025-10-02T02:49:09Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01612v1"
    },
    {
      "arxiv_id": "2510.01611v1",
      "title": "PychoBench: Evaluating the Psychology Intelligence of Large Language\n  Models",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: Can LLMs be effectively applied to psychological\ncounseling? To determine whether an LLM can effectively take on the role of a\npsychological counselor, the first step is to assess whether it meets the\nqualifications required for such a role, namely the ability to pass the U.S.\nNational Counselor Certification Exam (NCE). This is because, just as a human\ncounselor must pass a certification exam to practice, an LLM must demonstrate\nsufficient psychological knowledge to meet the standards required for such a\nrole. To address this, we introduce PsychoBench, a benchmark grounded in\nU.S.national counselor examinations, a licensure test for professional\ncounselors that requires about 70% accuracy to pass. PsychoBench comprises\napproximately 2,252 carefully curated single-choice questions, crafted to\nrequire deep understanding and broad enough to cover various sub-disciplines of\npsychology. This benchmark provides a comprehensive assessment of an LLM's\nability to function as a counselor. Our evaluation shows that advanced models\nsuch as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing\nthreshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)\nremain far below it. These results suggest that only frontier LLMs are\ncurrently capable of meeting counseling exam standards, highlighting both the\npromise and the challenges of developing psychology-oriented LLMs.",
      "authors": [
        "Min Zeng"
      ],
      "published": "2025-10-02T02:49:06Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01611v1"
    },
    {
      "arxiv_id": "2510.01606v1",
      "title": "Bridging Collaborative Filtering and Large Language Models with Dynamic\n  Alignment, Multimodal Fusion and Evidence-grounded Explanations",
      "summary": "Recent research has explored using Large Language Models for recommendation\ntasks by transforming user interaction histories and item metadata into text\nprompts, then having the LLM produce rankings or recommendations. A promising\napproach involves connecting collaborative filtering knowledge to LLM\nrepresentations through compact adapter networks, which avoids expensive\nfine-tuning while preserving the strengths of both components. Yet several\nchallenges persist in practice: collaborative filtering models often use static\nsnapshots that miss rapidly changing user preferences; many real-world items\ncontain rich visual and audio content beyond textual descriptions; and current\nsystems struggle to provide trustworthy explanations backed by concrete\nevidence. Our work introduces \\model{}, a framework that tackles these\nlimitations through three key innovations. We develop an online adaptation\nmechanism that continuously incorporates new user interactions through\nlightweight modules, avoiding the need to retrain large models. We create a\nunified representation that seamlessly combines collaborative signals with\nvisual and audio features, handling cases where some modalities may be\nunavailable. Finally, we design an explanation system that grounds\nrecommendations in specific collaborative patterns and item attributes,\nproducing natural language rationales users can verify. Our approach maintains\nthe efficiency of frozen base models while adding minimal computational\noverhead, making it practical for real-world deployment.",
      "authors": [
        "Bo Ma",
        "LuYao Liu",
        "Simon Lau",
        "Chandler Yuan",
        "and XueY Cui",
        "Rosie Zhang"
      ],
      "published": "2025-10-02T02:43:24Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01606v1"
    },
    {
      "arxiv_id": "2510.01600v1",
      "title": "A Comparison of Independent and Joint Fine-tuning Strategies for\n  Retrieval-Augmented Generation",
      "summary": "A Comparison of Independent and Joint Fine-tuning Strategies for\nRetrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,\nAnoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP\n2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0\nKeywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),\nFine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and\ncompare strategies for fine-tuning Retrieval Augmented Generation (RAG)\npipelines, including independent fine-tuning, joint fine-tuning, and two-phase\nfine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular\nframework for question answering that is powered by two large language models\n(LLMs): an embedding model that retrieves context documents from a database\nthat are relevant to a given question, and a generator model that uses the\nretrieved context to generate an answer to the question. Both the embedding and\ngenerator models can be fine-tuned to increase performance of a RAG pipeline on\na new task, but multiple fine-tuning strategies exist with different costs and\nbenefits. In this paper, we evaluate and compare several RAG fine-tuning\nstrategies, including independent, joint, and two-phase fine-tuning. In our\nexperiments, we observe that all of these strategies achieve about equal\nimprovement in EM and F1 generation quality metrics, although they have\nsignificantly different computational costs. We conclude the optimal\nfine-tuning strategy to use depends on whether the training dataset includes\ncontext labels and whether a grid search over the learning rates for the\nembedding and generator models is required.",
      "authors": [
        "Neal Gregory Lawton",
        "Alfy Samuel",
        "Anoop Kumar",
        "Daben Liu"
      ],
      "published": "2025-10-02T02:30:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01600v1"
    },
    {
      "arxiv_id": "2510.01591v1",
      "title": "CLUE: Non-parametric Verification from Experience via Hidden-State\n  Clustering",
      "summary": "Assessing the quality of Large Language Model (LLM) outputs presents a\ncritical challenge. Previous methods either rely on text-level information\n(e.g., reward models, majority voting), which can overfit to superficial cues,\nor on calibrated confidence from token probabilities, which would fail on\nless-calibrated models. Yet both of these signals are, in fact, partial\nprojections of a richer source of information: the model's internal hidden\nstates. Early layers, closer to token embeddings, preserve semantic and lexical\nfeatures that underpin text-based judgments, while later layers increasingly\nalign with output logits, embedding confidence-related information. This paper\nexplores hidden states directly as a unified foundation for verification. We\nshow that the correctness of a solution is encoded as a geometrically separable\nsignature within the trajectory of hidden activations. To validate this, we\npresent Clue (Clustering and Experience-based Verification), a deliberately\nminimalist, non-parametric verifier. With no trainable parameters, CLUE only\nsummarizes each reasoning trace by an hidden state delta and classifies\ncorrectness via nearest-centroid distance to ``success'' and ``failure''\nclusters formed from past experience. The simplicity of this method highlights\nthe strength of the underlying signal. Empirically, CLUE consistently\noutperforms LLM-as-a-judge baselines and matches or exceeds modern\nconfidence-based methods in reranking candidates, improving both top-1 and\nmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24\nwith a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%\n(top-maj@16).",
      "authors": [
        "Zhenwen Liang",
        "Ruosen Li",
        "Yujun Zhou",
        "Linfeng Song",
        "Dian Yu",
        "Xinya Du",
        "Haitao Mi",
        "Dong Yu"
      ],
      "published": "2025-10-02T02:14:33Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01591v1"
    },
    {
      "arxiv_id": "2510.01585v1",
      "title": "ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and\n  Long-Context Reasoning",
      "summary": "While Transformer architectures have demonstrated impressive scalability\nacross domains, they continue to face challenges in long-context reasoning,\ncomputational efficiency, and structural generalization - largely due to rigid\nlayer stacking, dense attention, and reliance on positional encodings. We\npresent ReSSFormer, a Recursive Sparse Structured Transformer that integrates\nthree complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for\niterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)\nfor efficient and focused context selection, and Self-Organizing Encoder\nStructure (SOES) for position-free structure induction. ReSSFormer replaces\nconventional depth stacking with recurrent inference, substitutes full\nattention with token- and expert-level sparsity, and models latent token\ntopology directly from content. Across language modeling, multi-hop QA, and\nstructure-sensitive tasks, ReSSFormer consistently outperforms strong baselines\nunder comparable FLOPs and parameter budgets, highlighting its scalability,\nefficiency, and structural flexibility.",
      "authors": [
        "Haochen You",
        "Baojing Liu"
      ],
      "published": "2025-10-02T02:05:30Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01585v1"
    },
    {
      "arxiv_id": "2510.01581v1",
      "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,\n  Attentive Compression",
      "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time\ncompute, but this scaling must be allocated in line with task difficulty. On\none hand, short reasoning (underthinking) leads to errors on harder problems\nthat require extended reasoning steps; but, excessively long reasoning\n(overthinking) can be token-inefficient, generating unnecessary steps even\nafter reaching a correct intermediate solution. We refer to this as\nunder-adaptivity, where the model fails to modulate its response length\nappropriately given problems of varying difficulty. To address under-adaptivity\nand strike a balance between under- and overthinking, we propose TRAAC (Think\nRight with Adaptive, Attentive Compression), an online post-training RL method\nthat leverages the model's self-attention over a long reasoning trajectory to\nidentify important steps and prune redundant ones. TRAAC also estimates\ndifficulty and incorporates it into training rewards, thereby learning to\nallocate reasoning budget commensurate with example difficulty. Our approach\nimproves accuracy, reduces reasoning steps, and enables adaptive thinking\ncompared to base models and other RL baselines. Across a variety of tasks\n(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute\naccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%\ncompared to the base model, and a 7.9% accuracy gain paired with a 29.4% length\ndrop compared to the best RL baseline. TRAAC also shows strong generalization:\nalthough our models are trained on math datasets, they show accuracy and\nefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,\nand OptimalThinkingBench. Our analysis further verifies that TRAAC provides\nfine-grained adjustments to thinking budget based on difficulty and that a\ncombination of task-difficulty calibration and attention-based compression\nyields gains across diverse tasks.",
      "authors": [
        "Joykirat Singh",
        "Justin Chih-Yao Chen",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Akshay Nambi",
        "Mohit Bansal"
      ],
      "published": "2025-10-02T02:00:20Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01581v1"
    },
    {
      "arxiv_id": "2510.01574v1",
      "title": "Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query\n  Autocomplete",
      "summary": "We introduce a data-centric approach for mitigating presentation bias in\nreal-time neural query autocomplete systems through the use of synthetic\nprefixes. These prefixes are generated from complete user queries collected\nduring regular search sessions where autocomplete was not active. This allows\nus to enrich the training data for learning to rank models with more diverse\nand less biased examples. This method addresses the inherent bias in engagement\nsignals collected from live query autocomplete interactions, where model\nsuggestions influence user behavior. Our neural ranker is optimized for\nreal-time deployment under strict latency constraints and incorporates a rich\nset of features, including query popularity, seasonality, fuzzy match scores,\nand contextual signals such as department affinity, device type, and vertical\nalignment with previous user queries. To support efficient training, we\nintroduce a task-specific simplification of the listwise loss, reducing\ncomputational complexity from $O(n^2)$ to $O(n)$ by leveraging the query\nautocomplete structure of having only one ground-truth selection per prefix.\nDeployed in a large-scale e-commerce setting, our system demonstrates\nstatistically significant improvements in user engagement, as measured by mean\nreciprocal rank and related metrics. Our findings show that synthetic prefixes\nnot only improve generalization but also provide a scalable path toward bias\nmitigation in other low-latency ranking tasks, including related searches and\nquery recommendations.",
      "authors": [
        "Adithya Rajan",
        "Xiaoyu Liu",
        "Prateek Verma",
        "Vibhu Arora"
      ],
      "published": "2025-10-02T01:44:44Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.01574v1"
    },
    {
      "arxiv_id": "2510.01569v1",
      "title": "InvThink: Towards AI Safety via Inverse Reasoning",
      "summary": "We present InvThink, a simple yet powerful approach that gives large language\nmodels (LLMs) the capability of inverse thinking: reasoning through failure\nmodes before generating responses. Unlike existing safety alignment methods\nthat optimize directly for safe response, InvThink instructs models to 1)\nenumerate potential harms, 2) analyze their consequences, and 3) generate safe\noutputs that proactively avoid these risks. Our method reveals three key\nfindings: (i) safety improvements show stronger scaling with model size\ncompared to existing safety methods. (ii) InvThink mitigates safety tax; by\ntraining models to systematically consider failure modes, it preserves general\nreasoning capabilities on standard benchmarks. (iii) beyond general safety\ntasks, InvThink excels in high-stakes domains including external-facing\n(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,\nachieving up to 15.7% reduction in harmful responses compared to baseline\nmethods like SafetyPrompt. We further implement InvThink via supervised\nfine-tuning, and reinforcement learning across three LLM families. These\nresults suggest that inverse reasoning provides a scalable and generalizable\npath toward safer, more capable language models.",
      "authors": [
        "Yubin Kim",
        "Taehan Kim",
        "Eugene Park",
        "Chunjong Park",
        "Cynthia Breazeal",
        "Daniel McDuff",
        "Hae Won Park"
      ],
      "published": "2025-10-02T01:26:53Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01569v1"
    },
    {
      "arxiv_id": "2510.01531v1",
      "title": "Information Seeking for Robust Decision Making under Partial\n  Observability",
      "summary": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io",
      "authors": [
        "Djengo Cyun-Jyun Fang",
        "Tsung-Wei Ke"
      ],
      "published": "2025-10-02T00:06:32Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01531v1"
    },
    {
      "arxiv_id": "2510.01526v1",
      "title": "One More Question is Enough, Expert Question Decomposition (EQD) Model\n  for Domain Quantitative Reasoning",
      "summary": "Domain-specific quantitative reasoning remains a major challenge for large\nlanguage models (LLMs), especially in fields requiring expert knowledge and\ncomplex question answering (QA). In this work, we propose Expert Question\nDecomposition (EQD), an approach designed to balance the use of domain\nknowledge with computational efficiency. EQD is built on a two-step fine-tuning\nframework and guided by a reward function that measures the effectiveness of\ngenerated sub-questions in improving QA outcomes. It requires only a few\nthousand training examples and a single A100 GPU for fine-tuning, with\ninference time comparable to zero-shot prompting. Beyond its efficiency, EQD\noutperforms state-of-the-art domain-tuned models and advanced prompting\nstrategies. We evaluate EQD in the financial domain, characterized by\nspecialized knowledge and complex quantitative reasoning, across four benchmark\ndatasets. Our method consistently improves QA performance by 0.6% to 10.5%\nacross different LLMs. Our analysis reveals an important insight: in\ndomain-specific QA, a single supporting question often provides greater benefit\nthan detailed guidance steps.",
      "authors": [
        "Mengyu Wang",
        "Sotirios Sabanis",
        "Miguel de Carvalho",
        "Shay B. Cohen",
        "Tiejun Ma"
      ],
      "published": "2025-10-01T23:45:45Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01526v1"
    },
    {
      "arxiv_id": "2510.01513v1",
      "title": "From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods\n  for Multimodal Content Analysis and Understanding",
      "summary": "Analysis of multi-modal content can be tricky, computationally expensive, and\nrequire a significant amount of engineering efforts. Lots of work with\npre-trained models on static data is out there, yet fusing these opensource\nmodels and methods with complex data such as videos is relatively challenging.\nIn this paper, we present a framework that enables efficiently prototyping\npipelines for multi-modal content analysis. We craft a candidate recipe for a\npipeline, marrying a set of pre-trained models, to convert videos into a\ntemporal semi-structured data format. We translate this structure further to a\nframe-level indexed knowledge graph representation that is query-able and\nsupports continual learning, enabling the dynamic incorporation of new\ndomain-specific knowledge through an interactive medium.",
      "authors": [
        "Basem Rizk",
        "Joel Walsh",
        "Mark Core",
        "Benjamin Nye"
      ],
      "published": "2025-10-01T23:20:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01513v1"
    },
    {
      "arxiv_id": "2510.01470v1",
      "title": "Extracting O*NET Features from the NLx Corpus to Build Public Use\n  Aggregate Labor Market Data",
      "summary": "Data from online job postings are difficult to access and are not built in a\nstandard or transparent manner. Data included in the standard taxonomy and\noccupational information database (O*NET) are updated infrequently and based on\nsmall survey samples. We adopt O*NET as a framework for building natural\nlanguage processing tools that extract structured information from job\npostings. We publish the Job Ad Analysis Toolkit (JAAT), a collection of\nopen-source tools built for this purpose, and demonstrate its reliability and\naccuracy in out-of-sample and LLM-as-a-Judge testing. We extract more than 10\nbillion data points from more than 155 million online job ads provided by the\nNational Labor Exchange (NLx) Research Hub, including O*NET tasks, occupation\ncodes, tools, and technologies, as well as wages, skills, industry, and more\nfeatures. We describe the construction of a dataset of occupation, state, and\nindustry level features aggregated by monthly active jobs from 2015 - 2025. We\nillustrate the potential for research and future uses in education and\nworkforce development.",
      "authors": [
        "Stephen Meisenbacher",
        "Svetlozar Nestorov",
        "Peter Norlander"
      ],
      "published": "2025-10-01T21:27:11Z",
      "primary_category": "cs.CY",
      "arxiv_url": "https://arxiv.org/abs/2510.01470v1"
    },
    {
      "arxiv_id": "2510.01469v1",
      "title": "A-VERT: Agnostic Verification with Embedding Ranking Targets",
      "summary": "The automatic evaluation of Language Model (LM) responses is a critical piece\nin the development of benchmarks and metrics, both for model training and\nquality assessment of production model endpoints. The current approaches to\nresponse classification relies on methods that are too expensive (i.e.\nLLM-as-a-Judge) or that are far from real-world conditions (string-matching,\nlogprob). In this paper, a structure-free evaluation method is presented. The\nmethod makes use of semantic embedding distances to match target candidates\nwith arbitrary LM-generated text, resulting in a robust classification of the\nresponse at a relatively low compute cost (embedding models of less than $10B$\nparameters). The results show a regression score of ~0.97 and an accuracy of\n~96% against human annotators, tested over 3 data sets and 3 different LM\narchitectures.",
      "authors": [
        "Nicolás Aguirre",
        "Ramiro Caso",
        "Ramiro Rodríguez Colmeiro",
        "Mauro Santelli",
        "Joaquín Toranzo Calderón"
      ],
      "published": "2025-10-01T21:26:03Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01469v1"
    },
    {
      "arxiv_id": "2510.01459v1",
      "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM\n  Reasoning",
      "summary": "Since the release of Deepseek-R1, reinforcement learning with verifiable\nrewards (RLVR) has become a central approach for training large language models\n(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss\nfunctions to make RLVR more efficient and effective. In this paper, motivated\nby studies of overthinking in LLMs, we propose Length-aware Sampling for Policy\nOptimization (LSPO), a novel meta-RLVR algorithm that dynamically selects\ntraining data at each step based on the average response length. We evaluate\nLSPO across multiple base models and datasets, demonstrating that it\nconsistently improves learning effectiveness. In addition, we conduct a\ndetailed ablation study to examine alternative ways of incorporating length\nsignals into dynamic sampling, offering further insights and highlighting\npromising directions for future research.",
      "authors": [
        "Weizhe Chen",
        "Sven Koenig",
        "Bistra Dilkina"
      ],
      "published": "2025-10-01T20:57:22Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01459v1"
    },
    {
      "arxiv_id": "2510.01444v1",
      "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning",
      "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce $\\textbf{VOGUE (Visual Uncertainty Guided\nExploration)}$, a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.",
      "authors": [
        "Rui Liu",
        "Dian Yu",
        "Tong Zheng",
        "Runpeng Dai",
        "Zongxia Li",
        "Wenhao Yu",
        "Zhenwen Liang",
        "Linfeng Song",
        "Haitao Mi",
        "Pratap Tokekar",
        "Dong Yu"
      ],
      "published": "2025-10-01T20:32:08Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01444v1"
    },
    {
      "arxiv_id": "2510.01394v1",
      "title": "Optimal Stopping vs Best-of-$N$ for Inference Time Optimization",
      "summary": "Large language model (LLM) generation often requires balancing output quality\nagainst inference cost, especially when using multiple generations. We\nintroduce a new framework for inference-time optimization based on the\nclassical Pandora's Box problem. Viewing each generation as opening a costly\n\"box\" with random reward, we develop algorithms that decide when to stop\ngenerating without knowing the underlying reward distribution. Our first\ncontribution is a UCB-style Pandora's Box algorithm, which achieves performance\nthat is provably close to Weitzman's algorithm, the optimal strategy when the\ndistribution is known. We further adapt this method to practical LLM settings\nby addressing reward scaling across prompts via a Bradley-Terry inspired\ntransformation. This leads to an adaptive inference-time optimization method\nthat normalizes rewards and learns stopping thresholds on the fly. Experiments\non the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,\nshow that our adaptive strategy can obtain the same performance as non-adaptive\nBest-of-N sampling while requiring 15-35 percent fewer generations on average.\nOur results establish a principled bridge between optimal stopping theory and\ninference-time scaling, providing both theoretical performance bounds and\npractical efficiency gains for LLM deployment.",
      "authors": [
        "Yusuf Kalayci",
        "Vinod Raman",
        "Shaddin Dughmi"
      ],
      "published": "2025-10-01T19:25:59Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01394v1"
    },
    {
      "arxiv_id": "2510.01391v1",
      "title": "TAG-EQA: Text-And-Graph for Event Question Answering via Structured\n  Prompting Strategies",
      "summary": "Large language models (LLMs) excel at general language tasks but often\nstruggle with event-based questions-especially those requiring causal or\ntemporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question\nAnswering), a prompting framework that injects causal event graphs into LLM\ninputs by converting structured relations into natural-language statements.\nTAG-EQA spans nine prompting configurations, combining three strategies\n(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,\ngraph-only, text+graph), enabling a systematic analysis of when and how\nstructured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA\nimproves accuracy by 5% on average over text-only baselines, with gains up to\n12% in zero-shot settings and 18% when graph-augmented CoT prompting is\neffective. While performance varies by model and configuration, our findings\nshow that causal graphs can enhance event reasoning in LLMs without\nfine-tuning, offering a flexible way to encode structure in prompt-based QA.",
      "authors": [
        "Maithili Kadam",
        "Francis Ferraro"
      ],
      "published": "2025-10-01T19:23:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01391v1"
    },
    {
      "arxiv_id": "2510.01375v1",
      "title": "Fine-tuning with RAG for Improving LLM Learning of New Skills",
      "summary": "Large language model (LLM) agents deployed for multi-step tasks frequently\nfail in predictable ways: attempting actions with unmet preconditions, issuing\nredundant commands, or mishandling environment constraints. While\nretrieval-augmented generation (RAG) can improve performance by providing\nruntime guidance, it requires maintaining external knowledge databases and adds\ncomputational overhead at every deployment. We propose a simple pipeline that\nconverts inference-time retrieval into learned competence through distillation.\nOur approach: (1) extracts compact, reusable hints from agent failures, (2)\nuses these hints to generate improved teacher trajectories via one-shot\nretrieval at episode start, and (3) trains student models on these trajectories\nwith hint strings removed, forcing internalization rather than memorization.\nAcross two interactive benchmarks, ALFWorld (household tasks) and WebShop\n(online shopping), distilled students consistently outperform baseline agents,\nachieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving\nWebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens\nthan retrieval-augmented teachers depending on the environment. The approach\ngeneralizes across model scales (7B/14B parameters) and agent architectures\n(ReAct/StateAct), demonstrating that retrieval benefits can be effectively\ninternalized through targeted fine-tuning without permanent runtime\ndependencies.",
      "authors": [
        "Humaid Ibrahim",
        "Nikolai Rozanov",
        "Marek Rei"
      ],
      "published": "2025-10-01T19:03:48Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01375v1"
    },
    {
      "arxiv_id": "2510.01367v1",
      "title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by\n  Measuring Reasoning Effort",
      "summary": "Reward hacking, where a reasoning model exploits loopholes in a reward\nfunction to achieve high rewards without solving the intended task, poses a\nsignificant threat. This behavior may be explicit, i.e. verbalized in the\nmodel's chain-of-thought (CoT), or implicit, where the CoT appears benign thus\nbypasses CoT monitors. To detect implicit reward hacking, we propose TRACE\n(Truncated Reasoning AUC Evaluation). Our key observation is that hacking\noccurs when exploiting the loophole is easier than solving the actual task.\nThis means that the model is using less `effort' than required to achieve high\nreward. TRACE quantifies effort by measuring how early a model's reasoning\nbecomes sufficient to pass a verifier. We progressively truncate a model's CoT\nat various lengths, force the model to answer, and measure the verifier-passing\nrate at each cutoff. A hacking model, which takes a shortcut, will achieve a\nhigh passing rate with only a small fraction of its CoT, yielding a large area\nunder the accuracy-vs-length curve. TRACE achieves over 65% gains over our\nstrongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B\nmonitor in coding. We further show that TRACE can discover unknown loopholes\nduring training. Overall, TRACE offers a scalable unsupervised approach for\noversight where current monitoring methods prove ineffective.",
      "authors": [
        "Xinpeng Wang",
        "Nitish Joshi",
        "Barbara Plank",
        "Rico Angell",
        "He He"
      ],
      "published": "2025-10-01T18:49:45Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01367v1"
    },
    {
      "arxiv_id": "2510.01354v1",
      "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
      "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.",
      "authors": [
        "Yinuo Liu",
        "Ruohan Xu",
        "Xilong Wang",
        "Yuqi Jia",
        "Neil Zhenqiang Gong"
      ],
      "published": "2025-10-01T18:34:06Z",
      "primary_category": "cs.CR",
      "arxiv_url": "https://arxiv.org/abs/2510.01354v1"
    },
    {
      "arxiv_id": "2510.01353v1",
      "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in\n  Multi-Platform Dynamic Agent Environments",
      "summary": "Recent works on context and memory benchmarking have primarily focused on\nconversational instances but the need for evaluating memory in dynamic\nenterprise environments is crucial for its effective application. We introduce\nMEMTRACK, a benchmark designed to evaluate long-term memory and state tracking\nin multi-platform agent environments. MEMTRACK models realistic organizational\nworkflows by integrating asynchronous events across multiple communication and\nproductivity platforms such as Slack, Linear and Git. Each benchmark instance\nprovides a chronologically platform-interleaved timeline, with noisy,\nconflicting, cross-referring information as well as potential\ncodebase/file-system comprehension and exploration. Consequently, our benchmark\ntests memory capabilities such as acquistion, selection and conflict\nresolution. We curate the MEMTRACK dataset through both manual expert driven\ndesign and scalable agent based synthesis, generating ecologically valid\nscenarios grounded in real world software development processes. We introduce\npertinent metrics for Correctness, Efficiency, and Redundancy that capture the\neffectiveness of memory mechanisms beyond simple QA performance. Experiments\nacross SoTA LLMs and memory backends reveal challenges in utilizing memory\nacross long horizons, handling cross-platform dependencies, and resolving\ncontradictions. Notably, the best performing GPT-5 model only achieves a 60\\%\nCorrectness score on MEMTRACK. This work provides an extensible framework for\nadvancing evaluation research for memory-augmented agents, beyond existing\nfocus on conversational setups, and sets the stage for multi-agent,\nmulti-platform memory benchmarking in complex organizational settings",
      "authors": [
        "Darshan Deshpande",
        "Varun Gangal",
        "Hersh Mehta",
        "Anand Kannappan",
        "Rebecca Qian",
        "Peng Wang"
      ],
      "published": "2025-10-01T18:34:03Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01353v1"
    },
    {
      "arxiv_id": "2510.01346v1",
      "title": "Aristotle: IMO-level Automated Theorem Proving",
      "summary": "We introduce Aristotle, an AI system that combines formal verification with\ninformal reasoning, achieving gold-medal-equivalent performance on the 2025\nInternational Mathematical Olympiad problems. Aristotle integrates three main\ncomponents: a Lean proof search system, an informal reasoning system that\ngenerates and formalizes lemmas, and a dedicated geometry solver. Our system\ndemonstrates state-of-the-art performance with favorable scaling properties for\nautomated theorem proving.",
      "authors": [
        "Tudor Achim",
        "Alex Best",
        "Kevin Der",
        "Mathïs Fédérico",
        "Sergei Gukov",
        "Daniel Halpern-Leister",
        "Kirsten Henningsgard",
        "Yury Kudryashov",
        "Alexander Meiburg",
        "Martin Michelsen",
        "Riley Patterson",
        "Eric Rodriguez",
        "Laura Scharff",
        "Vikram Shanker",
        "Vladmir Sicca",
        "Hari Sowrirajan",
        "Aidan Swope",
        "Matyas Tamas",
        "Vlad Tenev",
        "Jonathan Thomm",
        "Harold Williams",
        "Lawrence Wu"
      ],
      "published": "2025-10-01T18:21:13Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01346v1"
    },
    {
      "arxiv_id": "2510.01336v1",
      "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
      "summary": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.",
      "authors": [
        "Avinash Kumar",
        "Sujay Sanghavi",
        "Poulami Das"
      ],
      "published": "2025-10-01T18:04:14Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01336v1"
    },
    {
      "arxiv_id": "2510.01180v1",
      "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\ningredient for unlocking complex reasoning capabilities in large language\nmodels. Recent work ProRL has shown promise in scaling RL by increasing the\nnumber of training steps. However, performance plateaus after thousands of\nsteps, with clear diminishing returns from allocating more computation to\nadditional training. In this work, we investigate a complementary paradigm for\nscaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to\nexhaustively Broaden exploration, which yields continuous performance gains\nbeyond the saturation point observed in ProRL when scaling the number of\ntraining steps. Our approach is motivated by a mass balance equation analysis\nallowing us to characterize the rate of change in probability mass for correct\nand incorrect tokens during the reinforcement process. We show that under a\none-step RL assumption, sampled rollout tokens always contribute to\ncorrect-mass expansion, while unsampled tokens outside rollouts may lead to\ngains or losses depending on their distribution and the net reward balance.\nImportantly, as the number of rollouts per example N increases, the effect of\nunsampled terms diminishes, ensuring overall correct-mass expansion. To\nvalidate our theoretical analysis, we conduct simulations under more relaxed\nconditions and find that a sufficiently large rollout size N-corresponding to\nample exploration-guarantees an increase in the probability mass of all correct\ntokens. Empirically, BroRL revives models saturated after 3K ProRL training\nsteps and demonstrates robust, continuous improvement, achieving\nstate-of-the-art results for the 1.5B model across diverse benchmarks.",
      "authors": [
        "Jian Hu",
        "Mingjie Liu",
        "Ximing Lu",
        "Fang Wu",
        "Zaid Harchaoui",
        "Shizhe Diao",
        "Yejin Choi",
        "Pavlo Molchanov",
        "Jun Yang",
        "Jan Kautz",
        "Yi Dong"
      ],
      "published": "2025-10-01T17:59:02Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01180v1"
    },
    {
      "arxiv_id": "2510.01304v1",
      "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models",
      "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .",
      "authors": [
        "Yu Zeng",
        "Wenxuan Huang",
        "Shiting Huang",
        "Xikun Bao",
        "Yukun Qi",
        "Yiming Zhao",
        "Qiuchen Wang",
        "Lin Chen",
        "Zehui Chen",
        "Huaian Chen",
        "Wanli Ouyang",
        "Feng Zhao"
      ],
      "published": "2025-10-01T17:58:05Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01304v1"
    },
    {
      "arxiv_id": "2510.01179v1",
      "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments",
      "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.",
      "authors": [
        "Zhangchen Xu",
        "Adriana Meza Soria",
        "Shawn Tan",
        "Anurag Roy",
        "Ashish Sunil Agrawal",
        "Radha Poovendran",
        "Rameswar Panda"
      ],
      "published": "2025-10-01T17:58:03Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01179v1"
    },
    {
      "arxiv_id": "2510.01174v1",
      "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
      "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.",
      "authors": [
        "Yanzhe Chen",
        "Kevin Qinghong Lin",
        "Mike Zheng Shou"
      ],
      "published": "2025-10-01T17:56:48Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01174v1"
    },
    {
      "arxiv_id": "2510.01172v1",
      "title": "Energy-Regularized Sequential Model Editing on Hyperspheres",
      "summary": "Large language models (LLMs) require constant updates to remain aligned with\nevolving real-world knowledge. Model editing offers a lightweight alternative\nto retraining, but sequential editing often destabilizes representations and\ninduces catastrophic forgetting. In this work, we seek to better understand and\nmitigate performance degradation caused by sequential editing. We hypothesize\nthat hyperspherical uniformity, a property that maintains uniform distribution\nof neuron weights on a hypersphere, helps the model remain stable, retain prior\nknowledge, while still accommodate new updates. We use Hyperspherical Energy\n(HE) to quantify neuron uniformity during editing, and examine its correlation\nwith editing performance. Empirical studies across widely used editing methods\nreveals a strong correlation between HE dynamics and editing performance, with\nediting failures consistently coinciding with high HE fluctuations. We further\ntheoretically prove that HE dynamics impose a lower bound on the degradation of\npretrained knowledge, highlighting why HE stability is crucial for knowledge\nretention. Motivated by these insights, we propose SPHERE (Sparse Projection\nfor Hyperspherical Energy-Regularized Editing), an HE-driven regularization\nstrategy that stabilizes neuron weight distributions, ultimately preserving\nprior knowledge while enabling reliable sequential updates. Specifically,\nSPHERE identifies a sparse space complementary to the principal hyperspherical\ndirections of the pretrained weight matrices and projects new knowledge onto\nit, attenuating perturbations on the principal directions. Extensive\nexperiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the\nbest baseline in editing capability by an average of 16.41%, while most\nfaithfully preserving general model performance, thereby offering a principled\npath toward reliable large-scale knowledge editing.",
      "authors": [
        "Qingyuan Liu",
        "Jia-Chen Gu",
        "Yunzhi Yao",
        "Hong Wang",
        "Nanyun Peng"
      ],
      "published": "2025-10-01T17:55:43Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01172v1"
    },
    {
      "arxiv_id": "2510.01171v1",
      "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity",
      "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.",
      "authors": [
        "Jiayi Zhang",
        "Simon Yu",
        "Derek Chong",
        "Anthony Sicilia",
        "Michael R. Tomz",
        "Christopher D. Manning",
        "Weiyan Shi"
      ],
      "published": "2025-10-01T17:55:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01171v1"
    },
    {
      "arxiv_id": "2510.01167v1",
      "title": "Simultaneous Multi-objective Alignment Across Verifiable and\n  Non-verifiable Rewards",
      "summary": "Aligning large language models to human preferences is inherently\nmultidimensional, yet most pipelines collapse heterogeneous signals into a\nsingle optimizeable objective. We seek to answer what it would take to\nsimultaneously align a model across various domains spanning those with:\nverifiable rewards (mathematical accuracy), non-verifiable subjective\npreferences (human values), and complex interactive scenarios (multi-turn AI\ntutoring dialogues). Such multi-objective reinforcement learning setups are\noften plagued by the individual objectives being at odds with each other,\nresulting in inefficient training and little user control during inference. We\npropose a unified framework that: (i) standardizes {process reward model} (PRM)\ntraining across both verifiable and non-verifiable settings to better supervise\nmodels' chain-of-thought reasoning; (ii) performs {multi-objective alignment}\nby training the LLM with our $\\textbf{M}$ulti-$\\textbf{A}$ction-$\\textbf{H}$ead\n$\\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the\nvector correspond to the various objectives instead of a single scalar; and\n(iii) demonstrates how such a system provides fine-grained inference-time user\ncontrol. Experiments across math reasoning, value alignment, and multi-turn\ndialogue show that our framework improves performance across multiple\nobjectives simultaneously, while minimizing cross-objective trade-offs and\nenabling flexible inference time user control. The code can be found at\nhttps://github.com/pearls-lab/multiobj-align.",
      "authors": [
        "Yiran Shen",
        "Yu Xia",
        "Jonathan Chang",
        "Prithviraj Ammanabrolu"
      ],
      "published": "2025-10-01T17:54:15Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01167v1"
    },
    {
      "arxiv_id": "2510.01165v1",
      "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient\n  Few-Shot Reasoning",
      "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks,\nbut their effectiveness often depends on the quality of the provided context.\nRetrieval-Augmented Generation (RAG) enriches prompts with external\ninformation, but its reliance on static databases constrains adaptability and\ncan result in irrelevant demonstrations. In this work, we propose a Generative\nRetrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach\nwhere an LLM model is trained to generate input-specific concise\ndemonstrations. By tailoring demonstrations to each input, our method offers\nbetter contextual support than traditional RAG approaches. We demonstrate the\nsuperiority of GRAD under budget constraints, where we limit both the number of\ntokens used per demonstration and the number of tokens used for the final\noutput. Trained solely on a math dataset, GRAD consistently outperforms strong\nbaselines on Qwen2.5-14B across mathematical reasoning and advanced STEM\nquestions, highlighting GRAD's robust generalization to out-of-distribution\n(OOD) domains such as physics, chemistry, and computer science. Furthermore, we\nshow that demonstrations generated by trained smaller models can effectively\nguide larger target models, reducing training costs while maintaining\ncompetitive accuracy. Overall, this work introduces a scalable demonstration\ngenerator model presenting the first step toward a dynamic few-shot learning\nparadigm in resource-constrained settings. We release the code used for the\nproject.",
      "authors": [
        "Oussama Gabouj",
        "Kamel Charaf",
        "Ivan Zakazov",
        "Nicolas Baldwin",
        "Robert West"
      ],
      "published": "2025-10-01T17:52:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01165v1"
    },
    {
      "arxiv_id": "2510.01164v1",
      "title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social\n  Welfare",
      "summary": "Large language models (LLMs) are increasingly entrusted with high-stakes\ndecisions that affect human welfare. However, the principles and values that\nguide these models when distributing scarce societal resources remain largely\nunexamined. To address this, we introduce the Social Welfare Function (SWF)\nBenchmark, a dynamic simulation environment where an LLM acts as a sovereign\nallocator, distributing tasks to a heterogeneous community of recipients. The\nbenchmark is designed to create a persistent trade-off between maximizing\ncollective efficiency (measured by Return on Investment) and ensuring\ndistributive fairness (measured by the Gini coefficient). We evaluate 20\nstate-of-the-art LLMs and present the first leaderboard for social welfare\nallocation. Our findings reveal three key insights: (i) A model's general\nconversational ability, as measured by popular leaderboards, is a poor\npredictor of its allocation skill. (ii) Most LLMs exhibit a strong default\nutilitarian orientation, prioritizing group productivity at the expense of\nsevere inequality. (iii) Allocation strategies are highly vulnerable, easily\nperturbed by output-length constraints and social-influence framing. These\nresults highlight the risks of deploying current LLMs as societal\ndecision-makers and underscore the need for specialized benchmarks and targeted\nalignment for AI governance.",
      "authors": [
        "Zhengliang Shi",
        "Ruotian Ma",
        "Jen-tse Huang",
        "Xinbei Ma",
        "Xingyu Chen",
        "Mengru Wang",
        "Qu Yang",
        "Yue Wang",
        "Fanghua Ye",
        "Ziyang Chen",
        "Shanyi Wang",
        "Cixing Li",
        "Wenxuan Wang",
        "Zhaopeng Tu",
        "Xiaolong Li",
        "Zhaochun Ren",
        "Linus"
      ],
      "published": "2025-10-01T17:52:31Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01164v1"
    },
    {
      "arxiv_id": "2510.01157v1",
      "title": "Backdoor Attacks Against Speech Language Models",
      "summary": "Large Language Models (LLMs) and their multimodal extensions are becoming\nincreasingly popular. One common approach to enable multimodality is to cascade\ndomain-specific encoders with an LLM, making the resulting model inherit\nvulnerabilities from all of its components. In this work, we present the first\nsystematic study of audio backdoor attacks against speech language models. We\ndemonstrate its effectiveness across four speech encoders and three datasets,\ncovering four tasks: automatic speech recognition (ASR), speech emotion\nrecognition, and gender and age prediction. The attack consistently achieves\nhigh success rates, ranging from 90.76% to 99.41%. To better understand how\nbackdoors propagate, we conduct a component-wise analysis to identify the most\nvulnerable stages of the pipeline. Finally, we propose a fine-tuning-based\ndefense that mitigates the threat of poisoned pretrained encoders.",
      "authors": [
        "Alexandrine Fortier",
        "Thomas Thebaud",
        "Jesús Villalba",
        "Najim Dehak",
        "Patrick Cardinal"
      ],
      "published": "2025-10-01T17:45:04Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01157v1"
    },
    {
      "arxiv_id": "2510.01152v1",
      "title": "Pay-Per-Search Models are Abstention Models",
      "summary": "LLMs cannot reliably recognize their parametric knowledge boundaries and\noften hallucinate answers to outside-of-boundary questions. In contrast, humans\nrecognize their limitations and can either seek external help for such\nquestions or abstain. In this paper, we introduce MASH (Modeling Abstention via\nSelective Help-seeking), a training framework that readily extracts abstentions\nfrom LLMs. Our key idea is that any external help-seeking by an LLM, i.e.\nsearch tool use, can serve as a proxy for abstention if the external help\n(search) is appropriately penalized while simultaneously rewarding answer\naccuracy. MASH operationalizes this idea using reinforcement learning with a\npay-per-search reward.\n  We run experiments on three knowledge-intensive QA datasets. Our results show\nthat MASH substantially improves upon the selective help-seeking performance of\nprior efficient search approaches; on multi-hop datasets, MASH improves answer\naccuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf\nabstention -- it can distinguish between unanswerable/answerable questions and\nselectively generate responses for answerable questions -- showcasing behavior\nanalogous to specialized abstention approaches. We emphasize that contrary to\nprior abstention methods, MASH does not require pre-determining knowledge\nboundaries to construct training data. Instead, MASH's abstentions are a\nby-product of training for the auxiliary selective help-seeking task. Overall,\nwe show that MASH training effectively aligns search tool use with parametric\nknowledge, which can be successfully leveraged for making abstention decisions.",
      "authors": [
        "Mustafa Omer Gul",
        "Claire Cardie",
        "Tanya Goyal"
      ],
      "published": "2025-10-01T17:41:54Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01152v1"
    },
    {
      "arxiv_id": "2510.01146v1",
      "title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models",
      "summary": "Evaluation using Large Language Model (LLM) judges has been widely adopted in\nEnglish and shown to be effective for automatic evaluation. However, their\nperformance does not generalize well to non-English settings, and it remains\nunclear what constitutes effective multilingual training for such judges. In\nthis paper, we introduce mR3, a massively multilingual, rubric-agnostic reward\nreasoning model trained on 72 languages, achieving the broadest language\ncoverage in reward modeling to date. We present a comprehensive study of data\nand curriculum selection for training to identify effective strategies and data\nsources for building high-quality reward models, including the integration of\ntarget-language reasoning datasets. Our approach attains state-of-the-art\nperformance on multilingual reward model benchmarks, surpassing much larger\nmodels (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness\nis further confirmed through extensive ablation studies. Our models, data, and\ncode are available as open source at https://github.com/rubricreward/mr3.",
      "authors": [
        "David Anugraha",
        "Shou-Yi Hung",
        "Zilu Tang",
        "Annie En-Shiun Lee",
        "Derry Tanti Wijaya",
        "Genta Indra Winata"
      ],
      "published": "2025-10-01T17:36:59Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01146v1"
    },
    {
      "arxiv_id": "2510.01145v1",
      "title": "Automatic Speech Recognition (ASR) for African Low-Resource Languages: A\n  Systematic Literature Review",
      "summary": "ASR has achieved remarkable global progress, yet African low-resource\nlanguages remain rigorously underrepresented, producing barriers to digital\ninclusion across the continent with more than +2000 languages. This systematic\nliterature review (SLR) explores research on ASR for African languages with a\nfocus on datasets, models and training methods, evaluation techniques,\nchallenges, and recommends future directions. We employ the PRISMA 2020\nprocedures and search DBLP, ACM Digital Library, Google Scholar, Semantic\nScholar, and arXiv for studies published between January 2020 and July 2025. We\ninclude studies related to ASR datasets, models or metrics for African\nlanguages, while excluding non-African, duplicates, and low-quality studies\n(score <3/5). We screen 71 out of 2,062 records and we record a total of 74\ndatasets across 111 languages, encompassing approximately 11,206 hours of\nspeech. Fewer than 15% of research provided reproducible materials, and dataset\nlicensing is not clear. Self-supervised and transfer learning techniques are\npromising, but are hindered by limited pre-training data, inadequate coverage\nof dialects, and the availability of resources. Most of the researchers use\nWord Error Rate (WER), with very minimal use of linguistically informed scores\nsuch as Character Error Rate (CER) or Diacritic Error Rate (DER), and thus with\nlimited application in tonal and morphologically rich languages. The existing\nevidence on ASR systems is inconsistent, hindered by issues like dataset\navailability, poor annotations, licensing uncertainties, and limited\nbenchmarking. Nevertheless, the rise of community-driven initiatives and\nmethodological advancements indicates a pathway for improvement. Sustainable\ndevelopment for this area will also include stakeholder partnership, creation\nof ethically well-balanced datasets, use of lightweight modelling techniques,\nand active benchmarking.",
      "authors": [
        "Sukairaj Hafiz Imam",
        "Tadesse Destaw Belay",
        "Kedir Yassin Husse",
        "Ibrahim Said Ahmad",
        "Idris Abdulmumin",
        "Hadiza Ali Umar",
        "Muhammad Yahuza Bello",
        "Joyce Nakatumba-Nabende",
        "Seid Muhie Yimam",
        "Shamsuddeen Hassan Muhammad"
      ],
      "published": "2025-10-01T17:36:06Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01145v1"
    },
    {
      "arxiv_id": "2510.01135v1",
      "title": "Prompt Curriculum Learning for Efficient LLM Post-Training",
      "summary": "We introduce Prompt Curriculum Learning (PCL), a lightweight reinforcement\nlearning (RL) algorithm that selects intermediate-difficulty prompts using a\nlearned value model to post-train language models. Since post-training LLMs via\nRL remains sensitive to batching and prompt selection strategies, we first\nconduct a series of systematic experiments where we (1) determine the optimal\ntraining batch size that balances generation efficiency and gradient quality\nand (2) establish the importance of focusing on prompts of intermediate\ndifficulty for the policy. We build upon these results to design PCL, which\nidentifies prompts of intermediate difficulty for the current policy in an\non-policy manner by using a value model that is concurrently updated based on\nthe current policy. By focusing on informative prompts that yield high\neffective ratios, PCL achieves either the highest performance or requires\nsignificantly less time to reach comparable performance to its counterparts.\nCompared to rollout-based filtering methods, PCL avoids costly rollouts and\nachieves $12.1\\times$ and $16.9\\times$ faster speed on identifying\nintermediate-difficulty prompts when training on MATH and DeepScaleR,\nrespectively. We further demonstrate that our value model accurately predicts\nprompt difficulty and allows PCL to focus on progressively more challenging\nprompts during RL. Our results present a new methodology that delivers improved\ntradeoff between upper-bound performance and efficiency for reasoning-focused\nRL.",
      "authors": [
        "Zhaolin Gao",
        "Joongwon Kim",
        "Wen Sun",
        "Thorsten Joachims",
        "Sid Wang",
        "Richard Yuanzhe Pang",
        "Liang Tan"
      ],
      "published": "2025-10-01T17:24:28Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01135v1"
    },
    {
      "arxiv_id": "2510.01132v1",
      "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
      "summary": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
      "authors": [
        "Ruiyi Wang",
        "Prithviraj Ammanabrolu"
      ],
      "published": "2025-10-01T17:23:04Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01132v1"
    },
    {
      "arxiv_id": "2510.01076v1",
      "title": "Research on the Integration of Embodied Intelligence and Reinforcement\n  Learning in Textual Domains",
      "summary": "This article addresses embodied intelligence and reinforcement learning\nintegration in the field of text processing, aiming to enhance text handling\nwith more intelligence on the basis of embodied intelligence's perception and\naction superiority and reinforcement learning's decision optimization\ncapability. Through detailed theoretical explanation and experimental\nexploration, a novel integration model is introduced. This model has been\ndemonstrated to be very effective in a wide range oftext processing tasks,\nvalidating its applicative potential",
      "authors": [
        "Haonan Wang",
        "Junfeng Sun",
        "Mingjia Zhao",
        "Wei Liu"
      ],
      "published": "2025-10-01T16:21:04Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01076v1"
    },
    {
      "arxiv_id": "2510.01052v1",
      "title": "Hybrid Dialogue State Tracking for Persian Chatbots: A Language\n  Model-Based Approach",
      "summary": "Dialogue State Tracking (DST) is an essential element of conversational AI\nwith the objective of deeply understanding the conversation context and leading\nit toward answering user requests. Due to high demands for open-domain and\nmulti-turn chatbots, the traditional rule-based DST is not efficient enough,\nsince it cannot provide the required adaptability and coherence for human-like\nexperiences in complex conversations. This study proposes a hybrid DST model\nthat utilizes rule-based methods along with language models, including BERT for\nslot filling and intent detection, XGBoost for intent validation, GPT for DST,\nand online agents for real-time answer generation. This model is uniquely\ndesigned to be evaluated on a comprehensive Persian multi-turn dialogue dataset\nand demonstrated significantly improved accuracy and coherence over existing\nmethods in Persian-based chatbots. The results demonstrate how effectively a\nhybrid approach may improve DST capabilities, paving the way for conversational\nAI systems that are more customized, adaptable, and human-like.",
      "authors": [
        "Samin Mahdipour Aghabagher",
        "Saeedeh Momtazi"
      ],
      "published": "2025-10-01T15:57:19Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01052v1"
    },
    {
      "arxiv_id": "2510.01051v1",
      "title": "GEM: A Gym for Agentic LLMs",
      "summary": "The training paradigm for large language models (LLMs) is moving from static\ndatasets to experience-based learning, where agents acquire skills via\ninteracting with complex environments. To facilitate this transition we\nintroduce GEM (General Experience Maker), an open-source environment simulator\ndesigned for the age of LLMs. Analogous to OpenAI-Gym for traditional\nreinforcement learning (RL), GEM provides a standardized framework for the\nenvironment-agent interface, including asynchronous vectorized execution for\nhigh throughput, and flexible wrappers for easy extensibility. GEM also\nfeatures a diverse suite of environments, robust integrated tools, and\nsingle-file example scripts demonstrating using GEM with five popular RL\ntraining frameworks. Along with this, we also provide a set of baselines across\n24 environments using REINFORCE with Return Batch Normalization (ReBN), which\n-- unlike GRPO -- is compatible with the full RL setting of dense per-turn\nrewards and offers better credit assignment. We further conduct apple-to-apple\nbenchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings\nusing GEM to shed light on the algorithmic designs. Lastly, GEM also functions\nas a convenient evaluation toolkit besides a training environment. We hope this\nframework can help accelerate future agentic LLM research.",
      "authors": [
        "Zichen Liu",
        "Anya Sims",
        "Keyu Duan",
        "Changyu Chen",
        "Simon Yu",
        "Xiangxin Zhou",
        "Haotian Xu",
        "Shaopan Xiong",
        "Bo Liu",
        "Chenmien Tan",
        "Chuen Yang Beh",
        "Weixun Wang",
        "Hao Zhu",
        "Weiyan Shi",
        "Diyi Yang",
        "Michael Shieh",
        "Yee Whye Teh",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "published": "2025-10-01T15:55:57Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.01051v1"
    },
    {
      "arxiv_id": "2510.01048v1",
      "title": "Interpreting Language Models Through Concept Descriptions: A Survey",
      "summary": "Understanding the decision-making processes of neural networks is a central\ngoal of mechanistic interpretability. In the context of Large Language Models\n(LLMs), this involves uncovering the underlying mechanisms and identifying the\nroles of individual model components such as neurons and attention heads, as\nwell as model abstractions such as the learned sparse features extracted by\nSparse Autoencoders (SAEs). A rapidly growing line of work tackles this\nchallenge by using powerful generator models to produce open-vocabulary,\nnatural language concept descriptions for these components. In this paper, we\nprovide the first survey of the emerging field of concept descriptions for\nmodel components and abstractions. We chart the key methods for generating\nthese descriptions, the evolving landscape of automated and human metrics for\nevaluating them, and the datasets that underpin this research. Our synthesis\nreveals a growing demand for more rigorous, causal evaluation. By outlining the\nstate of the art and identifying key challenges, this survey provides a roadmap\nfor future research toward making models more transparent.",
      "authors": [
        "Nils Feldhus",
        "Laura Kopf"
      ],
      "published": "2025-10-01T15:51:44Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01048v1"
    },
    {
      "arxiv_id": "2510.01047v1",
      "title": "Authentic Discrete Diffusion Model",
      "summary": "We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally\nredefines prior pseudo-discrete approaches by preserving core diffusion\ncharacteristics directly in the one-hot space through a suite of coordinated\nmechanisms. Unlike conventional \"pseudo\" discrete diffusion (PDD) methods, ADD\nreformulates the diffusion input by directly using float-encoded one-hot class\ndata, without relying on diffusing in the continuous latent spaces or masking\npolicies. At its core, a timestep-conditioned cross-entropy loss is introduced\nbetween the diffusion model's outputs and the original one-hot labels. This\nsynergistic design establishes a bridge between discriminative and generative\nlearning. Our experiments demonstrate that ADD not only achieves superior\nperformance on classification tasks compared to the baseline, but also exhibits\nexcellent text generation capabilities on Image captioning. Extensive ablations\nvalidate the measurable gains of each component.",
      "authors": [
        "Xiao Li",
        "Jiaqi Zhang",
        "Shuxiang Zhang",
        "Tianshui Chen",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "published": "2025-10-01T15:51:10Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.01047v1"
    },
    {
      "arxiv_id": "2510.01028v1",
      "title": "Syntax-Guided Diffusion Language Models with User-Integrated\n  Personalization",
      "summary": "Large language models have made revolutionary progress in generating\nhuman-like text, yet their outputs often tend to be generic, exhibiting\ninsufficient structural diversity, which limits personalized expression. Recent\nadvances in diffusion models have opened new opportunities for improving\nlanguage generation beyond the limitations of autoregressive paradigms. In this\nwork, we propose a syntax-guided diffusion language model that integrates\nstructural supervision and personalized conditioning to enhance text quality,\ndiversity, and controllability. We introduce a cascaded framework that\ngenerates syntactic guidance before conditional text generation, and further\ngeneralize it to a novel noncascaded architecture for better alignment between\nstructure and content. By incorporating syntactic information in the generating\nprocess, the proposed model better captures the lexical and structural\ncharacteristics of stylistic sentence construction. To enable fine-grained\npersonalization, we develop a shared representation mechanism that facilitates\ninformation integration across users, supporting both faithful stylistic\ngeneration and generalizable zero-shot inference. Extensive experiments on\nmultiple tasks demonstrate the superiority of our approach in fluency,\ndiversity, and stylistic fidelity. Further qualitative analyses highlight its\ninterpretability and flexibility in learning personalized patterns.",
      "authors": [
        "Ruqian Zhang",
        "Yijiao Zhang",
        "Juan Shen",
        "Zhongyi Zhu",
        "Annie Qu"
      ],
      "published": "2025-10-01T15:33:12Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.01028v1"
    },
    {
      "arxiv_id": "2510.01025v1",
      "title": "Shape Happens: Automatic Feature Manifold Discovery in LLMs via\n  Supervised Multi-Dimensional Scaling",
      "summary": "The linear representation hypothesis states that language models (LMs) encode\nconcepts as directions in their latent space, forming organized,\nmultidimensional manifolds. Prior efforts focus on discovering specific\ngeometries for specific features, and thus lack generalization. We introduce\nSupervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to\nautomatically discover feature manifolds. We apply SMDS to temporal reasoning\nas a case study, finding that different features form various geometric\nstructures such as circles, lines, and clusters. SMDS reveals many insights on\nthese structures: they consistently reflect the properties of the concepts they\nrepresent; are stable across model families and sizes; actively support\nreasoning in models; and dynamically reshape in response to context changes.\nTogether, our findings shed light on the functional role of feature manifolds,\nsupporting a model of entity-based reasoning in which LMs encode and transform\nstructured representations.",
      "authors": [
        "Federico Tiblias",
        "Irina Bigoulaeva",
        "Jingcheng Niu",
        "Simone Balloccu",
        "Iryna Gurevych"
      ],
      "published": "2025-10-01T15:30:47Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.01025v1"
    },
    {
      "arxiv_id": "2510.01003v1",
      "title": "Improving Code Localization with Repository Memory",
      "summary": "Code localization is a fundamental challenge in repository-level software\nengineering tasks such as bug fixing. While existing methods equip language\nagents with comprehensive tools/interfaces to fetch information from the\nrepository, they overlook the critical aspect of memory, where each instance is\ntypically handled from scratch assuming no prior repository knowledge. In\ncontrast, human developers naturally build long-term repository memory, such as\nthe functionality of key modules and associations between various bug types and\ntheir likely fix locations. In this work, we augment language agents with such\nmemory by leveraging a repository's commit history - a rich yet underutilized\nresource that chronicles the codebase's evolution. We introduce tools that\nallow the agent to retrieve from a non-parametric memory encompassing recent\nhistorical commits and linked issues, as well as functionality summaries of\nactively evolving parts of the codebase identified via commit patterns. We\ndemonstrate that augmenting such a memory can significantly improve LocAgent, a\nstate-of-the-art localization framework, on both SWE-bench-verified and the\nmore recent SWE-bench-live benchmarks. Our research contributes towards\ndeveloping agents that can accumulate and leverage past experience for\nlong-horizon tasks, more closely emulating the expertise of human developers.",
      "authors": [
        "Boshi Wang",
        "Weijian Xu",
        "Yunsheng Li",
        "Mei Gao",
        "Yujia Xie",
        "Huan Sun",
        "Dongdong Chen"
      ],
      "published": "2025-10-01T15:10:15Z",
      "primary_category": "cs.SE",
      "arxiv_url": "https://arxiv.org/abs/2510.01003v1"
    },
    {
      "arxiv_id": "2510.00982v1",
      "title": "Spiralformer: Low Latency Encoder for Streaming Speech Recognition with\n  Circular Layer Skipping and Early Exiting",
      "summary": "For streaming speech recognition, a Transformer-based encoder has been widely\nused with block processing. Although many studies addressed improving emission\nlatency of transducers, little work has been explored for improving encoding\nlatency of the block processing. We seek to reduce latency by frequently\nemitting a chunk with a small shift rather than scarce large-chunk emissions,\nresulting in higher computational costs. To efficiently compute with the small\nchunk shift, we propose a new encoder, Spiralformer, tailored for block\nprocessing by combining layer dropping and early exiting. We skip layer\ncomputation in a cyclic manner and shift the computed layer in each block\nspirally, which completes computation for all the layers over the block\nprocessing. Experimentally, we observed that our method achieved 21.6%\nreduction in the averaged token emission delay in Librispeech, and 7.0% in CSJ,\ncompared with the baseline with similar computational cost and word error\nrates.",
      "authors": [
        "Emiru Tsunoo",
        "Hayato Futami",
        "Yosuke Kashiwagi",
        "Siddhant Arora",
        "Shinji Watanabe"
      ],
      "published": "2025-10-01T14:56:45Z",
      "primary_category": "eess.AS",
      "arxiv_url": "https://arxiv.org/abs/2510.00982v1"
    },
    {
      "arxiv_id": "2510.00977v1",
      "title": "It Takes Two: Your GRPO Is Secretly DPO",
      "summary": "Group Relative Policy Optimization (GRPO) is a prominent reinforcement\nlearning algorithm for post-training Large Language Models (LLMs). It is\ncommonly believed that GRPO necessitates a large group size to ensure stable\ntraining via precise statistical estimation, which incurs substantial\ncomputational overhead. In this work, we challenge this assumption by reframing\nGRPO as a form of contrastive learning, which reveals a fundamental connection\nto Direct Preference Optimization (DPO). Motivated by DPO's empirical success,\nwe investigate the minimal two-rollout case (2-GRPO), a configuration\npreviously deemed infeasible. We provide a rigorous theoretical analysis to\nvalidate 2-GRPO and demonstrate empirically that it achieves performance on par\nwith 16-GRPO, despite using only 1/8 of the rollouts and reducing training time\nby over 70%.",
      "authors": [
        "Yihong Wu",
        "Liheng Ma",
        "Lei Ding",
        "Muzhi Li",
        "Xinyu Wang",
        "Kejia Chen",
        "Zhan Su",
        "Zhanguang Zhang",
        "Chenyang Huang",
        "Yingxue Zhang",
        "Mark Coates",
        "Jian-Yun Nie"
      ],
      "published": "2025-10-01T14:52:11Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00977v1"
    },
    {
      "arxiv_id": "2510.00962v1",
      "title": "Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning\n  Benchmarks",
      "summary": "Large language models (LLMs) are ubiquitous in modern day natural language\nprocessing. However, previous work has shown degraded LLM performance for\nunder-represented English dialects. We analyze the effects of typifying\n\"standard\" American English language questions as non-\"standard\" dialectal\nvariants on multiple choice question answering tasks and find up to a 20%\nreduction in accuracy. Additionally, we investigate the grammatical basis of\nunder-performance in non-\"standard\" English questions. We find that individual\ngrammatical rules have varied effects on performance, but some are more\nconsequential than others: three specific grammar rules (existential \"it\", zero\ncopula, and y'all) can explain the majority of performance degradation observed\nin multiple dialects. We call for future work to investigate bias mitigation\nmethods focused on individual, high-impact grammatical structures.",
      "authors": [
        "Eileen Pan",
        "Anna Seo Gyeong Choi",
        "Maartje ter Hoeve",
        "Skyler Seto",
        "Allison Koenecke"
      ],
      "published": "2025-10-01T14:35:16Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00962v1"
    },
    {
      "arxiv_id": "2510.00931v1",
      "title": "Making, not Taking, the Best of N",
      "summary": "Obtaining high-quality generations in modern LLMs has largely been framed as\na selection problem: identifying a single winning generation from a diverse\npool of N samples, the Best-of-N (BoN). Yet, this approach is inherently\nzero-sum, discarding diverse and potentially useful information from the pool.\nInstead, we explore a collaborative setup, where all candidates can potentially\ncontribute to the final winning generation. To this end, we propose Fusion-of-N\n(FusioN): a method that uses a general LLM judge to synthesize the most\ninformative elements of each sample into a single final answer. We compare\nFusioN to BoN in two settings, (i) test-time scaling, where we sample and\naggregate from a single model at test-time (ii) synthetic data generation,\nwhere we fuse samples from a pool of diverse teachers to improve a student\nmodel. We extensively benchmark both setups across 11 languages, 3 diverse\ntasks and varying model scales. Across the bench, FusioN consistently\noutperforms BoN showing versatility and robustness both in test-time scaling\nand in downstream gains from synthetic data generation. We also perform\nextensive analysis on FusioN, where it shows surprising strengths and\nrobustness under challenging settings. These results show that we should shift\nhow we think about evaluating and utilizing LLM generations from a monolithic\nmeasure of quality, to embracing their polylithic nature. This shift allows us\nto integrate diverse strengths, unlock latent potential, and achieve\nimprovements that were previously inaccessible through selection alone.",
      "authors": [
        "Ammar Khairi",
        "Daniel D'souza",
        "Marzieh Fadaee",
        "Julia Kreutzer"
      ],
      "published": "2025-10-01T14:14:31Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00931v1"
    },
    {
      "arxiv_id": "2510.00919v2",
      "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in\n  Olympic-Level Physics Problem Solving",
      "summary": "Retrieval-augmented generation (RAG) with foundation models has achieved\nstrong performance across diverse tasks, but their capacity for expert-level\nreasoning-such as solving Olympiad-level physics problems-remains largely\nunexplored. Inspired by the way students prepare for competitions by reviewing\npast problems, we investigate the potential of RAG to enhance physics reasoning\nin foundation models. We introduce PhoPile, a high-quality multimodal dataset\nspecifically designed for Olympiad-level physics, enabling systematic study of\nretrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,\ncapturing the inherently multimodal nature of physics problem solving. Using\nPhoPile, we benchmark RAG-augmented foundation models, covering both large\nlanguage models (LLMs) and large multimodal models (LMMs) with multiple\nretrievers. Our results demonstrate that integrating retrieval with physics\ncorpora can improve model performance, while also highlighting challenges that\nmotivate further research in retrieval-augmented physics reasoning.",
      "authors": [
        "Shunfeng Zheng",
        "Yudi Zhang",
        "Meng Fang",
        "Zihan Zhang",
        "Zhitan Wu",
        "Mykola Pechenizkiy",
        "Ling Chen"
      ],
      "published": "2025-10-01T13:57:53Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00919v2"
    },
    {
      "arxiv_id": "2510.00908v1",
      "title": "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval\n  with Multilingual LLMs",
      "summary": "Cross-lingual information retrieval (CLIR) addresses the challenge of\nretrieving relevant documents written in languages different from that of the\noriginal query. Research in this area has typically framed the task as\nmonolingual retrieval augmented by translation, treating retrieval methods and\ncross-lingual capabilities in isolation. Both monolingual and cross-lingual\nretrieval usually follow a pipeline of query expansion, ranking, re-ranking\nand, increasingly, question answering. Recent advances, however, have shifted\nfrom translation-based methods toward embedding-based approaches and leverage\nmultilingual large language models (LLMs), for which aligning representations\nacross languages remains a central challenge. The emergence of cross-lingual\nembeddings and multilingual LLMs has introduced a new paradigm, offering\nimproved retrieval performance and enabling answer generation. This survey\nprovides a comprehensive overview of developments from early translation-based\nmethods to state-of-the-art embedding-driven and generative techniques. It\npresents a structured account of core CLIR components, evaluation practices,\nand available resources. Persistent challenges such as data imbalance and\nlinguistic variation are identified, while promising directions are suggested\nfor advancing equitable and effective cross-lingual information retrieval. By\nsituating CLIR within the broader landscape of information retrieval and\nmultilingual language processing, this work not only reviews current\ncapabilities but also outlines future directions for building retrieval systems\nthat are robust, inclusive, and adaptable.",
      "authors": [
        "Roksana Goworek",
        "Olivia Macmillan-Scott",
        "Eda B. Özyiğit"
      ],
      "published": "2025-10-01T13:50:05Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.00908v1"
    },
    {
      "arxiv_id": "2510.00890v1",
      "title": "Span-level Detection of AI-generated Scientific Text via Contrastive\n  Learning and Structural Calibration",
      "summary": "The rapid adoption of large language models (LLMs) in scientific writing\nraises serious concerns regarding authorship integrity and the reliability of\nscholarly publications. Existing detection approaches mainly rely on\ndocument-level classification or surface-level statistical cues; however, they\nneglect fine-grained span localization, exhibit weak calibration, and often\nfail to generalize across disciplines and generators. To address these\nlimitations, we present Sci-SpanDet, a structure-aware framework for detecting\nAI-generated scholarly texts. The proposed method combines section-conditioned\nstylistic modeling with multi-level contrastive learning to capture nuanced\nhuman-AI differences while mitigating topic dependence, thereby enhancing\ncross-domain robustness. In addition, it integrates BIO-CRF sequence labeling\nwith pointer-based boundary decoding and confidence calibration to enable\nprecise span-level detection and reliable probability estimates. Extensive\nexperiments on a newly constructed cross-disciplinary dataset of 100,000\nannotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,\nLLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with\nF1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows\nstrong resilience under adversarial rewriting and maintains balanced accuracy\nacross IMRaD sections and diverse disciplines, substantially surpassing\nexisting baselines. To ensure reproducibility and to foster further research on\nAI-generated text detection in scholarly documents, the curated dataset and\nsource code will be publicly released upon publication.",
      "authors": [
        "Zhen Yin",
        "Shenghua Wang"
      ],
      "published": "2025-10-01T13:35:14Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00890v1"
    },
    {
      "arxiv_id": "2510.00880v1",
      "title": "HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate\n  Hallucinations in Retrieval-Augmented Generation",
      "summary": "Large Language Models (LLMs) excel in many NLP tasks but remain prone to\nhallucinations, limiting trust in real-world applications. We present\nHalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating\nhallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies\ndocument-claim pairs as grounded or hallucinated and produces evidence-grounded\njustifications for transparency. Our approach combines (i) a domain-agnostic\nsynthetic dataset derived from FineWeb and refined through multi-stage curation\nand data reformation, (ii) synthetic grounded and hallucinated claims, and\n(iii) preference-based fine-tuning with Odds Ratio Preference Optimization to\ndistill large-model reasoning into a smaller backbone. On the RAGTruth subset\nof the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy\n(BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian\n3.3 (8B; 82.2%) while using roughly half their parameters. Over the full\nbenchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as\nGPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon\nacceptance.",
      "authors": [
        "Loris Bergeron",
        "Ioana Buhnila",
        "Jérôme François",
        "Radu State"
      ],
      "published": "2025-10-01T13:28:20Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00880v1"
    },
    {
      "arxiv_id": "2510.00866v2",
      "title": "The Data-Quality Illusion: Rethinking Classifier-Based Quality Filtering\n  for LLM Pretraining",
      "summary": "Large-scale models are pretrained on massive web-crawled datasets containing\ndocuments of mixed quality, making data filtering essential. A popular method\nis Classifier-based Quality Filtering (CQF), which trains a binary classifier\nto distinguish between pretraining data and a small, high-quality set. It\nassigns each pretraining document a quality score defined as the classifier's\nscore and retains only the top-scoring ones. We provide an in-depth analysis of\nCQF. We show that while CQF improves downstream task performance, it does not\nnecessarily enhance language modeling on the high-quality dataset. We explain\nthis paradox by the fact that CQF implicitly filters the high-quality dataset\nas well. We further compare the behavior of models trained with CQF to those\ntrained on synthetic data of increasing quality, obtained via random token\npermutations, and find starkly different trends. Our results challenge the view\nthat CQF captures a meaningful notion of data quality.",
      "authors": [
        "Thiziri Nait Saada",
        "Louis Bethune",
        "Michal Klein",
        "David Grangier",
        "Marco Cuturi",
        "Pierre Ablin"
      ],
      "published": "2025-10-01T13:15:15Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00866v2"
    },
    {
      "arxiv_id": "2510.00861v1",
      "title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented\n  LLMs",
      "summary": "While search-augmented large language models (LLMs) exhibit impressive\ncapabilities, their reliability in complex multi-hop reasoning remains limited.\nThis limitation arises from three fundamental challenges: decomposition errors,\nwhere tasks are incorrectly broken down; retrieval missing, where key evidence\nfails to be retrieved; and reasoning errors, where flawed logic propagates\nthrough the reasoning chain. A single failure in any of these stages can derail\nthe final answer. We propose Erasable Reinforcement Learning (ERL), a novel\nframework that transforms fragile reasoning into a robust process. ERL\nexplicitly identifies faulty steps, erases them, and regenerates reasoning in\nplace, preventing defective logic from propagating through the reasoning chain.\nThis targeted correction mechanism turns brittle reasoning into a more\nresilient process. Models trained with ERL, termed ESearch, achieve substantial\nimprovements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model\nachieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and\n+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest\nthat erasable reinforcement learning provides a powerful paradigm shift for\nrobust multi-step reasoning in LLMs.",
      "authors": [
        "Ziliang Wang",
        "Kang An",
        "Xuhui Zheng",
        "Faqiang Qian",
        "Weikun Zhang",
        "Cijun Ouyang",
        "Jialu Cai",
        "Yuhang Wang",
        "Yichao Wu"
      ],
      "published": "2025-10-01T13:10:36Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00861v1"
    },
    {
      "arxiv_id": "2510.00857v1",
      "title": "ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous\n  LLMs",
      "summary": "As large language models (LLMs) evolve from conversational assistants into\nautonomous agents, evaluating the safety of their actions becomes critical.\nPrior safety benchmarks have primarily focused on preventing generation of\nharmful content, such as toxic text. However, they overlook the challenge of\nagents taking harmful actions when the most effective path to an operational\ngoal conflicts with human safety. To address this gap, we introduce\nManagerBench, a benchmark that evaluates LLM decision-making in realistic,\nhuman-validated managerial scenarios. Each scenario forces a choice between a\npragmatic but harmful action that achieves an operational goal, and a safe\naction that leads to worse operational performance. A parallel control set,\nwhere potential harm is directed only at inanimate objects, measures a model's\npragmatism and identifies its tendency to be overly safe. Our findings indicate\nthat the frontier LLMs perform poorly when navigating this safety-pragmatism\ntrade-off. Many consistently choose harmful options to advance their\noperational goals, while others avoid harm only to become overly safe and\nineffective. Critically, we find this misalignment does not stem from an\ninability to perceive harm, as models' harm assessments align with human\njudgments, but from flawed prioritization. ManagerBench is a challenging\nbenchmark for a core component of agentic behavior: making safe choices when\noperational goals and alignment values incentivize conflicting actions.\nBenchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.",
      "authors": [
        "Adi Simhi",
        "Jonathan Herzig",
        "Martin Tutek",
        "Itay Itzhak",
        "Idan Szpektor",
        "Yonatan Belinkov"
      ],
      "published": "2025-10-01T13:08:33Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00857v1"
    },
    {
      "arxiv_id": "2510.00855v1",
      "title": "Can World Models Benefit VLMs for World Dynamics?",
      "summary": "Trained on internet-scale video data, generative world models are\nincreasingly recognized as powerful world simulators that can generate\nconsistent and plausible dynamics over structure, motion, and physics. This\nraises a natural question: with the advent of strong video foundational models,\nmight they supplant conventional vision encoder paradigms for general-purpose\nmultimodal understanding? While recent studies have begun to explore the\npotential of world models on common vision tasks, these explorations typically\nlack a systematic investigation of generic, multimodal tasks. In this work, we\nstrive to investigate the capabilities when world model priors are transferred\ninto Vision-Language Models: we re-purpose a video diffusion model as a\ngenerative encoder to perform a single denoising step and treat the resulting\nlatents as a set of visual embedding. We empirically investigate this class of\nmodels, which we refer to as World-Language Models (WorldLMs), and we find that\ngenerative encoders can capture latents useful for downstream understanding\nthat show distinctions from conventional encoders. Naming our best-performing\nvariant Dynamic Vision Aligner (DyVA), we further discover that this method\nsignificantly enhances spatial reasoning abilities and enables single-image\nmodels to perform multi-frame reasoning. Through the curation of a suite of\nvisual reasoning tasks, we find DyVA to surpass both open-source and\nproprietary baselines, achieving state-of-the-art or comparable performance. We\nattribute these gains to WorldLM's inherited motion-consistency internalization\nfrom video pre-training. Finally, we systematically explore extensive model\ndesigns to highlight promising directions for future work. We hope our study\ncan pave the way for a new family of VLMs that leverage priors from world\nmodels and are on a promising path towards generalist vision learners.",
      "authors": [
        "Kevin Zhang",
        "Kuangzhi Ge",
        "Xiaowei Chi",
        "Renrui Zhang",
        "Shaojun Shi",
        "Zhen Dong",
        "Sirui Han",
        "Shanghang Zhang"
      ],
      "published": "2025-10-01T13:07:05Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00855v1"
    },
    {
      "arxiv_id": "2510.00845v2",
      "title": "Mechanistic Interpretability as Statistical Estimation: A Variance\n  Analysis of EAP-IG",
      "summary": "The development of trustworthy artificial intelligence requires moving beyond\nblack-box performance metrics toward an understanding of models' internal\ncomputations. Mechanistic Interpretability (MI) aims to meet this need by\nidentifying the algorithmic mechanisms underlying model behaviors. Yet, the\nscientific rigor of MI critically depends on the reliability of its findings.\nIn this work, we argue that interpretability methods, such as circuit\ndiscovery, should be viewed as statistical estimators, subject to questions of\nvariance and robustness. To illustrate this statistical framing, we present a\nsystematic stability analysis of a state-of-the-art circuit discovery method:\nEAP-IG. We evaluate its variance and robustness through a comprehensive suite\nof controlled perturbations, including input resampling, prompt paraphrasing,\nhyperparameter variation, and injected noise within the causal analysis itself.\nAcross a diverse set of models and tasks, our results demonstrate that EAP-IG\nexhibits high structural variance and sensitivity to hyperparameters,\nquestioning the stability of its findings. Based on these results, we offer a\nset of best-practice recommendations for the field, advocating for the routine\nreporting of stability metrics to promote a more rigorous and statistically\ngrounded science of interpretability.",
      "authors": [
        "Maxime Méloux",
        "François Portet",
        "Maxime Peyrard"
      ],
      "published": "2025-10-01T12:55:34Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00845v2"
    },
    {
      "arxiv_id": "2510.00829v1",
      "title": "Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based\n  Machine Translation",
      "summary": "\\textbf{RE}trieval-\\textbf{A}ugmented \\textbf{L}LM-based \\textbf{M}achine\n\\textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like\nidiomatic translation, but its reliability under noisy retrieval contexts\nremains poorly understood despite this being a common challenge in real-world\ndeployment. To address this gap, we propose a noise synthesis framework and new\nmetrics to evaluate the robustness of REAL-MT systematically. Using this\nframework, we instantiate REAL-MT with Qwen-series models, including standard\nLLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate\ntheir performance on idiomatic translation across high-, medium-, and\nlow-resource language pairs under synthesized noise. Our results show that\nlow-resource language pairs, which rely more heavily on retrieved context,\ndegrade more severely under noise than high-resource ones and often produce\nnonsensical translations. Although LRMs possess enhanced reasoning\ncapabilities, they show no improvement in error correction and are even more\nsusceptible to noise, tending to rationalize incorrect contexts. We find that\nthis stems from an attention shift away from the source idiom to noisy content,\nwhile confidence increases despite declining accuracy, indicating poor\ncalibration. To mitigate these issues, we investigate training-free and\nfine-tuning strategies, which improve robustness at the cost of performance in\nclean contexts, revealing a fundamental trade-off. Our findings highlight the\nlimitations of current approaches, underscoring the need for self-verifying\nintegration mechanisms.",
      "authors": [
        "Yanming Sun",
        "Runzhe Zhan",
        "Chi Seng Cheang",
        "Han Wu",
        "Xuebo Liu",
        "Yuyao Niu",
        "Fengying Ye",
        "Kaixin Lan",
        "Lidia S. Chao",
        "Derek F. Wong"
      ],
      "published": "2025-10-01T12:43:55Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00829v1"
    },
    {
      "arxiv_id": "2510.00810v1",
      "title": "Family Matters: Language Transfer and Merging for Adapting Small LLMs to\n  Faroese",
      "summary": "We investigate how to adapt small, efficient LLMs to Faroese, a low-resource\nNorth Germanic language. Starting from English models, we continue pre-training\non related Scandinavian languages, either individually or combined via merging,\nbefore fine-tuning on Faroese. We compare full fine-tuning with\nparameter-efficient tuning using LoRA, evaluating their impact on both\nlinguistic accuracy and text comprehension. Due to the lack of existing Faroese\nevaluation data, we construct two new minimal-pair benchmarks from adapted and\nnewly collected datasets and complement them with human evaluations by Faroese\nlinguists. Our results demonstrate that transfer from related languages is\ncrucial, though the optimal source language depends on the task: Icelandic\nenhances linguistic accuracy, whereas Danish boosts comprehension. Similarly,\nthe choice between full fine-tuning and LoRA is task-dependent: LoRA improves\nlinguistic acceptability and slightly increases human evaluation scores on the\nbase model, while full fine-tuning yields stronger comprehension performance\nand better preserves model capabilities during downstream fine-tuning.",
      "authors": [
        "Jenny Kunz",
        "Iben Nyholm Debess",
        "Annika Simonsen"
      ],
      "published": "2025-10-01T12:17:09Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00810v1"
    },
    {
      "arxiv_id": "2510.00808v1",
      "title": "What You See is What You Ask: Evaluating Audio Descriptions",
      "summary": "Audio descriptions (ADs) narrate important visual details in movies, enabling\nBlind and Low Vision (BLV) users to understand narratives and appreciate visual\ndetails. Existing works in automatic AD generation mostly focus on few-second\ntrimmed clips, and evaluate them by comparing against a single ground-truth\nreference AD. However, writing ADs is inherently subjective. Through alignment\nand analysis of two independent AD tracks for the same movies, we quantify the\nsubjectivity in when and whether to describe, and what and how to highlight.\nThus, we show that working with trimmed clips is inadequate. We propose ADQA, a\nQA benchmark that evaluates ADs at the level of few-minute long, coherent video\nsegments, testing whether they would help BLV users understand the story and\nappreciate visual details. ADQA features visual appreciation (VA) questions\nabout visual facts and narrative understanding (NU) questions based on the\nplot. Through ADQA, we show that current AD generation methods lag far behind\nhuman-authored ADs. We conclude with several recommendations for future work\nand introduce a public leaderboard for benchmarking.",
      "authors": [
        "Divy Kala",
        "Eshika Khandelwal",
        "Makarand Tapaswi"
      ],
      "published": "2025-10-01T12:14:15Z",
      "primary_category": "cs.CV",
      "arxiv_url": "https://arxiv.org/abs/2510.00808v1"
    },
    {
      "arxiv_id": "2510.00743v1",
      "title": "From Scores to Preferences: Redefining MOS Benchmarking for Speech\n  Quality Reward Modeling",
      "summary": "Assessing the perceptual quality of synthetic speech is crucial for guiding\nthe development and refinement of speech generation models. However, it has\ntraditionally relied on human subjective ratings such as the Mean Opinion Score\n(MOS), which depend on manual annotations and often suffer from inconsistent\nrating standards and poor reproducibility. To address these limitations, we\nintroduce MOS-RMBench, a unified benchmark that reformulates diverse MOS\ndatasets into a preference-comparison setting, enabling rigorous evaluation\nacross different datasets. Building on MOS-RMBench, we systematically construct\nand evaluate three paradigms for reward modeling: scalar reward models,\nsemi-scalar reward models, and generative reward models (GRMs). Our experiments\nreveal three key findings: (1) scalar models achieve the strongest overall\nperformance, consistently exceeding 74% accuracy; (2) most models perform\nconsiderably worse on synthetic speech than on human speech; and (3) all models\nstruggle on pairs with very small MOS differences. To improve performance on\nthese challenging pairs, we propose a MOS-aware GRM that incorporates an\nMOS-difference-based reward function, enabling the model to adaptively scale\nrewards according to the difficulty of each sample pair. Experimental results\nshow that the MOS-aware GRM significantly improves fine-grained quality\ndiscrimination and narrows the gap with scalar models on the most challenging\ncases. We hope this work will establish both a benchmark and a methodological\nframework to foster more rigorous and scalable research in automatic speech\nquality assessment.",
      "authors": [
        "Yifei Cao",
        "Changhao Jiang",
        "Jiabao Zhuang",
        "Jiajun Sun",
        "Ming Zhang",
        "Zhiheng Xi",
        "Hui Li",
        "Shihan Dou",
        "Yuran Wang",
        "Yunke Zhang",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "published": "2025-10-01T10:27:51Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00743v1"
    },
    {
      "arxiv_id": "2510.00694v1",
      "title": "ALARB: An Arabic Legal Argument Reasoning Benchmark",
      "summary": "We introduce ALARB, a dataset and suite of tasks designed to evaluate the\nreasoning capabilities of large language models (LLMs) within the Arabic legal\ndomain. While existing Arabic benchmarks cover some knowledge-intensive tasks\nsuch as retrieval and understanding, substantial datasets focusing specifically\non multistep reasoning for Arabic LLMs, especially in open-ended contexts, are\nlacking. The dataset comprises over 13K commercial court cases from Saudi\nArabia, with each case including the facts presented, the reasoning of the\ncourt, the verdict, as well as the cited clauses extracted from the regulatory\ndocuments. We define a set of challenging tasks leveraging this dataset and\nreflecting the complexity of real-world legal reasoning, including verdict\nprediction, completion of reasoning chains in multistep legal arguments, and\nidentification of relevant regulations based on case facts. We benchmark a\nrepresentative selection of current open and closed Arabic LLMs on these tasks\nand demonstrate the dataset's utility for instruction tuning. Notably, we show\nthat instruction-tuning a modest 12B parameter model using ALARB significantly\nenhances its performance in verdict prediction and Arabic verdict generation,\nreaching a level comparable to that of GPT-4o.",
      "authors": [
        "Harethah Abu Shairah",
        "Somayah AlHarbi",
        "Abdulaziz AlHussein",
        "Sameer Alsabea",
        "Omar Shaqaqi",
        "Hebah AlShamlan",
        "Omar Knio",
        "George Turkiyyah"
      ],
      "published": "2025-10-01T09:15:41Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00694v1"
    },
    {
      "arxiv_id": "2510.00691v1",
      "title": "Inclusive Easy-to-Read Generation for Individuals with Cognitive\n  Impairments",
      "summary": "Ensuring accessibility for individuals with cognitive impairments is\nessential for autonomy, self-determination, and full citizenship. However,\nmanual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to\nscale, limiting access to crucial information in healthcare, education, and\ncivic life. AI-driven ETR generation offers a scalable solution but faces key\nchallenges, including dataset scarcity, domain adaptation, and balancing\nlightweight learning of Large Language Models (LLMs). In this paper, we\nintroduce ETR-fr, the first dataset for ETR text generation fully compliant\nwith European ETR guidelines. We implement parameter-efficient fine-tuning on\nPLMs and LLMs to establish generative baselines. To ensure high-quality and\naccessible outputs, we introduce an evaluation framework based on automatic\nmetrics supplemented by human assessments. The latter is conducted using a\n36-question evaluation form that is aligned with the guidelines. Overall\nresults show that PLMs perform comparably to LLMs and adapt effectively to\nout-of-domain texts.",
      "authors": [
        "François Ledoyen",
        "Gaël Dias",
        "Alexis Lechervy",
        "Jeremie Pantin",
        "Fabrice Maurel",
        "Youssef Chahir",
        "Elisa Gouzonnat",
        "Mélanie Berthelot",
        "Stanislas Moravac",
        "Armony Altinier",
        "Amy Khairalla"
      ],
      "published": "2025-10-01T09:13:18Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00691v1"
    },
    {
      "arxiv_id": "2510.00685v1",
      "title": "Stochastic Self-Organization in Multi-Agent Systems",
      "summary": "Multi-agent systems (MAS) based on Large Language Models (LLMs) have the\npotential to solve tasks that are beyond the reach of any single LLM. However,\nthis potential can only be realized when the collaboration mechanism between\nagents is optimized. Specifically, optimizing the communication structure\nbetween agents is critical for fruitful collaboration. Most existing approaches\nrely on fixed topologies, pretrained graph generators, optimization over edges,\nor employ external LLM judges, thereby adding to the complexity. In this work,\nwe introduce a response-conditioned framework that adapts communication\non-the-fly. Agents independently generate responses to the user query and\nassess peer contributions using an approximation of the Shapley value. A\ndirected acyclic graph (DAG) is then constructed to regulate the propagation of\nthe responses among agents, which ensures stable and efficient message\ntransmission from high-contributing agents to others. This graph is dynamically\nupdated based on the agent responses from the previous collaboration round.\nSince the proposed framework enables the self-organization of agents without\nadditional supervision or training, we refer to it as SelfOrg. The SelfOrg\nframework goes beyond task- and query-level optimization and takes into account\nthe stochastic nature of agent responses. Experiments with both strong and weak\nLLM backends demonstrate robust performance, with significant gains in the weak\nregime where prior methods collapse. We also theoretically show that multiple\nagents increase the chance of correctness and that the correct responses\nnaturally dominate the information flow.",
      "authors": [
        "Nurbek Tastan",
        "Samuel Horvath",
        "Karthik Nandakumar"
      ],
      "published": "2025-10-01T09:08:04Z",
      "primary_category": "cs.MA",
      "arxiv_url": "https://arxiv.org/abs/2510.00685v1"
    },
    {
      "arxiv_id": "2510.00671v1",
      "title": "Milco: Learned Sparse Retrieval Across Languages via a Multilingual\n  Connector",
      "summary": "Learned Sparse Retrieval (LSR) combines the efficiency of bi-encoders with\nthe transparency of lexical matching, but existing approaches struggle to scale\nbeyond English. We introduce MILCO, an LSR architecture that maps queries and\ndocuments from different languages into a shared English lexical space via a\nmultilingual connector. MILCO is trained with a specialized two-stage regime\nthat combines Sparse Alignment Pretraining with contrastive training to provide\nrepresentation transparency and effectiveness while mitigating semantic\ncollapse. Motivated by the observation that uncommon entities are often lost\nwhen projected into English, we propose a new LexEcho head, which enhances\nrobustness by augmenting the English lexical representation with a\nsource-language view obtained through a special [ECHO] token. MILCO achieves\nstate-of-the-art multilingual and cross-lingual LSR performance, outperforming\nleading dense, sparse, and multi-vector baselines such as BGE-M3 and\nQwen3-Embed on standard multilingual benchmarks, while supporting dynamic\nefficiency through post-hoc pruning. Notably, when using mass-based pruning to\nreduce document representations to only 30 active dimensions on average, MILCO\n560M outperforms the similarly-sized Qwen3-Embed 0.6B with 1024 dimensions.",
      "authors": [
        "Thong Nguyen",
        "Yibin Lei",
        "Jia-Huei Ju",
        "Eugene Yang",
        "Andrew Yates"
      ],
      "published": "2025-10-01T08:58:25Z",
      "primary_category": "cs.IR",
      "arxiv_url": "https://arxiv.org/abs/2510.00671v1"
    },
    {
      "arxiv_id": "2510.00662v1",
      "title": "Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to\n  Easy-to-Read Text Generation",
      "summary": "Simplifying complex texts is essential for ensuring equitable access to\ninformation, especially for individuals with cognitive impairments. The\nEasy-to-Read (ETR) initiative offers a framework for making content accessible\nto the neurodivergent population, but the manual creation of such texts remains\ntime-consuming and resource-intensive. In this work, we investigate the\npotential of large language models (LLMs) to automate the generation of ETR\ncontent. To address the scarcity of aligned corpora and the specificity of ETR\nconstraints, we propose a multi-task learning (MTL) approach that trains models\njointly on text summarization, text simplification, and ETR generation. We\nexplore two different strategies: multi-task retrieval-augmented generation\n(RAG) for in-context learning, and MTL-LoRA for parameter-efficient\nfine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a\nnew high-quality dataset, demonstrate the benefits of multi-task setups over\nsingle-task baselines across all configurations. Moreover, results show that\nthe RAG-based strategy enables generalization in out-of-domain settings, while\nMTL-LoRA outperforms all learning strategies within in-domain configurations.",
      "authors": [
        "François Ledoyen",
        "Gaël Dias",
        "Jeremie Pantin",
        "Alexis Lechervy",
        "Fabrice Maurel",
        "Youssef Chahir"
      ],
      "published": "2025-10-01T08:44:05Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00662v1"
    },
    {
      "arxiv_id": "2510.00647v1",
      "title": "MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for\n  Alt-text Generation",
      "summary": "The alt-text generation task produces concise, context-relevant descriptions\nof images, enabling blind and low-vision users to access online images. Despite\nthe capabilities of large vision-language models, alt-text generation\nperformance remains limited due to noisy user annotations, inconsistent\nstandards, and MLLMs' insensitivity to contextual information. Previous efforts\nto fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT\nrelies on accurate target annotations, which are often flawed in user-generated\nalt-text. To address this, we propose Multi-faceted Cross-modal Direct\nPreference Optimization (MCM-DPO), which improves alt-text generation by\nlearning to identify better options in preference pairs without requiring\nprecise annotations. MCM-DPO optimizes preferences across single, paired, and\nmulti-preference dimensions, covering textual, visual, and cross-modal factors.\nIn light of the scarcity of high-quality annotated and preference-labeled\ndatasets for alt-text, we constructed two large-scale, high-quality datasets\nnamed TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include\n202k annotated alt-text samples and 18k preference pairs that cover diverse\npreference dimensions, aiming to support further research in this domain.\nExperimental results show that our proposed MCM-DPO method consistently\noutperforms both DPO and SFT, establishing a new state of the art in alt-text\ngeneration. We release the code and data here:\nhttps://github.com/LVUGAI/MCM-DPO",
      "authors": [
        "Jinlan Fu",
        "Shenzhen Huangfu",
        "Hao Fei",
        "Yichong Huang",
        "Xiaoyu Shen",
        "Xipeng Qiu",
        "See-Kiong Ng"
      ],
      "published": "2025-10-01T08:25:18Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00647v1"
    },
    {
      "arxiv_id": "2510.00636v1",
      "title": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution",
      "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.",
      "authors": [
        "Alessio Devoto",
        "Maximilian Jeblick",
        "Simon Jégou"
      ],
      "published": "2025-10-01T08:12:14Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00636v1"
    },
    {
      "arxiv_id": "2510.00629v2",
      "title": "Tenyidie Syllabification corpus creation and deep learning applications",
      "summary": "The Tenyidie language is a low-resource language of the Tibeto-Burman family\nspoken by the Tenyimia Community of Nagaland in the north-eastern part of India\nand is considered a major language in Nagaland. It is tonal,\nSubject-Object-Verb, and highly agglutinative in nature. Being a low-resource\nlanguage, very limited research on Natural Language Processing (NLP) has been\nconducted. To the best of our knowledge, no work on syllabification has been\nreported for this language. Among the many NLP tasks, syllabification or\nsyllabication is an important task in which the given word syllables are\nidentified. The contribution of this work is the creation of 10,120 syllabified\nTenyidie words and the application of the Deep Learning techniques on the\ncreated corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and\nEncoder-decoder deep learning architectures on our created dataset. In our\ndataset split of 80:10:10 (train:validation:test) set, we achieved the highest\naccuracy of 99.21% with BLSTM model on the test set. This work will find its\napplication in numerous other NLP applications, such as morphological analysis,\npart-of-speech tagging, machine translation, etc, for the Tenyidie Language.\n  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF;\nEncoder-decoder",
      "authors": [
        "Teisovi Angami",
        "Kevisino Khate"
      ],
      "published": "2025-10-01T08:00:59Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00629v2"
    },
    {
      "arxiv_id": "2510.00628v1",
      "title": "Hearing the Order: Investigating Selection Bias in Large Audio-Language\n  Models",
      "summary": "Large audio-language models (LALMs) are often used in tasks that involve\nreasoning over ordered options. An open question is whether their predictions\nare influenced by the order of answer choices, which would indicate a form of\nselection bias and undermine their reliability. In this paper, we identify and\nanalyze this problem in LALMs. We demonstrate that no model is immune to this\nbias through extensive experiments on six LALMs across three widely used\nbenchmarks and their spoken counterparts. Shuffling the order of answer options\ncan cause performance fluctuations of up to 24% and even change model rankings,\nraising concerns about the reliability of current evaluation practices. We also\nstudy permutation-based strategies and show that they can mitigate bias in most\ncases. Our work represents the first systematic investigation of this issue in\nLALMs, and we hope it raises awareness and motivates further research in this\ndirection.",
      "authors": [
        "Yu-Xiang Lin",
        "Chen-An Li",
        "Sheng-Lun Wei",
        "Po-Chun Chen",
        "Hsin-Hsi Chen",
        "Hung-yi Lee"
      ],
      "published": "2025-10-01T08:00:58Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00628v1"
    },
    {
      "arxiv_id": "2510.00626v1",
      "title": "When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning\n  in Large Audio-Language Models",
      "summary": "Large audio-language models (LALMs) unify speech and text processing, but\ntheir robustness in noisy real-world settings remains underexplored. We\ninvestigate how irrelevant audio, such as silence, synthetic noise, and\nenvironmental sounds, affects text reasoning tasks where audio is unnecessary.\nAcross three text-based benchmarks, we find that even non-informative audio\nreduces accuracy and increases prediction volatility; the severity of\ninterference scales with longer durations, higher amplitudes, and elevated\ndecoding temperatures. Silence, often assumed neutral, destabilizes outputs as\nstrongly as synthetic noise. While larger models show greater resilience,\nvulnerabilities persist across all evaluated systems. We further test\nmitigation strategies and find that prompting shows limited effectiveness,\nwhereas self-consistency improves stability at the cost of increased\ncomputation. Our results reveal cross-modal interference as a key robustness\nchallenge and highlight the need for efficient fusion strategies that preserve\nreasoning performance in the presence of irrelevant inputs.",
      "authors": [
        "Chen-An Li",
        "Tzu-Han Lin",
        "Hung-yi Lee"
      ],
      "published": "2025-10-01T07:59:45Z",
      "primary_category": "cs.SD",
      "arxiv_url": "https://arxiv.org/abs/2510.00626v1"
    },
    {
      "arxiv_id": "2510.00620v1",
      "title": "HARPA: A Testability-Driven, Literature-Grounded Framework for Research\n  Ideation",
      "summary": "While there has been a surge of interest in automated scientific discovery\n(ASD), especially with the emergence of LLMs, it remains challenging for tools\nto generate hypotheses that are both testable and grounded in the scientific\nliterature. Additionally, existing ideation tools are not adaptive to prior\nexperimental outcomes. We developed HARPA to address these challenges by\nincorporating the ideation workflow inspired by human researchers. HARPA first\nidentifies emerging research trends through literature mining, then explores\nhypothesis design spaces, and finally converges on precise, testable hypotheses\nby pinpointing research gaps and justifying design choices. Our evaluations\nshow that HARPA-generated hypothesis-driven research proposals perform\ncomparably to a strong baseline AI-researcher across most qualitative\ndimensions (e.g., specificity, novelty, overall quality), but achieve\nsignificant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness\n(+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the\nASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11\nout of 40) and fewer failures (16 vs. 21 out of 40), showing that expert\nfeasibility judgments track with actual execution success. Furthermore, to\nsimulate how researchers continuously refine their understanding of what\nhypotheses are both testable and potentially interesting from experience, HARPA\nlearns a reward model that scores new hypotheses based on prior experimental\noutcomes, achieving approx. a 28\\% absolute gain over HARPA's untrained\nbaseline scorer. Together, these methods represent a step forward in the field\nof AI-driven scientific discovery.",
      "authors": [
        "Rosni Vasu",
        "Peter Jansen",
        "Pao Siangliulue",
        "Cristina Sarasua",
        "Abraham Bernstein",
        "Peter Clark",
        "Bhavana Dalvi Mishra"
      ],
      "published": "2025-10-01T07:52:19Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00620v1"
    },
    {
      "arxiv_id": "2510.00615v1",
      "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
      "summary": "Large language models (LLMs) are increasingly deployed as agents in dynamic,\nreal-world environments, where success requires both reasoning and effective\ntool use. A central challenge for agentic tasks is the growing context length,\nas agents must accumulate long histories of actions and observations. This\nexpansion raises costs and reduces efficiency in long-horizon tasks, yet prior\nwork on context compression has mostly focused on single-step tasks or narrow\napplications. We introduce Agent Context Optimization (ACON), a unified\nframework that optimally compresses both environment observations and\ninteraction histories into concise yet informative condensations. ACON\nleverages compression guideline optimization in natural language space: given\npaired trajectories where full context succeeds but compressed context fails,\ncapable LLMs analyze the causes of failure, and the compression guideline is\nupdated accordingly. Furthermore, we propose distilling the optimized LLM\ncompressor into smaller models to reduce the overhead of the additional module.\nExperiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON\nreduces memory usage by 26-54% (peak tokens) while largely preserving task\nperformance, preserves over 95% of accuracy when distilled into smaller\ncompressors, and enhances smaller LMs as long-horizon agents with up to 46%\nperformance improvement.",
      "authors": [
        "Minki Kang",
        "Wei-Ning Chen",
        "Dongge Han",
        "Huseyin A. Inan",
        "Lukas Wutschitz",
        "Yanzhi Chen",
        "Robert Sim",
        "Saravan Rajmohan"
      ],
      "published": "2025-10-01T07:43:49Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00615v1"
    },
    {
      "arxiv_id": "2510.00586v1",
      "title": "Eyes-on-Me: Scalable RAG Poisoning through Transferable\n  Attention-Steering Attractors",
      "summary": "Existing data poisoning attacks on retrieval-augmented generation (RAG)\nsystems scale poorly because they require costly optimization of poisoned\ndocuments for each target phrase. We introduce Eyes-on-Me, a modular attack\nthat decomposes an adversarial document into reusable Attention Attractors and\nFocus Regions. Attractors are optimized to direct attention to the Focus\nRegion. Attackers can then insert semantic baits for the retriever or malicious\ninstructions for the generator, adapting to new targets at near zero cost. This\nis achieved by steering a small subset of attention heads that we empirically\nidentify as strongly correlated with attack success. Across 18 end-to-end RAG\nsettings (3 datasets $\\times$ 2 retrievers $\\times$ 3 generators), Eyes-on-Me\nraises average attack success rates from 21.9 to 57.8 (+35.9 points,\n2.6$\\times$ over prior work). A single optimized attractor transfers to unseen\nblack box retrievers and generators without retraining. Our findings establish\na scalable paradigm for RAG data poisoning and show that modular, reusable\ncomponents pose a practical threat to modern AI systems. They also reveal a\nstrong link between attention concentration and model outputs, informing\ninterpretability research.",
      "authors": [
        "Yen-Shan Chen",
        "Sian-Yao Huang",
        "Cheng-Lin Yang",
        "Yun-Nung Chen"
      ],
      "published": "2025-10-01T07:07:22Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00586v1"
    },
    {
      "arxiv_id": "2510.00582v1",
      "title": "SAGE-LD: Towards Scalable and Generalizable End-to-End Language\n  Diarization via Simulated Data Augmentation",
      "summary": "In this paper, we present a neural spoken language diarization model that\nsupports an unconstrained span of languages within a single framework. Our\napproach integrates a learnable query-based architecture grounded in\nmultilingual awareness, with large-scale pretraining on simulated\ncode-switching data. By jointly leveraging these two components, our method\novercomes the limitations of conventional approaches in data scarcity and\narchitecture optimization, and generalizes effectively to real-world\nmultilingual settings across diverse environments. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance on several\nlanguage diarization benchmarks, with a relative performance improvement of 23%\nto 52% over previous methods. We believe that this work not only advances\nresearch in language diarization but also establishes a foundational framework\nfor code-switching speech technologies.",
      "authors": [
        "Sangmin Lee",
        "Woongjib Choi",
        "Jihyun Kim",
        "Hong-Goo Kang"
      ],
      "published": "2025-10-01T07:01:33Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00582v1"
    },
    {
      "arxiv_id": "2510.00579v1",
      "title": "CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs",
      "summary": "Chain-of-Thought (CoT) prompting has emerged as a powerful approach to\nenhancing the reasoning capabilities of Large Language Models (LLMs). However,\nexisting implementations, such as in-context learning and fine-tuning, remain\ncostly and inefficient. To improve CoT reasoning at a lower cost, and inspired\nby the task vector paradigm, we introduce CoT Vectors, compact representations\nthat encode task-general, multi-step reasoning knowledge. Through experiments\nwith Extracted CoT Vectors, we observe pronounced layer-wise instability,\nmanifesting as a U-shaped performance curve that reflects a systematic\nthree-stage reasoning process in LLMs. To address this limitation, we propose\nLearnable CoT Vectors, optimized under a teacher-student framework to provide\nmore stable and robust guidance. Extensive evaluations across diverse\nbenchmarks and models demonstrate that CoT Vectors not only outperform existing\nbaselines but also achieve performance comparable to parameter-efficient\nfine-tuning methods, while requiring fewer trainable parameters. Moreover, by\ntreating CoT Vectors as a probe, we uncover how their effectiveness varies due\nto latent space structure, information density, acquisition mechanisms, and\npre-training differences, offering new insights into the functional\norganization of multi-step reasoning in LLMs. The source code will be released.",
      "authors": [
        "Li Li",
        "Ziyi Wang",
        "Yongliang Wu",
        "Jianfei Cai",
        "Xu Yang"
      ],
      "published": "2025-10-01T06:58:23Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00579v1"
    },
    {
      "arxiv_id": "2510.00568v1",
      "title": "ReSeek: A Self-Correcting Framework for Search Agents with Instructive\n  Rewards",
      "summary": "Search agents powered by Large Language Models (LLMs) have demonstrated\nsignificant potential in tackling knowledge-intensive tasks. Reinforcement\nlearning (RL) has emerged as a powerful paradigm for training these agents to\nperform complex, multi-step reasoning. However, prior RL-based methods often\nrely on sparse or rule-based rewards, which can lead agents to commit to\nsuboptimal or erroneous reasoning paths without the ability to recover. To\naddress these limitations, we propose ReSeek, a novel self-correcting framework\nfor training search agents. Our framework introduces a self-correction\nmechanism that empowers the agent to dynamically identify and recover from\nerroneous search paths during an episode. By invoking a special JUDGE action,\nthe agent can judge the information and re-plan its search strategy. To guide\nthis process, we design a dense, instructive process reward function, which\ndecomposes into a correctness reward for retrieving factual information and a\nutility reward for finding information genuinely useful for the query.\nFurthermore, to mitigate the risk of data contamination in existing datasets,\nwe introduce FictionalHot, a new and challenging benchmark with recently\ncurated questions requiring complex reasoning. Being intuitively reasonable and\npractically simple, extensive experiments show that agents trained with ReSeek\nsignificantly outperform SOTA baselines in task success rate and path\nfaithfulness.",
      "authors": [
        "Shiyu Li",
        "Yang Tang",
        "Yifan Wang",
        "Peiming Li",
        "Xi Chen"
      ],
      "published": "2025-10-01T06:44:28Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00568v1"
    },
    {
      "arxiv_id": "2510.00567v1",
      "title": "Are Large Language Models Chronically Online Surfers? A Dataset for\n  Chinese Internet Meme Explanation",
      "summary": "Large language models (LLMs) are trained on vast amounts of text from the\nInternet, but do they truly understand the viral content that rapidly spreads\nonline -- commonly known as memes? In this paper, we introduce CHIME, a dataset\nfor CHinese Internet Meme Explanation. The dataset comprises popular\nphrase-based memes from the Chinese Internet, annotated with detailed\ninformation on their meaning, origin, example sentences, types, etc. To\nevaluate whether LLMs understand these memes, we designed two tasks. In the\nfirst task, we assessed the models' ability to explain a given meme, identify\nits origin, and generate appropriate example sentences. The results show that\nwhile LLMs can explain the meanings of some memes, their performance declines\nsignificantly for culturally and linguistically nuanced meme types.\nAdditionally, they consistently struggle to provide accurate origins for the\nmemes. In the second task, we created a set of multiple-choice questions (MCQs)\nrequiring LLMs to select the most appropriate meme to fill in a blank within a\ncontextual sentence. While the evaluated models were able to provide correct\nanswers, their performance remains noticeably below human levels. We have made\nCHIME public and hope it will facilitate future research on computational meme\nunderstanding.",
      "authors": [
        "Yubo Xie",
        "Chenkai Wang",
        "Zongyang Ma",
        "Fahui Miao"
      ],
      "published": "2025-10-01T06:41:46Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00567v1"
    },
    {
      "arxiv_id": "2510.00546v1",
      "title": "ThinkBrake: Mitigating Overthinking in Tool Reasoning",
      "summary": "Small reasoning models (SRMs) often overthink during tool use: they reach a\ncorrect tool-argument configuration, then continue reasoning and overwrite it\nwith an incorrect final call. We diagnose overthinking via oracle rollouts that\ninject </think> at sentence boundaries. On the Berkeley Function Calling\nLeaderboard (BFCL), this oracle termination lifts average accuracy from 85.8\\%\nto 94.2\\% while reducing tokens by 80-94\\%, revealing substantial recoverable\nheadroom and potential redundant reasoning. While prior work on concise\nreasoning has largely targeted mathematics, tool reasoning remains\nunderexplored. We adapt various early-termination baselines to tool use and\nintroduce ThinkBrake, a training-free decoding heuristic. ThinkBrake monitors\nthe log-probability margin between </think> and the current top token at\nsentence boundaries and triggers termination when this margin becomes small.\nAcross BFCL's single turn, non-live and live splits, ThinkBrake preserves or\nimproves accuracy while reducing tokens up to 25\\%, outperforming various\nbaselines.",
      "authors": [
        "Minjae Oh",
        "Sangjun Song",
        "Seungkyu Lee",
        "Sungmin Jo",
        "Yohan Jo"
      ],
      "published": "2025-10-01T06:04:57Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00546v1"
    },
    {
      "arxiv_id": "2510.00536v1",
      "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
      "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.",
      "authors": [
        "Kung-Hsiang Huang",
        "Haoyi Qiu",
        "Yutong Dai",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "published": "2025-10-01T05:37:54Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00536v1"
    },
    {
      "arxiv_id": "2510.00526v1",
      "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised\n  Fine-Tuning across the Model Capability Continuum",
      "summary": "Supervised fine-tuning (SFT) is the standard approach for post-training large\nlanguage models (LLMs), yet it often shows limited generalization. We trace\nthis limitation to its default training objective: negative log likelihood\n(NLL). While NLL is classically optimal when training from scratch,\npost-training operates in a different paradigm and could violate its optimality\nassumptions, where models already encode task-relevant priors and supervision\ncan be long and noisy. To this end, we study a general family of\nprobability-based objectives and characterize their effectiveness under\ndifferent conditions. Through comprehensive experiments and extensive ablation\nstudies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a\ncritical dimension that governs objective behavior: the model-capability\ncontinuum. Near the model-strong end, prior-leaning objectives that downweight\nlow-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants)\nconsistently outperform NLL; toward the model-weak end, NLL dominates; in\nbetween, no single objective prevails. Our theoretical analysis further\nelucidates how objectives trade places across the continuum, providing a\nprincipled foundation for adapting objectives to model capability. Our code is\navailable at https://github.com/GaotangLi/Beyond-Log-Likelihood.",
      "authors": [
        "Gaotang Li",
        "Ruizhong Qiu",
        "Xiusi Chen",
        "Heng Ji",
        "Hanghang Tong"
      ],
      "published": "2025-10-01T05:17:47Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00526v1"
    },
    {
      "arxiv_id": "2510.00514v1",
      "title": "EuroSpeech: A Multilingual Speech Corpus",
      "summary": "Recent progress in speech processing has highlighted that high-quality\nperformance across languages requires substantial training data for each\nindividual language. While existing multilingual datasets cover many languages,\nthey often contain insufficient data for most languages. Thus, trained models\nperform poorly on the majority of the supported languages. Our work addresses\nthis challenge by introducing a scalable pipeline for constructing speech\ndatasets from parliamentary recordings. The proposed pipeline includes robust\ncomponents for media retrieval and a two-stage alignment algorithm designed to\nhandle non-verbatim transcripts and long-form audio. Applying this pipeline to\nrecordings from 22 European parliaments, we extract over 61k hours of aligned\nspeech segments, achieving substantial per-language coverage with 19 languages\nexceeding 1k hours and 22 languages exceeding 500 hours of high-quality speech\ndata. We obtain an average 41.8\\% reduction in word error rates over baselines\nwhen finetuning an existing ASR model on our dataset, demonstrating the\nusefulness of our approach.",
      "authors": [
        "Samuel Pfisterer",
        "Florian Grötschla",
        "Luca A. Lanzendörfer",
        "Florian Yan",
        "Roger Wattenhofer"
      ],
      "published": "2025-10-01T04:51:45Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00514v1"
    },
    {
      "arxiv_id": "2510.00510v1",
      "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
      "summary": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness and adaptability. We\npropose a generalist agent architecture that integrates three core components:\na collective multi-agent framework combining planning and execution agents with\ncritic model voting, a hierarchical memory system spanning working, semantic,\nand procedural layers, and a refined tool suite for search, code execution, and\nmultimodal parsing. Evaluated on a comprehensive benchmark, our framework\nconsistently outperforms open-source baselines and approaches the performance\nof proprietary systems. These results demonstrate the importance of\nsystem-level integration and highlight a path toward scalable, resilient, and\nadaptive AI assistants capable of operating across diverse domains and tasks.",
      "authors": [
        "Jiarun Liu",
        "Shiyue Xu",
        "Shangkun Liu",
        "Yang Li",
        "Wen Liu",
        "Min Liu",
        "Xiaoqing Zhou",
        "Hanmin Wang",
        "Shilin Jia",
        "zhen Wang",
        "Shaohua Tian",
        "Hanhao Li",
        "Junbo Zhang",
        "Yongli Yu",
        "Peng Cao",
        "Haofen Wang"
      ],
      "published": "2025-10-01T04:41:58Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00510v1"
    },
    {
      "arxiv_id": "2510.00508v1",
      "title": "Copy-Paste to Mitigate Large Language Model Hallucinations",
      "summary": "While Retrieval-Augmented Generation (RAG) enables large language models\n(LLMs) to generate contextually grounded responses, contextual faithfulness\nremains challenging as LLMs may not consistently trust provided context,\nleading to hallucinations that undermine reliability. We observe an inverse\ncorrelation between response copying degree and context-unfaithful\nhallucinations on RAGTruth, suggesting that higher copying degrees reduce\nhallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,\nobtained through two-stage high-copying response preference training. We design\nthree prompting methods to enhance copying degree, demonstrating that\nhigh-copying responses achieve superior contextual faithfulness and\nhallucination control. These approaches enable a fully automated pipeline that\ntransforms generated responses into high-copying preference data for training\nCopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best\nperformance in both counterfactual and original contexts, remarkably with 12.2%\nto 24.5% accuracy improvements on FaithEval over the best baseline, while\nrequiring only 365 training samples -- 1/50th of baseline data. To elucidate\nCopyPasteLLM's effectiveness, we propose the Context-Parameter Copying\nCapturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates\nreliance on internal parametric knowledge rather than external knowledge during\ngeneration. All codes are available at\nhttps://github.com/longyongchao/CopyPasteLLM",
      "authors": [
        "Yongchao Long",
        "Xian Wu",
        "Yingying Zhang",
        "Xianbin Wen",
        "Yuxi Zhou",
        "Shenda Hong"
      ],
      "published": "2025-10-01T04:40:04Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00508v1"
    },
    {
      "arxiv_id": "2510.00507v1",
      "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via\n  Knowledge Graphs",
      "summary": "As multimodal LLM-driven agents continue to advance in autonomy and\ngeneralization, evaluation based on static datasets can no longer adequately\nassess their true capabilities in dynamic environments and diverse tasks.\nExisting LLM-based synthetic data methods are largely designed for LLM training\nand evaluation, and thus cannot be directly applied to agent tasks that require\ntool use and interactive capabilities. While recent studies have explored\nautomatic agent task generation with LLMs, most efforts remain limited to text\nor image analysis, without systematically modeling multi-step interactions in\nweb environments. To address these challenges, we propose Graph2Eval, a\nknowledge graph-based framework that automatically generates both multimodal\ndocument comprehension tasks and web interaction tasks, enabling comprehensive\nevaluation of agents' reasoning, collaboration, and interactive capabilities.\nIn our approach, knowledge graphs constructed from multi-source external data\nserve as the task space, where we translate semantic relations into structured\nmultimodal tasks using subgraph sampling, task templates, and meta-paths. A\nmulti-stage filtering pipeline based on node reachability, LLM scoring, and\nsimilarity analysis is applied to guarantee the quality and executability of\nthe generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of\nmultiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures\nreasoning, collaboration, and interaction capabilities. We instantiate the\nframework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning\ndocument comprehension and web interaction scenarios. Experiments show that\nGraph2Eval efficiently generates tasks that differentiate agent and model\nperformance, revealing gaps in reasoning, collaboration, and web interaction\nacross different settings and offering a new perspective for agent evaluation.",
      "authors": [
        "Yurun Chen",
        "Xavier Hu",
        "Yuhan Liu",
        "Ziqi Wang",
        "Zeyi Liao",
        "Lin Chen",
        "Feng Wei",
        "Yuxi Qian",
        "Bo Zheng",
        "Keting Yin",
        "Shengyu Zhang"
      ],
      "published": "2025-10-01T04:37:54Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00507v1"
    },
    {
      "arxiv_id": "2510.00499v2",
      "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
      "summary": "Spoken dialogue systems often rely on cascaded pipelines that transcribe,\nprocess, and resynthesize speech. While effective, this design discards\nparalinguistic cues and limits expressivity. Recent end-to-end methods reduce\nlatency and better preserve these cues, yet still rely on text intermediates,\ncreating a fundamental bottleneck. We present MOSS-Speech, a true\nspeech-to-speech large language model that directly understands and generates\nspeech without relying on text guidance. Our approach combines a modality-based\nlayer-splitting architecture with a frozen pre-training strategy, preserving\nthe reasoning and knowledge of pretrained text LLMs while adding native speech\ncapabilities. Experiments show that our model achieves state-of-the-art results\nin spoken question answering and delivers comparable speech-to-speech\nperformance relative to existing text-guided systems, while still maintaining\ncompetitive text performance. By narrowing the gap between text-guided and\ndirect speech generation, our work establishes a new paradigm for expressive\nand efficient end-to-end speech interaction.",
      "authors": [
        "Xingjian Zhao",
        "Zhe Xu",
        "Qinyuan Cheng",
        "Zhaoye Fei",
        "Luozhijie Jin",
        "Yang Wang",
        "Hanfu Chen",
        "Yaozhou Jiang",
        "Qinghui Gao",
        "Ke Chen",
        "Ruixiao Li",
        "Mingshu Chen",
        "Ruiming Wang",
        "Wenbo Zhang",
        "Yiyang Zhang",
        "Donghua Yu",
        "Yang Gao",
        "Xiaogui Yang",
        "Yitian Gong",
        "Yuanfan Xu",
        "Yaqian Zhou",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "published": "2025-10-01T04:32:37Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00499v2"
    },
    {
      "arxiv_id": "2510.00496v2",
      "title": "Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via\n  Sensitivity Perturbations",
      "summary": "Although numerous strategies have recently been proposed to enhance the\nautonomous interaction capabilities of multimodal agents in graphical user\ninterface (GUI), their reliability remains limited when faced with complex or\nout-of-domain tasks. This raises a fundamental question: Are existing\nmultimodal agents reasoning spuriously? In this paper, we propose\n\\textbf{Agent-ScanKit}, a systematic probing framework to unravel the memory\nand reasoning capabilities of multimodal agents under controlled perturbations.\nSpecifically, we introduce three orthogonal probing paradigms: visual-guided,\ntext-guided, and structure-guided, each designed to quantify the contributions\nof memorization and reasoning without requiring access to model internals. In\nfive publicly available GUI benchmarks involving 18 multimodal agents, the\nresults demonstrate that mechanical memorization often outweighs systematic\nreasoning. Most of the models function predominantly as retrievers of\ntraining-aligned knowledge, exhibiting limited generalization. Our findings\nunderscore the necessity of robust reasoning modeling for multimodal agents in\nreal-world scenarios, offering valuable insights toward the development of\nreliable multimodal agents.",
      "authors": [
        "Pengzhou Cheng",
        "Lingzhong Dong",
        "Zeng Wu",
        "Zongru Wu",
        "Zhuosheng Zhang",
        "Gongshen Liu"
      ],
      "published": "2025-10-01T04:29:39Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00496v2"
    },
    {
      "arxiv_id": "2510.00482v1",
      "title": "Agent Fine-tuning through Distillation for Domain-specific LLMs in\n  Microdomains",
      "summary": "Agentic large language models (LLMs) have become prominent for autonomously\ninteracting with external environments and performing multi-step reasoning\ntasks. Most approaches leverage these capabilities via in-context learning with\nfew-shot prompts, but this often results in lengthy inputs and higher\ncomputational costs. Agent fine-tuning offers an alternative by enabling LLMs\nto internalize procedural reasoning and domain-specific knowledge through\ntraining on relevant data and demonstration trajectories. While prior studies\nhave focused on general domains, their effectiveness in specialized technical\nmicrodomains remains unclear. This paper explores agent fine-tuning for domain\nadaptation within Hitachi's JP1 middleware, a microdomain for specialized IT\noperations. We fine-tuned LLMs using JP1-specific datasets derived from domain\nmanuals and distilled reasoning trajectories generated by LLMs themselves,\nenhancing decision making accuracy and search efficiency. During inference, we\nused an agentic prompt with retrieval-augmented generation and introduced a\ncontext-answer extractor to improve information relevance. On JP1 certification\nexam questions, our method achieved a 14% performance improvement over the base\nmodel, demonstrating the potential of agent fine-tuning for domain-specific\nreasoning in complex microdomains.",
      "authors": [
        "Yawen Xue",
        "Masaya Tsunokake",
        "Yuta Koreeda",
        "Ekant Muljibhai Amin",
        "Takashi Sumiyoshi",
        "Yasuhiro Sogawa"
      ],
      "published": "2025-10-01T04:04:53Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00482v1"
    },
    {
      "arxiv_id": "2510.00449v1",
      "title": "Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context\n  User Reviews",
      "summary": "Personalizing the outputs of large language models (LLMs) to align with\nindividual user preferences is an active research area. However, previous\nstudies have mainly focused on classification or ranking tasks and have not\nconsidered Likert-scale rating prediction, a regression task that requires both\nlanguage and mathematical reasoning to be solved effectively. This task has\nsignificant industrial applications, but the utilization of LLMs remains\nunderexplored, particularly regarding the capabilities of off-the-shelf LLMs.\nThis study investigates the performance of off-the-shelf LLMs on rating\nprediction, providing different in-context information. Through comprehensive\nexperiments with eight models across three datasets, we demonstrate that\nuser-written reviews significantly improve the rating prediction performance of\nLLMs. This result is comparable to traditional methods like matrix\nfactorization, highlighting the potential of LLMs as a promising solution for\nthe cold-start problem. We also find that the reviews for concrete items are\nmore effective than general preference descriptions that are not based on any\nspecific item. Furthermore, we discover that prompting LLMs to first generate a\nhypothetical review enhances the rating prediction performance. Our code is\navailable at https://github.com/ynklab/rating-prediction-with-reviews.",
      "authors": [
        "Koki Ryu",
        "Hitomi Yanaka"
      ],
      "published": "2025-10-01T03:04:20Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00449v1"
    },
    {
      "arxiv_id": "2510.00446v1",
      "title": "LongCodeZip: Compress Long Context for Code Language Models",
      "summary": "Code generation under long contexts is becoming increasingly critical as\nLarge Language Models (LLMs) are required to reason over extensive information\nin the codebase. While recent advances enable code LLMs to process long inputs,\nhigh API costs and generation latency remain substantial bottlenecks. Existing\ncontext pruning techniques, such as LLMLingua, achieve promising results for\ngeneral text but overlook code-specific structures and dependencies, leading to\nsuboptimal performance in programming tasks. In this paper, we propose\nLongCodeZip, a novel plug-and-play code compression framework designed\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\ncoarse-grained compression, which identifies and ranks function-level chunks\nusing conditional perplexity with respect to the instruction, retaining only\nthe most relevant functions; and (2) fine-grained compression, which segments\nretained functions into blocks based on perplexity and selects an optimal\nsubset under an adaptive token budget to maximize relevance. Evaluations across\nmultiple tasks, including code completion, summarization, and question\nanswering, show that LongCodeZip consistently outperforms baseline methods,\nachieving up to a 5.6x compression ratio without degrading task performance. By\neffectively reducing context size while preserving essential information,\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\nscenarios, advancing the efficiency and capability of code intelligence\napplications.",
      "authors": [
        "Yuling Shi",
        "Yichun Qian",
        "Hongyu Zhang",
        "Beijun Shen",
        "Xiaodong Gu"
      ],
      "published": "2025-10-01T02:54:57Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00446v1"
    },
    {
      "arxiv_id": "2510.00444v1",
      "title": "TokMem: Tokenized Procedural Memory for Large Language Models",
      "summary": "Large language models rely heavily on prompts to specify tasks, recall\nknowledge and guide reasoning. However, this reliance is inefficient as prompts\nmust be re-read at each step, scale poorly across tasks, and lack mechanisms\nfor modular reuse. We introduce TokMem, a tokenized procedural memory that\nstores recurring procedures as compact, trainable embeddings. Each memory token\nencodes both an address to a procedure and a control signal that steers\ngeneration, enabling targeted behavior with constant-size overhead. To support\ncontinual adaptation, TokMem keeps the backbone model frozen, allowing new\nprocedures to be added without interfering with existing ones. We evaluate\nTokMem on 1,000 tasks for atomic recall, and on function-calling tasks for\ncompositional recall, where it consistently outperforms retrieval-augmented\ngeneration while avoiding repeated context overhead, and fine-tuning with far\nfewer parameters. These results establish TokMem as a scalable and modular\nalternative to prompt engineering and fine-tuning, offering an explicit\nprocedural memory for LLMs.",
      "authors": [
        "Zijun Wu",
        "Yongchang Hao",
        "Lili Mou"
      ],
      "published": "2025-10-01T02:51:58Z",
      "primary_category": "cs.CL",
      "arxiv_url": "https://arxiv.org/abs/2510.00444v1"
    },
    {
      "arxiv_id": "2510.00436v1",
      "title": "Automated Evaluation can Distinguish the Good and Bad AI Responses to\n  Patient Questions about Hospitalization",
      "summary": "Automated approaches to answer patient-posed health questions are rising, but\nselecting among systems requires reliable evaluation. The current gold standard\nfor evaluating the free-text artificial intelligence (AI) responses--human\nexpert review--is labor-intensive and slow, limiting scalability. Automated\nmetrics are promising yet variably aligned with human judgments and often\ncontext-dependent. To address the feasibility of automating the evaluation of\nAI responses to hospitalization-related questions posed by patients, we\nconducted a large systematic study of evaluation approaches. Across 100 patient\ncases, we collected responses from 28 AI systems (2800 total) and assessed them\nalong three dimensions: whether a system response (1) answers the question, (2)\nappropriately uses clinical note evidence, and (3) uses general medical\nknowledge. Using clinician-authored reference answers to anchor metrics,\nautomated rankings closely matched expert ratings. Our findings suggest that\ncarefully designed automated evaluation can scale comparative assessment of AI\nsystems and support patient-clinician communication.",
      "authors": [
        "Sarvesh Soni",
        "Dina Demner-Fushman"
      ],
      "published": "2025-10-01T02:39:37Z",
      "primary_category": "cs.AI",
      "arxiv_url": "https://arxiv.org/abs/2510.00436v1"
    },
    {
      "arxiv_id": "2510.00404v2",
      "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features",
      "summary": "Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs.",
      "authors": [
        "Xudong Zhu",
        "Mohammad Mahdi Khalili",
        "Zhihui Zhu"
      ],
      "published": "2025-10-01T01:29:31Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00404v2"
    },
    {
      "arxiv_id": "2510.00374v1",
      "title": "GDLNN: Marriage of Programming Language and Neural Networks for Accurate\n  and Easy-to-Explain Graph Classification",
      "summary": "We present GDLNN, a new graph machine learning architecture, for graph\nclassification tasks. GDLNN combines a domain-specific programming language,\ncalled GDL, with neural networks. The main strength of GDLNN lies in its GDL\nlayer, which generates expressive and interpretable graph representations.\nSince the graph representation is interpretable, existing model explanation\ntechniques can be directly applied to explain GDLNN's predictions. Our\nevaluation shows that the GDL-based representation achieves high accuracy on\nmost graph classification benchmark datasets, outperforming dominant graph\nlearning methods such as GNNs. Applying an existing model explanation technique\nalso yields high-quality explanations of GDLNN's predictions. Furthermore, the\ncost of GDLNN is low when the explanation cost is included.",
      "authors": [
        "Minseok Jeon",
        "Seunghyun Park"
      ],
      "published": "2025-10-01T00:44:58Z",
      "primary_category": "cs.LG",
      "arxiv_url": "https://arxiv.org/abs/2510.00374v1"
    }
  ]
}